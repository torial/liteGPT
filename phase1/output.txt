BEGINNING (1681841684.668667): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6659, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6626, val loss 4.6637 [0.925250768661499 sec]
step 100: train loss 2.8679, val loss 2.9023 [2.383279800415039 sec]
step 200: train loss 2.6251, val loss 2.6913 [3.8530871868133545 sec]
step 300: train loss 2.5073, val loss 2.5887 [5.359282970428467 sec]
step 400: train loss 2.3999, val loss 2.4992 [6.8857200145721436 sec]
2.4385900497436523
Total Training Time: 7.466230392456055 seconds

wylend ilsis finco shes.
CCAg HHAUYE IgB ssta.% his hapmive th ou spired wing whe anta his al demyou
BEGINNING (1681841694.0982275): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6884, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7087, val loss 4.6956 [1.3841404914855957 sec]
step 100: train loss 2.7913, val loss 2.8379 [3.719820261001587 sec]
step 200: train loss 2.5765, val loss 2.6387 [6.021679639816284 sec]
step 300: train loss 2.4262, val loss 2.5066 [8.343538999557495 sec]
step 400: train loss 2.3294, val loss 2.4133 [10.599827289581299 sec]
2.293076992034912
Total Training Time: 11.553861618041992 seconds

"I P-ed uonout?"
"I iw sailens ofuEYew
ot WmiGthikd im oue sod. to Nid Sey Yit,as.
Afattor ay
pind o
BEGINNING (1681841705.9956844): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6687, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6378, val loss 4.6389 [1.882737159729004 sec]
step 100: train loss 2.7563, val loss 2.8009 [4.993450880050659 sec]
step 200: train loss 2.5610, val loss 2.6475 [8.175923585891724 sec]
step 300: train loss 2.3939, val loss 2.4810 [11.297141790390015 sec]
step 400: train loss 2.2728, val loss 2.3800 [14.430277347564697 sec]
2.292570114135742
Total Training Time: 15.76862907409668 seconds

thasyery hent would, exr brra eeneed afer you the the na sen
hains gef2e snem arte Gorse atingard. A
BEGINNING (1681841722.2563047): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6715, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6570, val loss 4.6612 [1.0097455978393555 sec]
step 100: train loss 2.8579, val loss 2.8993 [2.566605806350708 sec]
step 200: train loss 2.6550, val loss 2.7157 [4.128730297088623 sec]
step 300: train loss 2.5632, val loss 2.6385 [5.6904895305633545 sec]
step 400: train loss 2.4776, val loss 2.5578 [7.243569850921631 sec]
2.402296543121338
Total Training Time: 7.827090263366699 seconds

bwaca A€ediusth sos owond anis oneI br pntcowly
r ac ne vant sism fsat, wa ay 5g fovaemo s, Th lkini
BEGINNING (1681841730.2903962): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6391, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6500, val loss 4.6503 [1.4272615909576416 sec]
step 100: train loss 2.7833, val loss 2.8213 [3.8065664768218994 sec]
step 200: train loss 2.5945, val loss 2.6504 [6.238551616668701 sec]
step 300: train loss 2.5147, val loss 2.5817 [8.6420259475708 sec]
step 400: train loss 2.4372, val loss 2.5086 [11.036593437194824 sec]
2.386409282684326
Total Training Time: 12.016564846038818 seconds

fung crell wor olear s, to. sled llenen, lye w tan ay. Na t Teauther the g G siee heal s "
cenoiscas
BEGINNING (1681841742.66147): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6666, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6601, val loss 4.6612 [1.8345420360565186 sec]
step 100: train loss 2.7278, val loss 2.7699 [5.0603673458099365 sec]
step 200: train loss 2.5764, val loss 2.6389 [8.28493857383728 sec]
step 300: train loss 2.4707, val loss 2.5464 [11.557363510131836 sec]
step 400: train loss 2.3700, val loss 2.4585 [15.005210399627686 sec]
2.333615779876709
Total Training Time: 16.379209995269775 seconds

Tut o bere and. TEFe "THEACh
E Iw TH
TH
EE ge TE SeloniskO8tht
ber.
Re cK Th imal I
ifllle thevive.

BEGINNING (1681841759.5406976): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6036, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6065, val loss 4.6062 [1.0592916011810303 sec]
step 100: train loss 2.8255, val loss 2.8778 [2.754984140396118 sec]
step 200: train loss 2.6477, val loss 2.7137 [4.456526279449463 sec]
step 300: train loss 2.5679, val loss 2.6446 [6.144079208374023 sec]
step 400: train loss 2.5271, val loss 2.6064 [7.862748384475708 sec]
2.5315277576446533
Total Training Time: 8.50279974937439 seconds

hekavokerad and rof uth  snoraren "
"Y alass as tathemamamey y k danoop cok
Foilla reie ibus,
bonen 
BEGINNING (1681841768.2494977): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6163, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6240, val loss 4.6263 [1.4677116870880127 sec]
step 100: train loss 2.7754, val loss 2.8245 [3.991823673248291 sec]
step 200: train loss 2.6086, val loss 2.6666 [6.484327077865601 sec]
step 300: train loss 2.5416, val loss 2.6103 [8.990970134735107 sec]
step 400: train loss 2.4963, val loss 2.5705 [11.497361898422241 sec]
2.469306230545044
Total Training Time: 12.506807088851929 seconds

shied aas unrour ckeang."Whernt as Gxef sa aphevied Pe t thea latoinsowe thsawalorea
s t dve
a te b 
BEGINNING (1681841781.1083055): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6741, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6624, val loss 4.6676 [1.904604196548462 sec]
step 100: train loss 2.7581, val loss 2.8109 [5.2524049282073975 sec]
step 200: train loss 2.5899, val loss 2.6533 [8.575260877609253 sec]
step 300: train loss 2.5187, val loss 2.5957 [11.911898612976074 sec]
step 400: train loss 2.4690, val loss 2.5462 [15.248593091964722 sec]
2.455868721008301
Total Training Time: 16.64707589149475 seconds

kessat cal theeroff, a quhe(ary emy sowe hewive, ar
atitun s
of uuw!
"Chue watom ald nd The wsman ha
BEGINNING (1681841798.2684114): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6543, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6527, val loss 4.6550 [1.120002031326294 sec]
step 100: train loss 2.8928, val loss 2.9292 [2.904127836227417 sec]
step 200: train loss 2.6273, val loss 2.6979 [4.762504577636719 sec]
step 300: train loss 2.4708, val loss 2.5652 [6.731846332550049 sec]
step 400: train loss 2.3638, val loss 2.4717 [8.65457010269165 sec]
2.351583242416382
Total Training Time: 9.36990213394165 seconds

tuon san we sandis, hoo as sh
s,
matta cen and
tpoutheamithenuthis wen
wme tha iaceZas yomald, "n ge
BEGINNING (1681841807.9291933): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5718, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5623, val loss 4.5593 [1.7074098587036133 sec]
step 100: train loss 2.7520, val loss 2.8023 [4.460124969482422 sec]
step 200: train loss 2.5215, val loss 2.5902 [7.100600242614746 sec]
step 300: train loss 2.3548, val loss 2.4437 [9.762857913970947 sec]
step 400: train loss 2.2446, val loss 2.3626 [12.689803838729858 sec]
2.216295003890991
Total Training Time: 13.804631233215332 seconds

pinged the tPy cing an ut, a do souerrye sand gaing to themised the rr frousr mifbal youghe watif Ci
BEGINNING (1681841822.0915143): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5906, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5988, val loss 4.5852 [2.1332385540008545 sec]
step 100: train loss 2.7322, val loss 2.7643 [5.835412263870239 sec]
step 200: train loss 2.5082, val loss 2.5670 [9.295995712280273 sec]
step 300: train loss 2.3437, val loss 2.4260 [12.850209474563599 sec]
step 400: train loss 2.2194, val loss 2.3260 [16.74381113052368 sec]
2.2178494930267334
Total Training Time: 18.758331537246704 seconds

CMAT5TER The AY. in T"RA pand Det lak oow whiseOoosould Lomle warred theke seluchey ba7
wewes
beyu f
BEGINNING (1681841841.531122): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6301, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6405, val loss 4.6474 [1.5622518062591553 sec]
step 100: train loss 2.8225, val loss 2.8713 [3.9189677238464355 sec]
step 200: train loss 2.6134, val loss 2.6896 [5.786039352416992 sec]
step 300: train loss 2.5191, val loss 2.6021 [8.44544005393982 sec]
step 400: train loss 2.4171, val loss 2.5049 [11.090261697769165 sec]
2.4586379528045654
Total Training Time: 12.007609844207764 seconds

wand eubstad slieroulnans Tuoks ar G igh I h the egh'ou accke whe se sauris lind en
m are the the at
BEGINNING (1681841853.7887266): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6102, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6149, val loss 4.6056 [1.770759105682373 sec]
step 100: train loss 2.7773, val loss 2.8209 [4.591836929321289 sec]
step 200: train loss 2.5822, val loss 2.6420 [7.2473695278167725 sec]
step 300: train loss 2.4655, val loss 2.5384 [9.964890241622925 sec]
step 400: train loss 2.3417, val loss 2.4226 [12.88806939125061 sec]
2.2858452796936035
Total Training Time: 13.992790937423706 seconds

Pht
tharyrokithe est stllel asanthe er th youme,
agely. k, fo ad. "Ring he t ised ind hachout, thae 
BEGINNING (1681841868.2025099): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5863, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5924, val loss 4.5937 [2.1833369731903076 sec]
step 100: train loss 2.7337, val loss 2.7811 [5.671968698501587 sec]
step 200: train loss 2.5510, val loss 2.6230 [9.15811276435852 sec]
step 300: train loss 2.4392, val loss 2.5102 [12.586489915847778 sec]
step 400: train loss 2.3090, val loss 2.4099 [16.093968391418457 sec]
2.3336775302886963
Total Training Time: 17.47519302368164 seconds

sn 3MEA– hio the Peredws, gaked hin tjeatts foth ma da tor t wice rat atet a fre che alommor the'san
BEGINNING (1681841886.1940742): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6066, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6106, val loss 4.6110 [1.3577253818511963 sec]
step 100: train loss 2.8222, val loss 2.8721 [3.4631378650665283 sec]
step 200: train loss 2.6466, val loss 2.7125 [5.5523841381073 sec]
step 300: train loss 2.5666, val loss 2.6409 [7.6485137939453125 sec]
step 400: train loss 2.5233, val loss 2.5996 [9.715815305709839 sec]
2.501457929611206
Total Training Time: 10.489937543869019 seconds

Villong fphee bin5 wins hilofint ca
h haserouY
s cak, hasswhexllle
wio The thela ime bl t
t t ay o t
BEGINNING (1681841896.8965242): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6075, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6063, val loss 4.6104 [1.8797776699066162 sec]
step 100: train loss 2.7686, val loss 2.8210 [4.886672496795654 sec]
step 200: train loss 2.5980, val loss 2.6723 [7.8740034103393555 sec]
step 300: train loss 2.5237, val loss 2.5960 [10.895943880081177 sec]
step 400: train loss 2.4759, val loss 2.5547 [13.887117385864258 sec]
2.4503185749053955
Total Training Time: 15.04878282546997 seconds

s?" ouent ayth oout w to4 mig I Tundie m jurtekrpueriorinnwad
tofo
of wie h The eetes
Nimal toustale
BEGINNING (1681841912.336425): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5894, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5817, val loss 4.5761 [2.399813175201416 sec]
step 100: train loss 2.7275, val loss 2.7732 [6.3915276527404785 sec]
step 200: train loss 2.5701, val loss 2.6302 [10.31867003440857 sec]
step 300: train loss 2.5057, val loss 2.5696 [14.350429058074951 sec]
step 400: train loss 2.4446, val loss 2.5269 [18.528862714767456 sec]
2.392866611480713
Total Training Time: 20.47521209716797 seconds

uit leolde ghackist.
Ya Yinghad
ex oudsuware pal. Grilais s the ctace thare ttr tostalyopl wheunerad
BEGINNING (1681841933.486037): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6934, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7019, val loss 4.6980 [1.4977772235870361 sec]
step 100: train loss 2.8198, val loss 2.8596 [3.7993974685668945 sec]
step 200: train loss 2.5791, val loss 2.6380 [6.000741004943848 sec]
step 300: train loss 2.4156, val loss 2.5024 [8.266640901565552 sec]
step 400: train loss 2.3196, val loss 2.4136 [10.566774845123291 sec]
2.283771276473999
Total Training Time: 11.674651861190796 seconds

ws
what, " a biew cep enty andorheannd as
misnd thabe dit ay he
He orih smaps poveng
thechee nor faâ
BEGINNING (1681841945.428073): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5982, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6241, val loss 4.6319 [2.6636500358581543 sec]
step 100: train loss 2.7512, val loss 2.8021 [7.168569326400757 sec]
step 200: train loss 2.5118, val loss 2.5852 [11.756499290466309 sec]
step 300: train loss 2.3409, val loss 2.4347 [16.11495542526245 sec]
step 400: train loss 2.2159, val loss 2.3339 [20.593508005142212 sec]
2.219388961791992
Total Training Time: 22.29850935935974 seconds

SKE’ wfyout hames thew
lelad
ta pattaly. TheI haciad lewas natka s.
"Thitiderivmiding stokey ve goun
BEGINNING (1681841968.2809935): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5241, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5237, val loss 4.5223 [3.4099411964416504 sec]
step 100: train loss 2.7129, val loss 2.7567 [9.113139867782593 sec]
step 200: train loss 2.4611, val loss 2.5413 [14.602454423904419 sec]
step 300: train loss 2.2939, val loss 2.3873 [20.21062421798706 sec]
step 400: train loss 2.1618, val loss 2.2888 [25.86129093170166 sec]
2.226997137069702
Total Training Time: 28.146071672439575 seconds

nowd them. be, Ye lled on'ts cuth
to sefored tuuonn of s rin'e nor witwing, sais
butaece tatrisandes
BEGINNING (1681841997.281121): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6336, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6423, val loss 4.6366 [2.186163902282715 sec]
step 100: train loss 2.8188, val loss 2.8624 [5.542315721511841 sec]
step 200: train loss 2.6087, val loss 2.6754 [8.908549547195435 sec]
step 300: train loss 2.5093, val loss 2.5822 [12.224131345748901 sec]
step 400: train loss 2.3994, val loss 2.4769 [15.566974878311157 sec]
2.3797459602355957
Total Training Time: 16.72046184539795 seconds

to wing" The dlpeliinis t cam ook ske. led the thid ateer ureEdis 1eed red thisof ioor?"Fe. "" "CHAE
BEGINNING (1681842014.324981): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6729, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6634, val loss 4.6662 [2.857658624649048 sec]
step 100: train loss 2.7461, val loss 2.8043 [7.29742431640625 sec]
step 200: train loss 2.5713, val loss 2.6425 [11.754748344421387 sec]
step 300: train loss 2.4464, val loss 2.5300 [16.586776733398438 sec]
step 400: train loss 2.3061, val loss 2.4083 [21.25629687309265 sec]
2.293365955352783
Total Training Time: 23.041416883468628 seconds

beakeatins this, and im he an then spend t feluarof e.
"I bew derroorgs sGournan Bapen – townond san
BEGINNING (1681842037.9629822): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6676, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6762, val loss 4.6799 [3.567209243774414 sec]
step 100: train loss 2.7414, val loss 2.7830 [9.553656578063965 sec]
step 200: train loss 2.5336, val loss 2.6035 [15.584937572479248 sec]
step 300: train loss 2.4001, val loss 2.4876 [21.60936689376831 sec]
step 400: train loss 2.2694, val loss 2.3711 [27.965879678726196 sec]
2.2210912704467773
Total Training Time: 30.406832695007324 seconds

Grattent of co ar theiwwoutews A
hand ante on natt lo trearvearen.
Thexs Idias iit adsmatin. Tind A 
BEGINNING (1681842069.2124348): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.4817, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4836, val loss 4.4994 [2.449209451675415 sec]
step 100: train loss 2.7897, val loss 2.8417 [6.171370029449463 sec]
step 200: train loss 2.6304, val loss 2.6964 [9.963173627853394 sec]
step 300: train loss 2.5572, val loss 2.6355 [13.563180208206177 sec]
step 400: train loss 2.5123, val loss 2.5896 [17.2121160030365 sec]
2.521616220474243
Total Training Time: 18.5006685256958 seconds

hiof
at, te fa the cs acus ouscremachuf woulcere tesulur.
he astr, thiean d y oalate oul hid alesthe
BEGINNING (1681842088.072989): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6485, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6446, val loss 4.6523 [3.226529598236084 sec]
step 100: train loss 2.7706, val loss 2.8106 [8.308929443359375 sec]
step 200: train loss 2.5869, val loss 2.6556 [13.284112691879272 sec]
step 300: train loss 2.5191, val loss 2.5874 [18.313059091567993 sec]
step 400: train loss 2.4686, val loss 2.5406 [23.505468130111694 sec]
2.4477362632751465
Total Training Time: 25.376873254776 seconds

8"
Yus, wacow s had thas Y
ttano f ilented an hased hend thedr futtbend t,"€, ashtinde me s, acoungl
BEGINNING (1681842114.028392): Baseline LR(0.0006) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5172, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5279, val loss 4.5228 [4.002208232879639 sec]
step 100: train loss 2.7253, val loss 2.7810 [10.613957405090332 sec]
step 200: train loss 2.5641, val loss 2.6333 [16.666610717773438 sec]
step 300: train loss 2.4870, val loss 2.5627 [22.818260192871094 sec]
step 400: train loss 2.4121, val loss 2.4995 [29.240875005722046 sec]
2.354736328125
Total Training Time: 31.87300419807434 seconds

NGraatted sism An, TIf hat wat, hillell de. Grat t a gurant ag. ad ofurshtt thtoshio d isugacKaire f
BEGINNING (1681842146.8687968): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6356, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6412, val loss 4.6337 [2.201080322265625 sec]
step 100: train loss 2.6127, val loss 2.6636 [5.709295988082886 sec]
step 200: train loss 2.3584, val loss 2.4553 [10.080274820327759 sec]
step 300: train loss 2.2181, val loss 2.3378 [13.983315706253052 sec]
step 400: train loss 2.1176, val loss 2.2507 [17.87193536758423 sec]
2.252521514892578
Total Training Time: 19.458006143569946 seconds

TU The cenertt tall cabloked aighil?"kh Yrons he weried hasouls buo hy
hise bes. Yiond his bie?" Eop
BEGINNING (1681842166.9289403): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6802, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6705, val loss 4.6690 [3.528285264968872 sec]
step 100: train loss 2.5824, val loss 2.6499 [9.625849962234497 sec]
step 200: train loss 2.3294, val loss 2.4354 [15.478251457214355 sec]
step 300: train loss 2.1710, val loss 2.2937 [20.963117837905884 sec]
step 400: train loss 2.0444, val loss 2.1917 [26.418882608413696 sec]
2.074206590652466
Total Training Time: 28.827841997146606 seconds

bow them as hipled.
Grah was frest for ave th a mit theperell tuoke sming was we Toure with avo s, t
BEGINNING (1681842196.7407231): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6198, val loss 4.6211 [4.211029291152954 sec]
step 100: train loss 2.5510, val loss 2.6283 [11.559407472610474 sec]
step 200: train loss 2.3188, val loss 2.4192 [18.775519609451294 sec]
step 300: train loss 2.1399, val loss 2.2526 [26.154237031936646 sec]
step 400: train loss 1.9971, val loss 2.1625 [33.36809587478638 sec]
1.9288451671600342
Total Training Time: 36.34409046173096 seconds

theat. I nother buthem arcey."
23J Anayah so your
Pere Namand
pabled ardken and.
AAs.
Shas rot ristu
BEGINNING (1681842234.2097716): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6323, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6479, val loss 4.6446 [1.824286699295044 sec]
step 100: train loss 2.6222, val loss 2.6959 [4.9968202114105225 sec]
step 200: train loss 2.4508, val loss 2.5320 [8.121961116790771 sec]
step 300: train loss 2.2778, val loss 2.3798 [11.417035341262817 sec]
step 400: train loss 2.1494, val loss 2.2806 [14.651357650756836 sec]
2.1425087451934814
Total Training Time: 15.914585590362549 seconds

Anad gs heir the ileng thive Pls wiingerd le infe th. Grattielia rice. Hes an smorsep. "Toud
to
Grat
BEGINNING (1681842250.563265): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6074, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5788, val loss 4.5866 [2.6912200450897217 sec]
step 100: train loss 2.5664, val loss 2.6445 [7.761042594909668 sec]
step 200: train loss 2.4200, val loss 2.5002 [13.045811414718628 sec]
step 300: train loss 2.2331, val loss 2.3460 [17.845934629440308 sec]
step 400: train loss 2.0798, val loss 2.2114 [22.6334388256073 sec]
2.091905355453491
Total Training Time: 24.75593113899231 seconds

trand hily er to kis fred cuon the as sifelene.
Rech wank helo a sey Oecake uw
Renepa sor corriad an
BEGINNING (1681842276.0618935): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5696, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5675, val loss 4.5597 [3.569538116455078 sec]
step 100: train loss 2.5577, val loss 2.6257 [10.146907329559326 sec]
step 200: train loss 2.3910, val loss 2.4774 [16.756770133972168 sec]
step 300: train loss 2.1793, val loss 2.3056 [23.364073753356934 sec]
step 400: train loss 2.0173, val loss 2.1667 [29.88034200668335 sec]
1.9992250204086304
Total Training Time: 32.81570768356323 seconds

ell fust. Ond ithtiman will hee hatult.
"
Thet theim.
And fourphad und to man sto Arphifdedn, Gratta
BEGINNING (1681842309.9928265): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6255, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6305, val loss 4.6315 [2.083038091659546 sec]
step 100: train loss 2.6273, val loss 2.6935 [5.485934019088745 sec]
step 200: train loss 2.5161, val loss 2.5953 [8.855621099472046 sec]
step 300: train loss 2.4555, val loss 2.5350 [12.289877653121948 sec]
step 400: train loss 2.3670, val loss 2.4641 [15.66196084022522 sec]
2.3140642642974854
Total Training Time: 17.148255586624146 seconds

ce.
"DEont t NMyah ned hat thef the Iref.© We thit gogem as tord, thadindyouke te Nou the s Zat bede
BEGINNING (1681842327.6217947): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.4846, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4925, val loss 4.4926 [3.206331968307495 sec]
step 100: train loss 2.5912, val loss 2.6528 [8.556183576583862 sec]
step 200: train loss 2.4874, val loss 2.5646 [13.866913557052612 sec]
step 300: train loss 2.4131, val loss 2.5021 [19.13509964942932 sec]
step 400: train loss 2.2725, val loss 2.3776 [24.369332551956177 sec]
2.1657018661499023
Total Training Time: 26.580191373825073 seconds

THE
CYNad DUDou amed okerest "I coyo4 pes fideft the asthirsos chimd anses, cip ffath
h. Koedgousing
BEGINNING (1681842354.9756842): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6061, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6093, val loss 4.6050 [4.051049709320068 sec]
step 100: train loss 2.5747, val loss 2.6439 [11.368753910064697 sec]
step 200: train loss 2.4696, val loss 2.5449 [20.059725046157837 sec]
step 300: train loss 2.3604, val loss 2.4582 [27.145062923431396 sec]
step 400: train loss 2.1898, val loss 2.3125 [34.594693660736084 sec]
2.125612735748291
Total Training Time: 37.992321252822876 seconds

entaed nou caidhass ald, ybuts sanyepleelvele cn tooll cow
skic elipff ontoo omthe toio hans angaied
BEGINNING (1681842394.2637236): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6027, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6140, val loss 4.6204 [2.460566759109497 sec]
step 100: train loss 2.5931, val loss 2.6609 [5.968222379684448 sec]
step 200: train loss 2.3354, val loss 2.4321 [9.523010730743408 sec]
step 300: train loss 2.1848, val loss 2.3038 [13.138179779052734 sec]
step 400: train loss 2.0850, val loss 2.2241 [16.56869149208069 sec]
2.1975138187408447
Total Training Time: 17.909387588500977 seconds

if.
3"We ch sma frerour bst.
"Be creng of onon inele."Yah Thermid, war." Gortta turtta Pain, azer of
BEGINNING (1681842412.6394153): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.7427, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7396, val loss 4.7335 [3.1158804893493652 sec]
step 100: train loss 2.5377, val loss 2.6075 [8.424571990966797 sec]
step 200: train loss 2.2712, val loss 2.3779 [13.666501522064209 sec]
step 300: train loss 2.1005, val loss 2.2428 [18.851988554000854 sec]
step 400: train loss 1.9760, val loss 2.1429 [24.17951202392578 sec]
1.9701042175292969
Total Training Time: 26.37288188934326 seconds

Che Arad, Yeachesalrs bes sist2
ANana
juphothe poutioned was
not he maved ward your a !
"Yelly ther 
BEGINNING (1681842439.7988846): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6941, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6809, val loss 4.6707 [3.9968714714050293 sec]
step 100: train loss 2.5207, val loss 2.5866 [10.964418888092041 sec]
step 200: train loss 2.2220, val loss 2.3483 [17.92272186279297 sec]
step 300: train loss 2.0387, val loss 2.1936 [25.11922001838684 sec]
step 400: train loss 1.9260, val loss 2.1015 [32.001227378845215 sec]
1.992499828338623
Total Training Time: 34.92384910583496 seconds

ut, "He their, at helps the deet on neir
she groome camey solin, youidding the the caft him
sipplo, 
BEGINNING (1681842475.8385258): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5557, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5760, val loss 4.5719 [2.2119405269622803 sec]
step 100: train loss 2.6105, val loss 2.6797 [5.708080291748047 sec]
step 200: train loss 2.4476, val loss 2.5312 [9.286962270736694 sec]
step 300: train loss 2.2455, val loss 2.3505 [12.918296098709106 sec]
step 400: train loss 2.1026, val loss 2.2483 [16.57009744644165 sec]
2.058093309402466
Total Training Time: 17.947020769119263 seconds

celp t
hey ho. Grata weve ta speebe thoull and the cale fureaughimesed a thad
ars, ay mosto lon Bire
BEGINNING (1681842494.2347293): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5180, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5240, val loss 4.5276 [2.9909305572509766 sec]
step 100: train loss 2.5623, val loss 2.6385 [8.06337857246399 sec]
step 200: train loss 2.4057, val loss 2.4971 [12.882082462310791 sec]
step 300: train loss 2.1806, val loss 2.3075 [17.747682332992554 sec]
step 400: train loss 2.0120, val loss 2.1801 [22.65768837928772 sec]
1.985640525817871
Total Training Time: 24.80357551574707 seconds

hisy irs wistouts."
"I we thivest fit the toree stoment larny rethoun
of bre an. Ar:in
and you!"
"Da
BEGINNING (1681842519.7596524): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5635, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5598, val loss 4.5642 [4.051206111907959 sec]
step 100: train loss 2.5424, val loss 2.6149 [11.22392988204956 sec]
step 200: train loss 2.3276, val loss 2.4161 [18.243568181991577 sec]
step 300: train loss 2.0884, val loss 2.2267 [25.15617871284485 sec]
step 400: train loss 1.9229, val loss 2.0966 [32.21724796295166 sec]
1.9497065544128418
Total Training Time: 35.07062387466431 seconds

tarre cubsing freer
onereldge!
Peinanapt pas, and but and rat camf
bughand the arin. We carttaringhe
BEGINNING (1681842555.8606367): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6025, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5952, val loss 4.5981 [2.2897114753723145 sec]
step 100: train loss 2.6210, val loss 2.6883 [6.1898815631866455 sec]
step 200: train loss 2.5025, val loss 2.5790 [9.849469184875488 sec]
step 300: train loss 2.4025, val loss 2.4884 [13.512255668640137 sec]
step 400: train loss 2.2530, val loss 2.3618 [17.342742919921875 sec]
2.2020983695983887
Total Training Time: 18.81370711326599 seconds

ho nieredseeLfit the?" his claups cKist. Gratheve offoGratthibe thad him ane hatt has
couk, anot m '
BEGINNING (1681842575.1039593): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6224, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6242, val loss 4.6259 [3.4184200763702393 sec]
step 100: train loss 2.5883, val loss 2.6623 [9.177424669265747 sec]
step 200: train loss 2.4817, val loss 2.5597 [14.972559213638306 sec]
step 300: train loss 2.3991, val loss 2.4886 [20.908059120178223 sec]
step 400: train loss 2.2386, val loss 2.3485 [26.79895782470703 sec]
2.1375882625579834
Total Training Time: 29.172107458114624 seconds

sesit thes." Ta
peans anle." Tous wor, ectle fossumek an. Gratt biore taetat fooll ome ht aty yast a
BEGINNING (1681842605.079386): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6559, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6551, val loss 4.6521 [4.792985439300537 sec]
step 100: train loss 2.5652, val loss 2.6312 [13.15582275390625 sec]
step 200: train loss 2.4564, val loss 2.5343 [21.470128297805786 sec]
step 300: train loss 2.3081, val loss 2.4171 [29.530219078063965 sec]
step 400: train loss 2.0864, val loss 2.2315 [37.47905468940735 sec]
1.9806694984436035
Total Training Time: 40.70354175567627 seconds

thise speate. I newn thell. Ho not do washentad
th som chiegore the hime bile atee aidned kin anl an
BEGINNING (1681842646.9910054): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5176, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5395, val loss 4.5303 [2.360288381576538 sec]
step 100: train loss 2.5680, val loss 2.6220 [5.937348127365112 sec]
step 200: train loss 2.2784, val loss 2.3816 [9.585291862487793 sec]
step 300: train loss 2.1208, val loss 2.2520 [13.497931957244873 sec]
step 400: train loss 2.0117, val loss 2.1676 [17.18029284477234 sec]
1.9903173446655273
Total Training Time: 18.497824668884277 seconds

to to no had
the follow, ked usan and besa daOk. He
thards. Grat Mugh, He cubuswor anst
athe ody din
BEGINNING (1681842665.9613469): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5768, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6037, val loss 4.6091 [3.278055429458618 sec]
step 100: train loss 2.5225, val loss 2.5917 [8.56890344619751 sec]
step 200: train loss 2.2141, val loss 2.3259 [13.974480390548706 sec]
step 300: train loss 2.0412, val loss 2.1954 [19.364486694335938 sec]
step 400: train loss 1.9141, val loss 2.0966 [24.654886484146118 sec]
1.893991231918335
Total Training Time: 26.833850383758545 seconds

thim.
Af Gratta, leand cor nafwase doty rrechemplible d hum:.
Al with jupordile momsed gzied nard."

BEGINNING (1681842693.461836): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6997, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6935, val loss 4.6946 [4.090252161026001 sec]
step 100: train loss 2.4915, val loss 2.5638 [11.051966428756714 sec]
step 200: train loss 2.2064, val loss 2.3249 [18.17810368537903 sec]
step 300: train loss 2.0127, val loss 2.1702 [25.238580465316772 sec]
step 400: train loss 1.8870, val loss 2.0616 [32.638723850250244 sec]
1.9126166105270386
Total Training Time: 35.68158507347107 seconds

chief, et a enaid. Will he dey. They
trabarieck at orfaed in
abure stop!
Spouin quick. Gratta said
h
BEGINNING (1681842730.2664216): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6825, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6832, val loss 4.6942 [2.443556547164917 sec]
step 100: train loss 2.6015, val loss 2.6791 [6.240679502487183 sec]
step 200: train loss 2.4082, val loss 2.4966 [10.003572225570679 sec]
step 300: train loss 2.1981, val loss 2.3206 [13.862083435058594 sec]
step 400: train loss 2.0579, val loss 2.2096 [17.65627670288086 sec]
2.110149383544922
Total Training Time: 18.938390970230103 seconds

FE Chir Tats hey cucous on rearsel oferopaces hae swerribe urst and and to ofownge thef thad sig cas
BEGINNING (1681842749.6074998): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6081, val loss 4.5987 [3.0678720474243164 sec]
step 100: train loss 2.5463, val loss 2.6227 [8.651395559310913 sec]
step 200: train loss 2.3097, val loss 2.4129 [14.42650032043457 sec]
step 300: train loss 2.0956, val loss 2.2315 [20.17578959465027 sec]
step 400: train loss 1.9406, val loss 2.1069 [25.84190082550049 sec]
1.9466997385025024
Total Training Time: 28.217673778533936 seconds

Aslionsed fire Vice said hour fel.
Thas waull ye back. Md for spale soldidring ste."
y, triyour. Hit
BEGINNING (1681842778.6158497): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6578, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6550, val loss 4.6548 [4.553995847702026 sec]
step 100: train loss 2.5368, val loss 2.6020 [12.5625638961792 sec]
step 200: train loss 2.2972, val loss 2.3888 [20.41362690925598 sec]
step 300: train loss 2.0416, val loss 2.1916 [27.56443500518799 sec]
step 400: train loss 1.8953, val loss 2.0698 [35.26336431503296 sec]
1.8660269975662231
Total Training Time: 38.42017579078674 seconds

piar!" Hew the. Gratta dyeek py. They and laars ye
wirus a looke wimnerss, the hus dacke hak and we 
BEGINNING (1681842818.1790178): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6500, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6527, val loss 4.6460 [2.923814296722412 sec]
step 100: train loss 2.6190, val loss 2.6805 [7.572195768356323 sec]
step 200: train loss 2.4923, val loss 2.5605 [12.17259669303894 sec]
step 300: train loss 2.3842, val loss 2.4696 [16.695444583892822 sec]
step 400: train loss 2.1968, val loss 2.3104 [21.29821228981018 sec]
2.130920886993408
Total Training Time: 22.99007821083069 seconds

s atuon!
"Gid, inould tur he nomed ofg navereded fore woutthe wiexre fan les
he ced asthe a. Tavell 
BEGINNING (1681842841.6370811): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6280, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6308, val loss 4.6239 [4.282204866409302 sec]
step 100: train loss 2.5747, val loss 2.6353 [11.307549953460693 sec]
step 200: train loss 2.4549, val loss 2.5393 [18.49494743347168 sec]
step 300: train loss 2.3058, val loss 2.4082 [25.56189799308777 sec]
step 400: train loss 2.0988, val loss 2.2408 [32.592411518096924 sec]
2.0420610904693604
Total Training Time: 35.41052031517029 seconds

walleou pme tas and ovexf of for wif looke apenty as te andeeed tallacor
fwith, to nce to cres."
"I 
BEGINNING (1681842878.0158186): Baseline LR(0.0006) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6383, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6352, val loss 4.6368 [5.673066854476929 sec]
step 100: train loss 2.5563, val loss 2.6279 [14.661148071289062 sec]
step 200: train loss 2.4384, val loss 2.5265 [23.800079107284546 sec]
step 300: train loss 2.2761, val loss 2.3833 [32.97546172142029 sec]
step 400: train loss 2.0491, val loss 2.2081 [42.11427569389343 sec]
2.0041234493255615
Total Training Time: 45.889976501464844 seconds

pa."
Kiddd hid lout o ast. Turnect usin ce sfor, We ind Aha,
the fen. Arpadllowled Arph
chadreare
an
BEGINNING (1681842925.1088908): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6102, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6148, val loss 4.6072 [2.1998443603515625 sec]
step 100: train loss 2.5330, val loss 2.6177 [5.911078691482544 sec]
step 200: train loss 2.2536, val loss 2.3622 [9.55697774887085 sec]
step 300: train loss 2.1101, val loss 2.2393 [13.312906742095947 sec]
step 400: train loss 2.0117, val loss 2.1701 [17.047260999679565 sec]
1.9787938594818115
Total Training Time: 18.60086417198181 seconds

my.phats for. G he tuon to
culoziniythis the be that oan gerid the smes ofir trodt. Thellangual'sk, 
BEGINNING (1681842944.3143272): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6735, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7088, val loss 4.7082 [3.5076799392700195 sec]
step 100: train loss 2.4722, val loss 2.5439 [9.569243907928467 sec]
step 200: train loss 2.1976, val loss 2.3259 [15.657571077346802 sec]
step 300: train loss 2.0261, val loss 2.1778 [21.81445837020874 sec]
step 400: train loss 1.8999, val loss 2.0875 [27.984883308410645 sec]
1.9373265504837036
Total Training Time: 30.647648334503174 seconds

would oned werlalt mating the wothented Ve wearvand a che mome
caret ot you ceptch
auick.
"Mc chieni
BEGINNING (1681842975.9732034): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5789, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5653, val loss 4.5674 [4.6902549266815186 sec]
step 100: train loss 2.4716, val loss 2.5686 [13.385266304016113 sec]
step 200: train loss 2.1542, val loss 2.2852 [21.95306158065796 sec]
step 300: train loss 1.9845, val loss 2.1608 [30.384859561920166 sec]
step 400: train loss 1.8726, val loss 2.0483 [38.830334424972534 sec]
1.8385226726531982
Total Training Time: 42.573546409606934 seconds

side had the Too that Ao Pels fould be nodded caws Even's.
Anayah rsoth were oves. Arphad gresadde, 
BEGINNING (1681843020.044658): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5893, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6218, val loss 4.6224 [2.2988462448120117 sec]
step 100: train loss 2.5449, val loss 2.6193 [6.1724326610565186 sec]
step 200: train loss 2.3417, val loss 2.4394 [10.05127739906311 sec]
step 300: train loss 2.1344, val loss 2.2577 [13.818317890167236 sec]
step 400: train loss 2.0153, val loss 2.1865 [17.69226312637329 sec]
2.033435821533203
Total Training Time: 19.28019380569458 seconds

to befill jas isus ard kn ay. Grattarta carnir hung himed, ws cam42.
"B“
Geath ter ta ing com The ar
BEGINNING (1681843039.8753276): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5898, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5738, val loss 4.5662 [3.5828583240509033 sec]
step 100: train loss 2.5040, val loss 2.5721 [9.756226539611816 sec]
step 200: train loss 2.2602, val loss 2.3767 [16.181498050689697 sec]
step 300: train loss 2.0478, val loss 2.1852 [22.2959086894989 sec]
step 400: train loss 1.9093, val loss 2.0953 [29.410006523132324 sec]
1.9063469171524048
Total Training Time: 32.1458306312561 seconds

!" Grattraoved Anayah whor hies dors
and as splows at?"
"Yougl as, bas
ondidgident plom the fret die
BEGINNING (1681843073.064138): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5973, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5764, val loss 4.5756 [4.839412689208984 sec]
step 100: train loss 2.4910, val loss 2.5702 [13.223719120025635 sec]
step 200: train loss 2.2541, val loss 2.3705 [21.75763702392578 sec]
step 300: train loss 2.0264, val loss 2.2017 [30.35200309753418 sec]
step 400: train loss 1.8705, val loss 2.0473 [39.289456367492676 sec]
1.8636014461517334
Total Training Time: 43.236263036727905 seconds

cabefor hish and in mage andend our withe wan. Lifim, he
moaush wiwned devild bach, andight, but int
BEGINNING (1681843118.037056): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5965, val loss 4.6054 [2.7440552711486816 sec]
step 100: train loss 2.5587, val loss 2.6285 [7.065692663192749 sec]
step 200: train loss 2.4627, val loss 2.5470 [11.306693315505981 sec]
step 300: train loss 2.3340, val loss 2.4413 [15.383480787277222 sec]
step 400: train loss 2.1288, val loss 2.2663 [19.59554624557495 sec]
2.056124448776245
Total Training Time: 21.3098087310791 seconds

thEVArmils en cand ncey,"
Whis moince rep.
"If. IKRint The the yout ong cor ank knein't
ownentake cu
BEGINNING (1681843139.9437516): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6254, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6382, val loss 4.6459 [3.927334785461426 sec]
step 100: train loss 2.5257, val loss 2.5947 [10.786372423171997 sec]
step 200: train loss 2.4287, val loss 2.5121 [17.70837378501892 sec]
step 300: train loss 2.2263, val loss 2.3515 [24.48433756828308 sec]
step 400: train loss 2.0173, val loss 2.1806 [31.301929712295532 sec]
1.980928659439087
Total Training Time: 34.206435441970825 seconds

Buch inis they wele and pas wey gainctaright airs.
"I mance grat was sed min't agake min thuons
riy,
BEGINNING (1681843175.2197187): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6493, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6386, val loss 4.6399 [5.399578332901001 sec]
step 100: train loss 2.5111, val loss 2.5872 [15.184927225112915 sec]
step 200: train loss 2.3794, val loss 2.4739 [24.473228216171265 sec]
step 300: train loss 2.1637, val loss 2.2888 [33.90856719017029 sec]
step 400: train loss 1.9503, val loss 2.1201 [43.308507442474365 sec]
1.8766289949417114
Total Training Time: 47.29432225227356 seconds

"Whad ard werentand parear
hood fucke.h in the Eve theing.
Geral the tuong kna knon ammeuinge sure
o
BEGINNING (1681843224.129619): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5934, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5843, val loss 4.5783 [2.4724345207214355 sec]
step 100: train loss 2.4708, val loss 2.5490 [6.569629669189453 sec]
step 200: train loss 2.1774, val loss 2.3135 [10.708582639694214 sec]
step 300: train loss 2.0307, val loss 2.1867 [14.926808595657349 sec]
step 400: train loss 1.9305, val loss 2.0850 [19.072860956192017 sec]
1.8700460195541382
Total Training Time: 20.725868463516235 seconds

penttrof wxen toold leosculnibcond no to as you tu sacand alundied and siined
with a to in to aldins
BEGINNING (1681843245.4811952): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5634, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5494, val loss 4.5475 [3.841249704360962 sec]
step 100: train loss 2.4316, val loss 2.5016 [10.700188875198364 sec]
step 200: train loss 2.1178, val loss 2.2568 [17.30084228515625 sec]
step 300: train loss 1.9392, val loss 2.0993 [24.091179609298706 sec]
step 400: train loss 1.8336, val loss 2.0235 [30.632732152938843 sec]
1.9012843370437622
Total Training Time: 33.40327286720276 seconds

the tencupers, oashil in a him trebremy have. Butsts of give
to and scromed."
"You allly." He mavere
BEGINNING (1681843279.9310606): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6732, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6803, val loss 4.6761 [5.23732852935791 sec]
step 100: train loss 2.4068, val loss 2.4905 [14.160444021224976 sec]
step 200: train loss 2.0944, val loss 2.2359 [23.19735598564148 sec]
step 300: train loss 1.9134, val loss 2.0917 [32.11639189720154 sec]
step 400: train loss 1.7966, val loss 1.9936 [41.53554034233093 sec]
1.8340673446655273
Total Training Time: 45.44488739967346 seconds

Aran it of Gratta gooked Gratta Awayah Pas and smalked your
isfor!" Anaftamal day moke the the of
th
BEGINNING (1681843326.9465435): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5843, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5881, val loss 4.5812 [2.6446924209594727 sec]
step 100: train loss 2.5254, val loss 2.6041 [6.976054906845093 sec]
step 200: train loss 2.2742, val loss 2.3781 [11.372814655303955 sec]
step 300: train loss 2.0607, val loss 2.2129 [15.718677997589111 sec]
step 400: train loss 1.9371, val loss 2.1196 [20.058445930480957 sec]
1.9337471723556519
Total Training Time: 21.739248275756836 seconds

thilight hat him gould and ast ared-enow low lazilaka th bed arackmos ead whs spred aws this gly.
Gr
BEGINNING (1681843349.2831457): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5863, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6027, val loss 4.6050 [3.8523387908935547 sec]
step 100: train loss 2.4854, val loss 2.5547 [10.667080402374268 sec]
step 200: train loss 2.2109, val loss 2.3225 [17.435831308364868 sec]
step 300: train loss 1.9779, val loss 2.1459 [24.114007234573364 sec]
step 400: train loss 1.8312, val loss 2.0227 [30.879913568496704 sec]
1.7940387725830078
Total Training Time: 33.961419343948364 seconds

their, and fes worned fape!
Aread. Arphad, sed aland tisoned a grow and loke mifted fom the
Pyrrany?
BEGINNING (1681843384.373127): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5401, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5358, val loss 4.5288 [5.397691011428833 sec]
step 100: train loss 2.4814, val loss 2.5488 [14.479065179824829 sec]
step 200: train loss 2.1697, val loss 2.2991 [23.641403675079346 sec]
step 300: train loss 1.9254, val loss 2.0996 [32.86307382583618 sec]
step 400: train loss 1.7718, val loss 1.9894 [42.16206097602844 sec]
1.710486888885498
Total Training Time: 46.29439401626587 seconds

NEANBHE
Chaptef The rang one be a in mosew
to het Triall seectes the sto thumpritestifny
allo."
Grat
BEGINNING (1681843432.2761154): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6915, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6800, val loss 4.6787 [3.0820159912109375 sec]
step 100: train loss 2.5465, val loss 2.6188 [8.131789445877075 sec]
step 200: train loss 2.4377, val loss 2.5230 [13.230979919433594 sec]
step 300: train loss 2.2995, val loss 2.4050 [18.44305992126465 sec]
step 400: train loss 2.0854, val loss 2.2281 [23.438920974731445 sec]
2.0704355239868164
Total Training Time: 25.401941537857056 seconds

anoncome bef themper, an fat muldere they hame we whallavelned
tred the ad taled an) to fooked th't.
BEGINNING (1681843458.263833): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6859, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6778, val loss 4.6740 [4.769459009170532 sec]
step 100: train loss 2.5233, val loss 2.6000 [12.755997896194458 sec]
step 200: train loss 2.4028, val loss 2.4928 [20.731959342956543 sec]
step 300: train loss 2.1479, val loss 2.2718 [28.739609241485596 sec]
step 400: train loss 1.9316, val loss 2.1049 [36.710373878479004 sec]
1.884392261505127
Total Training Time: 40.02540349960327 seconds

Cyert herve tell ad aw." The idelve hust lageguards
the was stamand ageors." Grata his wexver as ist
BEGINNING (1681843499.3754494): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6272, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6238, val loss 4.6207 [6.433477163314819 sec]
step 100: train loss 2.5097, val loss 2.5903 [17.558911561965942 sec]
step 200: train loss 2.3471, val loss 2.4511 [28.682363986968994 sec]
step 300: train loss 2.0742, val loss 2.2225 [39.49440574645996 sec]
step 400: train loss 1.8583, val loss 2.0533 [50.388784408569336 sec]
1.784981369972229
Total Training Time: 54.98299598693848 seconds

 triyah ald. It wap out with swelked do cam the of
CoR Re outent filleblegate. Whe
ChiHAn BehieTHATr
BEGINNING (1681843555.943615): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6318, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6239, val loss 4.6348 [2.8654673099517822 sec]
step 100: train loss 2.4525, val loss 2.5375 [7.38282322883606 sec]
step 200: train loss 2.1642, val loss 2.2907 [11.831506729125977 sec]
step 300: train loss 2.0005, val loss 2.1617 [16.461663484573364 sec]
step 400: train loss 1.8978, val loss 2.0796 [21.082629680633545 sec]
1.86720609664917
Total Training Time: 22.832066535949707 seconds

und that
tured amon the and was a haghiall, and his flalt dep a to hot hare as, thim, "Pyran with an
BEGINNING (1681843579.45784): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6768, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6832, val loss 4.6891 [4.521650314331055 sec]
step 100: train loss 2.4102, val loss 2.4984 [11.477321863174438 sec]
step 200: train loss 2.0667, val loss 2.2134 [18.62126636505127 sec]
step 300: train loss 1.8982, val loss 2.0735 [25.570005655288696 sec]
step 400: train loss 1.7884, val loss 1.9932 [32.61545419692993 sec]
1.7536859512329102
Total Training Time: 35.50366973876953 seconds

be?"
SEANGMcKAY
CHAPTER AVTE, CHE DEG GEANS!"
Geatta's core
was wobl."
22
CEAY
RUOI sits monestree a
BEGINNING (1681843616.0192401): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6844, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6581, val loss 4.6640 [5.49493145942688 sec]
step 100: train loss 2.3831, val loss 2.4710 [14.885666370391846 sec]
step 200: train loss 2.0489, val loss 2.2052 [24.2878520488739 sec]
step 300: train loss 1.8602, val loss 2.0582 [34.01287531852722 sec]
step 400: train loss 1.7474, val loss 1.9592 [43.46343374252319 sec]
1.763039231300354
Total Training Time: 47.51388359069824 seconds

was nods is wend dong, but he somermee exp aseurest acher. Arphad wen
Yah prompling wither
some. "Hi
BEGINNING (1681843665.0882497): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5960, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6052, val loss 4.5935 [3.0227277278900146 sec]
step 100: train loss 2.5128, val loss 2.5893 [7.9241931438446045 sec]
step 200: train loss 2.2107, val loss 2.3275 [12.662943363189697 sec]
step 300: train loss 2.0109, val loss 2.1673 [17.596271753311157 sec]
step 400: train loss 1.8927, val loss 2.0777 [22.45540142059326 sec]
1.9146900177001953
Total Training Time: 24.325533390045166 seconds

gae. Glaid the with a9 The ubs sprou list the
ploternehat bechost the agbes borion,
arey matwoout ye
BEGINNING (1681843690.0200174): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6034, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6226, val loss 4.6112 [4.351349592208862 sec]
step 100: train loss 2.4714, val loss 2.5523 [11.616069555282593 sec]
step 200: train loss 2.1540, val loss 2.2820 [18.98369598388672 sec]
step 300: train loss 1.9293, val loss 2.1041 [26.627723455429077 sec]
step 400: train loss 1.7833, val loss 1.9921 [33.82997512817383 sec]
1.7773997783660889
Total Training Time: 36.79832863807678 seconds

gl's rivetimed the crmeay tapss repitior
ling of dung xtlatt Arphayah; tridg smome his from whe was 
BEGINNING (1681843727.914904): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6097, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6137, val loss 4.6073 [5.784016370773315 sec]
step 100: train loss 2.4596, val loss 2.5366 [15.505466222763062 sec]
step 200: train loss 2.1241, val loss 2.2529 [25.26253056526184 sec]
step 300: train loss 1.8844, val loss 2.0674 [35.132123708724976 sec]
step 400: train loss 1.7316, val loss 1.9549 [44.91999411582947 sec]
1.7496229410171509
Total Training Time: 49.18233942985535 seconds

wo and that waughthers. Namal dikeft ho the gezitindere
Zechariys.
Ars frour the will you stued. Tha
BEGINNING (1681843778.8203685): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6149, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6093, val loss 4.6066 [3.711151361465454 sec]
step 100: train loss 2.5416, val loss 2.6143 [10.027595281600952 sec]
step 200: train loss 2.4098, val loss 2.5039 [16.05775260925293 sec]
step 300: train loss 2.1802, val loss 2.3020 [22.010464906692505 sec]
step 400: train loss 1.9924, val loss 2.1617 [27.9393789768219 sec]
2.006284236907959
Total Training Time: 30.211267709732056 seconds

Chimwasendned we ent that. Gratta dome led he the warthe
cans the Com wer shis iseving froinfithe Pi
BEGINNING (1681843809.6265147): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6329, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6274, val loss 4.6302 [5.737295866012573 sec]
step 100: train loss 2.5132, val loss 2.5846 [15.331836462020874 sec]
step 200: train loss 2.3534, val loss 2.4516 [24.84799027442932 sec]
step 300: train loss 2.0788, val loss 2.2241 [34.51024675369263 sec]
step 400: train loss 1.8701, val loss 2.0531 [44.15404915809631 sec]
1.8077501058578491
Total Training Time: 48.05488395690918 seconds

y trect was in able arrirrir. So mae
the catervenced ies faict prish ye suyeturt, as art hav
twepons
BEGINNING (1681843858.8845885): Baseline LR(0.0006) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5543, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5374, val loss 4.5447 [7.5631937980651855 sec]
step 100: train loss 2.4899, val loss 2.5639 [20.428196668624878 sec]
step 200: train loss 2.3163, val loss 2.4191 [33.63676953315735 sec]
step 300: train loss 2.0017, val loss 2.1610 [47.21157169342041 sec]
step 400: train loss 1.7895, val loss 1.9952 [60.859753131866455 sec]
1.7182815074920654
Total Training Time: 66.47039985656738 seconds

ANSF."
A for didgekel then To as havinnice, the brans to me dand
with
not any sempene of I!" The Pyr
BEGINNING (1681843926.9267101): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5297, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5337, val loss 4.5420 [2.6950485706329346 sec]
step 100: train loss 2.4235, val loss 2.5065 [6.829092025756836 sec]
step 200: train loss 2.1496, val loss 2.2736 [11.486775636672974 sec]
step 300: train loss 1.9957, val loss 2.1650 [15.532184839248657 sec]
step 400: train loss 1.8915, val loss 2.0786 [19.73551058769226 sec]
1.8605682849884033
Total Training Time: 21.422399520874023 seconds

CHS
Veld the und at pay." Namalled hut an.
"And oked the tow tent segol thed. It shor hell, tumilir.
BEGINNING (1681843949.015816): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5995, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5891, val loss 4.5776 [3.698835611343384 sec]
step 100: train loss 2.4258, val loss 2.5109 [10.775610446929932 sec]
step 200: train loss 2.1066, val loss 2.2406 [18.419368267059326 sec]
step 300: train loss 1.9415, val loss 2.1371 [26.299884796142578 sec]
step 400: train loss 1.8399, val loss 2.0358 [34.20921540260315 sec]
1.8246864080429077
Total Training Time: 37.61711049079895 seconds

theeked the calina. All shie cKac sow afe that
folourns ive now his what mie the
torearly spoice his
BEGINNING (1681843987.9324636): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5562, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5592, val loss 4.5638 [5.9588706493377686 sec]
step 100: train loss 2.3927, val loss 2.4786 [16.752434968948364 sec]
step 200: train loss 2.0906, val loss 2.2303 [27.581076622009277 sec]
step 300: train loss 1.9139, val loss 2.1061 [38.3077826499939 sec]
step 400: train loss 1.8081, val loss 2.0074 [49.06524872779846 sec]
1.750884771347046
Total Training Time: 53.54633903503418 seconds

theing at hhe
move th mandies.
"Hal fek havereen, he deed his ave. "I weon Any milow, bat be if Grat
BEGINNING (1681844043.2851217): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6059, val loss 4.6157 [2.352299213409424 sec]
step 100: train loss 2.4968, val loss 2.5801 [6.768066644668579 sec]
step 200: train loss 2.2222, val loss 2.3376 [10.735198974609375 sec]
step 300: train loss 2.0210, val loss 2.1861 [14.593615055084229 sec]
step 400: train loss 1.9001, val loss 2.0730 [18.536443948745728 sec]
1.9548122882843018
Total Training Time: 20.088844537734985 seconds

He had them tuw calls Nt dithes ars, anay to he pard
jumaed Nabore plans.
Gramatta sme. Heichad, n s
BEGINNING (1681844063.9701006): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5420, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5380, val loss 4.5326 [3.837797164916992 sec]
step 100: train loss 2.4776, val loss 2.5615 [10.031911134719849 sec]
step 200: train loss 2.1649, val loss 2.2895 [16.753991842269897 sec]
step 300: train loss 1.9500, val loss 2.1293 [23.55595302581787 sec]
step 400: train loss 1.7990, val loss 2.0089 [30.21093487739563 sec]
1.7177702188491821
Total Training Time: 33.140074014663696 seconds

So naubt."
Goul neal theare this the I gothries up. "I youning fillen to
"I you, an yahe racess?ow a
BEGINNING (1681844098.268547): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5974, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6024, val loss 4.6001 [5.189054727554321 sec]
step 100: train loss 2.4654, val loss 2.5516 [14.956631898880005 sec]
step 200: train loss 2.1685, val loss 2.2892 [24.397010564804077 sec]
step 300: train loss 1.9265, val loss 2.1188 [33.88791012763977 sec]
step 400: train loss 1.7633, val loss 1.9735 [43.43283557891846 sec]
1.7504582405090332
Total Training Time: 47.65914249420166 seconds

43M%OEE;S."
PEAN, be smill mpely back one ck nolreage to stend
be dart, to as eapes witribling. If w
BEGINNING (1681844147.6975355): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5966, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5882, val loss 4.5829 [2.6717445850372314 sec]
step 100: train loss 2.5183, val loss 2.5959 [7.150588512420654 sec]
step 200: train loss 2.3759, val loss 2.4787 [11.688788652420044 sec]
step 300: train loss 2.1247, val loss 2.2669 [16.116453409194946 sec]
step 400: train loss 1.9702, val loss 2.1493 [20.70382809638977 sec]
1.8969274759292603
Total Training Time: 22.587032794952393 seconds

A hunad shad s son yests.?" Weas athe by
NaYes soy -int. Grath the the clenerace. He an iengnc
bed.

BEGINNING (1681844170.939634): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6837, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6826, val loss 4.6807 [4.603244781494141 sec]
step 100: train loss 2.5038, val loss 2.5801 [12.469395875930786 sec]
step 200: train loss 2.3756, val loss 2.4742 [20.1763699054718 sec]
step 300: train loss 2.1111, val loss 2.2517 [27.920554876327515 sec]
step 400: train loss 1.8951, val loss 2.0914 [35.723742961883545 sec]
1.831834077835083
Total Training Time: 39.07614040374756 seconds

and bouter chbe mand so. Nal a withemal turesta sain.
Gratta sirly te horka, nopos and
aftled. The c
BEGINNING (1681844211.177758): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6120, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6080, val loss 4.6140 [6.197413444519043 sec]
step 100: train loss 2.4870, val loss 2.5652 [16.89444875717163 sec]
step 200: train loss 2.3164, val loss 2.4173 [27.4654700756073 sec]
step 300: train loss 2.0108, val loss 2.1846 [37.728190898895264 sec]
step 400: train loss 1.8074, val loss 2.0311 [47.281511545181274 sec]
1.735183596611023
Total Training Time: 51.1452271938324 seconds

2AN quetidit." Gratta sowed."
Gened insted fent foden int in to Grom backerach at
Aid an, ander the 
BEGINNING (1681844263.6359181): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5655, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5667, val loss 4.5651 [1.9108467102050781 sec]
step 100: train loss 2.4007, val loss 2.4844 [5.1621317863464355 sec]
step 200: train loss 2.1037, val loss 2.2552 [8.338799238204956 sec]
step 300: train loss 1.9376, val loss 2.1195 [11.47885513305664 sec]
step 400: train loss 1.8420, val loss 2.0572 [14.632563352584839 sec]
1.8606332540512085
Total Training Time: 15.846882820129395 seconds

the stusbet toook."
Taaka siare your mand will, and he sund had
Clags neart be his
for ampelliand hi
BEGINNING (1681844279.928834): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6572, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6650, val loss 4.6636 [3.046220541000366 sec]
step 100: train loss 2.3836, val loss 2.4590 [8.178730249404907 sec]
step 200: train loss 2.0346, val loss 2.2004 [13.346572399139404 sec]
step 300: train loss 1.8564, val loss 2.0629 [18.520301818847656 sec]
step 400: train loss 1.7456, val loss 1.9619 [23.643787622451782 sec]
1.7368078231811523
Total Training Time: 25.899292469024658 seconds

heneon tushim. What in you are a clabounhout
thans, the cults for a note and dow of the but lant loo
BEGINNING (1681844306.7792494): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6937, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7031, val loss 4.7061 [4.387905836105347 sec]
step 100: train loss 2.3592, val loss 2.4531 [11.676166534423828 sec]
step 200: train loss 2.0136, val loss 2.1741 [18.87850332260132 sec]
step 300: train loss 1.8303, val loss 2.0081 [26.1277494430542 sec]
step 400: train loss 1.7036, val loss 1.9225 [33.31406545639038 sec]
1.7520661354064941
Total Training Time: 36.459797620773315 seconds

I thake pare by sound till feathathed, will intany his his faiter a guato compan
he clund ast boody 
BEGINNING (1681844344.477045): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6242, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6349, val loss 4.6423 [2.0588080883026123 sec]
step 100: train loss 2.4842, val loss 2.5557 [5.40488862991333 sec]
step 200: train loss 2.1405, val loss 2.2843 [8.723334312438965 sec]
step 300: train loss 1.9570, val loss 2.1331 [12.019102573394775 sec]
step 400: train loss 1.8374, val loss 2.0301 [15.36917519569397 sec]
1.8431510925292969
Total Training Time: 16.642526865005493 seconds

allight. It und pawas bothen aid at comppe samand to yre
he nrelted pass ared. Emare forey
sphard an
BEGINNING (1681844361.586844): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6255, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6073, val loss 4.6222 [3.264387369155884 sec]
step 100: train loss 2.4288, val loss 2.5055 [8.815691471099854 sec]
step 200: train loss 2.0724, val loss 2.2281 [14.69029712677002 sec]
step 300: train loss 1.8421, val loss 2.0369 [20.260637283325195 sec]
step 400: train loss 1.7114, val loss 1.9296 [26.559875965118408 sec]
1.6660975217819214
Total Training Time: 29.009469509124756 seconds

Gratta let had leary, aske yough Gratta's to the
Their surn0
7370
SEAMcKAY
• KAN McKAY
CLASALL McKAT
BEGINNING (1681844391.4698954): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5703, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5681, val loss 4.5655 [4.410457372665405 sec]
step 100: train loss 2.4329, val loss 2.5112 [12.152579069137573 sec]
step 200: train loss 2.0660, val loss 2.2136 [19.896224975585938 sec]
step 300: train loss 1.8240, val loss 2.0231 [27.61884903907776 sec]
step 400: train loss 1.6675, val loss 1.9060 [35.34363341331482 sec]
1.6909993886947632
Total Training Time: 38.642571210861206 seconds

Ekinaly postut to ther the had frourl fir the tescity
from compenatial queity." Anayah my iffell the
BEGINNING (1681844431.4671476): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6238, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6196, val loss 4.6239 [2.8551852703094482 sec]
step 100: train loss 2.5092, val loss 2.5822 [7.714221954345703 sec]
step 200: train loss 2.3612, val loss 2.4594 [12.5090651512146 sec]
step 300: train loss 2.0773, val loss 2.2224 [17.283082246780396 sec]
step 400: train loss 1.8949, val loss 2.0862 [22.053743600845337 sec]
1.8345204591751099
Total Training Time: 23.950019359588623 seconds

on smeat. "Wuched the whys them gribor and him." The
loweriker, their werigh tir, fat frour args the
BEGINNING (1681844455.858168): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6768, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6770, val loss 4.6710 [4.858135223388672 sec]
step 100: train loss 2.4840, val loss 2.5603 [13.165516376495361 sec]
step 200: train loss 2.2817, val loss 2.3885 [21.466082334518433 sec]
step 300: train loss 1.9710, val loss 2.1486 [29.77645444869995 sec]
step 400: train loss 1.7807, val loss 2.0038 [38.083534955978394 sec]
1.7575182914733887
Total Training Time: 41.55134439468384 seconds

The rack theeif stood ban Tuon gain is I now, Not Chief
Ar can, weltered, hief yourna/ as and lis fo
BEGINNING (1681844498.2900121): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6861, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6963, val loss 4.6968 [6.879714727401733 sec]
step 100: train loss 2.4743, val loss 2.5557 [18.74782419204712 sec]
step 200: train loss 2.2702, val loss 2.3807 [30.548260927200317 sec]
step 300: train loss 1.9311, val loss 2.1057 [43.34509348869324 sec]
step 400: train loss 1.7216, val loss 1.9607 [58.960659980773926 sec]
1.6843746900558472
Total Training Time: 64.72771072387695 seconds

anded, leee but in hiedh afend Sour
agazing qup the humans.
Ardsed rand as mors the forte.
"Srome up
BEGINNING (1681844565.3563213): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6264, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6202, val loss 4.6191 [3.563709259033203 sec]
step 100: train loss 2.3625, val loss 2.4470 [8.86072039604187 sec]
step 200: train loss 2.0662, val loss 2.2279 [13.408788204193115 sec]
step 300: train loss 1.9036, val loss 2.0907 [17.933589220046997 sec]
step 400: train loss 1.7962, val loss 2.0019 [22.00419330596924 sec]
1.714148759841919
Total Training Time: 23.592642545700073 seconds

beed sa do's any him hand aspopout befacterhy in a
voleaty the sminerniuted a
daree, scabs wait the 
BEGINNING (1681844589.5589674): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6713, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6585, val loss 4.6589 [4.522324562072754 sec]
step 100: train loss 2.3232, val loss 2.4121 [12.221882343292236 sec]
step 200: train loss 1.9895, val loss 2.1476 [19.902365684509277 sec]
step 300: train loss 1.8101, val loss 2.0183 [26.820074558258057 sec]
step 400: train loss 1.6975, val loss 1.9305 [33.42126655578613 sec]
1.5796797275543213
Total Training Time: 36.210946798324585 seconds

"Soing that for fiblany tritake. Canced and solding
two not groundded Ga solvor to from as would the
BEGINNING (1681844626.8589103): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5663, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5600, val loss 4.5719 [5.792025089263916 sec]
step 100: train loss 2.3150, val loss 2.4206 [17.46097755432129 sec]
step 200: train loss 1.9604, val loss 2.1289 [29.918986082077026 sec]
step 300: train loss 1.7779, val loss 1.9804 [42.31232523918152 sec]
step 400: train loss 1.6664, val loss 1.9016 [53.625159740448 sec]
1.709262728691101
Total Training Time: 57.60591959953308 seconds

nest her of alon for om his. His who Rluing of that him stozet of the the mas smell.
Bears,
wit osh 
BEGINNING (1681844685.8412545): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5704, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5794, val loss 4.5801 [3.0775959491729736 sec]
step 100: train loss 2.4409, val loss 2.5260 [7.18918251991272 sec]
step 200: train loss 2.0899, val loss 2.2375 [11.077325820922852 sec]
step 300: train loss 1.8972, val loss 2.0861 [15.18932819366455 sec]
step 400: train loss 1.7665, val loss 1.9892 [19.5514976978302 sec]
1.7155141830444336
Total Training Time: 21.134145736694336 seconds

trenter tearionsiffor. Hise uped to mist iff Toriyah
beire
joutteresterstoung fing follegive b9
Pele
BEGINNING (1681844707.4489512): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6373, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6416, val loss 4.6357 [3.9597725868225098 sec]
step 100: train loss 2.4000, val loss 2.4896 [10.550816059112549 sec]
step 200: train loss 2.0259, val loss 2.1910 [17.195701122283936 sec]
step 300: train loss 1.8066, val loss 2.0116 [23.610389709472656 sec]
step 400: train loss 1.6607, val loss 1.8981 [30.063154220581055 sec]
1.698898434638977
Total Training Time: 32.726402044296265 seconds

6"
SEAN. Theas eyes ha“ frill,
sommany the sones helirs Arayah the faif thelp
tors baths befoll ind 
BEGINNING (1681844741.0294945): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6250, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6150, val loss 4.6111 [5.165331602096558 sec]
step 100: train loss 2.3973, val loss 2.4921 [14.328541994094849 sec]
step 200: train loss 1.9854, val loss 2.1496 [24.3118793964386 sec]
step 300: train loss 1.7524, val loss 1.9695 [33.47001528739929 sec]
step 400: train loss 1.6072, val loss 1.8704 [42.39209032058716 sec]
1.6533524990081787
Total Training Time: 46.45985174179077 seconds

morthey under oner. "My your is Why illane?
Buon fam a light tuon he rest, what them after to
feasur
BEGINNING (1681844788.7743244): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5735, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5776, val loss 4.5796 [3.499319314956665 sec]
step 100: train loss 2.5006, val loss 2.5814 [9.321701526641846 sec]
step 200: train loss 2.3240, val loss 2.4307 [15.173492193222046 sec]
step 300: train loss 2.0271, val loss 2.1840 [20.990890741348267 sec]
step 400: train loss 1.8513, val loss 2.0506 [26.81333327293396 sec]
1.8167001008987427
Total Training Time: 29.15164279937744 seconds

Mymes stail and ablinh smillie." I was there from lood the
"Grall Aiddescolled simed and. Tharound.

BEGINNING (1681844818.3825603): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6002, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5911, val loss 4.5951 [5.911109685897827 sec]
step 100: train loss 2.4728, val loss 2.5575 [16.074598789215088 sec]
step 200: train loss 2.2501, val loss 2.3631 [26.259814023971558 sec]
step 300: train loss 1.9335, val loss 2.1145 [36.39420747756958 sec]
step 400: train loss 1.7341, val loss 1.9672 [46.54863452911377 sec]
1.702471375465393
Total Training Time: 50.806976556777954 seconds

Arthe snight thes'st west samen ent.
General wou relliefored the was nexon yes aving Prhana
peot has
BEGINNING (1681844870.0705738): Baseline LR(0.0006) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6021, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6000, val loss 4.6126 [8.426787614822388 sec]
step 100: train loss 2.4685, val loss 2.5501 [23.530235528945923 sec]
step 200: train loss 2.1928, val loss 2.3110 [38.670432806015015 sec]
step 300: train loss 1.8629, val loss 2.0553 [53.76666593551636 sec]
step 400: train loss 1.6599, val loss 1.9203 [68.86607122421265 sec]
1.6056791543960571
Total Training Time: 75.28528928756714 seconds

they to ta ran thesthrought in to care, "The many it of
dsaysecernotered thelp youred."
“Anart, will
BEGINNING (1681844946.621963): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6146, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6231, val loss 4.6175 [2.195091485977173 sec]
step 100: train loss 2.3520, val loss 2.4393 [5.944828510284424 sec]
step 200: train loss 2.0569, val loss 2.2213 [9.68291711807251 sec]
step 300: train loss 1.9209, val loss 2.0939 [13.392315864562988 sec]
step 400: train loss 1.8277, val loss 2.0480 [17.13953185081482 sec]
1.8676018714904785
Total Training Time: 18.67746090888977 seconds

dermaeys.L for ood wortath him the Pyring as didsing saddews rouidessling the cubs,
and ationned.. I
BEGINNING (1681844965.9624252): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6407, val loss 4.6484 [4.00523042678833 sec]
step 100: train loss 2.3258, val loss 2.4243 [10.685664892196655 sec]
step 200: train loss 2.0218, val loss 2.1929 [17.303269147872925 sec]
step 300: train loss 1.8675, val loss 2.0596 [23.934282541275024 sec]
step 400: train loss 1.7376, val loss 1.9716 [30.57754921913147 sec]
1.6835538148880005
Total Training Time: 33.43128943443298 seconds

of the guard sling his raE Thas will you
the warrie" carse. If the criords ah raeby
ound the Tne tuo
BEGINNING (1681845000.6051977): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5966, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6183, val loss 4.6310 [5.277590990066528 sec]
step 100: train loss 2.3337, val loss 2.4335 [14.949662446975708 sec]
step 200: train loss 2.0230, val loss 2.1950 [24.429048538208008 sec]
step 300: train loss 1.8540, val loss 2.0595 [34.34391689300537 sec]
step 400: train loss 1.7445, val loss 1.9780 [44.00944399833679 sec]
1.7939934730529785
Total Training Time: 48.28667092323303 seconds

CEAN OF McKAY
A551ED. The worihe a from Live king of chis. He dares, of the on grow non
slang." "A s
BEGINNING (1681845050.6635385): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5960, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6024, val loss 4.5864 [2.3550631999969482 sec]
step 100: train loss 2.4237, val loss 2.5223 [6.27616024017334 sec]
step 200: train loss 2.0677, val loss 2.2194 [10.19606328010559 sec]
step 300: train loss 1.8847, val loss 2.0804 [14.170852899551392 sec]
step 400: train loss 1.7825, val loss 2.0012 [18.087356090545654 sec]
1.6829555034637451
Total Training Time: 19.7038676738739 seconds

gaiew. I joy wormight, be foods wans werert at Tuont Cort said, It lang they mome
Thriess
riere the 
BEGINNING (1681845070.9836473): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6190, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6201, val loss 4.6268 [4.015523433685303 sec]
step 100: train loss 2.3984, val loss 2.4893 [10.962057828903198 sec]
step 200: train loss 2.0316, val loss 2.2018 [17.96336793899536 sec]
step 300: train loss 1.8151, val loss 2.0382 [25.00269055366516 sec]
step 400: train loss 1.6815, val loss 1.9447 [32.28830528259277 sec]
1.620497226715088
Total Training Time: 35.28147339820862 seconds

nat the of the them. Gratta a leftly2hes with ne the Ne Arphad
crow wain
you. We wildins, and quir f
BEGINNING (1681845107.4372182): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5764, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5780, val loss 4.5762 [5.692034006118774 sec]
step 100: train loss 2.4190, val loss 2.5025 [15.746756076812744 sec]
step 200: train loss 2.0196, val loss 2.1799 [25.817180633544922 sec]
step 300: train loss 1.8101, val loss 2.0276 [35.82830548286438 sec]
step 400: train loss 1.6542, val loss 1.9191 [45.83867621421814 sec]
1.5613112449645996
Total Training Time: 50.28531813621521 seconds

sking.."
"We the he wight aboughed they humanss, the seen ace
shroff, werpw swidgon saps. Gratta as 
BEGINNING (1681845159.5232868): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6042, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6170, val loss 4.6086 [3.169969081878662 sec]
step 100: train loss 2.4774, val loss 2.5533 [8.725983381271362 sec]
step 200: train loss 2.2704, val loss 2.3977 [14.211257457733154 sec]
step 300: train loss 1.9630, val loss 2.1448 [19.636974334716797 sec]
step 400: train loss 1.8041, val loss 2.0206 [25.0829975605011 sec]
1.7903316020965576
Total Training Time: 27.357997179031372 seconds

Eve beargace; maysay in,
"Slow You in cant me omer fornicith and dond yoursed him.
"Mon tom buzit. N
BEGINNING (1681845187.4899561): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6991, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6988, val loss 4.7024 [5.613828420639038 sec]
step 100: train loss 2.4663, val loss 2.5484 [15.580701351165771 sec]
step 200: train loss 2.1971, val loss 2.3229 [25.49071955680847 sec]
step 300: train loss 1.8940, val loss 2.0803 [36.33899450302124 sec]
step 400: train loss 1.6921, val loss 1.9318 [47.163859128952026 sec]
1.6257306337356567
Total Training Time: 51.734778881073 seconds

HE KAYMk loook Ta's If that – have pasoong of thrun you
ences uplow, the ge whare and of cil and dri
BEGINNING (1681845240.4694562): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5982, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5903, val loss 4.6056 [8.72391128540039 sec]
step 100: train loss 2.4797, val loss 2.5628 [24.03547239303589 sec]
step 200: train loss 2.2440, val loss 2.3567 [39.912137031555176 sec]
step 300: train loss 1.8859, val loss 2.0950 [55.116214990615845 sec]
step 400: train loss 1.6649, val loss 1.9365 [69.9988784790039 sec]
1.618118166923523
Total Training Time: 76.4512197971344 seconds

Gratta chented sinded thin. "The wivere
repanery maefore witior trace ot's al you dozed trelf
as, ab
BEGINNING (1681845319.0486462): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5849, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5756, val loss 4.5776 [2.5556013584136963 sec]
step 100: train loss 2.2594, val loss 2.3829 [6.704955101013184 sec]
step 200: train loss 1.9916, val loss 2.1619 [10.826030492782593 sec]
step 300: train loss 1.8349, val loss 2.0277 [15.13092827796936 sec]
step 400: train loss 1.7420, val loss 1.9681 [19.860267877578735 sec]
1.656628966331482
Total Training Time: 21.622228145599365 seconds

 his cre plant
the fraby to the isentched tuon teantered
what woure. I haveirn it lens they the Tori
BEGINNING (1681845341.4271636): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6253, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6190, val loss 4.6265 [4.251048564910889 sec]
step 100: train loss 2.2250, val loss 2.3328 [11.44640588760376 sec]
step 200: train loss 1.9334, val loss 2.1113 [18.757667064666748 sec]
step 300: train loss 1.7708, val loss 1.9872 [26.490181922912598 sec]
step 400: train loss 1.6622, val loss 1.9095 [33.74389147758484 sec]
1.599305272102356
Total Training Time: 37.201507806777954 seconds

Irmall weas in feaw a ertuing to gralling
bac;.rowereded aronced we creda wart ton entowmised. Zecha
BEGINNING (1681845379.9630415): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6106, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5944, val loss 4.5905 [5.449320077896118 sec]
step 100: train loss 2.2562, val loss 2.3745 [16.226836442947388 sec]
step 200: train loss 1.9230, val loss 2.1068 [28.144170999526978 sec]
step 300: train loss 1.7524, val loss 1.9592 [39.82109785079956 sec]
step 400: train loss 1.6440, val loss 1.9111 [51.62931299209595 sec]
1.7386623620986938
Total Training Time: 56.572566509246826 seconds

mome his was forther some, ~2, and sugging. I with gave have your fared that
him."
Gratta chened tak
BEGINNING (1681845438.5192401): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5862, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5820, val loss 4.5836 [2.9740982055664062 sec]
step 100: train loss 2.3755, val loss 2.4750 [8.174914598464966 sec]
step 200: train loss 2.0070, val loss 2.1704 [13.257534503936768 sec]
step 300: train loss 1.8042, val loss 2.0078 [18.676485538482666 sec]
step 400: train loss 1.6731, val loss 1.9309 [24.433149337768555 sec]
1.7792457342147827
Total Training Time: 26.757830142974854 seconds

pawlind liging a straiverled "I as weter the sarelans. Thls had to gew said
"Ded smen beguld will an
BEGINNING (1681845465.9771974): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6373, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6287, val loss 4.6301 [5.0746448040008545 sec]
step 100: train loss 2.3695, val loss 2.4705 [13.900686979293823 sec]
step 200: train loss 1.9406, val loss 2.1265 [22.883557081222534 sec]
step 300: train loss 1.7142, val loss 1.9377 [32.632766008377075 sec]
step 400: train loss 1.5845, val loss 1.8514 [41.33619046211243 sec]
1.5879238843917847
Total Training Time: 44.87491297721863 seconds

more back, will from be be faing dast was
Anayah's. Gor catta dise."
Gratta whidded brood and gated 
BEGINNING (1681845512.0793922): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5524, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5379, val loss 4.5417 [6.693362474441528 sec]
step 100: train loss 2.3938, val loss 2.4882 [19.053658723831177 sec]
step 200: train loss 1.9616, val loss 2.1453 [31.439790725708008 sec]
step 300: train loss 1.7208, val loss 1.9547 [43.4143385887146 sec]
step 400: train loss 1.5716, val loss 1.8613 [55.197290658950806 sec]
1.5778366327285767
Total Training Time: 60.260749101638794 seconds

grate to Ais him affe to the Pyrran Ar
%le and glies, and he some gressid dazzing outing the cubs ba
BEGINNING (1681845574.1651032): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6203, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6224, val loss 4.6223 [4.473459243774414 sec]
step 100: train loss 2.4622, val loss 2.5542 [12.009627342224121 sec]
step 200: train loss 2.1976, val loss 2.3334 [19.52396011352539 sec]
step 300: train loss 1.9076, val loss 2.0999 [27.203850507736206 sec]
step 400: train loss 1.7325, val loss 1.9698 [34.70033550262451 sec]
1.677626609802246
Total Training Time: 37.819581031799316 seconds

theng. Aopon buard ance comperwas buded. The in
offe Thats a smil. Kan maeuw waske fure your. Gratta
BEGINNING (1681845612.6716955): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6289, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6312, val loss 4.6355 [7.800374507904053 sec]
step 100: train loss 2.4495, val loss 2.5333 [21.323327779769897 sec]
step 200: train loss 2.1172, val loss 2.2664 [34.87305998802185 sec]
step 300: train loss 1.7814, val loss 2.0099 [48.68741059303284 sec]
step 400: train loss 1.6027, val loss 1.8807 [63.2680082321167 sec]
1.5932897329330444
Total Training Time: 69.38141918182373 seconds

Pri spong the the smoon alloon. They maue soups
eemalie of the purple of mange ablie rews. The
metur
BEGINNING (1681845683.5244455): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5906, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5940, val loss 4.5950 [11.585362195968628 sec]
step 100: train loss 2.4556, val loss 2.5418 [33.08146691322327 sec]
step 200: train loss 2.1479, val loss 2.2799 [53.09586143493652 sec]
step 300: train loss 1.7704, val loss 1.9891 [73.51913070678711 sec]
step 400: train loss 1.5641, val loss 1.8495 [93.84472489356995 sec]
1.5415029525756836
Total Training Time: 102.66799473762512 seconds

his has was warriors spead has smilath intedgs to
did to this athe rem ormoragd. He firmsed my fine

BEGINNING (1681845788.1303926): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6205, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6391, val loss 4.6377 [2.7679975032806396 sec]
step 100: train loss 2.2277, val loss 2.3446 [7.5098559856414795 sec]
step 200: train loss 1.9319, val loss 2.1152 [12.396131038665771 sec]
step 300: train loss 1.7932, val loss 2.0073 [17.480087995529175 sec]
step 400: train loss 1.6908, val loss 1.9381 [22.394940614700317 sec]
1.6631536483764648
Total Training Time: 24.31005048751831 seconds

rome bofind!" General neptopling. "Tak he THE TER VI – V
PER ING this look
Anayah splee," for he end
BEGINNING (1681845813.0914788): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5715, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5919, val loss 4.5960 [4.66284704208374 sec]
step 100: train loss 2.1963, val loss 2.3286 [12.266045570373535 sec]
step 200: train loss 1.8654, val loss 2.0571 [20.176619052886963 sec]
step 300: train loss 1.7069, val loss 1.9404 [27.94144082069397 sec]
step 400: train loss 1.5993, val loss 1.8694 [35.26900029182434 sec]
1.6986368894577026
Total Training Time: 38.225595235824585 seconds

enerned and Gera! I way a disked
to the tuon warre to the cad you, broughtt quicklus. The eyes
waill
BEGINNING (1681845852.5340838): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6098, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6226, val loss 4.6291 [5.90374493598938 sec]
step 100: train loss 2.2340, val loss 2.3563 [16.45876431465149 sec]
step 200: train loss 1.8818, val loss 2.0923 [27.272862195968628 sec]
step 300: train loss 1.7003, val loss 1.9464 [37.84307527542114 sec]
step 400: train loss 1.5849, val loss 1.8629 [49.05424928665161 sec]
1.5569766759872437
Total Training Time: 53.57826805114746 seconds

Pack GAPTER ISPEAN JEEKING PEATE –I – SEAN
TEAN
MEAN MERAGE CHAPTEATE
GRAFDEIOL THE OF TEATEozing Go
BEGINNING (1681845907.9012332): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6460, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6547, val loss 4.6529 [3.4388957023620605 sec]
step 100: train loss 2.3236, val loss 2.4207 [9.180317401885986 sec]
step 200: train loss 1.9569, val loss 2.1412 [15.121864557266235 sec]
step 300: train loss 1.7687, val loss 2.0061 [20.91324234008789 sec]
step 400: train loss 1.6359, val loss 1.8967 [26.737908363342285 sec]
1.610572338104248
Total Training Time: 29.047275066375732 seconds

Hrook is the tuon he centand thos the cave,
"Whes strough scomp-ear wass a patess he thad
frothe you
BEGINNING (1681845937.5845668): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6794, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6789, val loss 4.6688 [5.718602418899536 sec]
step 100: train loss 2.3272, val loss 2.4319 [15.684243202209473 sec]
step 200: train loss 1.9064, val loss 2.1021 [25.881276607513428 sec]
step 300: train loss 1.6845, val loss 1.9283 [36.248716831207275 sec]
step 400: train loss 1.5485, val loss 1.8423 [46.13241744041443 sec]
1.5325052738189697
Total Training Time: 50.38777256011963 seconds

EAVN )UBI chieuw your earge, he tuons wisch take liking
attion bow! We
ccompart, ched his untraved a
BEGINNING (1681845989.2624462): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5443, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5404, val loss 4.5474 [8.033782720565796 sec]
step 100: train loss 2.3543, val loss 2.4537 [22.033065795898438 sec]
step 200: train loss 1.8976, val loss 2.0889 [36.30197596549988 sec]
step 300: train loss 1.6564, val loss 1.9089 [50.73074769973755 sec]
step 400: train loss 1.5032, val loss 1.8179 [65.32477736473083 sec]
1.47506582736969
Total Training Time: 71.7783932685852 seconds

midled folvingeor and Gratta. "I will!" I e cleture to
they raid shond appeated them borded bignt th
BEGINNING (1681846063.180281): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6930, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6875, val loss 4.6810 [5.519848823547363 sec]
step 100: train loss 2.4413, val loss 2.5295 [15.363715410232544 sec]
step 200: train loss 2.1123, val loss 2.2681 [25.18719744682312 sec]
step 300: train loss 1.8254, val loss 2.0354 [34.9709153175354 sec]
step 400: train loss 1.6480, val loss 1.9112 [44.953322410583496 sec]
1.6274573802947998
Total Training Time: 48.9779007434845 seconds

nor ar jumile. "Yess." The smanta loooking hid of
that trounds don the Triefore part. Thas foug fil 
BEGINNING (1681846112.8816926): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6094, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6099, val loss 4.6101 [10.20827341079712 sec]
step 100: train loss 2.4416, val loss 2.5324 [28.042176961898804 sec]
step 200: train loss 2.0536, val loss 2.2115 [45.426759243011475 sec]
step 300: train loss 1.7384, val loss 1.9769 [61.7312171459198 sec]
step 400: train loss 1.5364, val loss 1.8394 [80.02454710006714 sec]
1.5018301010131836
Total Training Time: 87.7439775466919 seconds

have feemans ther!" Ting bared to his shoNm foure
him maeuw suppeded sho con and the mace. The the c
BEGINNING (1681846201.9298284): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6492, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6494, val loss 4.6464 [14.045045375823975 sec]
step 100: train loss 2.4562, val loss 2.5422 [39.5670211315155 sec]
step 200: train loss 2.0999, val loss 2.2402 [63.533565282821655 sec]
step 300: train loss 1.7387, val loss 1.9659 [88.47161650657654 sec]
step 400: train loss 1.5101, val loss 1.8286 [113.62318062782288 sec]
1.4802707433700562
Total Training Time: 124.2701199054718 seconds

Zecby racked of the tuons warriant ace trioss we arroor,
and hoods eving straction them wabe?"
Chape
BEGINNING (1681846328.3195121): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6124, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6063, val loss 4.6048 [3.001716375350952 sec]
step 100: train loss 2.2491, val loss 2.3613 [7.935568332672119 sec]
step 200: train loss 1.9908, val loss 2.1463 [12.826404809951782 sec]
step 300: train loss 1.8652, val loss 2.0802 [17.86669898033142 sec]
step 400: train loss 1.7521, val loss 1.9992 [22.9733464717865 sec]
1.7604725360870361
Total Training Time: 25.419102430343628 seconds

ERANS Rpick stoven. I
was decards a."
Taka lepaking tuand wed to to
moreter coul prust fill, the wac
BEGINNING (1681846354.7235174): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5771, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5760, val loss 4.5717 [5.288280963897705 sec]
step 100: train loss 2.2672, val loss 2.3719 [13.871844291687012 sec]
step 200: train loss 1.9750, val loss 2.1614 [22.356132984161377 sec]
step 300: train loss 1.8161, val loss 2.0320 [30.955125093460083 sec]
step 400: train loss 1.7130, val loss 1.9604 [39.491271018981934 sec]
1.5819329023361206
Total Training Time: 43.2512526512146 seconds

pecthrops. I his had, foonother againjummes, but a stold anroughty criquicked, and the Pring. Gratta
BEGINNING (1681846399.5163617): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6081, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6039, val loss 4.5986 [6.704629421234131 sec]
step 100: train loss 2.3576, val loss 2.4508 [19.210820198059082 sec]
step 200: train loss 2.0284, val loss 2.1960 [31.790988206863403 sec]
step 300: train loss 1.8704, val loss 2.0609 [44.59062361717224 sec]
step 400: train loss 1.7359, val loss 1.9644 [58.49074459075928 sec]
1.7492115497589111
Total Training Time: 64.8309497833252 seconds

His rompling for moldded before in top mol of The saeuw to deasty
hin wan uster himpled a magation
b
BEGINNING (1681846466.9501972): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6390, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6450, val loss 4.6540 [3.113400936126709 sec]
step 100: train loss 2.3545, val loss 2.4473 [8.959017038345337 sec]
step 200: train loss 1.9979, val loss 2.1731 [15.366471529006958 sec]
step 300: train loss 1.8041, val loss 2.0320 [21.73793888092041 sec]
step 400: train loss 1.6659, val loss 1.9206 [28.095778465270996 sec]
1.5947988033294678
Total Training Time: 30.80480670928955 seconds

Chief."
• Arpay a small sueg. But blearly wher.'O WE capped a saons and up and the humon
weache. Pel
BEGINNING (1681846498.676776): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5882, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6003, val loss 4.5970 [6.296282768249512 sec]
step 100: train loss 2.3892, val loss 2.4880 [17.122037649154663 sec]
step 200: train loss 1.9911, val loss 2.1650 [28.173287391662598 sec]
step 300: train loss 1.7749, val loss 1.9970 [38.80756330490112 sec]
step 400: train loss 1.6336, val loss 1.9021 [49.90363073348999 sec]
1.6132179498672485
Total Training Time: 54.84800744056702 seconds

thand combat was. The was guarded to befolese a nointicned
sing.
"I have They a pang Y€u lerow Yahwe
BEGINNING (1681846555.2345335): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5780, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5701, val loss 4.5698 [9.127918004989624 sec]
step 100: train loss 2.4498, val loss 2.5408 [24.281436443328857 sec]
step 200: train loss 2.0675, val loss 2.2319 [39.49456763267517 sec]
step 300: train loss 1.8338, val loss 2.0521 [54.953781843185425 sec]
step 400: train loss 1.6603, val loss 1.9085 [70.978839635849 sec]
1.6607904434204102
Total Training Time: 78.10433149337769 seconds

lidge Arpreaty." Namal the head caty at red nown
this cry inginnck uppratt!" Grattatta" Natta's salG
BEGINNING (1681846635.6483378): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6568, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6524, val loss 4.6477 [4.480685472488403 sec]
step 100: train loss 2.4365, val loss 2.5260 [12.285548686981201 sec]
step 200: train loss 2.1213, val loss 2.2732 [20.101598739624023 sec]
step 300: train loss 1.8547, val loss 2.0661 [28.13498568534851 sec]
step 400: train loss 1.6845, val loss 1.9297 [36.22571396827698 sec]
1.6269644498825073
Total Training Time: 39.730995893478394 seconds

but hEvem. HE is woomet." Anu not my tred
that the smile liked. Grattled Gratta turned to asolly.
"G
BEGINNING (1681846676.1790245): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5447, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5505, val loss 4.5530 [8.82746958732605 sec]
step 100: train loss 2.4613, val loss 2.5511 [25.985496759414673 sec]
step 200: train loss 2.1684, val loss 2.3152 [42.70472502708435 sec]
step 300: train loss 1.8267, val loss 2.0445 [59.58409905433655 sec]
step 400: train loss 1.6303, val loss 1.9162 [76.65851020812988 sec]
1.6190276145935059
Total Training Time: 84.02495455741882 seconds

and yeleancy. Nower the clabried of of the laust."
Gratta salook apped and as cust him sice manisir 
BEGINNING (1681846762.2032704): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6586, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6644, val loss 4.6644 [13.09328579902649 sec]
step 100: train loss 2.4753, val loss 2.5635 [37.66358160972595 sec]
step 200: train loss 2.2425, val loss 2.3713 [61.432639598846436 sec]
step 300: train loss 1.8699, val loss 2.0821 [83.91775178909302 sec]
step 400: train loss 1.6239, val loss 1.9008 [106.3303747177124 sec]
1.553715705871582
Total Training Time: 117.03461456298828 seconds

he tuon warking of ar cers of the humans, was if mil.
He a smell campent that grat of was unater us 
BEGINNING (1681846881.807902): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6779, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6597, val loss 4.6526 [3.7312049865722656 sec]
step 100: train loss 2.1913, val loss 2.3318 [9.528158187866211 sec]
step 200: train loss 1.9337, val loss 2.1359 [15.487802982330322 sec]
step 300: train loss 1.7710, val loss 2.0040 [21.545955181121826 sec]
step 400: train loss 1.6783, val loss 1.9406 [27.65727424621582 sec]
1.648110270500183
Total Training Time: 30.39087963104248 seconds

IA NEW CE:HESPERATE ACBATE CHAPTER S– A DGUd by the one alsire to to
Coubly be to to thate's.
HeG Pe
BEGINNING (1681846913.0877783): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6025, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6186, val loss 4.6152 [5.780858993530273 sec]
step 100: train loss 2.2090, val loss 2.3427 [15.663347959518433 sec]
step 200: train loss 1.9165, val loss 2.1188 [25.734299182891846 sec]
step 300: train loss 1.7471, val loss 1.9743 [36.228904247283936 sec]
step 400: train loss 1.6356, val loss 1.9001 [45.505494832992554 sec]
1.6564332246780396
Total Training Time: 49.692492961883545 seconds

Yes our sturnied me underrown his c gattion of sinked!'"
Is nord took thim Gratta like to do.
"It im
BEGINNING (1681846964.4045107): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6016, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6026, val loss 4.5969 [8.257413625717163 sec]
step 100: train loss 2.2825, val loss 2.3900 [22.16089391708374 sec]
step 200: train loss 1.9289, val loss 2.1225 [35.193010568618774 sec]
step 300: train loss 1.7449, val loss 1.9920 [48.31865477561951 sec]
step 400: train loss 1.6347, val loss 1.9064 [61.60752630233765 sec]
1.60390043258667
Total Training Time: 67.41220736503601 seconds

Chiefor I Yah Elyon Kiven Arphad not
cobEc. His he firne the
he ~illy but and at them gater a
CHAPTE
BEGINNING (1681847034.1737857): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5787, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5870, val loss 4.5790 [3.7669577598571777 sec]
step 100: train loss 2.2891, val loss 2.3980 [10.247977018356323 sec]
step 200: train loss 1.9150, val loss 2.1135 [16.978737592697144 sec]
step 300: train loss 1.7087, val loss 1.9525 [23.55717182159424 sec]
step 400: train loss 1.5901, val loss 1.8771 [30.149359464645386 sec]
1.62349271774292
Total Training Time: 32.86713147163391 seconds

dis to hid prowls. We humans the fillieft tuons. I the mup Ely
for 1 Roive thems,
gation he tuons of
BEGINNING (1681847067.823918): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6039, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6080, val loss 4.5962 [6.687689304351807 sec]
step 100: train loss 2.3471, val loss 2.4549 [18.571514129638672 sec]
step 200: train loss 1.8855, val loss 2.0703 [30.62510895729065 sec]
step 300: train loss 1.6687, val loss 1.8982 [42.77607011795044 sec]
step 400: train loss 1.5348, val loss 1.8385 [54.943127393722534 sec]
1.4816855192184448
Total Training Time: 60.2370924949646 seconds

tment, Taka has look at to see.
Anayah al arui?"
"My maste!" Chief Arphad abow will snee go berent. 
BEGINNING (1681847129.649496): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6591, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6543, val loss 4.6444 [9.80472207069397 sec]
step 100: train loss 2.4100, val loss 2.5034 [27.513476848602295 sec]
step 200: train loss 1.9451, val loss 2.1206 [45.16522026062012 sec]
step 300: train loss 1.7123, val loss 1.9558 [62.799129009246826 sec]
step 400: train loss 1.5373, val loss 1.8393 [80.4551465511322 sec]
1.4293190240859985
Total Training Time: 88.27894234657288 seconds

turn't humans again, looked, to hey have noter oned the
are. "Fortherss were attent Elight, des tist
BEGINNING (1681847220.3404782): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6228, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6223, val loss 4.6199 [6.100757360458374 sec]
step 100: train loss 2.4166, val loss 2.5148 [16.96657085418701 sec]
step 200: train loss 2.0323, val loss 2.1989 [27.989842414855957 sec]
step 300: train loss 1.7717, val loss 2.0007 [38.87475061416626 sec]
step 400: train loss 1.5993, val loss 1.8754 [50.9412887096405 sec]
1.5870246887207031
Total Training Time: 55.97734975814819 seconds

"You Mays, mome follow nany be they to people a roples. Muzities only bridge of which
youars besen o
BEGINNING (1681847277.2538793): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5957, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5960, val loss 4.5777 [11.945594549179077 sec]
step 100: train loss 2.4406, val loss 2.5271 [34.072675466537476 sec]
step 200: train loss 2.0569, val loss 2.2205 [56.03911256790161 sec]
step 300: train loss 1.7368, val loss 1.9738 [76.8683717250824 sec]
step 400: train loss 1.5364, val loss 1.8320 [97.06738495826721 sec]
1.4525989294052124
Total Training Time: 105.88827633857727 seconds

as repain deck ble fect his forwas
geed and said to bity finds, their back. When huch was
was had hi
BEGINNING (1681847384.8642175): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5673, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5546, val loss 4.5625 [17.567829847335815 sec]
step 100: train loss 2.4745, val loss 2.5549 [46.59581732749939 sec]
step 200: train loss 2.1697, val loss 2.2978 [75.98774528503418 sec]
step 300: train loss 1.7822, val loss 2.0166 [105.15190196037292 sec]
step 400: train loss 1.5413, val loss 1.8573 [134.13162779808044 sec]
1.497671127319336
Total Training Time: 147.03484511375427 seconds

66
they tunarly to up I thond?"
"What! And sir! into thim?"
"No," them it if reasing Anayah think th
BEGINNING (1681847534.5452578): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.7373, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7398, val loss 4.7339 [3.592242479324341 sec]
step 100: train loss 2.1391, val loss 2.2813 [9.838767290115356 sec]
step 200: train loss 1.8711, val loss 2.0666 [15.947932958602905 sec]
step 300: train loss 1.7162, val loss 1.9517 [21.907947063446045 sec]
step 400: train loss 1.6336, val loss 1.9116 [27.92224144935608 sec]
1.5888570547103882
Total Training Time: 30.364397525787354 seconds

a care lrosed, efur bovery askmer. The
now. I ugat through.
"These party is gimatick now sow thround
BEGINNING (1681847565.7323632): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5222, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5090, val loss 4.5016 [5.696688652038574 sec]
step 100: train loss 2.1762, val loss 2.3068 [16.03512740135193 sec]
step 200: train loss 1.8483, val loss 2.0605 [26.70471215248108 sec]
step 300: train loss 1.6814, val loss 1.9493 [37.12251830101013 sec]
step 400: train loss 1.5772, val loss 1.8750 [48.30326223373413 sec]
1.507091760635376
Total Training Time: 52.94361686706543 seconds

when. I woll, staps attaking Kinayah had who sold his.
The trubess anot Gratta wall that
of their wo
BEGINNING (1681847620.3702724): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6361, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6314, val loss 4.6175 [8.790102243423462 sec]
step 100: train loss 2.2003, val loss 2.3262 [23.259811878204346 sec]
step 200: train loss 1.8616, val loss 2.0557 [37.71939206123352 sec]
step 300: train loss 1.6715, val loss 1.9435 [51.902549266815186 sec]
step 400: train loss 1.5541, val loss 1.8612 [66.66552090644836 sec]
1.5059566497802734
Total Training Time: 73.07317352294922 seconds

"I sir!"
"Hoshy come at their."
Ghatta stowly nears perobling me, they stet now us frief an her viki
BEGINNING (1681847695.9747324): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6181, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6266, val loss 4.6326 [4.851006269454956 sec]
step 100: train loss 2.2464, val loss 2.3628 [13.019323587417603 sec]
step 200: train loss 1.8487, val loss 2.0465 [21.624818086624146 sec]
step 300: train loss 1.6655, val loss 1.9065 [30.270896434783936 sec]
step 400: train loss 1.5371, val loss 1.8387 [38.940632820129395 sec]
1.5065293312072754
Total Training Time: 42.456361055374146 seconds

"noGEatta kning?" Perana said Bam whis trips fate
in helpenting Anayah at his treaty hare to was fro
BEGINNING (1681847739.3067365): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5685, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5680, val loss 4.5642 [8.550719261169434 sec]
step 100: train loss 2.2885, val loss 2.3945 [24.178046464920044 sec]
step 200: train loss 1.8436, val loss 2.0455 [39.95039916038513 sec]
step 300: train loss 1.6017, val loss 1.8769 [55.58141827583313 sec]
step 400: train loss 1.4696, val loss 1.8085 [71.98993444442749 sec]
1.455416202545166
Total Training Time: 79.09204149246216 seconds

ban accapnearly. As they not earches of quons ame friefush
Zechariyah's
rare9y in in course and stro
BEGINNING (1681847820.3981066): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6379, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6124, val loss 4.6136 [13.456186294555664 sec]
step 100: train loss 2.4149, val loss 2.5146 [36.29151368141174 sec]
step 200: train loss 1.9302, val loss 2.1112 [59.34874129295349 sec]
step 300: train loss 1.6502, val loss 1.9228 [81.13309812545776 sec]
step 400: train loss 1.4850, val loss 1.8341 [102.88189005851746 sec]
1.4311314821243286
Total Training Time: 112.49882936477661 seconds

to the wall.
"There was the Pyrranger, Chief 1." Theaâ
SEAN smallier – of (any the trabowirred. "Has
BEGINNING (1681847935.4431562): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5988, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5945, val loss 4.5945 [7.84251856803894 sec]
step 100: train loss 2.4000, val loss 2.5002 [21.583975553512573 sec]
step 200: train loss 1.9888, val loss 2.1719 [35.867993116378784 sec]
step 300: train loss 1.7178, val loss 1.9591 [49.721673250198364 sec]
step 400: train loss 1.5401, val loss 1.8395 [63.27533793449402 sec]
1.5276734828948975
Total Training Time: 69.42747354507446 seconds

whad steash!" Anded. "Yes, the legmelt sips you. Aftern pare proansed on
the crourds weire a no send
BEGINNING (1681848005.8226466): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6077, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6075, val loss 4.6002 [14.645266056060791 sec]
step 100: train loss 2.4224, val loss 2.5124 [39.17267966270447 sec]
step 200: train loss 1.9734, val loss 2.1542 [63.77634596824646 sec]
step 300: train loss 1.6562, val loss 1.9175 [88.17570328712463 sec]
step 400: train loss 1.4615, val loss 1.8131 [113.27426242828369 sec]
1.41730797290802
Total Training Time: 123.86658883094788 seconds

Gratta in to him. This unsher. He tent Gratta looked at def-1
Chief Arphad, they men arrount ago? On
BEGINNING (1681848131.3724363): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5862, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5830, val loss 4.5826 [21.440950632095337 sec]
step 100: train loss 2.4556, val loss 2.5445 [57.399576902389526 sec]
step 200: train loss 2.0544, val loss 2.2144 [93.43599390983582 sec]
step 300: train loss 1.6819, val loss 1.9354 [128.7241792678833 sec]
step 400: train loss 1.4380, val loss 1.8133 [163.79535293579102 sec]
1.3853012323379517
Total Training Time: 178.51314902305603 seconds

"Whan with pass in they seep side inste." Gratta scaid,
"I ave all Hi. We my cau eseen with a my
sco
BEGINNING (1681848312.50905): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5776, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5587, val loss 4.5654 [4.0573577880859375 sec]
step 100: train loss 2.1856, val loss 2.3446 [11.36988353729248 sec]
step 200: train loss 1.9225, val loss 2.1262 [18.629550457000732 sec]
step 300: train loss 1.8070, val loss 2.0330 [26.043543815612793 sec]
step 400: train loss 1.6996, val loss 1.9617 [33.296913862228394 sec]
1.6720877885818481
Total Training Time: 36.56912970542908 seconds

is luned Gratta sold
t mance
have to fear prested. He town worchis grespopprittly recoredsurnted. Hi
BEGINNING (1681848350.344939): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5876, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6042, val loss 4.6013 [7.170487403869629 sec]
step 100: train loss 2.2764, val loss 2.4034 [20.469815492630005 sec]
step 200: train loss 1.9855, val loss 2.1680 [33.731764793395996 sec]
step 300: train loss 1.8363, val loss 2.0500 [46.998934268951416 sec]
step 400: train loss 1.7236, val loss 1.9646 [60.46214747428894 sec]
1.631126880645752
Total Training Time: 66.64276599884033 seconds

the cround it name, any owly be reasniing lowhat deve aigh the tuon, "Torief, to had be to be are in
BEGINNING (1681848419.456858): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5610, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5679, val loss 4.5658 [10.052441835403442 sec]
step 100: train loss 2.5254, val loss 2.6020 [29.195573091506958 sec]
step 200: train loss 2.1444, val loss 2.2970 [48.51195406913757 sec]
step 300: train loss 1.9717, val loss 2.1840 [67.66421365737915 sec]
step 400: train loss 1.8512, val loss 2.0863 [86.65918016433716 sec]
1.6928049325942993
Total Training Time: 95.71388697624207 seconds

the mutcould down yuran he
is he tuo hapt. Out?" Thaving to fave a someresticner vetold an tha. We
r
BEGINNING (1681848518.8099155): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6076, val loss 4.6145 [4.694451093673706 sec]
step 100: train loss 2.2586, val loss 2.3982 [13.455374956130981 sec]
step 200: train loss 1.8866, val loss 2.0901 [22.18804669380188 sec]
step 300: train loss 1.7146, val loss 1.9760 [30.86861777305603 sec]
step 400: train loss 1.5867, val loss 1.8900 [39.541842222213745 sec]
1.6212472915649414
Total Training Time: 43.48876190185547 seconds

with would ned to-cubirs.
Taking in wallong, wide surply be niffrely the cible."
Aidnon't foll the n
BEGINNING (1681848563.5751355): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5905, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5822, val loss 4.5746 [8.70462679862976 sec]
step 100: train loss 2.4209, val loss 2.5302 [25.00086784362793 sec]
step 200: train loss 2.0106, val loss 2.1881 [41.38946008682251 sec]
step 300: train loss 1.7721, val loss 2.0096 [57.70940351486206 sec]
step 400: train loss 1.6352, val loss 1.9133 [74.05559754371643 sec]
1.6217529773712158
Total Training Time: 81.63268995285034 seconds

Juso I arnager wite. Gratta pare him anminded. Wout
hambe.
Fery ween we oshoo his do to erched."
Aid
BEGINNING (1681848647.6476483): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6500, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6673, val loss 4.6639 [12.678061246871948 sec]
step 100: train loss 2.5343, val loss 2.6042 [36.581419944763184 sec]
step 200: train loss 2.1792, val loss 2.3203 [60.39127445220947 sec]
step 300: train loss 1.8994, val loss 2.1206 [84.30240321159363 sec]
step 400: train loss 1.7061, val loss 1.9658 [108.2790539264679 sec]
1.627685546875
Total Training Time: 119.47609567642212 seconds

that taked in yould out geturn with sheen tot a
reath. Gratta ward had was will. But had side had ra
BEGINNING (1681848770.8216472): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6015, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5931, val loss 4.5871 [7.791372060775757 sec]
step 100: train loss 2.4162, val loss 2.5278 [22.060975790023804 sec]
step 200: train loss 1.9794, val loss 2.1633 [36.967748403549194 sec]
step 300: train loss 1.7314, val loss 1.9908 [51.81322622299194 sec]
step 400: train loss 1.5601, val loss 1.8748 [66.35703802108765 sec]
1.4705408811569214
Total Training Time: 72.88230609893799 seconds

Pelana's we cleanime came slical are
breate othing and chis in
bewithould of the would not over, cou
BEGINNING (1681848844.9749014): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6697, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6592, val loss 4.6654 [15.069718837738037 sec]
step 100: train loss 2.4797, val loss 2.5635 [41.853108167648315 sec]
step 200: train loss 2.1977, val loss 2.3319 [68.83433890342712 sec]
step 300: train loss 1.8118, val loss 2.0370 [96.1257483959198 sec]
step 400: train loss 1.6032, val loss 1.8897 [123.32923889160156 sec]
1.5128306150436401
Total Training Time: 135.09735321998596 seconds

are nodded, "They dorthen stip in componterectiance o
pisace."
Gratta as who without nations nodded 
BEGINNING (1681848982.5416574): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6362, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6378, val loss 4.6305 [22.207146406173706 sec]
step 100: train loss 2.5606, val loss 2.6311 [59.11551570892334 sec]
step 200: train loss 2.4111, val loss 2.5091 [95.58053803443909 sec]
step 300: train loss 2.0469, val loss 2.2212 [131.76957058906555 sec]
step 400: train loss 1.7375, val loss 1.9896 [167.0667369365692 sec]
1.5966298580169678
Total Training Time: 183.21304750442505 seconds

the cubs on wulls alm. Took the furne all had befor with dean the
the nare. Surely to the tend them 
BEGINNING (1681849169.512966): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5757, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5689, val loss 4.5817 [4.502771377563477 sec]
step 100: train loss 2.1035, val loss 2.2608 [12.45330548286438 sec]
step 200: train loss 1.8336, val loss 2.0379 [20.593483209609985 sec]
step 300: train loss 1.6951, val loss 1.9480 [28.92571258544922 sec]
step 400: train loss 1.5969, val loss 1.8836 [36.94249963760376 sec]
1.5239605903625488
Total Training Time: 40.53174328804016 seconds

what through propied, sund well evere
witheef to his forest him this pare
ehopins, and gate rode bre
BEGINNING (1681849211.2880385): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6908, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6783, val loss 4.6818 [7.805231809616089 sec]
step 100: train loss 2.2246, val loss 2.3635 [22.83296251296997 sec]
step 200: train loss 1.9254, val loss 2.1124 [37.985459089279175 sec]
step 300: train loss 1.7555, val loss 1.9897 [53.061458110809326 sec]
step 400: train loss 1.6380, val loss 1.9310 [68.00098538398743 sec]
1.5884205102920532
Total Training Time: 74.86384463310242 seconds

in anyon, and be stood and be the onlaparces.
He the sceek did and stalund to them, wait other to
bo
BEGINNING (1681849288.5606406): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5673, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5534, val loss 4.5618 [11.180041313171387 sec]
step 100: train loss 2.4581, val loss 2.5585 [32.50108003616333 sec]
step 200: train loss 2.0432, val loss 2.2303 [54.32660984992981 sec]
step 300: train loss 1.8536, val loss 2.0846 [75.64978194236755 sec]
step 400: train loss 1.7141, val loss 1.9893 [97.13451647758484 sec]
1.6026419401168823
Total Training Time: 107.43564629554749 seconds

watenes.
"Vere, he 1 or war; of up the plans,
of his wellls noth have tows the
tell stolds, and his 
BEGINNING (1681849399.707505): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5671, val loss 4.5721 [6.503348350524902 sec]
step 100: train loss 2.1974, val loss 2.3262 [18.016945362091064 sec]
step 200: train loss 1.8080, val loss 2.0190 [29.485413074493408 sec]
step 300: train loss 1.6113, val loss 1.8861 [41.01056432723999 sec]
step 400: train loss 1.4905, val loss 1.8386 [52.689204692840576 sec]
1.5183111429214478
Total Training Time: 57.86013460159302 seconds

To Thas a guagainstion that this preson."
Perhaps werhaps themble gain fintifue."
Gratta light smile
BEGINNING (1681849458.9125803): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6293, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6284, val loss 4.6194 [12.087270736694336 sec]
step 100: train loss 2.3630, val loss 2.4657 [33.90953087806702 sec]
step 200: train loss 1.9085, val loss 2.1010 [55.859480142593384 sec]
step 300: train loss 1.6796, val loss 1.9208 [77.70338296890259 sec]
step 400: train loss 1.5188, val loss 1.8319 [100.11565113067627 sec]
1.5063732862472534
Total Training Time: 110.26112532615662 seconds

the will maeuws sther hadked marks diffinces of and
Taka formor
tund taction to the over blaigh not 
BEGINNING (1681849571.7067342): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5746, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5706, val loss 4.5732 [17.48576521873474 sec]
step 100: train loss 2.5337, val loss 2.6207 [49.80155372619629 sec]
step 200: train loss 2.0872, val loss 2.2349 [82.17753577232361 sec]
step 300: train loss 1.8081, val loss 2.0095 [114.33135461807251 sec]
step 400: train loss 1.6027, val loss 1.8851 [146.75733876228333 sec]
1.5640844106674194
Total Training Time: 161.1918122768402 seconds

hid ston the natip. Sleft, sway well are tall with
to
enor the curch ecerfaterencerely, look a flam 
BEGINNING (1681849736.652391): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.7153, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7155, val loss 4.7160 [11.711865186691284 sec]
step 100: train loss 2.3843, val loss 2.4790 [31.578962087631226 sec]
step 200: train loss 1.9044, val loss 2.0970 [51.36904740333557 sec]
step 300: train loss 1.6395, val loss 1.9158 [71.08886003494263 sec]
step 400: train loss 1.4612, val loss 1.8302 [91.13127589225769 sec]
1.3955656290054321
Total Training Time: 99.71412634849548 seconds

had not bee thim. How dew deenot;
the mansile's aready find. "It Keannemy, his we
clowly, move turne
BEGINNING (1681849837.7134068): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6225, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6321, val loss 4.6268 [21.627049684524536 sec]
step 100: train loss 2.4643, val loss 2.5678 [56.98909044265747 sec]
step 200: train loss 2.0838, val loss 2.2435 [92.73939299583435 sec]
step 300: train loss 1.7176, val loss 1.9591 [128.48392009735107 sec]
step 400: train loss 1.5045, val loss 1.8676 [163.63437867164612 sec]
1.4394186735153198
Total Training Time: 179.26859998703003 seconds

"Gratta? Whild an to smels unave
"Thany splacks. I a mumberbackly Ke
hav in Anayah have you."
"Eve G
BEGINNING (1681850019.6329277): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6263, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6194, val loss 4.6202 [28.5099515914917 sec]
step 100: train loss 2.5413, val loss 2.6227 [102.2973427772522 sec]
step 200: train loss 2.3502, val loss 2.4683 [195.28787279129028 sec]
step 300: train loss 1.9312, val loss 2.1368 [279.1532738208771 sec]
step 400: train loss 1.6115, val loss 1.9112 [355.87221217155457 sec]
1.4821597337722778
Total Training Time: 394.41914558410645 seconds

IF This beare
mily and boicate morted just a not givle to the pad."
Cominow, Gratta loked and hif Ta
BEGINNING (1681850418.060517): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6161, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6211, val loss 4.6227 [7.15950083732605 sec]
step 100: train loss 2.0540, val loss 2.2175 [20.326936960220337 sec]
step 200: train loss 1.7811, val loss 2.0164 [31.693277597427368 sec]
step 300: train loss 1.6286, val loss 1.8856 [42.737366914749146 sec]
step 400: train loss 1.5419, val loss 1.8719 [53.458332777023315 sec]
1.5353667736053467
Total Training Time: 58.18331217765808 seconds

wipTy, to we ouon. Evensiwere.
This amal were doubt. With wen the like traie, and
youse the graps of
BEGINNING (1681850477.5583112): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6252, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6135, val loss 4.6107 [10.951731443405151 sec]
step 100: train loss 2.2019, val loss 2.3366 [30.68756604194641 sec]
step 200: train loss 1.8512, val loss 2.0541 [50.656768798828125 sec]
step 300: train loss 1.6778, val loss 1.9349 [70.41624212265015 sec]
step 400: train loss 1.5714, val loss 1.8838 [90.25730919837952 sec]
1.5816874504089355
Total Training Time: 99.0351414680481 seconds

generals a fached fewteld, andthered melk and they not
have and forer osh the Tuon paws
was unTuons.
BEGINNING (1681850579.4923606): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6307, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6198, val loss 4.6141 [15.18028473854065 sec]
step 100: train loss 2.4341, val loss 2.5177 [44.623154401779175 sec]
step 200: train loss 2.0098, val loss 2.1886 [73.66871094703674 sec]
step 300: train loss 1.7723, val loss 2.0127 [103.53170204162598 sec]
step 400: train loss 1.6288, val loss 1.9242 [131.90341544151306 sec]
1.6064131259918213
Total Training Time: 145.51884031295776 seconds

Gratta know was or his cheart, bunna safe to be
kfee the sothing. "He Lamek have light, the yof of o
BEGINNING (1681850728.77396): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6052, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6183, val loss 4.6172 [9.217276811599731 sec]
step 100: train loss 2.1264, val loss 2.2726 [27.04098868370056 sec]
step 200: train loss 1.7451, val loss 1.9783 [44.0067572593689 sec]
step 300: train loss 1.5677, val loss 1.8797 [59.90606665611267 sec]
step 400: train loss 1.4370, val loss 1.8088 [76.46900653839111 sec]
1.4903209209442139
Total Training Time: 83.29297423362732 seconds

that Too sa ginu armiled misen hat
troupicts. The t should evere. We will be was mumberfur
gares. Gr
BEGINNING (1681850813.2989988): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5472, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5520, val loss 4.5467 [15.823508977890015 sec]
step 100: train loss 2.3333, val loss 2.4292 [44.45333218574524 sec]
step 200: train loss 1.8593, val loss 2.0695 [72.58461260795593 sec]
step 300: train loss 1.6005, val loss 1.8838 [100.92160201072693 sec]
step 400: train loss 1.4470, val loss 1.8143 [130.07687950134277 sec]
1.4118952751159668
Total Training Time: 143.09352111816406 seconds

HU". This befroore did no speeplad had sell.
"Syee shile you till carms before it!
But alliany of Pe
BEGINNING (1681850958.858685): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5461, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5468, val loss 4.5408 [22.866641998291016 sec]
step 100: train loss 2.5331, val loss 2.6113 [62.31919050216675 sec]
step 200: train loss 2.1059, val loss 2.2586 [102.19920492172241 sec]
step 300: train loss 1.7778, val loss 2.0159 [141.20260214805603 sec]
step 400: train loss 1.5584, val loss 1.8688 [180.37592840194702 sec]
1.5041964054107666
Total Training Time: 197.47730112075806 seconds

ISkaing of any had and he head torcharchilm.
“Gratta quiet would and this wealish.
"May Yons." Yes, 
BEGINNING (1681851160.0029383): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5894, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5864, val loss 4.5878 [14.770391941070557 sec]
step 100: train loss 2.3613, val loss 2.4590 [43.55616283416748 sec]
step 200: train loss 1.8452, val loss 2.0494 [76.37761378288269 sec]
step 300: train loss 1.5721, val loss 1.8848 [104.50525236129761 sec]
step 400: train loss 1.3914, val loss 1.7797 [131.9989311695099 sec]
1.3415518999099731
Total Training Time: 146.70204877853394 seconds

Gratta reval Beariyah. He wall humander him morders,
they to remies with Gratta. "What a gar gi~e yo
BEGINNING (1681851308.294542): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6681, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6785, val loss 4.6794 [29.90352749824524 sec]
step 100: train loss 2.4509, val loss 2.5438 [81.84228491783142 sec]
step 200: train loss 2.0375, val loss 2.2042 [129.8702003955841 sec]
step 300: train loss 1.6721, val loss 1.9430 [180.85693097114563 sec]
step 400: train loss 1.4421, val loss 1.8093 [238.3562126159668 sec]
1.3518056869506836
Total Training Time: 258.3100061416626 seconds

Your fur."
"Noot ther you must proct human to the was powed operfuren
with them, he sail. I have the
BEGINNING (1681851569.1790717): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5997, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5872, val loss 4.5813 [1.122215747833252 sec]
step 100: train loss 3.0848, val loss 3.1122 [2.876368522644043 sec]
step 200: train loss 2.8137, val loss 2.8498 [4.828449964523315 sec]
step 300: train loss 2.6795, val loss 2.7375 [6.688588619232178 sec]
step 400: train loss 2.5836, val loss 2.6486 [8.661787271499634 sec]
2.5572099685668945
Total Training Time: 9.413568019866943 seconds

AiI roberithes ched thisat dath. in r at bin ith tond Payulakareng the ooumed
hentDa n he ba"miscaki
BEGINNING (1681851578.861454): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6556, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6420, val loss 4.6459 [1.806182622909546 sec]
step 100: train loss 3.0356, val loss 3.0600 [4.558847427368164 sec]
step 200: train loss 2.7649, val loss 2.8195 [7.253243923187256 sec]
step 300: train loss 2.6314, val loss 2.6997 [10.08616018295288 sec]
step 400: train loss 2.5480, val loss 2.6090 [12.707521438598633 sec]
2.5000698566436768
Total Training Time: 13.696079969406128 seconds

nigh wray charts Phesa banie areegisratclat nd ye awer cerapowm tesloraDgend ck hiy anod cema sthemo
BEGINNING (1681851592.9440436): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5538, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5467, val loss 4.5528 [2.0550272464752197 sec]
step 100: train loss 2.9611, val loss 3.0018 [5.445210695266724 sec]
step 200: train loss 2.6999, val loss 2.7465 [8.849931240081787 sec]
step 300: train loss 2.5725, val loss 2.6363 [12.213661909103394 sec]
step 400: train loss 2.4781, val loss 2.5499 [15.550723552703857 sec]
2.4301743507385254
Total Training Time: 16.991016149520874 seconds

on manprslyed the
at "The ake br tinr whiern.""
I imMmt the
e sme o,
tthe nile m~Nad fy- ced shakeFo
BEGINNING (1681851610.4750857): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6340, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6112, val loss 4.6200 [1.0440073013305664 sec]
step 100: train loss 3.0641, val loss 3.0995 [2.727753162384033 sec]
step 200: train loss 2.8155, val loss 2.8670 [4.433638334274292 sec]
step 300: train loss 2.6910, val loss 2.7401 [6.254061460494995 sec]
step 400: train loss 2.6214, val loss 2.6872 [7.907074689865112 sec]
2.6700122356414795
Total Training Time: 8.578840970993042 seconds

thais th l. orred I and adistthespeute
Anri samat bed ke titerner. humasrangHisgwels
ned ht tht d
 w
BEGINNING (1681851619.2859263): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6980, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6829, val loss 4.6829 [1.571004867553711 sec]
step 100: train loss 2.9871, val loss 3.0130 [4.12492299079895 sec]
step 200: train loss 2.7467, val loss 2.7912 [6.711618423461914 sec]
step 300: train loss 2.6365, val loss 2.6930 [9.298064947128296 sec]
step 400: train loss 2.5718, val loss 2.6315 [11.923304796218872 sec]
2.5312368869781494
Total Training Time: 13.005078554153442 seconds

e
EECPe malto ororf mi7thube mo!J
CEaud re, t atillyr tand
cf n
had and wa
ted samee iveouathedoad c
BEGINNING (1681851632.6621566): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5972, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6096, val loss 4.6118 [2.122300148010254 sec]
step 100: train loss 2.9729, val loss 2.9982 [5.387942314147949 sec]
step 200: train loss 2.7306, val loss 2.7767 [8.70724105834961 sec]
step 300: train loss 2.6222, val loss 2.6810 [12.001555442810059 sec]
step 400: train loss 2.5550, val loss 2.6219 [15.46498727798462 sec]
2.515550136566162
Total Training Time: 16.933126211166382 seconds

thig te a do tof ad f listhader thind wgay worokdomes t." s Pine f we Te F rid !I be thV lais"AP nma
BEGINNING (1681851650.1465461): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6645, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6635, val loss 4.6618 [1.10469388961792 sec]
step 100: train loss 3.0464, val loss 3.0793 [2.9220964908599854 sec]
step 200: train loss 2.7933, val loss 2.8437 [4.709988117218018 sec]
step 300: train loss 2.6975, val loss 2.7601 [6.476518154144287 sec]
step 400: train loss 2.6382, val loss 2.6935 [8.239376306533813 sec]
2.6243042945861816
Total Training Time: 8.947555780410767 seconds

"
AANI.-", cas "- fen"W
Gratheoueipeotoxer heley
t YGe thacin het hache tid. Eonore ak"Y as urthi6 a
BEGINNING (1681851659.3300939): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6495, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6451, val loss 4.6466 [1.6058299541473389 sec]
step 100: train loss 2.9911, val loss 3.0229 [4.197393894195557 sec]
step 200: train loss 2.7532, val loss 2.7992 [6.791590213775635 sec]
step 300: train loss 2.6490, val loss 2.7180 [9.360782384872437 sec]
step 400: train loss 2.5952, val loss 2.6556 [12.120846509933472 sec]
2.5860180854797363
Total Training Time: 13.456314086914062 seconds

cen2 mes ws wlytnt; tofphe a hine teg t yond ch "
pler and date a he gQurok0 tisraioilafoustl th, Pe
BEGINNING (1681851673.2282524): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6387, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6461, val loss 4.6423 [4.794492244720459 sec]
step 100: train loss 2.9458, val loss 2.9747 [8.707465171813965 sec]
step 200: train loss 2.7187, val loss 2.7644 [12.15767526626587 sec]
step 300: train loss 2.6225, val loss 2.6872 [16.312820434570312 sec]
step 400: train loss 2.5725, val loss 2.6397 [20.06552267074585 sec]
2.5138583183288574
Total Training Time: 21.647132635116577 seconds

bnonesadd e thte waZefumowonot r ts, y.
Chemiu, hanat mas met ce edpathepar. as t fue n ite ind ande
BEGINNING (1681851695.4973834): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5956, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5881, val loss 4.5820 [1.3740081787109375 sec]
step 100: train loss 3.0708, val loss 3.0997 [3.353743553161621 sec]
step 200: train loss 2.7967, val loss 2.8399 [5.359936475753784 sec]
step 300: train loss 2.6682, val loss 2.7178 [7.568066835403442 sec]
step 400: train loss 2.5552, val loss 2.6247 [9.510754346847534 sec]
2.4990317821502686
Total Training Time: 10.212840795516968 seconds

tie torilfuwo slarmeaconeof ttas.
hle achesk beantimera hetur atghepestheed io t harr whe tuTek bart
BEGINNING (1681851705.932223): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5387, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5550, val loss 4.5547 [1.5818464756011963 sec]
step 100: train loss 3.0045, val loss 3.0353 [4.241421699523926 sec]
step 200: train loss 2.7256, val loss 2.7708 [6.826473236083984 sec]
step 300: train loss 2.5961, val loss 2.6445 [9.510626077651978 sec]
step 400: train loss 2.5003, val loss 2.5700 [12.180759191513062 sec]
2.4442665576934814
Total Training Time: 13.227463006973267 seconds

sthan wgyie an t5Yis anound.d Th s dechaiy wis rind nong eo’oriond mats als, th gh
marif pleponok. "
BEGINNING (1681851719.5276837): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6311, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6033, val loss 4.6152 [2.082322359085083 sec]
step 100: train loss 2.9394, val loss 2.9734 [5.561661005020142 sec]
step 200: train loss 2.6774, val loss 2.7448 [9.066465616226196 sec]
step 300: train loss 2.5617, val loss 2.6259 [12.499545097351074 sec]
step 400: train loss 2.4609, val loss 2.5345 [15.989269733428955 sec]
2.414557695388794
Total Training Time: 17.470454692840576 seconds

onqeve n be. Grattceratestand. “e j, €he oturon, nd."0(or A7 st fore tey st ou tho malferre tionedel
BEGINNING (1681851737.5955615): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6659, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6830, val loss 4.6828 [1.2060649394989014 sec]
step 100: train loss 3.0824, val loss 3.1147 [3.0528981685638428 sec]
step 200: train loss 2.7987, val loss 2.8511 [4.933598279953003 sec]
step 300: train loss 2.6917, val loss 2.7508 [6.812759160995483 sec]
step 400: train loss 2.6093, val loss 2.6790 [8.776130676269531 sec]
2.5730741024017334
Total Training Time: 9.482437372207642 seconds

Gstpy.."S
A
ANan f ay AYh was thineisounthoohe uis."
Che, catharhe4AA
Nouily Thkot brret w this."
"i
BEGINNING (1681851747.3049974): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6191, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6100, val loss 4.6021 [1.7257180213928223 sec]
step 100: train loss 2.9634, val loss 2.9908 [4.418219089508057 sec]
step 200: train loss 2.7217, val loss 2.7686 [7.084265232086182 sec]
step 300: train loss 2.6208, val loss 2.6752 [9.773716688156128 sec]
step 400: train loss 2.5449, val loss 2.6225 [12.456498146057129 sec]
2.540659189224243
Total Training Time: 13.522526502609253 seconds

plergallad nan slsighis. Tsebowamonn, le freravewe Then. Grroueneske akiplike bup whe Tilsedyeowenst
BEGINNING (1681851761.256526): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6751, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6653, val loss 4.6601 [2.2406609058380127 sec]
step 100: train loss 2.9268, val loss 2.9563 [5.946683645248413 sec]
step 200: train loss 2.6861, val loss 2.7455 [9.58376431465149 sec]
step 300: train loss 2.5866, val loss 2.6455 [13.30358076095581 sec]
step 400: train loss 2.5255, val loss 2.5851 [16.998090505599976 sec]
2.486058473587036
Total Training Time: 18.499372005462646 seconds

ow iben serismof hed ntrer aNhid h Thethorerd aree niharede le mt, for hathe t, h thc4e Atotuilpegif
BEGINNING (1681851780.299562): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6109, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6236, val loss 4.6310 [1.4462122917175293 sec]
step 100: train loss 3.0639, val loss 3.0969 [3.6857805252075195 sec]
step 200: train loss 2.8117, val loss 2.8575 [5.87764310836792 sec]
step 300: train loss 2.7007, val loss 2.7573 [8.00778865814209 sec]
step 400: train loss 2.6347, val loss 2.7027 [10.144298791885376 sec]
2.646057367324829
Total Training Time: 10.912879705429077 seconds

heirathisd the baoftes Tokarppa t batid Mowoco t oount hinll m."Ra bede tharemin ben 6lg
sarig hm ad
BEGINNING (1681851791.4314404): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6941, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6817, val loss 4.6800 [1.8650143146514893 sec]
step 100: train loss 2.9631, val loss 3.0008 [4.904451131820679 sec]
step 200: train loss 2.7324, val loss 2.7856 [7.928786277770996 sec]
step 300: train loss 2.6375, val loss 2.6957 [10.959337949752808 sec]
step 400: train loss 2.5833, val loss 2.6497 [14.047920942306519 sec]
2.5703165531158447
Total Training Time: 15.273848533630371 seconds

hhe
anra
tah, ord,e hedichig EAwsthenyabe bI s g w haifid makauoutred wans
""DAYAmahemif ie ve. and 
BEGINNING (1681851807.07429): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6324, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6397, val loss 4.6453 [2.4449117183685303 sec]
step 100: train loss 2.9073, val loss 2.9467 [6.509851932525635 sec]
step 200: train loss 2.7032, val loss 2.7506 [10.496719360351562 sec]
step 300: train loss 2.6080, val loss 2.6686 [14.48800277709961 sec]
step 400: train loss 2.5548, val loss 2.6195 [18.58270573616028 sec]
2.540140151977539
Total Training Time: 20.20815896987915 seconds

tk to
clyake ain stouto-he.n Tr
beleoumad cabuiethYe ameste lls red, I itegs "rl weu/ y Ai%oraatheyo
BEGINNING (1681851827.8240871): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6352, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6450, val loss 4.6428 [1.4527671337127686 sec]
step 100: train loss 3.0806, val loss 3.1137 [3.5537495613098145 sec]
step 200: train loss 2.7968, val loss 2.8429 [5.701747894287109 sec]
step 300: train loss 2.6674, val loss 2.7225 [7.718784332275391 sec]
step 400: train loss 2.5558, val loss 2.6272 [9.862849712371826 sec]
2.512054681777954
Total Training Time: 10.642499208450317 seconds

hs noqThe, mig
bond piod Arthe agte ’ rubincyora qa the wotadmathed to ang "
Lecit iufta9ibooona ti.
BEGINNING (1681851838.7192144): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6648, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6649, val loss 4.6725 [1.92897367477417 sec]
step 100: train loss 2.9680, val loss 3.0006 [4.82801365852356 sec]
step 200: train loss 2.6753, val loss 2.7409 [7.7172181606292725 sec]
step 300: train loss 2.5308, val loss 2.6075 [10.551215887069702 sec]
step 400: train loss 2.4257, val loss 2.5111 [13.504025936126709 sec]
2.409303665161133
Total Training Time: 14.657788276672363 seconds

cy mout the brokingis
the thad yo ose a car benof but
thigeran tren hanouis nighg is
f,ont is dusser
BEGINNING (1681851853.7792416): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5571, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5688, val loss 4.5665 [2.266225576400757 sec]
step 100: train loss 2.8950, val loss 2.9304 [6.06671667098999 sec]
step 200: train loss 2.6550, val loss 2.7104 [9.908409118652344 sec]
step 300: train loss 2.5112, val loss 2.5881 [14.103759765625 sec]
step 400: train loss 2.3886, val loss 2.4720 [17.96805477142334 sec]
2.327986240386963
Total Training Time: 19.446654319763184 seconds

hmad odist ickeme pa8 w2Km, wors the surkthan at'rpep the natt Grald hillow eon acums wally." bied T
BEGINNING (1681851873.767673): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5819, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5873, val loss 4.5921 [1.4277746677398682 sec]
step 100: train loss 3.0639, val loss 3.1006 [3.5063540935516357 sec]
step 200: train loss 2.7921, val loss 2.8408 [5.624129056930542 sec]
step 300: train loss 2.6641, val loss 2.7221 [7.745402097702026 sec]
step 400: train loss 2.5985, val loss 2.6585 [9.819329738616943 sec]
2.604609966278076
Total Training Time: 10.54780888557434 seconds

/AaI mte sowe atn l( ul“nmed asarp b eyo ted, He of in.
PTurnelahe th t wawft owaw bo, r whm tas the
BEGINNING (1681851884.5314834): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6599, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6599, val loss 4.6501 [1.8125741481781006 sec]
step 100: train loss 3.0137, val loss 3.0543 [4.750149488449097 sec]
step 200: train loss 2.7225, val loss 2.7738 [7.769274473190308 sec]
step 300: train loss 2.6084, val loss 2.6699 [10.918416261672974 sec]
step 400: train loss 2.5398, val loss 2.6081 [14.054484605789185 sec]
2.4638843536376953
Total Training Time: 15.237053155899048 seconds

APxlad eris
wo Gryo Thatanthathd ssid ca, anl.""I thoZato seme, Ide a s d ea"yt toiaked wes spag
Gho
BEGINNING (1681851900.1375425): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5769, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5697, val loss 4.5740 [2.2497992515563965 sec]
step 100: train loss 2.9206, val loss 2.9483 [6.014522552490234 sec]
step 200: train loss 2.7042, val loss 2.7630 [9.785590171813965 sec]
step 300: train loss 2.5954, val loss 2.6606 [13.532712936401367 sec]
step 400: train loss 2.5241, val loss 2.5948 [17.413024187088013 sec]
2.5616583824157715
Total Training Time: 18.985469579696655 seconds

HA0Yo frit aE-ong and th us,˜"N thepn wve blyke ashew ce, thnlld miy He o b waled hed wacephasthe th
BEGINNING (1681851919.6903803): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6910, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6799, val loss 4.6870 [1.6743698120117188 sec]
step 100: train loss 3.0636, val loss 3.0963 [4.260449409484863 sec]
step 200: train loss 2.8258, val loss 2.8656 [6.785839319229126 sec]
step 300: train loss 2.6965, val loss 2.7465 [9.318399906158447 sec]
step 400: train loss 2.6297, val loss 2.6906 [11.87941288948059 sec]
2.613847494125366
Total Training Time: 12.780936002731323 seconds

oubrmeour Ue ouitider f ofeO cHanoyotem ffLed masemr kled t asave ker. tangke bs ur Rd bgumisith brr
BEGINNING (1681851932.6903837): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5996, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5940, val loss 4.5924 [2.3138768672943115 sec]
step 100: train loss 2.9767, val loss 3.0076 [6.035512208938599 sec]
step 200: train loss 2.7246, val loss 2.7717 [9.78514552116394 sec]
step 300: train loss 2.6151, val loss 2.6778 [13.584197044372559 sec]
step 400: train loss 2.5651, val loss 2.6289 [17.393078565597534 sec]
2.552041530609131
Total Training Time: 18.783340454101562 seconds

pasalsmatausthisorer taedee,
bofo Pum agh."U Grce
6t
" ansWan.(.AriINandeve Grsthir an he Aruioofere
BEGINNING (1681851951.846657): Baseline LR(0.0003) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6277, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6215, val loss 4.6196 [2.9494235515594482 sec]
step 100: train loss 2.9334, val loss 2.9650 [7.815696716308594 sec]
step 200: train loss 2.6978, val loss 2.7444 [12.746819734573364 sec]
step 300: train loss 2.5950, val loss 2.6604 [17.63035011291504 sec]
step 400: train loss 2.5414, val loss 2.6066 [22.574101448059082 sec]
2.547104597091675
Total Training Time: 24.516599416732788 seconds

te h˜tons Tha a'the
bqune Ferorimea he
Mat, prithe wi?"
ond.
be he; ENAPAY l
"HiBenscomahes cev6
win
BEGINNING (1681851976.893977): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5800, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5705, val loss 4.5689 [1.2026045322418213 sec]
step 100: train loss 2.7715, val loss 2.8167 [3.2520673274993896 sec]
step 200: train loss 2.5877, val loss 2.6544 [5.25664496421814 sec]
step 300: train loss 2.4395, val loss 2.5107 [7.362155914306641 sec]
step 400: train loss 2.3334, val loss 2.4245 [9.397651195526123 sec]
2.272207021713257
Total Training Time: 10.190999746322632 seconds

urrase tra ced yopou bele sight."
Gramy sednouid chatthempphd ar ain the ay blyonewth Groaldottaithe
BEGINNING (1681851987.3774705): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5908, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6068, val loss 4.5945 [1.894637107849121 sec]
step 100: train loss 2.7036, val loss 2.7585 [5.309736013412476 sec]
step 200: train loss 2.5146, val loss 2.5852 [9.104900598526001 sec]
step 300: train loss 2.3521, val loss 2.4415 [12.39629316329956 sec]
step 400: train loss 2.2527, val loss 2.3551 [15.695729970932007 sec]
2.216815233230591
Total Training Time: 17.51342010498047 seconds

toex she mif
chistis itean yo cot slidgurn. thaif . "Y
"Durned, foldseined and in“ of but becou;t an
BEGINNING (1681852005.4970286): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6263, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6387, val loss 4.6327 [3.6500356197357178 sec]
step 100: train loss 2.6701, val loss 2.7341 [9.095930814743042 sec]
step 200: train loss 2.4826, val loss 2.5627 [14.184926509857178 sec]
step 300: train loss 2.3105, val loss 2.4132 [19.384877681732178 sec]
step 400: train loss 2.1906, val loss 2.3112 [24.885573863983154 sec]
2.1287283897399902
Total Training Time: 27.220348358154297 seconds

hick hars hsp or if hiese call, and
whac all moof the ounc. hatrit Elalg a ine ant one in wosy the w
BEGINNING (1681852033.6307354): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5332, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5343, val loss 4.5353 [1.3904674053192139 sec]
step 100: train loss 2.7469, val loss 2.8057 [3.7809767723083496 sec]
step 200: train loss 2.5975, val loss 2.6617 [6.215575933456421 sec]
step 300: train loss 2.5087, val loss 2.5848 [8.505423069000244 sec]
step 400: train loss 2.4437, val loss 2.5276 [10.847357749938965 sec]
2.4052300453186035
Total Training Time: 11.806309223175049 seconds

caned rou lers on as. ah hats
Ara bed at Gro
lenappasth low ls s
basis 3~ mead.
"
fnen Thand
Pran gh
BEGINNING (1681852045.7622283): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6404, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6316, val loss 4.6353 [2.0772902965545654 sec]
step 100: train loss 2.7174, val loss 2.7683 [5.808659076690674 sec]
step 200: train loss 2.5477, val loss 2.6128 [9.53499460220337 sec]
step 300: train loss 2.4414, val loss 2.5270 [13.17420506477356 sec]
step 400: train loss 2.3409, val loss 2.4344 [16.920100927352905 sec]
2.3499436378479004
Total Training Time: 18.59888792037964 seconds

y7"IHE Ak tuacR EANCG3z
O0 Ais be Swhelaritond hill. The
The tala he hid, Hencawhat foy and. The wou
BEGINNING (1681852064.9544199): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5763, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5801, val loss 4.5915 [2.783015251159668 sec]
step 100: train loss 2.6702, val loss 2.7341 [7.978797197341919 sec]
step 200: train loss 2.5177, val loss 2.5872 [13.02196717262268 sec]
step 300: train loss 2.4162, val loss 2.5030 [18.095947980880737 sec]
step 400: train loss 2.3023, val loss 2.4091 [23.419734001159668 sec]
2.2893483638763428
Total Training Time: 25.79426145553589 seconds

herrgeng ad. 
"– oowim wecotsp wate frrourd hed risf –
to lento willd oube he cirsilb. "Ple is hode 
BEGINNING (1681852091.6056976): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5887, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5854, val loss 4.5853 [1.7088112831115723 sec]
step 100: train loss 2.7655, val loss 2.8172 [4.724585294723511 sec]
step 200: train loss 2.6141, val loss 2.6830 [7.385497331619263 sec]
step 300: train loss 2.5516, val loss 2.6164 [10.012037992477417 sec]
step 400: train loss 2.5032, val loss 2.5804 [12.727456331253052 sec]
2.5234951972961426
Total Training Time: 13.847304582595825 seconds

thead aroronoke lhara as aseld on ca
pl diled breursantoranolpens'thedid m t whithe dthes, mat acot,
BEGINNING (1681852105.7839954): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5867, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5749, val loss 4.5715 [2.7056329250335693 sec]
step 100: train loss 2.7039, val loss 2.7691 [7.072831869125366 sec]
step 200: train loss 2.5704, val loss 2.6363 [12.00476622581482 sec]
step 300: train loss 2.5100, val loss 2.5829 [16.601778507232666 sec]
step 400: train loss 2.4649, val loss 2.5422 [21.132776737213135 sec]
2.491065502166748
Total Training Time: 23.08799457550049 seconds

Seaisllod sbly wh m!"Yolwerrime gat cirspe ile des asmed omen hans ngncotun, inea any, w.
ongeracip 
BEGINNING (1681852129.4568212): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.7219, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7296, val loss 4.7299 [3.3958327770233154 sec]
step 100: train loss 2.6832, val loss 2.7391 [9.375823259353638 sec]
step 200: train loss 2.5497, val loss 2.6201 [15.817175388336182 sec]
step 300: train loss 2.4843, val loss 2.5684 [21.860486268997192 sec]
step 400: train loss 2.4291, val loss 2.5083 [27.714659690856934 sec]
2.394777297973633
Total Training Time: 30.301985263824463 seconds

Ch fepah he a gritidocel parstareent orde ad mirsheap, ghe sumarewan
akait tof oteain.
"Surin orofur
BEGINNING (1681852160.6442993): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6103, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6215, val loss 4.6217 [1.5474073886871338 sec]
step 100: train loss 2.7500, val loss 2.8002 [4.039953947067261 sec]
step 200: train loss 2.5598, val loss 2.6348 [6.528971910476685 sec]
step 300: train loss 2.3993, val loss 2.4752 [9.111549139022827 sec]
step 400: train loss 2.2893, val loss 2.3812 [11.55366063117981 sec]
2.2354981899261475
Total Training Time: 12.555802345275879 seconds

leilliebelais cingse tuos madd careme ensy hes
tharmis griewes ised, is, hall,
"I neaillqDoelidd
Te 
BEGINNING (1681852173.517678): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6740, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6650, val loss 4.6612 [2.2815206050872803 sec]
step 100: train loss 2.7023, val loss 2.7499 [6.178347587585449 sec]
step 200: train loss 2.4788, val loss 2.5355 [10.111612558364868 sec]
step 300: train loss 2.3050, val loss 2.3956 [13.922873497009277 sec]
step 400: train loss 2.1758, val loss 2.3001 [17.93459391593933 sec]
2.068331003189087
Total Training Time: 19.62511944770813 seconds

nothe cicas a wal willin wibanic2
Tou st
f%op do%mes, ain's nom
cough!"
Aved.
Suond dit fere
tround 
BEGINNING (1681852193.7389338): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6201, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6548, val loss 4.6566 [3.0032224655151367 sec]
step 100: train loss 2.6584, val loss 2.7153 [8.374118089675903 sec]
step 200: train loss 2.4419, val loss 2.5282 [13.808679342269897 sec]
step 300: train loss 2.2728, val loss 2.3721 [19.146578073501587 sec]
step 400: train loss 2.1380, val loss 2.2763 [24.607974529266357 sec]
2.151764154434204
Total Training Time: 26.95393204689026 seconds

aread he." Iven sire mol nisyackiuncest aon.. Cht Greata
fore crom pand
dely cred ne belen ro aved. 
BEGINNING (1681852221.595385): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5589, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5658, val loss 4.5736 [1.7262489795684814 sec]
step 100: train loss 2.7581, val loss 2.8154 [4.449536561965942 sec]
step 200: train loss 2.5896, val loss 2.6559 [7.148858308792114 sec]
step 300: train loss 2.4837, val loss 2.5594 [9.758119344711304 sec]
step 400: train loss 2.3823, val loss 2.4672 [12.38830304145813 sec]
2.341686248779297
Total Training Time: 13.392303943634033 seconds

in si
5of on'at w, co tha sisst aticefallidd lim. Tuou no
d
himet Aithererf tist stres. Eutid be th 
BEGINNING (1681852235.3086872): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6346, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6254, val loss 4.6286 [2.295107841491699 sec]
step 100: train loss 2.7052, val loss 2.7592 [6.195360422134399 sec]
step 200: train loss 2.5323, val loss 2.6083 [10.117521524429321 sec]
step 300: train loss 2.4315, val loss 2.5126 [14.02727484703064 sec]
step 400: train loss 2.2922, val loss 2.3978 [17.941855907440186 sec]
2.239198684692383
Total Training Time: 19.61611843109131 seconds

bet ruat tren, Kayou no drded deXiiish you ra mangllled ofe,"
wast bse had w. Non me wVairlDazle."
"
BEGINNING (1681852255.5104082): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5870, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5864, val loss 4.5913 [3.0433497428894043 sec]
step 100: train loss 2.6521, val loss 2.7063 [8.730507135391235 sec]
step 200: train loss 2.4993, val loss 2.5665 [14.65840744972229 sec]
step 300: train loss 2.3849, val loss 2.4698 [20.44530415534973 sec]
step 400: train loss 2.2419, val loss 2.3490 [25.9657301902771 sec]
2.20487117767334
Total Training Time: 28.381933212280273 seconds

mallak jut. Iok amick gu k!" Andkin it and alfe smoryahe splek coudlitesh u€ed up can." tul, butroup
BEGINNING (1681852284.7442048): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5774, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5802, val loss 4.5846 [2.415132999420166 sec]
step 100: train loss 2.7466, val loss 2.7957 [6.129024028778076 sec]
step 200: train loss 2.5924, val loss 2.6604 [9.913807392120361 sec]
step 300: train loss 2.5276, val loss 2.6064 [13.443802833557129 sec]
step 400: train loss 2.4836, val loss 2.5567 [17.190281867980957 sec]
2.462371587753296
Total Training Time: 18.484768390655518 seconds

ay sarreprild. "are sur ou tJerer bmilat sthe ceetrid tha t fisupenranded caertore, orarrbefind
fut 
BEGINNING (1681852303.6269956): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.7452, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7386, val loss 4.7419 [3.5102076530456543 sec]
step 100: train loss 2.6956, val loss 2.7587 [9.633402347564697 sec]
step 200: train loss 2.5614, val loss 2.6235 [15.373071670532227 sec]
step 300: train loss 2.4930, val loss 2.5683 [20.819074153900146 sec]
step 400: train loss 2.4364, val loss 2.5236 [26.818986177444458 sec]
2.44716477394104
Total Training Time: 29.296510934829712 seconds

Burceytha wed wheruelakes the win he t hanof th je adwal.
"
tharitancou aned thta am enis s. s. seru
BEGINNING (1681852333.600806): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6037, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5980, val loss 4.5879 [4.750839710235596 sec]
step 100: train loss 2.6707, val loss 2.7278 [13.330846309661865 sec]
step 200: train loss 2.5318, val loss 2.6096 [21.729978322982788 sec]
step 300: train loss 2.4641, val loss 2.5445 [29.63858723640442 sec]
step 400: train loss 2.3884, val loss 2.4803 [37.74934649467468 sec]
2.324528217315674
Total Training Time: 41.031394243240356 seconds

tulit thel. Haweg wata shels ysor oughth the st thico be chey meurs re thamak
nouas Pe owprddllldsit
BEGINNING (1681852375.5640826): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5275, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5395, val loss 4.5447 [1.7749311923980713 sec]
step 100: train loss 2.7318, val loss 2.7882 [4.583795547485352 sec]
step 200: train loss 2.5141, val loss 2.5988 [7.252248764038086 sec]
step 300: train loss 2.3371, val loss 2.4380 [9.92046856880188 sec]
step 400: train loss 2.2316, val loss 2.3453 [12.612037181854248 sec]
2.285547971725464
Total Training Time: 13.687999725341797 seconds

Ohatyor in ou win thes hempef cere die thas avperord fhif ad tod ava saicire tp Ms
walecpBe her, its
BEGINNING (1681852389.5637655): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5926, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5833, val loss 4.5978 [2.6044023036956787 sec]
step 100: train loss 2.6777, val loss 2.7269 [6.931952714920044 sec]
step 200: train loss 2.4509, val loss 2.5236 [11.044489622116089 sec]
step 300: train loss 2.2729, val loss 2.3685 [15.189360618591309 sec]
step 400: train loss 2.1390, val loss 2.2592 [19.277479648590088 sec]
2.158841609954834
Total Training Time: 21.00325632095337 seconds

Gratta we ating, his klarfuise, afly lo be
stles of foor He the makgnteple ershowelo hrout. 4ist a s
BEGINNING (1681852411.1617038): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6569, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6461, val loss 4.6383 [3.200141429901123 sec]
step 100: train loss 2.6447, val loss 2.7105 [8.801179885864258 sec]
step 200: train loss 2.4043, val loss 2.4815 [14.531961679458618 sec]
step 300: train loss 2.2152, val loss 2.3305 [20.105908393859863 sec]
step 400: train loss 2.0800, val loss 2.2247 [25.669201374053955 sec]
2.0586323738098145
Total Training Time: 28.101428508758545 seconds

his st and. They toccrice, ham hown whallave wastae% they ham hus ing." Asken! Grared.
Dour Iured wn
BEGINNING (1681852440.1122146): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6709, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6681, val loss 4.6673 [1.9703831672668457 sec]
step 100: train loss 2.7376, val loss 2.7749 [5.5335705280303955 sec]
step 200: train loss 2.5646, val loss 2.6288 [8.693254709243774 sec]
step 300: train loss 2.4732, val loss 2.5437 [11.86582088470459 sec]
step 400: train loss 2.3646, val loss 2.4519 [15.03957748413086 sec]
2.3358540534973145
Total Training Time: 16.339701652526855 seconds

and.
Mramout Grahtandd aweitt
s the yok.
CAon The ther
ars ake ailighs hist we fey. Aneved. Grasith,
BEGINNING (1681852456.768384): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6538, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6569, val loss 4.6504 [2.9206836223602295 sec]
step 100: train loss 2.7009, val loss 2.7501 [7.823336839675903 sec]
step 200: train loss 2.5186, val loss 2.5873 [12.72275710105896 sec]
step 300: train loss 2.3930, val loss 2.4704 [17.26438808441162 sec]
step 400: train loss 2.2506, val loss 2.3673 [21.807597398757935 sec]
2.2212700843811035
Total Training Time: 23.602921962738037 seconds

of; TEI Appor Em hi da she bead- sinto blay freg., ane smat foodile
wantit thoowht
ho mil s maw wayo
BEGINNING (1681852480.9876573): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5843, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5749, val loss 4.5724 [4.106513261795044 sec]
step 100: train loss 2.6480, val loss 2.7083 [10.440089464187622 sec]
step 200: train loss 2.4819, val loss 2.5535 [16.773675441741943 sec]
step 300: train loss 2.3437, val loss 2.4368 [22.991928577423096 sec]
step 400: train loss 2.1828, val loss 2.3095 [29.443902254104614 sec]
2.118549346923828
Total Training Time: 31.926154136657715 seconds

toun mat his him, is, wile yore en sice arn four lon the an ath
jor ass briild, in
Che Grata padinge
BEGINNING (1681852513.764181): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6107, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6154, val loss 4.6188 [2.420374631881714 sec]
step 100: train loss 2.7536, val loss 2.7981 [6.613083362579346 sec]
step 200: train loss 2.5931, val loss 2.6608 [10.773093938827515 sec]
step 300: train loss 2.5271, val loss 2.5990 [14.568166017532349 sec]
step 400: train loss 2.4807, val loss 2.5605 [18.664031744003296 sec]
2.4801905155181885
Total Training Time: 20.220185041427612 seconds

Gan wiereresm fr at!"lld Weya herne Cce spave we fo a theadezookerout
cate peely ard t.omatonde
ered
BEGINNING (1681852534.3844914): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5721, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5669, val loss 4.5739 [3.789207935333252 sec]
step 100: train loss 2.6922, val loss 2.7493 [10.240076541900635 sec]
step 200: train loss 2.5472, val loss 2.6194 [16.52445936203003 sec]
step 300: train loss 2.4805, val loss 2.5545 [23.158720016479492 sec]
step 400: train loss 2.4242, val loss 2.5085 [29.55988645553589 sec]
2.4172186851501465
Total Training Time: 32.16302680969238 seconds

ucn trekl ed rof thiq uaned.
Tuad hed Yes slsurzat me ayofoubed ue tesed aves sa,
bist, aya t sint w
BEGINNING (1681852567.1705868): Baseline LR(0.0003) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6613, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6652, val loss 4.6584 [5.649446249008179 sec]
step 100: train loss 2.6782, val loss 2.7316 [14.393967151641846 sec]
step 200: train loss 2.5295, val loss 2.6018 [23.190571784973145 sec]
step 300: train loss 2.4551, val loss 2.5352 [31.78751516342163 sec]
step 400: train loss 2.3677, val loss 2.4545 [40.51170253753662 sec]
2.290395498275757
Total Training Time: 43.98456573486328 seconds

us byDe w˜h bl thak fey. âin.
"Gojathes wo igh. Arad owaid m hour renerof Nand Naveced
ad he trllllp
BEGINNING (1681852612.0617208): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6635, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6368, val loss 4.6275 [1.6890239715576172 sec]
step 100: train loss 2.6597, val loss 2.7077 [4.511748790740967 sec]
step 200: train loss 2.4526, val loss 2.5311 [7.479746580123901 sec]
step 300: train loss 2.2906, val loss 2.4028 [10.411498785018921 sec]
step 400: train loss 2.1756, val loss 2.3009 [13.417961835861206 sec]
2.2383813858032227
Total Training Time: 14.57099986076355 seconds

"Teead heis saw urcang the gom herous ief tyok s over, of Pus you his.idr smeftrontiang hav or oun t
BEGINNING (1681852627.0393875): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6413, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6571, val loss 4.6650 [2.8283920288085938 sec]
step 100: train loss 2.5947, val loss 2.6645 [7.618791580200195 sec]
step 200: train loss 2.3873, val loss 2.4650 [12.486392498016357 sec]
step 300: train loss 2.2169, val loss 2.3242 [17.313910722732544 sec]
step 400: train loss 2.0836, val loss 2.2267 [22.1444993019104 sec]
2.0134754180908203
Total Training Time: 24.202311277389526 seconds

Toklow, well, smeneas the wish youhn hvell
hn trour Grata the thir trick louered ta Eed and trind co
BEGINNING (1681852651.9844844): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6039, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5797, val loss 4.5860 [3.686833381652832 sec]
step 100: train loss 2.5856, val loss 2.6483 [10.41498064994812 sec]
step 200: train loss 2.3718, val loss 2.4549 [17.049615383148193 sec]
step 300: train loss 2.2003, val loss 2.3215 [23.55959725379944 sec]
step 400: train loss 2.0645, val loss 2.2181 [29.995206832885742 sec]
1.9459123611450195
Total Training Time: 33.08526301383972 seconds

hil stion wis."
S1 Pyour EA ISElent CHAMV save An indin marned. Anayah mas the soeeforearn to co sha
BEGINNING (1681852686.2701406): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6585, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6417, val loss 4.6359 [1.757561445236206 sec]
step 100: train loss 2.6584, val loss 2.7185 [4.69463586807251 sec]
step 200: train loss 2.5119, val loss 2.5844 [7.639402627944946 sec]
step 300: train loss 2.4114, val loss 2.4991 [10.577953100204468 sec]
step 400: train loss 2.2859, val loss 2.3988 [13.54095721244812 sec]
2.229776620864868
Total Training Time: 14.706820726394653 seconds

para gh le. Geane smale, bose na and ea att,Ses!" ay Co this d
TherO
Vithly ger of l's allorece mali
BEGINNING (1681852701.394806): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6569, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6481, val loss 4.6441 [2.7081258296966553 sec]
step 100: train loss 2.6089, val loss 2.6723 [7.5605552196502686 sec]
step 200: train loss 2.4545, val loss 2.5389 [12.524351835250854 sec]
step 300: train loss 2.3170, val loss 2.4151 [17.44813632965088 sec]
step 400: train loss 2.1712, val loss 2.2939 [22.208619594573975 sec]
2.130122661590576
Total Training Time: 24.279053926467896 seconds

mype gum. The uand Gratta vinersh mall. Whe
"Wentar meus tosked whas aved won we thil.
"Soledse hang
BEGINNING (1681852726.4325995): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.7205, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7285, val loss 4.7318 [3.7588202953338623 sec]
step 100: train loss 2.5853, val loss 2.6580 [10.181275367736816 sec]
step 200: train loss 2.4379, val loss 2.5226 [16.618969678878784 sec]
step 300: train loss 2.2747, val loss 2.3849 [23.79104733467102 sec]
step 400: train loss 2.1110, val loss 2.2426 [30.683725357055664 sec]
2.0688042640686035
Total Training Time: 33.86811399459839 seconds

hil Tak ar hin thouts tl cemegatst, and his es mene kpoulp."
Ked the hoy fres dle, had dimpat€al hop
BEGINNING (1681852761.4773502): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.7011, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6951, val loss 4.7007 [1.9978892803192139 sec]
step 100: train loss 2.6520, val loss 2.7140 [5.551867961883545 sec]
step 200: train loss 2.5370, val loss 2.6204 [9.610875129699707 sec]
step 300: train loss 2.4810, val loss 2.5590 [13.313494682312012 sec]
step 400: train loss 2.4226, val loss 2.5142 [17.046992778778076 sec]
2.4003052711486816
Total Training Time: 18.420066595077515 seconds

with istheld sie. Yond therreasede wheande
sdeam. It
some
afurell
tin at trnd a. soup TEAY
Awacst he
BEGINNING (1681852780.3625011): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6627, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6769, val loss 4.6777 [3.8928935527801514 sec]
step 100: train loss 2.6062, val loss 2.6716 [10.243745565414429 sec]
step 200: train loss 2.5007, val loss 2.5750 [16.49801015853882 sec]
step 300: train loss 2.4312, val loss 2.5205 [22.65568971633911 sec]
step 400: train loss 2.3496, val loss 2.4528 [29.574033737182617 sec]
2.2854771614074707
Total Training Time: 31.96177387237549 seconds

rear in, amonermoaty lowed toorse
on an arebs dan. Aront id nara horere aiveff meril
ot bs.
"Yenenee
BEGINNING (1681852813.124337): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6478, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6400, val loss 4.6399 [5.3666603565216064 sec]
step 100: train loss 2.5869, val loss 2.6523 [14.298041820526123 sec]
step 200: train loss 2.4844, val loss 2.5568 [22.929447412490845 sec]
step 300: train loss 2.4035, val loss 2.4889 [32.63410186767578 sec]
step 400: train loss 2.2963, val loss 2.3996 [41.34418964385986 sec]
2.199227809906006
Total Training Time: 44.73115277290344 seconds

he subfin as to was cren dien hack prin aght sfof cub hie byoug tof
beriljuI y. Grttaimacett, d herr
BEGINNING (1681852859.0547905): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5764, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5870, val loss 4.5863 [1.8428740501403809 sec]
step 100: train loss 2.6302, val loss 2.6932 [4.813944101333618 sec]
step 200: train loss 2.3901, val loss 2.4767 [7.86274790763855 sec]
step 300: train loss 2.2252, val loss 2.3387 [11.541764497756958 sec]
step 400: train loss 2.1144, val loss 2.2497 [14.497504711151123 sec]
2.142271041870117
Total Training Time: 15.689349174499512 seconds

atad la. SEAIX PivyUh
A1Mt. KACCHAATEANQG YE jERB way fred AY
TEKAPMcKV7ITM,I A
S;ing Graiatta surhe
BEGINNING (1681852875.1684365): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6071, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6308, val loss 4.6285 [2.702859878540039 sec]
step 100: train loss 2.5872, val loss 2.6576 [7.5827577114105225 sec]
step 200: train loss 2.3445, val loss 2.4316 [12.40713906288147 sec]
step 300: train loss 2.1497, val loss 2.2743 [17.132766008377075 sec]
step 400: train loss 2.0252, val loss 2.1713 [21.860295057296753 sec]
1.9654102325439453
Total Training Time: 23.908953189849854 seconds

may.
SEA Glatte as wony thad to you
Pyr%on? Nime Chal brrong
to smeidg as I Yathen
Ayah or,
anatirin
BEGINNING (1681852899.8221388): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5140, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5262, val loss 4.5224 [3.6224846839904785 sec]
step 100: train loss 2.5458, val loss 2.6262 [10.191552639007568 sec]
step 200: train loss 2.2985, val loss 2.4023 [16.6864595413208 sec]
step 300: train loss 2.1068, val loss 2.2415 [23.155887365341187 sec]
step 400: train loss 1.9675, val loss 2.1448 [29.70341467857361 sec]
1.9586719274520874
Total Training Time: 32.6643590927124 seconds

gay reare
ve cabe loke sa6 He siff notherp Pesire ras the ol ragaed. Namalk.
Anayah agfaif suled gun
BEGINNING (1681852933.5990038): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6516, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6457, val loss 4.6476 [1.958139419555664 sec]
step 100: train loss 2.6382, val loss 2.6975 [5.340243816375732 sec]
step 200: train loss 2.4810, val loss 2.5561 [8.466496467590332 sec]
step 300: train loss 2.3393, val loss 2.4374 [11.770590782165527 sec]
step 400: train loss 2.1834, val loss 2.2958 [15.124366998672485 sec]
2.179053783416748
Total Training Time: 16.384507656097412 seconds

brin lomp an com they
mim farayet anst wh hs pof wa looncel on acker wor hadeubser."
Anoce ea's same
BEGINNING (1681852950.4127285): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6971, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6849, val loss 4.6865 [3.004138469696045 sec]
step 100: train loss 2.5924, val loss 2.6609 [8.035420894622803 sec]
step 200: train loss 2.4356, val loss 2.5195 [12.966471195220947 sec]
step 300: train loss 2.2675, val loss 2.3738 [17.95198154449463 sec]
step 400: train loss 2.1024, val loss 2.2466 [22.951602458953857 sec]
2.0097343921661377
Total Training Time: 25.08772349357605 seconds

whed gand coll silled bell toghtons anent, aluy This
lan't thesis ionen." "Cand shas bere8
Pithe jub
BEGINNING (1681852976.2563133): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6020, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5973, val loss 4.5899 [4.0666258335113525 sec]
step 100: train loss 2.5629, val loss 2.6335 [11.340206384658813 sec]
step 200: train loss 2.3891, val loss 2.4785 [18.36015272140503 sec]
step 300: train loss 2.1959, val loss 2.3176 [25.348264932632446 sec]
step 400: train loss 2.0253, val loss 2.1811 [32.28648233413696 sec]
1.968438744544983
Total Training Time: 35.62399935722351 seconds

6ATEKAPThisQ of Grata hademmenef the ters oup 1
43
cor TEAY
–APE
AN wallynegan briy, Ack ploker.
Che
BEGINNING (1681853013.0454588): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6545, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6503, val loss 4.6562 [3.0276198387145996 sec]
step 100: train loss 2.6422, val loss 2.7003 [7.564083099365234 sec]
step 200: train loss 2.5237, val loss 2.5997 [12.027697324752808 sec]
step 300: train loss 2.4528, val loss 2.5324 [16.683650970458984 sec]
step 400: train loss 2.3704, val loss 2.4596 [21.35866379737854 sec]
2.2831900119781494
Total Training Time: 23.35786771774292 seconds

Penoncs wout ah cekvirswas, ck a yruston" on ow."
"Esell wed this wis hht sisto 7hhe tas thilatis ad
BEGINNING (1681853036.849722): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6597, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6564, val loss 4.6456 [5.138645887374878 sec]
step 100: train loss 2.6099, val loss 2.6727 [13.173642873764038 sec]
step 200: train loss 2.4961, val loss 2.5670 [21.354928493499756 sec]
step 300: train loss 2.4123, val loss 2.4960 [29.76768732070923 sec]
step 400: train loss 2.2972, val loss 2.3955 [38.08572268486023 sec]
2.246762275695801
Total Training Time: 41.1677303314209 seconds

CAMM3(
CAPTSEARIV "Wher min AN lichaghas midnt t has ricid co
3E DEr J€E APKAAPCAPTEAN AME4R we SN T
BEGINNING (1681853079.0259542): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6364, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6346, val loss 4.6268 [6.637903213500977 sec]
step 100: train loss 2.5797, val loss 2.6517 [17.512123346328735 sec]
step 200: train loss 2.4711, val loss 2.5465 [27.953883409500122 sec]
step 300: train loss 2.3733, val loss 2.4658 [38.647778034210205 sec]
step 400: train loss 2.2103, val loss 2.3364 [48.94529438018799 sec]
2.13649845123291
Total Training Time: 53.36959767341614 seconds

toded and ordokr haras, smis.
"Tay hys sas somekere
soubsay thaing te an sle. "Ye sins hiss gal-, ha
BEGINNING (1681853133.6548526): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5644, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5615, val loss 4.5705 [2.2657270431518555 sec]
step 100: train loss 2.6087, val loss 2.6755 [5.615673303604126 sec]
step 200: train loss 2.3531, val loss 2.4506 [8.952571868896484 sec]
step 300: train loss 2.1780, val loss 2.3095 [12.311346054077148 sec]
step 400: train loss 2.0745, val loss 2.2274 [15.891379594802856 sec]
2.0575077533721924
Total Training Time: 17.18027091026306 seconds

a fernadd. I fors hap nad and ta. Wliok. He was you moked and ad and on do s degilp. Ml4
I TEl the f
BEGINNING (1681853151.2518346): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5964, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6026, val loss 4.6042 [3.1438634395599365 sec]
step 100: train loss 2.5467, val loss 2.6134 [8.298224925994873 sec]
step 200: train loss 2.2830, val loss 2.3866 [13.575217485427856 sec]
step 300: train loss 2.0948, val loss 2.2327 [18.59486961364746 sec]
step 400: train loss 1.9777, val loss 2.1329 [23.84086585044861 sec]
2.0108320713043213
Total Training Time: 25.974947929382324 seconds

larre fat the rubs Faxch turney w
sall hie coung callls fentent your at ton to willy, yua3 Namall
ut
BEGINNING (1681853177.9556515): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6589, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6618, val loss 4.6595 [4.07543158531189 sec]
step 100: train loss 2.5343, val loss 2.6020 [11.451013803482056 sec]
step 200: train loss 2.2653, val loss 2.3740 [18.79493737220764 sec]
step 300: train loss 2.0625, val loss 2.2108 [25.721386432647705 sec]
step 400: train loss 1.9235, val loss 2.0852 [32.66229748725891 sec]
1.9487942457199097
Total Training Time: 35.78611612319946 seconds

Compare atat to of took
a's tor, as stican of I
ce to
his stiowle!" Aid smon thas rellows ilf Thes o
BEGINNING (1681853214.936586): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6516, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6439, val loss 4.6345 [2.1940252780914307 sec]
step 100: train loss 2.6299, val loss 2.6913 [5.977914333343506 sec]
step 200: train loss 2.4591, val loss 2.5384 [9.767279148101807 sec]
step 300: train loss 2.2834, val loss 2.3890 [13.488269567489624 sec]
step 400: train loss 2.1346, val loss 2.2677 [17.253741979599 sec]
2.1064412593841553
Total Training Time: 18.735552549362183 seconds

"Cowaspill anat the us pow's mangut
was there
gor, Grathate.""Yan ille he fouls ince
nah md wittilly
BEGINNING (1681853234.1006522): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5624, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5722, val loss 4.5647 [3.4976720809936523 sec]
step 100: train loss 2.5775, val loss 2.6453 [10.084563970565796 sec]
step 200: train loss 2.4069, val loss 2.4906 [16.33691930770874 sec]
step 300: train loss 2.1997, val loss 2.3232 [22.553702354431152 sec]
step 400: train loss 2.0232, val loss 2.1901 [28.78599977493286 sec]
2.0156235694885254
Total Training Time: 31.29752278327942 seconds

to Rubordion the hap with saw awill brobe was to abe foreations smed he
cht ive bece hing hate had."
BEGINNING (1681853266.2869503): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5702, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5651, val loss 4.5589 [4.786728858947754 sec]
step 100: train loss 2.5539, val loss 2.6279 [13.574745416641235 sec]
step 200: train loss 2.3711, val loss 2.4635 [21.63466167449951 sec]
step 300: train loss 2.1616, val loss 2.2985 [29.71521806716919 sec]
step 400: train loss 1.9861, val loss 2.1580 [38.61301255226135 sec]
1.9479014873504639
Total Training Time: 41.96842074394226 seconds

A6f – HAYou.f Theuws harchells
wo dell, the ce wes ren and uncirth.
"I bice as rast dmavernst heppan
BEGINNING (1681853309.453786): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5397, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5444, val loss 4.5606 [3.652665853500366 sec]
step 100: train loss 2.6342, val loss 2.7026 [9.741934299468994 sec]
step 200: train loss 2.5159, val loss 2.5928 [15.556607723236084 sec]
step 300: train loss 2.4506, val loss 2.5316 [21.817853927612305 sec]
step 400: train loss 2.3650, val loss 2.4619 [27.491891622543335 sec]
2.298219919204712
Total Training Time: 29.88616108894348 seconds

This aly – anxd ight takmelfer. "Bissil bu dist hith?"
y asterouon ieis des G
Alof fos winen mean ow
BEGINNING (1681853339.7970202): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5907, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5863, val loss 4.5976 [5.829906225204468 sec]
step 100: train loss 2.6028, val loss 2.6705 [15.924468517303467 sec]
step 200: train loss 2.4816, val loss 2.5626 [25.836472272872925 sec]
step 300: train loss 2.3819, val loss 2.4763 [36.464499711990356 sec]
step 400: train loss 2.2195, val loss 2.3377 [47.0208306312561 sec]
2.1681880950927734
Total Training Time: 51.53495264053345 seconds

ug had mill. To "We ith miel to fa.
Arallen'sf ifidndened lonke wecainrerted was them shts
sand ste 
BEGINNING (1681853392.1767223): Baseline LR(0.0003) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6328, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6271, val loss 4.6368 [8.179237127304077 sec]
step 100: train loss 2.5704, val loss 2.6389 [22.967235803604126 sec]
step 200: train loss 2.4481, val loss 2.5320 [37.4401695728302 sec]
step 300: train loss 2.3384, val loss 2.4372 [52.72168850898743 sec]
step 400: train loss 2.1656, val loss 2.2890 [67.11155581474304 sec]
2.074859619140625
Total Training Time: 73.0722975730896 seconds

fell ant to arden buretentelyah coy he
plasmithe wrous sower und and as schis alarg, abed biaigh
alo
BEGINNING (1681853466.4085875): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5967, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5941, val loss 4.5931 [1.9278254508972168 sec]
step 100: train loss 2.5872, val loss 2.6524 [5.179841041564941 sec]
step 200: train loss 2.3414, val loss 2.4312 [8.654113292694092 sec]
step 300: train loss 2.1946, val loss 2.3237 [12.04141116142273 sec]
step 400: train loss 2.0809, val loss 2.2319 [15.332396030426025 sec]
2.013308048248291
Total Training Time: 16.66723871231079 seconds

had tao millike aries wir could an the Gratratta. Atta wos ladd,  we hity hin han deneastwit om ther
BEGINNING (1681853483.5681672): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6659, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6483, val loss 4.6412 [3.087820053100586 sec]
step 100: train loss 2.5487, val loss 2.6075 [8.702360391616821 sec]
step 200: train loss 2.3017, val loss 2.4008 [14.500226020812988 sec]
step 300: train loss 2.1280, val loss 2.2510 [20.155683517456055 sec]
step 400: train loss 1.9858, val loss 2.1640 [25.974419355392456 sec]
1.9433560371398926
Total Training Time: 28.566452026367188 seconds

that yous Grod fal weorts El V We in Grata
cuzs had the as beatrountroon looke proover skids. The at
BEGINNING (1681853513.0958223): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6109, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6174, val loss 4.6219 [4.3250038623809814 sec]
step 100: train loss 2.5217, val loss 2.6066 [12.059939622879028 sec]
step 200: train loss 2.2680, val loss 2.3757 [19.853853464126587 sec]
step 300: train loss 2.0786, val loss 2.2364 [27.713595151901245 sec]
step 400: train loss 1.9523, val loss 2.1198 [35.86587858200073 sec]
1.8643720149993896
Total Training Time: 39.36903405189514 seconds

Oce not the he fouve vo thats's whek
insst, noded the o werrporght?"
Atheer lanced the lowned back h
BEGINNING (1681853553.8277466): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6332, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6285, val loss 4.6221 [1.9144558906555176 sec]
step 100: train loss 2.5833, val loss 2.6558 [5.277011156082153 sec]
step 200: train loss 2.4385, val loss 2.5199 [8.676952123641968 sec]
step 300: train loss 2.2843, val loss 2.3917 [12.088653326034546 sec]
step 400: train loss 2.1290, val loss 2.2667 [15.45106029510498 sec]
2.070894956588745
Total Training Time: 16.82425332069397 seconds

AN
EC I Youw buthiorsin has allsewn dEy over beabs Grat n't acke smarhe
warkeas, ak the the coupt Gr
BEGINNING (1681853571.2387364): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6819, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6757, val loss 4.6737 [3.3864500522613525 sec]
step 100: train loss 2.5634, val loss 2.6251 [9.032899379730225 sec]
step 200: train loss 2.3965, val loss 2.4863 [14.822964668273926 sec]
step 300: train loss 2.2044, val loss 2.3279 [20.58727717399597 sec]
step 400: train loss 2.0442, val loss 2.1893 [26.12042760848999 sec]
2.0481760501861572
Total Training Time: 28.580722332000732 seconds

smile bere ucheve it the twerien not
to in forwassthad ran w thernowan waosith.
©rirs noter3S
SEAB~E
BEGINNING (1681853600.7584245): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6603, val loss 4.6563 [4.316728353500366 sec]
step 100: train loss 2.5264, val loss 2.5918 [12.47998595237732 sec]
step 200: train loss 2.3730, val loss 2.4665 [20.64479660987854 sec]
step 300: train loss 2.1719, val loss 2.3021 [28.60802721977234 sec]
step 400: train loss 1.9966, val loss 2.1556 [36.728041887283325 sec]
1.9561891555786133
Total Training Time: 40.37937331199646 seconds

in of the, bo witiugh driestress. "Ch viefur do fe the st ot that
of kip they, up as tuom ias.
The f
BEGINNING (1681853642.5188859): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5770, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5665, val loss 4.5624 [2.4590752124786377 sec]
step 100: train loss 2.5855, val loss 2.6580 [6.950765609741211 sec]
step 200: train loss 2.4850, val loss 2.5691 [11.263065814971924 sec]
step 300: train loss 2.4122, val loss 2.5063 [15.669050931930542 sec]
step 400: train loss 2.2956, val loss 2.4045 [20.5003821849823 sec]
2.245884418487549
Total Training Time: 22.21201992034912 seconds

3wast war erald to witenshke. And
d hy to dey trin ayat imaidyout Ond, Twou
milonisell "Nit ther the
BEGINNING (1681853665.2440991): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5664, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5744, val loss 4.5695 [4.224117994308472 sec]
step 100: train loss 2.5486, val loss 2.6207 [11.561996698379517 sec]
step 200: train loss 2.4625, val loss 2.5395 [18.97969698905945 sec]
step 300: train loss 2.3704, val loss 2.4599 [26.3887140750885 sec]
step 400: train loss 2.2171, val loss 2.3346 [33.57383394241333 sec]
2.146226167678833
Total Training Time: 36.689380168914795 seconds

lecrshis brogh af wa Tt hackead e wily."His Arqut,
Nais le id aror thee Chey
mouwarilncis il ht ing

BEGINNING (1681853702.9947162): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5695, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5719, val loss 4.5759 [5.690501689910889 sec]
step 100: train loss 2.5493, val loss 2.6177 [16.334016799926758 sec]
step 200: train loss 2.4443, val loss 2.5304 [26.980373859405518 sec]
step 300: train loss 2.3311, val loss 2.4378 [37.65979719161987 sec]
step 400: train loss 2.1286, val loss 2.2612 [48.06309461593628 sec]
2.04482364654541
Total Training Time: 52.41234230995178 seconds

of bith aveghe s heell an. Wh. The tuoupl in on. He is gwill arot
bew cat mad sprat.
9"Youist me ple
BEGINNING (1681853756.8625078): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6083, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6125, val loss 4.6121 [2.1123533248901367 sec]
step 100: train loss 2.5613, val loss 2.6389 [5.559316158294678 sec]
step 200: train loss 2.2913, val loss 2.3895 [9.198911666870117 sec]
step 300: train loss 2.1233, val loss 2.2638 [13.153609037399292 sec]
step 400: train loss 2.0108, val loss 2.1702 [16.489635944366455 sec]
1.9942048788070679
Total Training Time: 17.862176418304443 seconds

Aithe mastranyah fors shuig
Anayyah's ondyer und hin hom culland
amenaempls.
"I marty he call aid th
BEGINNING (1681853775.223683): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6098, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6192, val loss 4.6239 [3.134028673171997 sec]
step 100: train loss 2.5062, val loss 2.5850 [8.765014886856079 sec]
step 200: train loss 2.2278, val loss 2.3425 [14.85150694847107 sec]
step 300: train loss 2.0434, val loss 2.1989 [20.258249282836914 sec]
step 400: train loss 1.9183, val loss 2.0898 [25.680046796798706 sec]
1.8721240758895874
Total Training Time: 27.92374610900879 seconds

a fare cubs this smTerraill thes.
• sut9 Thell umpled?" "Do
"Ve you strostrely arly Pyrrased anked
y
BEGINNING (1681853804.0376651): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.7469, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7405, val loss 4.7421 [4.935555458068848 sec]
step 100: train loss 2.4822, val loss 2.5675 [12.531441926956177 sec]
step 200: train loss 2.2217, val loss 2.3326 [20.021595001220703 sec]
step 300: train loss 2.0320, val loss 2.1821 [27.771095275878906 sec]
step 400: train loss 1.8908, val loss 2.0788 [35.67971396446228 sec]
1.7898637056350708
Total Training Time: 39.01409125328064 seconds

Cle berud word his paws duzls of arriithem."
"Anayah as sood look cound alled
strached to of the Pyr
BEGINNING (1681853844.3368826): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6725, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6681, val loss 4.6657 [2.1530535221099854 sec]
step 100: train loss 2.5795, val loss 2.6476 [5.593204975128174 sec]
step 200: train loss 2.3905, val loss 2.4785 [9.033350706100464 sec]
step 300: train loss 2.1942, val loss 2.3177 [12.68388056755066 sec]
step 400: train loss 2.0692, val loss 2.2164 [16.091910362243652 sec]
1.9936271905899048
Total Training Time: 17.450404167175293 seconds

unbowng wed arlooke of stby en thor Che Py Ell fithe
morid and ce po Pir him. "El; mou gy of the Ret
BEGINNING (1681853862.256829): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6171, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6160, val loss 4.6311 [3.5346293449401855 sec]
step 100: train loss 2.5459, val loss 2.6196 [9.392065525054932 sec]
step 200: train loss 2.3564, val loss 2.4504 [15.176226377487183 sec]
step 300: train loss 2.1375, val loss 2.2668 [21.233198404312134 sec]
step 400: train loss 1.9623, val loss 2.1251 [27.202672958374023 sec]
1.9247077703475952
Total Training Time: 29.585965156555176 seconds

chamen graig and lookies read, thed.
"
Though efor and woor pusemarnoon a nowed frocquit
doned.
"I P
BEGINNING (1681853892.7682955): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5851, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5851, val loss 4.5864 [4.596438407897949 sec]
step 100: train loss 2.5063, val loss 2.5729 [12.60338306427002 sec]
step 200: train loss 2.3096, val loss 2.4054 [20.479859113693237 sec]
step 300: train loss 2.0801, val loss 2.2288 [28.360459566116333 sec]
step 400: train loss 1.8931, val loss 2.0689 [36.26256203651428 sec]
1.8311983346939087
Total Training Time: 39.69192337989807 seconds

CEDION "D. W" nown my iceny of onf an ton
Chang bahed our, bot the befrom tookire
for the care dist.
BEGINNING (1681853933.8105085): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6487, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6452, val loss 4.6483 [2.8886847496032715 sec]
step 100: train loss 2.5853, val loss 2.6528 [7.81833291053772 sec]
step 200: train loss 2.4780, val loss 2.5542 [12.71448802947998 sec]
step 300: train loss 2.3958, val loss 2.4864 [17.72563147544861 sec]
step 400: train loss 2.2819, val loss 2.3901 [22.69140100479126 sec]
2.1699230670928955
Total Training Time: 24.788320064544678 seconds

C2ANTE Na €of 3S
CHiourppiAYMDled as had spared be overnides t aved hepprears wo fin
4
CEID˜"TEUY
SI
BEGINNING (1681853959.1459608): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6109, val loss 4.6124 [5.038485050201416 sec]
step 100: train loss 2.5422, val loss 2.6207 [14.0628080368042 sec]
step 200: train loss 2.4308, val loss 2.5139 [22.83912467956543 sec]
step 300: train loss 2.3012, val loss 2.4103 [31.488813400268555 sec]
step 400: train loss 2.1104, val loss 2.2561 [40.113088846206665 sec]
2.0678937435150146
Total Training Time: 43.74685883522034 seconds

are and ae sme trowill goroucl-t wing
youtre El dore youhis pegh ther aircourviegs. Yeimon is
will g
BEGINNING (1681854003.845151): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6213, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6287, val loss 4.6195 [7.041625261306763 sec]
step 100: train loss 2.5320, val loss 2.6067 [19.427764892578125 sec]
step 200: train loss 2.4201, val loss 2.5071 [31.99845552444458 sec]
step 300: train loss 2.2620, val loss 2.3672 [44.61405348777771 sec]
step 400: train loss 2.0223, val loss 2.1831 [57.3023898601532 sec]
1.9305453300476074
Total Training Time: 62.792269229888916 seconds

G7˜'MB SEM)
EAN – fleKA lart bow mould fard faind elled sa3, a E
Anathaf if BuBenyacopheropll sow! m
BEGINNING (1681854068.0528557): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6668, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6621, val loss 4.6605 [2.300351619720459 sec]
step 100: train loss 2.5400, val loss 2.6119 [6.133792161941528 sec]
step 200: train loss 2.2524, val loss 2.3582 [9.709055662155151 sec]
step 300: train loss 2.0777, val loss 2.2186 [13.21105408668518 sec]
step 400: train loss 1.9621, val loss 2.1306 [16.895291328430176 sec]
2.0192739963531494
Total Training Time: 18.18380904197693 seconds

tuor
goed
to dide.
Gratta ato in frow a scorthe rigem. Tikn the Gratta. Hickatt Bearse the
juPrumare
BEGINNING (1681854086.7132397): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6458, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6461, val loss 4.6556 [3.26755952835083 sec]
step 100: train loss 2.4851, val loss 2.5629 [8.903992891311646 sec]
step 200: train loss 2.1871, val loss 2.3054 [14.583806276321411 sec]
step 300: train loss 1.9928, val loss 2.1512 [20.460873126983643 sec]
step 400: train loss 1.8679, val loss 2.0549 [26.317796230316162 sec]
1.8470585346221924
Total Training Time: 28.79699420928955 seconds

QuN of rachue.
Gratta s bnothed up intirst. Thaking thors
saifterealYed Anayah now at the ot
enugh a
BEGINNING (1681854116.4961638): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5036, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4908, val loss 4.5032 [4.586143970489502 sec]
step 100: train loss 2.4465, val loss 2.5261 [12.528931856155396 sec]
step 200: train loss 2.1518, val loss 2.2779 [20.494678735733032 sec]
step 300: train loss 1.9450, val loss 2.1102 [28.289628267288208 sec]
step 400: train loss 1.7976, val loss 2.0185 [37.208064556121826 sec]
1.7206817865371704
Total Training Time: 41.679396867752075 seconds

EAY N as and lown. I will look "Gotterpha smor we
but wile nottled tima fend. Whe ut
a you dumS
sise
BEGINNING (1681854159.7641346): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6512, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6488, val loss 4.6601 [2.4235122203826904 sec]
step 100: train loss 2.5700, val loss 2.6419 [6.4637978076934814 sec]
step 200: train loss 2.3987, val loss 2.4845 [10.716254949569702 sec]
step 300: train loss 2.1887, val loss 2.3076 [15.015125036239624 sec]
step 400: train loss 2.0431, val loss 2.2011 [19.356969833374023 sec]
2.021007776260376
Total Training Time: 21.06497097015381 seconds

SERSER for and thee maned?"
"I Gratta thes severe orshis teppoined.
Thinfte this crut Efacen Chieffu
BEGINNING (1681854181.3411398): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6005, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6027, val loss 4.5967 [3.965897560119629 sec]
step 100: train loss 2.5211, val loss 2.5954 [11.047303676605225 sec]
step 200: train loss 2.3185, val loss 2.4157 [18.416332483291626 sec]
step 300: train loss 2.0772, val loss 2.2330 [26.545499324798584 sec]
step 400: train loss 1.9009, val loss 2.0815 [33.770686626434326 sec]
1.8434982299804688
Total Training Time: 36.80144906044006 seconds

fat bre pret waul elitin the had alke the
walt be gach frosshat famie, ham he ladde
tuone thatwings 
BEGINNING (1681854219.1343555): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6582, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6365, val loss 4.6446 [5.680153131484985 sec]
step 100: train loss 2.4995, val loss 2.5771 [15.499892473220825 sec]
step 200: train loss 2.2927, val loss 2.4003 [24.859912395477295 sec]
step 300: train loss 2.0362, val loss 2.1882 [34.04306244850159 sec]
step 400: train loss 1.8576, val loss 2.0447 [43.22107458114624 sec]
1.8212047815322876
Total Training Time: 47.05850625038147 seconds

ashim plige inkion is the."
Gratta led taked per won't ind smelid stededep. Ot an as bs
his maeuust 
BEGINNING (1681854267.5885344): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6251, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6297, val loss 4.6244 [3.5540740489959717 sec]
step 100: train loss 2.5777, val loss 2.6500 [9.708284139633179 sec]
step 200: train loss 2.4609, val loss 2.5412 [15.878959655761719 sec]
step 300: train loss 2.3569, val loss 2.4515 [22.123456478118896 sec]
step 400: train loss 2.1925, val loss 2.3120 [28.26217794418335 sec]
2.083629846572876
Total Training Time: 30.75474262237549 seconds

wasemmantt treaka san, tably."
A hed phazirs, weaI cailsed, and yebat use
ruchece malit Nommat?
Grab
BEGINNING (1681854298.832395): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6527, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6620, val loss 4.6667 [6.08430027961731 sec]
step 100: train loss 2.5443, val loss 2.6155 [16.918898582458496 sec]
step 200: train loss 2.4291, val loss 2.5206 [27.694571018218994 sec]
step 300: train loss 2.2921, val loss 2.4008 [38.63969898223877 sec]
step 400: train loss 2.0969, val loss 2.2392 [56.163191080093384 sec]
1.9856997728347778
Total Training Time: 61.225687980651855 seconds

thow seek. The afoughfe el."
Gratta theyarded his Plearlall
the byahe sief at wou mas The a an no so
BEGINNING (1681854360.956237): Baseline LR(0.0003) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6292, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6296, val loss 4.6307 [9.784896612167358 sec]
step 100: train loss 2.5234, val loss 2.5992 [24.783017873764038 sec]
step 200: train loss 2.3985, val loss 2.4924 [40.060648679733276 sec]
step 300: train loss 2.2111, val loss 2.3297 [55.14509606361389 sec]
step 400: train loss 2.0045, val loss 2.1766 [70.17443680763245 sec]
1.917988896369934
Total Training Time: 76.5898334980011 seconds

les thoure the crobles,
fald then en had farr og as a dold the allin takesthe
seow thir ourteny of G
BEGINNING (1681854438.843307): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5631, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5544, val loss 4.5647 [2.2616753578186035 sec]
step 100: train loss 2.4843, val loss 2.5692 [6.1417396068573 sec]
step 200: train loss 2.2094, val loss 2.3467 [9.909277200698853 sec]
step 300: train loss 2.0391, val loss 2.2098 [13.670936584472656 sec]
step 400: train loss 1.9389, val loss 2.1289 [17.425074577331543 sec]
1.8539713621139526
Total Training Time: 18.938769578933716 seconds

o’ Weuzirs." Go6 Takne four mesaid, coved enceb, at forias monary to rable, ange thevin wiir
cors ma
BEGINNING (1681854458.4132047): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6011, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5872, val loss 4.5826 [3.868527889251709 sec]
step 100: train loss 2.4538, val loss 2.5303 [10.678956985473633 sec]
step 200: train loss 2.1760, val loss 2.2997 [17.346736669540405 sec]
step 300: train loss 1.9980, val loss 2.1552 [23.972167253494263 sec]
step 400: train loss 1.8678, val loss 2.0607 [30.65676498413086 sec]
1.8304108381271362
Total Training Time: 33.52352738380432 seconds

werelted and the manaps. He twermes
all anuSle." Gratta lookionly a pood, ass ivear, lifing, facks h
BEGINNING (1681854493.1111543): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5712, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6016, val loss 4.5949 [5.370494604110718 sec]
step 100: train loss 2.4424, val loss 2.5317 [14.93116021156311 sec]
step 200: train loss 2.1497, val loss 2.2891 [24.53974485397339 sec]
step 300: train loss 1.9517, val loss 2.1239 [34.13324999809265 sec]
step 400: train loss 1.8400, val loss 2.0450 [45.41561508178711 sec]
1.7775318622589111
Total Training Time: 49.712780714035034 seconds

17
SeAN THE Arpassal sewil oupon and out the tume. Namamalted to nelh manuw,
and now as rellegive."

BEGINNING (1681854544.6231434): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6589, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6614, val loss 4.6470 [2.3603100776672363 sec]
step 100: train loss 2.5104, val loss 2.5908 [6.360522031784058 sec]
step 200: train loss 2.3144, val loss 2.4208 [10.41017746925354 sec]
step 300: train loss 2.1018, val loss 2.2536 [14.514812469482422 sec]
step 400: train loss 1.9685, val loss 2.1478 [18.628090620040894 sec]
1.8790861368179321
Total Training Time: 20.217392206192017 seconds

Pele som they atheuzriall
soll, byahke oleght laas nor thee for twed thiss.
Anas hickleclfted. Gratt
BEGINNING (1681854565.548748): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6085, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6253, val loss 4.6294 [4.070418357849121 sec]
step 100: train loss 2.4889, val loss 2.5579 [11.207351684570312 sec]
step 200: train loss 2.2733, val loss 2.3749 [18.79898977279663 sec]
step 300: train loss 2.0258, val loss 2.1896 [26.057719707489014 sec]
step 400: train loss 1.8675, val loss 2.0503 [33.18951416015625 sec]
1.8084166049957275
Total Training Time: 36.36460995674133 seconds

k brwout to the rivirss cubve be core still, but witurn
of that maeuwn, ban gany have seradred to li
BEGINNING (1681854603.164948): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5515, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5167, val loss 4.5043 [5.850981712341309 sec]
step 100: train loss 2.4838, val loss 2.5513 [16.4219229221344 sec]
step 200: train loss 2.2621, val loss 2.3690 [26.763235807418823 sec]
step 300: train loss 2.0036, val loss 2.1648 [37.14405179023743 sec]
step 400: train loss 1.8326, val loss 2.0315 [47.58132219314575 sec]
1.8144830465316772
Total Training Time: 52.06710600852966 seconds

cheet to. Whis we could fer slooke whightery was
k yah nuet with butts mans you the thace his
nort f
BEGINNING (1681854657.054409): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5528, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5556, val loss 4.5569 [3.2196664810180664 sec]
step 100: train loss 2.5330, val loss 2.6087 [8.974913358688354 sec]
step 200: train loss 2.4212, val loss 2.5013 [14.547420024871826 sec]
step 300: train loss 2.2876, val loss 2.3904 [20.03036117553711 sec]
step 400: train loss 2.1019, val loss 2.2505 [25.647557735443115 sec]
2.0157175064086914
Total Training Time: 28.03950786590576 seconds

"Wo shisuon treat, "Yould and none che
lownight pary. It she wole wor hor this. 36
CHANa– HAPTEAPT T
BEGINNING (1681854685.7449262): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6207, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6401, val loss 4.6372 [5.860076904296875 sec]
step 100: train loss 2.5123, val loss 2.5964 [16.139185190200806 sec]
step 200: train loss 2.4044, val loss 2.4941 [26.472888946533203 sec]
step 300: train loss 2.2188, val loss 2.3341 [36.80023384094238 sec]
step 400: train loss 1.9973, val loss 2.1649 [47.25548505783081 sec]
1.9162380695343018
Total Training Time: 51.75837159156799 seconds

tecer ithey caven of tle."
Grat, ving to cany a cored ind fabe that him se
grebut hat smonl. ârdeyer
BEGINNING (1681854738.8117113): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5781, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5813, val loss 4.5725 [8.299761295318604 sec]
step 100: train loss 2.5002, val loss 2.5965 [24.356939792633057 sec]
step 200: train loss 2.3734, val loss 2.4730 [40.14207124710083 sec]
step 300: train loss 2.1447, val loss 2.2817 [55.759682178497314 sec]
step 400: train loss 1.9316, val loss 2.1109 [71.17620491981506 sec]
1.8956836462020874
Total Training Time: 77.71267414093018 seconds

Gratta towned couldented mauws wathe bursChimef unne yound
ge to seek aswerms bation, wan' inly fom 
BEGINNING (1681854818.3496082): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5813, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5823, val loss 4.5692 [2.5314390659332275 sec]
step 100: train loss 2.4597, val loss 2.5380 [6.611963510513306 sec]
step 200: train loss 2.1540, val loss 2.2819 [10.656956434249878 sec]
step 300: train loss 1.9854, val loss 2.1554 [14.648728370666504 sec]
step 400: train loss 1.8786, val loss 2.0611 [18.616238832473755 sec]
1.9039031267166138
Total Training Time: 20.199169158935547 seconds

prives."
Gratta trabound bet looked buzile a
from therust. Him heave er
elpody Greath had to Tne to 
BEGINNING (1681854839.2004797): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.7007, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6986, val loss 4.6955 [3.899508237838745 sec]
step 100: train loss 2.4203, val loss 2.5054 [10.684946775436401 sec]
step 200: train loss 2.0867, val loss 2.2282 [17.42916440963745 sec]
step 300: train loss 1.9013, val loss 2.0850 [24.39404582977295 sec]
step 400: train loss 1.7790, val loss 2.0004 [31.32076072692871 sec]
1.6959201097488403
Total Training Time: 34.19265174865723 seconds

ruidinsPelp a and oness. Gratta startabned and than The astall repoongiver deir
inper with they noth
BEGINNING (1681854874.568297): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6576, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6560, val loss 4.6573 [5.410451412200928 sec]
step 100: train loss 2.4105, val loss 2.4856 [15.08727741241455 sec]
step 200: train loss 2.0877, val loss 2.2359 [24.783007621765137 sec]
step 300: train loss 1.8818, val loss 2.0488 [34.38932013511658 sec]
step 400: train loss 1.7529, val loss 1.9741 [44.128782749176025 sec]
1.721822738647461
Total Training Time: 48.37830090522766 seconds

and Elyor this do left this anreat -ort But int The
Surnes landsea's."
Genta rall!"
"What your ty
hi
BEGINNING (1681854924.71039): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.7533, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7531, val loss 4.7558 [2.7324862480163574 sec]
step 100: train loss 2.5065, val loss 2.5730 [7.546774387359619 sec]
step 200: train loss 2.2696, val loss 2.3746 [12.188916444778442 sec]
step 300: train loss 2.0423, val loss 2.2004 [16.90870690345764 sec]
step 400: train loss 1.8997, val loss 2.0907 [21.4633731842041 sec]
1.944749355316162
Total Training Time: 23.33448815345764 seconds

2S in they todden, und
slord humps arearts a him onsent tly for whiorn, and anday hoon,
have thookin
BEGINNING (1681854948.6959658): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5354, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5372, val loss 4.5435 [4.656712293624878 sec]
step 100: train loss 2.4696, val loss 2.5560 [12.691555976867676 sec]
step 200: train loss 2.2152, val loss 2.3304 [20.912248611450195 sec]
step 300: train loss 1.9466, val loss 2.1193 [28.989037036895752 sec]
step 400: train loss 1.7799, val loss 1.9809 [37.059454917907715 sec]
1.7424521446228027
Total Training Time: 40.47619652748108 seconds

"Some, "I ackint the we sneoach alvo foll ing tuon.
He to e ofatcil amy any exce semped, end
a Anaya
BEGINNING (1681854990.3847556): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5963, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5929, val loss 4.5965 [6.722794532775879 sec]
step 100: train loss 2.4494, val loss 2.5328 [18.531712532043457 sec]
step 200: train loss 2.1614, val loss 2.2893 [30.13825511932373 sec]
step 300: train loss 1.8896, val loss 2.0777 [41.71078395843506 sec]
step 400: train loss 1.7292, val loss 1.9461 [53.30528163909912 sec]
1.670946478843689
Total Training Time: 58.317256927490234 seconds

70 THE SE BIP"Ned ound Kana's weaturne
onterser to Trie or had no. So eyas met off-
woure freacith t
BEGINNING (1681855050.4803572): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6449, val loss 4.6462 [4.266475200653076 sec]
step 100: train loss 2.5227, val loss 2.6062 [11.661876678466797 sec]
step 200: train loss 2.4038, val loss 2.4999 [19.413682460784912 sec]
step 300: train loss 2.2263, val loss 2.3517 [27.05466103553772 sec]
step 400: train loss 2.0063, val loss 2.1706 [34.78782367706299 sec]
1.9272311925888062
Total Training Time: 37.97529911994934 seconds

a was to withaiklow len of ow, noyah our didgat bous any. El
twek are laterncrrich othad diesfuon sh
BEGINNING (1681855089.1059968): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5970, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5817, val loss 4.5895 [7.855226516723633 sec]
step 100: train loss 2.4994, val loss 2.5810 [21.822222471237183 sec]
step 200: train loss 2.3660, val loss 2.4672 [35.73250102996826 sec]
step 300: train loss 2.1297, val loss 2.2749 [49.6799578666687 sec]
step 400: train loss 1.9146, val loss 2.0993 [63.632354974746704 sec]
1.8649684190750122
Total Training Time: 69.62800526618958 seconds

aflower Gratta figh ing the had Arphadned ave snstsicem.
"Atted, usk and but beuchs tuone't
to at ll
BEGINNING (1681855159.9740686): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6175, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6184, val loss 4.6220 [11.201815128326416 sec]
step 100: train loss 2.4881, val loss 2.5676 [31.20216178894043 sec]
step 200: train loss 2.3381, val loss 2.4442 [51.10005259513855 sec]
step 300: train loss 2.0582, val loss 2.2094 [71.0220673084259 sec]
step 400: train loss 1.8421, val loss 2.0464 [90.9553689956665 sec]
1.7820051908493042
Total Training Time: 99.64791297912598 seconds

Aidd cus wouldirgh Kan King trough
Prrias and tas saure sque ities, and deche sled tining the
side. 
BEGINNING (1681855261.5443747): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6457, val loss 4.6407 [2.734769344329834 sec]
step 100: train loss 2.4280, val loss 2.5170 [7.139407634735107 sec]
step 200: train loss 2.1096, val loss 2.2581 [11.497699737548828 sec]
step 300: train loss 1.9421, val loss 2.1012 [15.84170413017273 sec]
step 400: train loss 1.8356, val loss 2.0405 [20.201796293258667 sec]
1.8472110033035278
Total Training Time: 21.86656379699707 seconds

the aron exd teled and puon him pur the
cir.
Rou Verowline the wal have so dide onty ten, thas smaul
BEGINNING (1681855284.0386865): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6169, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5863, val loss 4.5861 [4.266903877258301 sec]
step 100: train loss 2.3695, val loss 2.4634 [11.632018327713013 sec]
step 200: train loss 2.0489, val loss 2.2052 [18.954484224319458 sec]
step 300: train loss 1.8556, val loss 2.0380 [26.38750171661377 sec]
step 400: train loss 1.7466, val loss 1.9742 [33.7012825012207 sec]
1.6598109006881714
Total Training Time: 36.76312732696533 seconds

to notho the toward. Anayah recent the that. I donse pries ivly to no
esso neling. Gratta so to sto 
BEGINNING (1681855321.9923582): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6641, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6509, val loss 4.6466 [6.199763059616089 sec]
step 100: train loss 2.3632, val loss 2.4530 [16.680464506149292 sec]
step 200: train loss 2.0175, val loss 2.1738 [27.017348289489746 sec]
step 300: train loss 1.8168, val loss 2.0196 [37.377241134643555 sec]
step 400: train loss 1.6939, val loss 1.9400 [47.76087808609009 sec]
1.6720412969589233
Total Training Time: 52.16831040382385 seconds

"Selt. "Ioke has he mive. Gratta waked.
Alposed. Fing Gratta to came two finuld his corfulse. Gor, b
BEGINNING (1681855375.912053): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6276, val loss 4.6348 [3.3215930461883545 sec]
step 100: train loss 2.4781, val loss 2.5662 [8.941609621047974 sec]
step 200: train loss 2.2185, val loss 2.3338 [14.570482015609741 sec]
step 300: train loss 1.9948, val loss 2.1720 [20.31050682067871 sec]
step 400: train loss 1.8533, val loss 2.0485 [25.904778718948364 sec]
1.807375431060791
Total Training Time: 28.203728675842285 seconds

his allsliss, bout and whis I mave rided
Arphned for the traused, "Re the cubs, seed thep two smille
BEGINNING (1681855404.7465277): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5617, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5617, val loss 4.5546 [5.67908239364624 sec]
step 100: train loss 2.4587, val loss 2.5423 [15.514485597610474 sec]
step 200: train loss 2.1605, val loss 2.2921 [25.383597373962402 sec]
step 300: train loss 1.8856, val loss 2.0700 [35.22950220108032 sec]
step 400: train loss 1.7279, val loss 1.9500 [45.051615953445435 sec]
1.714967131614685
Total Training Time: 49.210514068603516 seconds

"Take my fives maeu hem beincal. But knot tooked
hotly un
poube of ingen ald dittrast, the degantion
BEGINNING (1681855455.156298): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6221, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6033, val loss 4.6028 [8.103482246398926 sec]
step 100: train loss 2.4378, val loss 2.5280 [22.43674874305725 sec]
step 200: train loss 2.1208, val loss 2.2620 [37.02239966392517 sec]
step 300: train loss 1.8688, val loss 2.0707 [51.63735556602478 sec]
step 400: train loss 1.6894, val loss 1.9245 [66.22739577293396 sec]
1.6904642581939697
Total Training Time: 72.62718367576599 seconds

bit sight the from hist frearsed, he suered. Beriyah
will he? Thar woill Dof mauens of thu ens eneen
BEGINNING (1681855529.599501): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5768, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5767, val loss 4.5896 [5.301442384719849 sec]
step 100: train loss 2.5140, val loss 2.5928 [14.824597835540771 sec]
step 200: train loss 2.3743, val loss 2.4699 [24.44105315208435 sec]
step 300: train loss 2.1624, val loss 2.2886 [34.009865522384644 sec]
step 400: train loss 1.9556, val loss 2.1275 [43.51002788543701 sec]
1.9417186975479126
Total Training Time: 47.49542450904846 seconds

choperat led the tuon the Ove. That y snowl hanty smaeuch re deptode
azition.
Grtta then the and. Wi
BEGINNING (1681855577.721453): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5997, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6050, val loss 4.6018 [9.656742572784424 sec]
step 100: train loss 2.4890, val loss 2.5659 [26.800925493240356 sec]
step 200: train loss 2.3297, val loss 2.4347 [43.81882977485657 sec]
step 300: train loss 2.0474, val loss 2.2018 [60.8772234916687 sec]
step 400: train loss 1.8342, val loss 2.0328 [77.84603524208069 sec]
1.7352087497711182
Total Training Time: 85.24655508995056 seconds

youn com tuon intre. He hother penters! Jurs as Ana
floirph, they go carely winge ons Taka ple. He s
BEGINNING (1681855664.2297938): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.4928, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4949, val loss 4.4949 [13.925005435943604 sec]
step 100: train loss 2.4762, val loss 2.5537 [38.38779664039612 sec]
step 200: train loss 2.2936, val loss 2.4018 [62.96182179450989 sec]
step 300: train loss 1.9846, val loss 2.1539 [87.70930624008179 sec]
step 400: train loss 1.7678, val loss 1.9927 [112.02367615699768 sec]
1.6906037330627441
Total Training Time: 122.4789969921112 seconds

tho cle
turned anstattansed the ratty's tuons charries.
"Gor was a pragabt, as cherdses head wait
th
BEGINNING (1681855788.7585301): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6810, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6790, val loss 4.6696 [3.18446946144104 sec]
step 100: train loss 2.4375, val loss 2.5232 [8.326495170593262 sec]
step 200: train loss 2.1369, val loss 2.2726 [12.952327728271484 sec]
step 300: train loss 1.9850, val loss 2.1546 [17.613081455230713 sec]
step 400: train loss 1.8850, val loss 2.0903 [22.32852530479431 sec]
1.863070011138916
Total Training Time: 24.277870416641235 seconds

would. The ou wermy in a llooke emains, ind's and gent the
of This ang €ir Anayah. I whelpagaion ild
BEGINNING (1681855813.8275719): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6599, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6591, val loss 4.6570 [4.678164005279541 sec]
step 100: train loss 2.4101, val loss 2.5086 [13.045279264450073 sec]
step 200: train loss 2.0870, val loss 2.2342 [21.491451263427734 sec]
step 300: train loss 1.9005, val loss 2.0940 [29.82105541229248 sec]
step 400: train loss 1.7982, val loss 2.0099 [38.179609060287476 sec]
1.7268309593200684
Total Training Time: 42.12913918495178 seconds

Kinjught."
Kname have hadid as not lisg nech.
"Comm tuon the Pestiand lood. Srived
the whowle toorke
BEGINNING (1681855857.5333421): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6769, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6740, val loss 4.6751 [6.60408878326416 sec]
step 100: train loss 2.4204, val loss 2.5072 [18.68246364593506 sec]
step 200: train loss 2.0954, val loss 2.2469 [30.63377070426941 sec]
step 300: train loss 1.8923, val loss 2.0911 [42.626132011413574 sec]
step 400: train loss 1.7778, val loss 2.0008 [54.73681330680847 sec]
1.6547799110412598
Total Training Time: 60.19693636894226 seconds

is tuons. This plie to uch to crickly cable his, and This quinge.
Gratta a
moved af hore at hurad re
BEGINNING (1681855919.948094): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5992, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6117, val loss 4.6110 [3.145209550857544 sec]
step 100: train loss 2.4805, val loss 2.5604 [8.413597106933594 sec]
step 200: train loss 2.2194, val loss 2.3512 [13.501622200012207 sec]
step 300: train loss 1.9871, val loss 2.1723 [18.6033616065979 sec]
step 400: train loss 1.8517, val loss 2.0641 [23.709556579589844 sec]
1.8582518100738525
Total Training Time: 25.88489842414856 seconds

civen ituon tries mite courded oplans andere assed he
in no-to Prinay speeow so two awas at spid too
BEGINNING (1681855946.558032): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6714, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6485, val loss 4.6490 [5.276038646697998 sec]
step 100: train loss 2.4719, val loss 2.5533 [14.659741163253784 sec]
step 200: train loss 2.1739, val loss 2.3125 [24.101399898529053 sec]
step 300: train loss 1.9292, val loss 2.1139 [33.44809937477112 sec]
step 400: train loss 1.7661, val loss 1.9834 [42.97981333732605 sec]
1.7765991687774658
Total Training Time: 47.204548597335815 seconds

walking Pyrrana Arphad, But
forning I now you a that detientesn stle.
Namalfed smeted.
"I a kme for 
BEGINNING (1681855995.2419255): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6376, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6311, val loss 4.6210 [7.4121057987213135 sec]
step 100: train loss 2.4704, val loss 2.5566 [21.019993543624878 sec]
step 200: train loss 2.1724, val loss 2.3110 [34.67669463157654 sec]
step 300: train loss 1.9131, val loss 2.1070 [48.25856161117554 sec]
step 400: train loss 1.7501, val loss 1.9656 [62.0546658039093 sec]
1.6745986938476562
Total Training Time: 68.26671314239502 seconds

wors derst. He sould for to the drent iterslesslet eenaming
to fillite to that. Gratta nearal st and
BEGINNING (1681856065.6167803): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6067, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6001, val loss 4.5967 [4.467283725738525 sec]
step 100: train loss 2.4995, val loss 2.5817 [12.311131954193115 sec]
step 200: train loss 2.3748, val loss 2.4761 [20.168559551239014 sec]
step 300: train loss 2.1716, val loss 2.3039 [28.069029569625854 sec]
step 400: train loss 1.9822, val loss 2.1588 [35.91652750968933 sec]
1.9203048944473267
Total Training Time: 39.30856657028198 seconds

jorbeked the a K,.
"Shook ra qas cumod's at fored fill gnis I their.
"The nod Gratto beeit, wat of y
BEGINNING (1681856105.6709633): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5356, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5410, val loss 4.5475 [8.229044914245605 sec]
step 100: train loss 2.4845, val loss 2.5694 [23.001891613006592 sec]
step 200: train loss 2.3432, val loss 2.4530 [37.717073917388916 sec]
step 300: train loss 2.0759, val loss 2.2330 [52.438148498535156 sec]
step 400: train loss 1.8715, val loss 2.0740 [67.15276622772217 sec]
1.8098978996276855
Total Training Time: 73.6519045829773 seconds

•Th.1 walk weth that He for for the Tray gratta. Suse. Iff the
goares! Namall 18
"En Ely Rowe to p
r
BEGINNING (1681856180.8002193): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6136, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6229, val loss 4.6131 [11.99628758430481 sec]
step 100: train loss 2.4870, val loss 2.5747 [33.58935070037842 sec]
step 200: train loss 2.3467, val loss 2.4505 [55.159379720687866 sec]
step 300: train loss 2.0573, val loss 2.2142 [76.77180886268616 sec]
step 400: train loss 1.8157, val loss 2.0132 [98.27769351005554 sec]
1.7465581893920898
Total Training Time: 107.88794660568237 seconds

manet. "Do tho havincery AnE his had. Beriyah
trist the your ench therst, us ghbe habore ter thens.

BEGINNING (1681856290.864409): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6131, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5999, val loss 4.6022 [2.866778612136841 sec]
step 100: train loss 2.3757, val loss 2.4684 [7.703073263168335 sec]
step 200: train loss 2.0606, val loss 2.2174 [12.526915311813354 sec]
step 300: train loss 1.9067, val loss 2.0905 [17.38863182067871 sec]
step 400: train loss 1.7981, val loss 2.0050 [22.220656871795654 sec]
1.7487834692001343
Total Training Time: 24.235973596572876 seconds

the sir, all ankently." Gratta seep. Dols befter Anayah. El Ro he ple look to the flom.
A Thak his f
BEGINNING (1681856315.84038): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6315, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6318, val loss 4.6423 [5.221371412277222 sec]
step 100: train loss 2.3586, val loss 2.4485 [13.926461935043335 sec]
step 200: train loss 2.0177, val loss 2.1825 [22.506417751312256 sec]
step 300: train loss 1.8346, val loss 2.0417 [31.132131814956665 sec]
step 400: train loss 1.7160, val loss 1.9630 [40.32337212562561 sec]
1.7268203496932983
Total Training Time: 44.158551931381226 seconds

DPEACCORDV A PAiSPEAY
be reling the." The celftere.t Le Afters. One Frough the guards, hunty horcen 
BEGINNING (1681856361.4110174): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5309, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5443, val loss 4.5489 [7.516490697860718 sec]
step 100: train loss 2.3527, val loss 2.4462 [20.102683782577515 sec]
step 200: train loss 2.0016, val loss 2.1625 [32.86018967628479 sec]
step 300: train loss 1.8003, val loss 2.0163 [45.503620862960815 sec]
step 400: train loss 1.6820, val loss 1.9150 [58.385629653930664 sec]
1.6857476234436035
Total Training Time: 65.60373711585999 seconds

swarrang tenty scould and mokennion forwhat chered
tell innto the trocklew." Graatta swided the turn
BEGINNING (1681856429.7496774): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5917, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5860, val loss 4.5984 [3.844527006149292 sec]
step 100: train loss 2.4562, val loss 2.5384 [12.251247882843018 sec]
step 200: train loss 2.1393, val loss 2.2840 [19.937974214553833 sec]
step 300: train loss 1.9306, val loss 2.1163 [27.13212752342224 sec]
step 400: train loss 1.7919, val loss 2.0105 [34.443960428237915 sec]
1.7383670806884766
Total Training Time: 37.18698573112488 seconds

mort they be plaking ehapped waty aso ching be." movened the had of yillegans,
would ars, and wenote
BEGINNING (1681856467.7281384): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6588, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6614, val loss 4.6670 [6.930608034133911 sec]
step 100: train loss 2.4259, val loss 2.5177 [19.548441410064697 sec]
step 200: train loss 2.1043, val loss 2.2561 [31.578813076019287 sec]
step 300: train loss 1.8402, val loss 2.0550 [43.66856789588928 sec]
step 400: train loss 1.6905, val loss 1.9292 [56.20143699645996 sec]
1.6799238920211792
Total Training Time: 61.46092677116394 seconds

"Siefore with?
Chy Can Chief Gratta cleaking for the gestrane and reast
wo cold host have snict Chie
BEGINNING (1681856530.8111303): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6220, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6293, val loss 4.6359 [10.099198579788208 sec]
step 100: train loss 2.4339, val loss 2.5246 [28.16319441795349 sec]
step 200: train loss 2.1067, val loss 2.2599 [45.48361301422119 sec]
step 300: train loss 1.8268, val loss 2.0274 [63.57692503929138 sec]
step 400: train loss 1.6560, val loss 1.8987 [85.00974345207214 sec]
1.6333084106445312
Total Training Time: 93.63440823554993 seconds

to a Hish Polessing the hattain fimanic thalzes hat
their har's.
They han you have gate to he his mo
BEGINNING (1681856626.713477): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6490, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6511, val loss 4.6554 [6.01886248588562 sec]
step 100: train loss 2.4931, val loss 2.5814 [17.70217251777649 sec]
step 200: train loss 2.3425, val loss 2.4479 [34.04347825050354 sec]
step 300: train loss 2.0995, val loss 2.2494 [44.637890100479126 sec]
step 400: train loss 1.8850, val loss 2.0831 [55.20484662055969 sec]
1.791082739830017
Total Training Time: 59.77889966964722 seconds

pe whoung the not Toth Telabdieng a eve way to tuone campled
weld hums and nericited trought tazed t
BEGINNING (1681856687.28943): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6160, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6134, val loss 4.6144 [11.056556940078735 sec]
step 100: train loss 2.4746, val loss 2.5615 [31.266783475875854 sec]
step 200: train loss 2.2692, val loss 2.3872 [54.04095673561096 sec]
step 300: train loss 1.9779, val loss 2.1621 [78.23252582550049 sec]
step 400: train loss 1.7689, val loss 1.9987 [101.11090850830078 sec]
1.6929634809494019
Total Training Time: 109.74350547790527 seconds

"Kied, What Gor the remance."
The soaf nong aldieght melining-the tand to haves had
"Por lan! them b
BEGINNING (1681856798.5908778): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6074, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6089, val loss 4.6220 [16.097614765167236 sec]
step 100: train loss 2.4768, val loss 2.5631 [44.01795768737793 sec]
step 200: train loss 2.2964, val loss 2.4107 [74.41000747680664 sec]
step 300: train loss 1.9439, val loss 2.1245 [106.5984570980072 sec]
step 400: train loss 1.7057, val loss 1.9361 [138.93310809135437 sec]
1.6164922714233398
Total Training Time: 150.76418018341064 seconds

hif guarded wels it for the vagate amon, mucheuw lighed
took leg, had? Grattap the was cowasted to t
BEGINNING (1681856951.678507): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6101, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6101, val loss 4.6090 [3.2665679454803467 sec]
step 100: train loss 2.3485, val loss 2.4500 [8.727230548858643 sec]
step 200: train loss 2.0167, val loss 2.1874 [14.397799015045166 sec]
step 300: train loss 1.8492, val loss 2.0527 [21.4519145488739 sec]
step 400: train loss 1.7375, val loss 1.9530 [27.683322429656982 sec]
1.7033915519714355
Total Training Time: 30.200907230377197 seconds

Fish degate they was werphat, has he
cloBering
oney, has head a cover oris feegratt a them age lolde
BEGINNING (1681856982.746439): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6639, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6647, val loss 4.6657 [6.085728406906128 sec]
step 100: train loss 2.3010, val loss 2.4151 [16.74293613433838 sec]
step 200: train loss 1.9600, val loss 2.1277 [27.06050157546997 sec]
step 300: train loss 1.7842, val loss 1.9896 [37.318984270095825 sec]
step 400: train loss 1.6645, val loss 1.9057 [47.37649655342102 sec]
1.6178090572357178
Total Training Time: 52.1311411857605 seconds

Soodder comple a minncition both eviled of
the veriour you with bet, his knanng one the magaeu?s
Ste
BEGINNING (1681857036.7108955): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6031, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6016, val loss 4.5992 [8.251484155654907 sec]
step 100: train loss 2.3241, val loss 2.4390 [23.23157048225403 sec]
step 200: train loss 1.9574, val loss 2.1249 [37.361814975738525 sec]
step 300: train loss 1.7535, val loss 1.9881 [52.91477990150452 sec]
step 400: train loss 1.6199, val loss 1.8857 [68.33327078819275 sec]
1.5801517963409424
Total Training Time: 74.81360459327698 seconds

smole they to the Tuon
Coull the Juseame quibes are out hideres some." Fisht stame
no muwish. Fame o
BEGINNING (1681857113.866226): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6725, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6704, val loss 4.6662 [4.588711500167847 sec]
step 100: train loss 2.4350, val loss 2.5190 [12.835005521774292 sec]
step 200: train loss 2.0887, val loss 2.2496 [21.849841833114624 sec]
step 300: train loss 1.8738, val loss 2.0836 [31.19506311416626 sec]
step 400: train loss 1.7314, val loss 1.9786 [39.364792585372925 sec]
1.7077275514602661
Total Training Time: 42.83879590034485 seconds

cimkned to the him talls be the cuckly were cans of
came, and and him lookiel two the
otho Re Vavila
BEGINNING (1681857157.4782941): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5893, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5677, val loss 4.5786 [8.387559652328491 sec]
step 100: train loss 2.4064, val loss 2.4970 [22.964649438858032 sec]
step 200: train loss 2.0248, val loss 2.1953 [39.445130348205566 sec]
step 300: train loss 1.7753, val loss 1.9909 [56.16269302368164 sec]
step 400: train loss 1.6224, val loss 1.8841 [74.82198333740234 sec]
1.5783803462982178
Total Training Time: 81.11728405952454 seconds

on habt you turned onciter?"
You looked to This bight a leaws your to spedingled
oing endess, that a
BEGINNING (1681857240.3147118): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6656, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6583, val loss 4.6621 [12.629774808883667 sec]
step 100: train loss 2.4141, val loss 2.5072 [34.1319100856781 sec]
step 200: train loss 2.0457, val loss 2.2071 [58.548781633377075 sec]
step 300: train loss 1.7567, val loss 1.9969 [84.5175039768219 sec]
step 400: train loss 1.5845, val loss 1.8790 [105.79706168174744 sec]
1.5272154808044434
Total Training Time: 115.12994718551636 seconds

his ensege time. They ware maright to
the raw sall their cubs were ried. The tuon waurrisr date goon
BEGINNING (1681857357.7034197): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5723, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5631, val loss 4.5725 [7.847107648849487 sec]
step 100: train loss 2.4855, val loss 2.5606 [22.231430292129517 sec]
step 200: train loss 2.3177, val loss 2.4177 [35.37224292755127 sec]
step 300: train loss 2.0368, val loss 2.1951 [48.5153443813324 sec]
step 400: train loss 1.8430, val loss 2.0399 [61.631187915802 sec]
1.7795387506484985
Total Training Time: 67.33649110794067 seconds

om approost this wern a ours. Miffing itay him up his mayse
Gratta gratta sped, Grattatta, "Shatta'N
BEGINNING (1681857425.8534584): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6537, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6527, val loss 4.6497 [16.170132398605347 sec]
step 100: train loss 2.4621, val loss 2.5514 [46.4129319190979 sec]
step 200: train loss 2.2103, val loss 2.3403 [72.54666590690613 sec]
step 300: train loss 1.8968, val loss 2.0942 [98.00107431411743 sec]
step 400: train loss 1.7007, val loss 1.9392 [122.53081822395325 sec]
1.651210069656372
Total Training Time: 133.10840773582458 seconds

laked, he pond hiss frace was moment.
of the sondrobed his head. Now. They poumanif Gratta the le
bo
BEGINNING (1681857560.6370158): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6246, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6164, val loss 4.6105 [22.80783724784851 sec]
step 100: train loss 2.4659, val loss 2.5532 [61.84952235221863 sec]
step 200: train loss 2.2502, val loss 2.3680 [94.84049010276794 sec]
step 300: train loss 1.8786, val loss 2.0712 [127.79828453063965 sec]
step 400: train loss 1.6520, val loss 1.9253 [160.76755809783936 sec]
1.5808731317520142
Total Training Time: 174.65131211280823 seconds

ovelow men igno frome the centy."
Aidden was good humans herplannne!" The hrood
arde, look a vlagoic
BEGINNING (1681857737.673162): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6155, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6136, val loss 4.6097 [3.7254016399383545 sec]
step 100: train loss 2.3297, val loss 2.4331 [10.400885581970215 sec]
step 200: train loss 2.0446, val loss 2.2039 [17.088961839675903 sec]
step 300: train loss 1.8790, val loss 2.0868 [23.75918674468994 sec]
step 400: train loss 1.7850, val loss 2.0073 [30.4720618724823 sec]
1.7129769325256348
Total Training Time: 33.48447275161743 seconds

whas, lertly phad ust pas and would seked awouth his wagauh. Sfor's mulsh
the milew of fas inces. "B
BEGINNING (1681857772.3440247): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6054, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5928, val loss 4.5951 [6.710895776748657 sec]
step 100: train loss 2.3541, val loss 2.4558 [19.12378716468811 sec]
step 200: train loss 2.0312, val loss 2.1969 [31.525776147842407 sec]
step 300: train loss 1.8418, val loss 2.0405 [43.909456968307495 sec]
step 400: train loss 1.7413, val loss 1.9802 [56.295217990875244 sec]
1.7230277061462402
Total Training Time: 62.08208608627319 seconds

stiked is have a long tror to pate treats priop op with eminath his with as all betod but ive one
va
BEGINNING (1681857836.6897762): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5995, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6138, val loss 4.6164 [9.63346266746521 sec]
step 100: train loss 2.4143, val loss 2.5054 [27.83305835723877 sec]
step 200: train loss 2.0464, val loss 2.2122 [45.89837694168091 sec]
step 300: train loss 1.8437, val loss 2.0500 [64.08528208732605 sec]
step 400: train loss 1.7294, val loss 1.9509 [82.26172661781311 sec]
1.7354090213775635
Total Training Time: 90.80389261245728 seconds

WHE RES I you maeuw, have
straded. Gratta
smived hoid, Anayah, what sake some
the peystsemer?"
Gratt
BEGINNING (1681857930.8923671): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6629, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6608, val loss 4.6581 [4.568539619445801 sec]
step 100: train loss 2.4157, val loss 2.5143 [13.009361982345581 sec]
step 200: train loss 2.0743, val loss 2.2255 [21.45851492881775 sec]
step 300: train loss 1.8688, val loss 2.0839 [29.917373657226562 sec]
step 400: train loss 1.7404, val loss 1.9793 [38.391154766082764 sec]
1.695868730545044
Total Training Time: 42.2618043422699 seconds

lition, the Taka puchad rist of the mustroYah
lage to athe hump caund surved."
Aidded al the bisht t
BEGINNING (1681857974.3083856): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6353, val loss 4.6316 [8.506048440933228 sec]
step 100: train loss 2.4462, val loss 2.5344 [24.484299659729004 sec]
step 200: train loss 2.0714, val loss 2.2360 [40.521095991134644 sec]
step 300: train loss 1.8228, val loss 2.0235 [57.13444423675537 sec]
step 400: train loss 1.6733, val loss 1.9307 [73.63065767288208 sec]
1.6291850805282593
Total Training Time: 81.40724515914917 seconds

the reep tuons soome. As this were be proctsign for tor camies, any
• Arphad Elde I heneas bemaughed
BEGINNING (1681858058.2355216): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5519, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5511, val loss 4.5580 [12.672335386276245 sec]
step 100: train loss 2.4609, val loss 2.5472 [36.80540895462036 sec]
step 200: train loss 2.1273, val loss 2.2705 [60.83338284492493 sec]
step 300: train loss 1.8396, val loss 2.0533 [85.11122155189514 sec]
step 400: train loss 1.6779, val loss 1.9384 [109.3377115726471 sec]
1.6563823223114014
Total Training Time: 120.6327497959137 seconds

offived. The the camae barek, the cught would but that
bick. They counce." seer Gen-eging to Taka Na
BEGINNING (1681858182.4950933): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6605, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6465, val loss 4.6536 [7.770219564437866 sec]
step 100: train loss 2.4716, val loss 2.5632 [21.990033864974976 sec]
step 200: train loss 2.2755, val loss 2.3986 [36.17984652519226 sec]
step 300: train loss 1.9902, val loss 2.1722 [50.42450737953186 sec]
step 400: train loss 1.7912, val loss 2.0283 [64.70627403259277 sec]
1.756529688835144
Total Training Time: 71.30192971229553 seconds

Ohen wouted aftarierss, "Chief Gratta just this camp bry
atek? You hard Vor dories. "Shen yen groves
BEGINNING (1681858255.0119104): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6633, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6738, val loss 4.6672 [14.920832395553589 sec]
step 100: train loss 2.4835, val loss 2.5664 [42.075764656066895 sec]
step 200: train loss 2.2695, val loss 2.3793 [68.80078625679016 sec]
step 300: train loss 1.9353, val loss 2.1185 [95.15790724754333 sec]
step 400: train loss 1.7141, val loss 1.9580 [121.5622136592865 sec]
1.5787787437438965
Total Training Time: 133.01360368728638 seconds

hild Chief Pelarast Anayah staknaid, "What nat yah low
ho beere are returned some
icaution thamany i
BEGINNING (1681858390.4068918): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5743, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5775, val loss 4.5847 [20.97026777267456 sec]
step 100: train loss 2.4905, val loss 2.5751 [55.26330757141113 sec]
step 200: train loss 2.3295, val loss 2.4389 [88.19184803962708 sec]
step 300: train loss 1.9843, val loss 2.1828 [122.17426109313965 sec]
step 400: train loss 1.7140, val loss 1.9487 [157.57788610458374 sec]
1.6505258083343506
Total Training Time: 173.5488314628601 seconds

for will and armed. It losebanned oxthe ner guard and the wire
robabloy." Beriyah of the sighe the h
BEGINNING (1681858567.5398316): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5958, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5963, val loss 4.5988 [4.201451778411865 sec]
step 100: train loss 2.2718, val loss 2.3890 [12.075779676437378 sec]
step 200: train loss 1.9593, val loss 2.1459 [19.741852521896362 sec]
step 300: train loss 1.8091, val loss 2.0147 [27.816812753677368 sec]
step 400: train loss 1.6935, val loss 1.9554 [35.64461302757263 sec]
1.7219740152359009
Total Training Time: 39.53038215637207 seconds

W0"
"Whey back at with to you tuon wan to
a grong abbefile. "Ch will turd not smellay well eyes hon.
BEGINNING (1681858608.2741888): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6152, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6321, val loss 4.6323 [7.709238052368164 sec]
step 100: train loss 2.2853, val loss 2.4004 [22.15929627418518 sec]
step 200: train loss 1.9401, val loss 2.1242 [36.516579151153564 sec]
step 300: train loss 1.7715, val loss 1.9803 [50.810081005096436 sec]
step 400: train loss 1.6415, val loss 1.9168 [65.39244604110718 sec]
1.652577519416809
Total Training Time: 71.92375922203064 seconds

SEAN THR PEAC RASSEATI – BNARM
spood as and leake threat traughts, the coven any his smiling
aboff T
BEGINNING (1681858682.4827209): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6894, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6849, val loss 4.6878 [10.72702407836914 sec]
step 100: train loss 2.3459, val loss 2.4452 [31.25464177131653 sec]
step 200: train loss 1.9435, val loss 2.1153 [51.988091230392456 sec]
step 300: train loss 1.7803, val loss 2.0084 [72.7170422077179 sec]
step 400: train loss 1.6579, val loss 1.9231 [94.69106459617615 sec]
1.6483503580093384
Total Training Time: 105.42744898796082 seconds

planed to the deapcier, Gratta were's thas? Gratta
wound is light oar the rempany moves, and
with si
BEGINNING (1681858791.80822): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6813, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6812, val loss 4.6821 [6.4340105056762695 sec]
step 100: train loss 2.3823, val loss 2.4750 [17.44650411605835 sec]
step 200: train loss 1.9911, val loss 2.1547 [28.410993814468384 sec]
step 300: train loss 1.7924, val loss 2.0325 [39.51129984855652 sec]
step 400: train loss 1.6525, val loss 1.9085 [50.76242113113403 sec]
1.602485179901123
Total Training Time: 55.739463567733765 seconds

hensned. He incely would worrus al hall relseedziter.
Chief Pencesse wit is to a before incermed
dit
BEGINNING (1681858848.746876): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6528, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6680, val loss 4.6640 [11.509283065795898 sec]
step 100: train loss 2.3937, val loss 2.4874 [31.87160015106201 sec]
step 200: train loss 1.9552, val loss 2.1515 [52.19246745109558 sec]
step 300: train loss 1.7216, val loss 1.9676 [72.46753478050232 sec]
step 400: train loss 1.5642, val loss 1.8790 [92.6731948852539 sec]
1.6034377813339233
Total Training Time: 101.7704963684082 seconds

His do reain. We of
alme moorn a deat Hight off. "Toria twas abough, his pof their
see hur reaving t
BEGINNING (1681858952.7847254): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6243, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6277, val loss 4.6269 [16.315361261367798 sec]
step 100: train loss 2.4279, val loss 2.5243 [48.28483200073242 sec]
step 200: train loss 2.0175, val loss 2.1812 [79.90299606323242 sec]
step 300: train loss 1.7501, val loss 1.9927 [111.2900903224945 sec]
step 400: train loss 1.5741, val loss 1.8693 [146.56248474121094 sec]
1.5139788389205933
Total Training Time: 161.33459329605103 seconds

Gon!" Gratta turned and hire nodded eves and
low said, "Sly for you hiff and cordies sque conimblan,
BEGINNING (1681859117.6219263): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5939, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5979, val loss 4.6034 [8.631796836853027 sec]
step 100: train loss 2.4527, val loss 2.5342 [27.46665906906128 sec]
step 200: train loss 2.2025, val loss 2.3254 [45.66366410255432 sec]
step 300: train loss 1.8852, val loss 2.0841 [69.22508478164673 sec]
step 400: train loss 1.6950, val loss 1.9532 [92.72992372512817 sec]
1.6676875352859497
Total Training Time: 101.51215934753418 seconds

Kestion, "Chief twilf
othe – Taka the moreatueund fursed, as not moding
their bacom, boing he stod o
BEGINNING (1681859220.425256): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5624, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5624, val loss 4.5674 [24.755611419677734 sec]
step 100: train loss 2.4675, val loss 2.5548 [65.89284157752991 sec]
step 200: train loss 2.2373, val loss 2.3646 [97.10231041908264 sec]
step 300: train loss 1.8509, val loss 2.0601 [129.76592254638672 sec]
step 400: train loss 1.6205, val loss 1.8924 [172.58274269104004 sec]
1.5762711763381958
Total Training Time: 189.7082941532135 seconds

87
Veembled thave to looked at oin a back of your for beforsive
onces. He lest ift yenty would with 
BEGINNING (1681859412.688549): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5515, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5456, val loss 4.5426 [35.13350558280945 sec]
step 100: train loss 2.4795, val loss 2.5704 [89.7467098236084 sec]
step 200: train loss 2.2605, val loss 2.3813 [138.01942348480225 sec]
step 300: train loss 1.8756, val loss 2.0801 [187.82292699813843 sec]
step 400: train loss 1.6187, val loss 1.8839 [235.44453644752502 sec]
1.5051202774047852
Total Training Time: 259.51544284820557 seconds

bowere turned his How glaved stopecather. And Ar
beuw was saxighfor! The you recopled upought. Jucco
BEGINNING (1681859676.1481936): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6225, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6099, val loss 4.6301 [4.972118377685547 sec]
step 100: train loss 2.2103, val loss 2.3479 [13.977778673171997 sec]
step 200: train loss 1.9130, val loss 2.1088 [22.887938737869263 sec]
step 300: train loss 1.7502, val loss 1.9791 [31.788763761520386 sec]
step 400: train loss 1.6461, val loss 1.8992 [40.68874740600586 sec]
1.650684118270874
Total Training Time: 44.65124988555908 seconds

meaked to chtake! Toridur. "I was do.
Gratta sulted. Gratta need the gating their chapt waile, and d
BEGINNING (1681859721.9953983): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6748, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6726, val loss 4.6660 [8.913833379745483 sec]
step 100: train loss 2.2408, val loss 2.3652 [28.443510055541992 sec]
step 200: train loss 1.8917, val loss 2.0973 [45.512259006500244 sec]
step 300: train loss 1.6937, val loss 1.9391 [65.31902599334717 sec]
step 400: train loss 1.5784, val loss 1.8744 [81.81705832481384 sec]
1.5753610134124756
Total Training Time: 89.9285717010498 seconds

have as a capprong nor a spraty. Gratta will the spect. Anayah knead the sugh?"
Chief Taka enoughed 
BEGINNING (1681859814.432691): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5857, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5888, val loss 4.5820 [13.471733331680298 sec]
step 100: train loss 2.2964, val loss 2.4093 [37.611610412597656 sec]
step 200: train loss 1.9150, val loss 2.1167 [61.809773683547974 sec]
step 300: train loss 1.7231, val loss 1.9647 [87.61073112487793 sec]
step 400: train loss 1.5949, val loss 1.8790 [114.09478664398193 sec]
1.5403549671173096
Total Training Time: 126.19751381874084 seconds

ut new witersed to the cubbs would jarger to tright bridge,
another loudgerened. "I the gant exbover
BEGINNING (1681859944.2413008): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5792, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5849, val loss 4.5853 [7.799210548400879 sec]
step 100: train loss 2.3424, val loss 2.4517 [22.061115503311157 sec]
step 200: train loss 1.9456, val loss 2.1292 [36.179248332977295 sec]
step 300: train loss 1.7379, val loss 1.9744 [51.006913900375366 sec]
step 400: train loss 1.6121, val loss 1.9043 [65.15599727630615 sec]
1.567029356956482
Total Training Time: 72.15795922279358 seconds

is borses.." "I that as and Gratta didn't Gor him.
"Taka he arood. BiThat of you to no to you." Grat
BEGINNING (1681860017.728849): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6026, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6061, val loss 4.6016 [15.759456157684326 sec]
step 100: train loss 2.3823, val loss 2.4802 [43.63302230834961 sec]
step 200: train loss 1.9122, val loss 2.1055 [71.64688444137573 sec]
step 300: train loss 1.6697, val loss 1.9205 [98.59040260314941 sec]
step 400: train loss 1.5183, val loss 1.8530 [125.39574861526489 sec]
1.5094428062438965
Total Training Time: 137.02227115631104 seconds

Arna. I wiskerest Gratta much the walked agater fly
warriors offainver to the captamb judgled as Gne
BEGINNING (1681860157.090137): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6025, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6051, val loss 4.6101 [21.44745969772339 sec]
step 100: train loss 2.4172, val loss 2.5138 [58.182750940322876 sec]
step 200: train loss 1.9873, val loss 2.1626 [95.20165419578552 sec]
step 300: train loss 1.6977, val loss 1.9490 [131.71944856643677 sec]
step 400: train loss 1.5195, val loss 1.8456 [176.70533776283264 sec]
1.512678623199463
Total Training Time: 195.96679520606995 seconds

their bewn Pyrrans a command, little aht he with the purtoon
humas and he maeredrayfainests a desvis
BEGINNING (1681860356.8800447): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6319, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6362, val loss 4.6257 [14.656315565109253 sec]
step 100: train loss 2.4360, val loss 2.5326 [46.13836598396301 sec]
step 200: train loss 2.1579, val loss 2.3035 [72.55127763748169 sec]
step 300: train loss 1.8333, val loss 2.0486 [95.0560200214386 sec]
step 400: train loss 1.6427, val loss 1.9153 [122.00277709960938 sec]
1.629685640335083
Total Training Time: 133.30947852134705 seconds

50 Sto the tuons a Namalsinge hows cep of the onic
wing the the Torialess my, buthed Aiddel4
mase Pr
BEGINNING (1681860491.5440474): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5974, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6045, val loss 4.6129 [24.816895484924316 sec]
step 100: train loss 2.4569, val loss 2.5462 [74.09247350692749 sec]
step 200: train loss 2.1650, val loss 2.3114 [127.55310845375061 sec]
step 300: train loss 1.7715, val loss 1.9992 [183.28080940246582 sec]
step 400: train loss 1.5509, val loss 1.8500 [239.4814591407776 sec]
1.5439088344573975
Total Training Time: 257.68623447418213 seconds

timal the Prran to the Ell, shump he as the
heled cares and the eyesen to they. And way growlet rela
BEGINNING (1681860751.8455548): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6269, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6248, val loss 4.6191 [1.0851058959960938 sec]
step 100: train loss 3.1576, val loss 3.1810 [2.7467565536499023 sec]
step 200: train loss 2.9057, val loss 2.9351 [4.4076738357543945 sec]
step 300: train loss 2.7481, val loss 2.7962 [6.089785575866699 sec]
step 400: train loss 2.6624, val loss 2.7183 [7.844204902648926 sec]
2.691190242767334
Total Training Time: 8.467487812042236 seconds

Ar."I t sou tdin con
hguvointooP "I is-touh t be
CuTp adg€ula'a.X W
pthecoweD8raB"mt thatr aEW t lan
BEGINNING (1681860760.5570412): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6219, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6101, val loss 4.6118 [1.6757221221923828 sec]
step 100: train loss 3.0822, val loss 3.1087 [4.629796743392944 sec]
step 200: train loss 2.8183, val loss 2.8593 [7.210994005203247 sec]
step 300: train loss 2.6887, val loss 2.7476 [10.030158996582031 sec]
step 400: train loss 2.6064, val loss 2.6603 [12.698299646377563 sec]
2.4918580055236816
Total Training Time: 13.901904106140137 seconds

hee A
mato nous o-d pekel siorm8uwa , t s oso tare d Chalere wanTorole uhuny mielke Thir hikkdothed.
BEGINNING (1681860774.9027786): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6911, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6823, val loss 4.6743 [2.170171022415161 sec]
step 100: train loss 3.0333, val loss 3.0578 [5.961992263793945 sec]
step 200: train loss 2.7575, val loss 2.8021 [10.83112621307373 sec]
step 300: train loss 2.6190, val loss 2.6667 [14.544432163238525 sec]
step 400: train loss 2.5248, val loss 2.5903 [18.192570447921753 sec]
2.51491117477417
Total Training Time: 19.691477060317993 seconds

touunt. .Xe Sy ifod Whdeiacumaluemh. "Twin., ’r neprllavee s aiPonantit ) ovoged guhy r s Wucul hero
BEGINNING (1681860795.1769109): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7108, val loss 4.7085 [1.0690970420837402 sec]
step 100: train loss 3.1391, val loss 3.1698 [2.7218685150146484 sec]
step 200: train loss 2.8942, val loss 2.9459 [4.4843950271606445 sec]
step 300: train loss 2.7413, val loss 2.7921 [6.512037038803101 sec]
step 400: train loss 2.6725, val loss 2.7293 [8.371791362762451 sec]
2.6528079509735107
Total Training Time: 9.092692852020264 seconds

or,,oke gins pind aigasd "
'two t thed foked itold! t widuroledconno nf swhindath t ak
fud
of t t ae
BEGINNING (1681860804.5035968): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6422, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6339, val loss 4.6328 [1.622528314590454 sec]
step 100: train loss 3.0964, val loss 3.1204 [4.344703435897827 sec]
step 200: train loss 2.8160, val loss 2.8460 [7.133367300033569 sec]
step 300: train loss 2.6829, val loss 2.7285 [10.091683149337769 sec]
step 400: train loss 2.6014, val loss 2.6574 [13.112436532974243 sec]
2.630533218383789
Total Training Time: 14.268554925918579 seconds

heiy."X: g I bend : /Yat
cutor ienKeUeme.
G wowamherr'n, Pf Heheedat
"'odled
Coulat€rataneaf as'DuI 
BEGINNING (1681860819.2734928): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6909, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6612, val loss 4.6589 [2.1395390033721924 sec]
step 100: train loss 3.0541, val loss 3.0899 [5.785534620285034 sec]
step 200: train loss 2.7679, val loss 2.8193 [9.189075231552124 sec]
step 300: train loss 2.6503, val loss 2.7056 [12.68439531326294 sec]
step 400: train loss 2.5792, val loss 2.6457 [16.156044006347656 sec]
2.5499815940856934
Total Training Time: 17.54021143913269 seconds

btherrok le
the tthit thaloa tibttthhe an t age wind,
cutheinofhed Sitoulil'd ttoudpq wert moo ju ve
BEGINNING (1681860837.3836944): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6104, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6220, val loss 4.6288 [1.1485404968261719 sec]
step 100: train loss 3.1128, val loss 3.1476 [2.99049711227417 sec]
step 200: train loss 2.8610, val loss 2.9043 [4.828671216964722 sec]
step 300: train loss 2.7440, val loss 2.7952 [6.6640355587005615 sec]
step 400: train loss 2.6778, val loss 2.7356 [8.471020221710205 sec]
2.6514055728912354
Total Training Time: 9.176119804382324 seconds

Afow."T“d Rad t, t wo srdElarirowat as tous f s sthaf The lyoupm wisust ar hof t m9
prapiRfitld,
 fe
BEGINNING (1681860846.7898061): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6688, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6765, val loss 4.6689 [1.6026995182037354 sec]
step 100: train loss 3.0703, val loss 3.1001 [4.487297773361206 sec]
step 200: train loss 2.7999, val loss 2.8543 [7.124479293823242 sec]
step 300: train loss 2.6968, val loss 2.7504 [9.754519701004028 sec]
step 400: train loss 2.6261, val loss 2.6887 [12.389114379882812 sec]
2.5671210289001465
Total Training Time: 13.47082781791687 seconds

Gtys6outhy e mratorop y tamiamhs coun w te A d
Sed se urlord Lande loinsind.
s h CEedd GNan miod. Pw
BEGINNING (1681860860.6397173): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5841, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6017, val loss 4.6102 [2.049159049987793 sec]
step 100: train loss 2.9986, val loss 3.0296 [5.72315788269043 sec]
step 200: train loss 2.7556, val loss 2.7968 [9.560950756072998 sec]
step 300: train loss 2.6488, val loss 2.7012 [13.051151990890503 sec]
step 400: train loss 2.5959, val loss 2.6562 [16.789491653442383 sec]
2.5353353023529053
Total Training Time: 18.32315731048584 seconds

at
gos ssus nan oume
Arfaid s thilof owen. isThed
Cf t bg thilpr “aloverell
t R bininallatono eide n
BEGINNING (1681860879.5058665): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6418, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6431, val loss 4.6461 [1.2044517993927002 sec]
step 100: train loss 3.1226, val loss 3.1498 [3.102663516998291 sec]
step 200: train loss 2.8366, val loss 2.8796 [4.980875730514526 sec]
step 300: train loss 2.7004, val loss 2.7428 [6.851324796676636 sec]
step 400: train loss 2.6110, val loss 2.6713 [8.70332407951355 sec]
2.6253883838653564
Total Training Time: 9.408031702041626 seconds

Grasmiey a sa ris fhecatpesioiuchoun
av. yGouI all orcie t.Vcata9ins."I
h, ig va Vcacouy.E"d cu%leif
BEGINNING (1681860889.1371098): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6102, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6118, val loss 4.6162 [1.655395746231079 sec]
step 100: train loss 3.0732, val loss 3.1064 [4.4318273067474365 sec]
step 200: train loss 2.8001, val loss 2.8336 [7.140237808227539 sec]
step 300: train loss 2.6525, val loss 2.7088 [9.822778940200806 sec]
step 400: train loss 2.5510, val loss 2.6164 [12.537476301193237 sec]
2.489433526992798
Total Training Time: 13.586150169372559 seconds

rupriI
phe med.
bof withThe oner
Gr Thasoude Thes sat t gota"
Jand andenl thatkebt meghad us t o€ he
BEGINNING (1681860903.1368802): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5721, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5758, val loss 4.5814 [2.1554148197174072 sec]
step 100: train loss 3.0286, val loss 3.0462 [5.7106828689575195 sec]
step 200: train loss 2.7505, val loss 2.7896 [9.250599145889282 sec]
step 300: train loss 2.6027, val loss 2.6672 [12.835983037948608 sec]
step 400: train loss 2.5024, val loss 2.5696 [16.36046528816223 sec]
2.4008572101593018
Total Training Time: 17.773364305496216 seconds

Wtif qunge rreanig th fo:"
bed way tharinouA– P. shan
gout on ur to2 C"Vavor thes
ar sasu nt fil we 
BEGINNING (1681860921.4313316): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6321, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6443, val loss 4.6391 [1.2397487163543701 sec]
step 100: train loss 3.1065, val loss 3.1323 [3.1642234325408936 sec]
step 200: train loss 2.8547, val loss 2.8819 [5.23015832901001 sec]
step 300: train loss 2.7210, val loss 2.7720 [7.288000822067261 sec]
step 400: train loss 2.6551, val loss 2.7146 [9.227907180786133 sec]
2.6220204830169678
Total Training Time: 10.083344221115112 seconds

"
wAGrenda alto pcArf
Yeralitth, thouo Gwice imoo'e T:d bwag
uan Tthkirat a halI t ma towae “ea2l˜at
BEGINNING (1681860931.823669): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6315, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6236, val loss 4.6271 [2.2117505073547363 sec]
step 100: train loss 3.0528, val loss 3.0761 [5.311537981033325 sec]
step 200: train loss 2.7925, val loss 2.8395 [8.403140783309937 sec]
step 300: train loss 2.6748, val loss 2.7247 [11.372637271881104 sec]
step 400: train loss 2.5993, val loss 2.6553 [14.796276330947876 sec]
2.595689535140991
Total Training Time: 15.985618829727173 seconds

-elthal. moricaced wasnane
w d tad aviarnhe cos w."
E
genwointod we
Yoka, nshene
currye
Cht tad hd O
BEGINNING (1681860948.2254553): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6901, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6972, val loss 4.6948 [3.8556694984436035 sec]
step 100: train loss 2.9868, val loss 3.0171 [8.138047456741333 sec]
step 200: train loss 2.7505, val loss 2.7917 [13.117386102676392 sec]
step 300: train loss 2.6323, val loss 2.6963 [17.414242267608643 sec]
step 400: train loss 2.5670, val loss 2.6279 [21.6191508769989 sec]
2.5524253845214844
Total Training Time: 23.402255058288574 seconds

FBESIwhe s coucraand Vig wie d Grca TEQet xgercas tis wJoveim1 as omee  huss de 1onulerarerd spre sj
BEGINNING (1681860972.180766): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6414, val loss 4.6368 [2.2355594635009766 sec]
step 100: train loss 3.1077, val loss 3.1396 [4.392811059951782 sec]
step 200: train loss 2.8606, val loss 2.9072 [7.582834720611572 sec]
step 300: train loss 2.7369, val loss 2.7902 [9.834462642669678 sec]
step 400: train loss 2.6720, val loss 2.7245 [12.991557836532593 sec]
2.66821551322937
Total Training Time: 14.084558010101318 seconds

wo hWhi athaiabeuneous by cioat s neve bl c w teras, as ll"Zatwrug tomistaoutamhe pVbt hious kis, f 
BEGINNING (1681860986.485877): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6698, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6758, val loss 4.6712 [2.025908946990967 sec]
step 100: train loss 3.0479, val loss 3.0778 [5.352789640426636 sec]
step 200: train loss 2.7816, val loss 2.8264 [8.639336585998535 sec]
step 300: train loss 2.6721, val loss 2.7270 [11.870455980300903 sec]
step 400: train loss 2.6115, val loss 2.6716 [15.433655500411987 sec]
2.6226229667663574
Total Training Time: 16.7378408908844 seconds

ENyUN/wxpa
d t weut benmor EN8e matheite llo srnd's, id xUp, Thtreifthesawerd
had intede Grephant•he
BEGINNING (1681861003.6352372): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6055, val loss 4.6042 [2.789785385131836 sec]
step 100: train loss 3.0143, val loss 3.0428 [8.601952314376831 sec]
step 200: train loss 2.7550, val loss 2.8025 [13.66035509109497 sec]
step 300: train loss 2.6516, val loss 2.7089 [18.227713346481323 sec]
step 400: train loss 2.5881, val loss 2.6537 [23.531497955322266 sec]
2.6027286052703857
Total Training Time: 26.704957962036133 seconds

wed ya ld pldertninsavef an athed.
bof eraveriralopird s brrilon w Gfued, ga ane st "
le,ae
Attheut 
BEGINNING (1681861030.9782417): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6532, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6541, val loss 4.6486 [1.592604637145996 sec]
step 100: train loss 3.1341, val loss 3.1563 [3.9633846282958984 sec]
step 200: train loss 2.8343, val loss 2.8755 [6.052074432373047 sec]
step 300: train loss 2.7037, val loss 2.7587 [8.132014989852905 sec]
step 400: train loss 2.6050, val loss 2.6668 [10.240809679031372 sec]
2.521669626235962
Total Training Time: 11.02340292930603 seconds

bitbokp? Ne okiun wag. wof Ttungerimers ldsto
peve on, he lleI sh
d Hod Dthenf"2 Nrsh this hie, thou
BEGINNING (1681861042.2246392): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5955, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6039, val loss 4.6024 [2.042703151702881 sec]
step 100: train loss 3.0462, val loss 3.0659 [5.050281286239624 sec]
step 200: train loss 2.7608, val loss 2.7974 [7.954069375991821 sec]
step 300: train loss 2.6110, val loss 2.6607 [10.707769393920898 sec]
step 400: train loss 2.4940, val loss 2.5575 [13.619535446166992 sec]
2.41520357131958
Total Training Time: 14.749964714050293 seconds

’ana sceran
ttanahind s ht
utseaf Thowre thitt
s inAry selmc alleroan."e drisped the cfilnay botey l
BEGINNING (1681861057.4009418): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5709, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5809, val loss 4.5869 [2.2658181190490723 sec]
step 100: train loss 3.0023, val loss 3.0284 [6.071725845336914 sec]
step 200: train loss 2.7216, val loss 2.7678 [10.117497205734253 sec]
step 300: train loss 2.5936, val loss 2.6552 [14.124906063079834 sec]
step 400: train loss 2.4992, val loss 2.5645 [18.0042724609375 sec]
2.499464511871338
Total Training Time: 19.53297233581543 seconds

too won dlgbet scoin thar isirZm.lld.
S Bean rsged. I PETUas ars YArandeasdat fred as ake ins. Hal w
BEGINNING (1681861077.5129006): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5864, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5899, val loss 4.5968 [1.4242453575134277 sec]
step 100: train loss 3.1239, val loss 3.1533 [3.665534019470215 sec]
step 200: train loss 2.8623, val loss 2.9019 [5.916319370269775 sec]
step 300: train loss 2.7274, val loss 2.7814 [8.335565328598022 sec]
step 400: train loss 2.6514, val loss 2.7110 [10.51950216293335 sec]
2.66684627532959
Total Training Time: 11.322718143463135 seconds

hes fer l. w) 0n; hilisefarilhindcofre, spat thelLerl teche
"h
Bpaldis d?Pmky an taathe had. wacthis
BEGINNING (1681861089.0571706): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6194, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6143, val loss 4.6063 [1.9370510578155518 sec]
step 100: train loss 3.0595, val loss 3.0872 [5.012522220611572 sec]
step 200: train loss 2.7649, val loss 2.8141 [8.103386402130127 sec]
step 300: train loss 2.6415, val loss 2.7005 [11.202799558639526 sec]
step 400: train loss 2.5702, val loss 2.6308 [14.411621809005737 sec]
2.550394296646118
Total Training Time: 15.693395853042603 seconds

Naverouven tt hico tala thandey I ng ceareas kout, huso ro
t thed
ce worengbathet fonesngavay t tate
BEGINNING (1681861105.121707): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5650, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5518, val loss 4.5480 [2.3994226455688477 sec]
step 100: train loss 2.9770, val loss 3.0108 [6.378363132476807 sec]
step 200: train loss 2.7385, val loss 2.7855 [10.389017581939697 sec]
step 300: train loss 2.6194, val loss 2.6762 [14.286339044570923 sec]
step 400: train loss 2.5511, val loss 2.6147 [18.325488567352295 sec]
2.5097663402557373
Total Training Time: 19.899153470993042 seconds

basheredwst ralcpe bmar."
Heromer oroutt d f ote ga sto
av, aurc,"
j shand. toshesthiSI Ind "
 peran
BEGINNING (1681861125.5509546): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6328, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6354, val loss 4.6247 [1.744267225265503 sec]
step 100: train loss 3.0887, val loss 3.1132 [4.776095867156982 sec]
step 200: train loss 2.8523, val loss 2.8873 [9.778508186340332 sec]
step 300: train loss 2.7283, val loss 2.7780 [12.939616441726685 sec]
step 400: train loss 2.6588, val loss 2.7131 [15.94194769859314 sec]
2.661792039871216
Total Training Time: 16.98802900314331 seconds

SEhleln Ton I furhithoug Gnca Tand alJatq€Thareng
gailll I soufu,EI whe bs
.I d t:d s anctheopaf t t
BEGINNING (1681861142.7589753): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5504, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5553, val loss 4.5501 [2.389761447906494 sec]
step 100: train loss 3.0580, val loss 3.0829 [6.325252532958984 sec]
step 200: train loss 2.8044, val loss 2.8486 [10.297228336334229 sec]
step 300: train loss 2.6827, val loss 2.7379 [14.302273035049438 sec]
step 400: train loss 2.6071, val loss 2.6767 [18.088189601898193 sec]
2.583153486251831
Total Training Time: 19.511214017868042 seconds

hanged alave as. tha. therores, thed t st acthid P":he, age phebl ckin t wad tiser s th ave sthat fl
BEGINNING (1681861162.6521864): Baseline LR(0.00025) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5365, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5455, val loss 4.5461 [2.9667088985443115 sec]
step 100: train loss 2.9701, val loss 3.0060 [9.28342318534851 sec]
step 200: train loss 2.7309, val loss 2.7836 [17.498344898223877 sec]
step 300: train loss 2.6255, val loss 2.6905 [25.85013771057129 sec]
step 400: train loss 2.5709, val loss 2.6392 [32.425089597702026 sec]
2.570228099822998
Total Training Time: 34.45897817611694 seconds

MyaN ckm. Gy yIuong phra-an fonde "
ris torloush tey mid.
hatam aed s hare cure g d. asld a ty,
"
wa
BEGINNING (1681861197.664163): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5791, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5915, val loss 4.5893 [1.357635259628296 sec]
step 100: train loss 2.8355, val loss 2.8700 [3.4726130962371826 sec]
step 200: train loss 2.6310, val loss 2.6864 [5.539713144302368 sec]
step 300: train loss 2.5036, val loss 2.5659 [7.868889570236206 sec]
step 400: train loss 2.3965, val loss 2.4805 [10.029913187026978 sec]
2.3380630016326904
Total Training Time: 10.902902603149414 seconds

tut lie nicats wo she t. 
SE "Yerelat wuthe uld, d with gr poound ang wind is shind. I Prne
, hispar
BEGINNING (1681861208.9532871): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.7297, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6986, val loss 4.6953 [2.2245614528656006 sec]
step 100: train loss 2.7622, val loss 2.8229 [5.929955959320068 sec]
step 200: train loss 2.5615, val loss 2.6224 [9.25770616531372 sec]
step 300: train loss 2.4096, val loss 2.5093 [12.877973079681396 sec]
step 400: train loss 2.3073, val loss 2.4116 [16.39630889892578 sec]
2.2044742107391357
Total Training Time: 17.96552324295044 seconds

noundd was weroose akew dhe mof lishe figh sunsllimpe grlomiih seesed anmantes al tere. Boun."Ancini
BEGINNING (1681861227.456699): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6779, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6635, val loss 4.6640 [2.804103136062622 sec]
step 100: train loss 2.7289, val loss 2.7682 [7.763989210128784 sec]
step 200: train loss 2.5371, val loss 2.6038 [12.528385162353516 sec]
step 300: train loss 2.3954, val loss 2.4716 [17.35700035095215 sec]
step 400: train loss 2.2720, val loss 2.3769 [22.382585048675537 sec]
2.244450569152832
Total Training Time: 24.58428692817688 seconds

Kandemypend, mre lrantens
cavefeve woid tefespampe hal
Grarattacta luon ofgel'cks im fof sondd
"Bull
BEGINNING (1681861252.806049): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6537, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6374, val loss 4.6506 [1.3298301696777344 sec]
step 100: train loss 2.8132, val loss 2.8567 [3.4135589599609375 sec]
step 200: train loss 2.6406, val loss 2.7020 [5.438241958618164 sec]
step 300: train loss 2.5458, val loss 2.6212 [7.862927436828613 sec]
step 400: train loss 2.4819, val loss 2.5657 [10.093836784362793 sec]
2.4361190795898438
Total Training Time: 11.05471134185791 seconds

fllljewyurifh lind, an
bancy ke cor cono nd curd h fouf hinUs the Thinst med. Pionye Hig sA
qoofofru
BEGINNING (1681861264.23634): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6357, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6638, val loss 4.6715 [2.1139307022094727 sec]
step 100: train loss 2.7519, val loss 2.8114 [5.668548345565796 sec]
step 200: train loss 2.5818, val loss 2.6538 [8.94062614440918 sec]
step 300: train loss 2.4880, val loss 2.5770 [12.374111890792847 sec]
step 400: train loss 2.3978, val loss 2.4889 [15.734291076660156 sec]
2.2731995582580566
Total Training Time: 17.17188024520874 seconds

u/s 1KY
KANAived furtiolced at Thic, gin hekeche t
ccer. Thed tmar Elimorumaculdon? se “ th heread s
BEGINNING (1681861281.9802117): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6065, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5923, val loss 4.5810 [2.6055588722229004 sec]
step 100: train loss 2.7233, val loss 2.7665 [7.248858451843262 sec]
step 200: train loss 2.5576, val loss 2.6279 [11.773755073547363 sec]
step 300: train loss 2.4627, val loss 2.5365 [16.242308855056763 sec]
step 400: train loss 2.3783, val loss 2.4625 [20.921642303466797 sec]
2.315876007080078
Total Training Time: 22.883826971054077 seconds

thes ble see rsovaitidim. 4g. Grattt pusitfeyeus lefour Gry. Pitha nounstat
was talsara paertey as t
BEGINNING (1681861305.6103406): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6043, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6084, val loss 4.6069 [1.4003639221191406 sec]
step 100: train loss 2.8030, val loss 2.8417 [3.7581517696380615 sec]
step 200: train loss 2.6359, val loss 2.7018 [6.074116945266724 sec]
step 300: train loss 2.5602, val loss 2.6311 [8.369640111923218 sec]
step 400: train loss 2.5201, val loss 2.5872 [10.67333984375 sec]
2.535468101501465
Total Training Time: 11.558413743972778 seconds

Ucto yhatha tidcka eythad ine ats."I w Te
Ifzang slia Shagasand, emithanoutand, mun fro a t an ted y
BEGINNING (1681861317.4838843): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5965, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5853, val loss 4.5751 [2.1489624977111816 sec]
step 100: train loss 2.7414, val loss 2.7890 [5.7817089557647705 sec]
step 200: train loss 2.6034, val loss 2.6630 [9.47253155708313 sec]
step 300: train loss 2.5302, val loss 2.5954 [13.292291164398193 sec]
step 400: train loss 2.4815, val loss 2.5582 [17.077040433883667 sec]
2.4548323154449463
Total Training Time: 18.595703125 seconds

qhe fllkerthed ar totey t. hele folon fs watue y offes hiw
"."Biskerth therre f Crse, s f Gr, The ke
BEGINNING (1681861336.6125736): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6009, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6163, val loss 4.6107 [2.8695905208587646 sec]
step 100: train loss 2.7189, val loss 2.7671 [7.847129583358765 sec]
step 200: train loss 2.5709, val loss 2.6416 [12.96196961402893 sec]
step 300: train loss 2.5073, val loss 2.5772 [19.093130111694336 sec]
step 400: train loss 2.4612, val loss 2.5363 [24.750314474105835 sec]
2.4204821586608887
Total Training Time: 27.67912983894348 seconds

ty y
tfo es atkere yoher and thasm a8
car fofuo ald a wais flit gisimedowr rtame plyuremsapracas tau
BEGINNING (1681861365.2173195): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6321, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6061, val loss 4.6035 [1.6063575744628906 sec]
step 100: train loss 2.8315, val loss 2.8687 [4.137883901596069 sec]
step 200: train loss 2.6116, val loss 2.6646 [6.524044990539551 sec]
step 300: train loss 2.4624, val loss 2.5371 [8.946491718292236 sec]
step 400: train loss 2.3353, val loss 2.4323 [11.29373574256897 sec]
2.3072593212127686
Total Training Time: 12.302855730056763 seconds

Aas!AS
"Kin tha - Per thithe tarat Ca. "
"Yeaund. Arpu toker
ching w m 5t arnt hegansy hea pfittouon
BEGINNING (1681861377.882163): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5513, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5606, val loss 4.5675 [2.30635929107666 sec]
step 100: train loss 2.7528, val loss 2.7932 [6.057167291641235 sec]
step 200: train loss 2.5254, val loss 2.5808 [9.65162467956543 sec]
step 300: train loss 2.3727, val loss 2.4619 [13.230589389801025 sec]
step 400: train loss 2.2472, val loss 2.3550 [16.816577434539795 sec]
2.222477674484253
Total Training Time: 18.46362829208374 seconds

Thoups ufns snaprote bis aungat ont. Nas, ak to, thalabors imigst ernf ter da forraechede niyser rou
BEGINNING (1681861396.9148521): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6354, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6484, val loss 4.6454 [3.0185420513153076 sec]
step 100: train loss 2.6829, val loss 2.7302 [7.853971719741821 sec]
step 200: train loss 2.4844, val loss 2.5467 [12.645369529724121 sec]
step 300: train loss 2.3204, val loss 2.4212 [17.764769315719604 sec]
step 400: train loss 2.2014, val loss 2.3180 [22.607285499572754 sec]
2.218357801437378
Total Training Time: 24.652561902999878 seconds

sus list sulo than
thinng
couWh his me
culd fattrolore
be the cay
where t wan ttif cled hemamy twend
BEGINNING (1681861422.3534098): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6189, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6221, val loss 4.6284 [1.548149824142456 sec]
step 100: train loss 2.7819, val loss 2.8338 [3.9219417572021484 sec]
step 200: train loss 2.6157, val loss 2.6815 [7.213799238204956 sec]
step 300: train loss 2.5253, val loss 2.5975 [10.484678030014038 sec]
step 400: train loss 2.4518, val loss 2.5237 [13.171887874603271 sec]
2.438227653503418
Total Training Time: 14.159412622451782 seconds

gopseethe ck his thaior oudd wht Aisthe garielondill tnihey. He maled comaus, Gret bare umin D
tTHim
BEGINNING (1681861436.8138273): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6380, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6320, val loss 4.6305 [2.3686959743499756 sec]
step 100: train loss 2.7313, val loss 2.7743 [6.376305818557739 sec]
step 200: train loss 2.5552, val loss 2.6248 [10.133878231048584 sec]
step 300: train loss 2.4593, val loss 2.5399 [13.713994264602661 sec]
step 400: train loss 2.3520, val loss 2.4451 [17.24173665046692 sec]
2.2757790088653564
Total Training Time: 18.673197984695435 seconds

ecfted, hier "Y puod lour opnd ga mal ; mie
bof lie sok
m iednosthe wid suthte he ken d, red wind wi
BEGINNING (1681861456.0001159): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5964, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6078, val loss 4.6012 [2.8635363578796387 sec]
step 100: train loss 2.6927, val loss 2.7472 [8.437113046646118 sec]
step 200: train loss 2.5362, val loss 2.6010 [13.456822633743286 sec]
step 300: train loss 2.4344, val loss 2.5146 [18.43686580657959 sec]
step 400: train loss 2.3239, val loss 2.4211 [24.212832927703857 sec]
2.2321643829345703
Total Training Time: 26.343430757522583 seconds

wlouthe wownge he. Theawe ware. – ses.
Zefuthin
hereft mard romakny hkn. Arumaeshaway, Gredthe.y htu
BEGINNING (1681861483.1195822): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6120, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6086, val loss 4.6112 [1.884368658065796 sec]
step 100: train loss 2.7908, val loss 2.8344 [4.790735483169556 sec]
step 200: train loss 2.6222, val loss 2.6818 [7.782299280166626 sec]
step 300: train loss 2.5548, val loss 2.6284 [10.738510608673096 sec]
step 400: train loss 2.5138, val loss 2.5893 [13.903239011764526 sec]
2.5099799633026123
Total Training Time: 15.009396314620972 seconds

atede miply, T't hed ozuttthepsen ve hr whaly ifisie shem. Cly
F ay wand is t, fegedld d lubyoned t 
BEGINNING (1681861498.4272032): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6480, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6546, val loss 4.6588 [2.7838029861450195 sec]
step 100: train loss 2.7426, val loss 2.7948 [7.4955713748931885 sec]
step 200: train loss 2.5937, val loss 2.6521 [12.247898578643799 sec]
step 300: train loss 2.5266, val loss 2.5930 [16.876923322677612 sec]
step 400: train loss 2.4797, val loss 2.5602 [21.575547218322754 sec]
2.4273874759674072
Total Training Time: 23.418374061584473 seconds

ount fof bat bye. Gllamow cfe ratadenged, peatondind l fow
oum."Red gtnveppatulet ove d ten ha ida l
BEGINNING (1681861522.3831477): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6913, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6972, val loss 4.7067 [3.730332851409912 sec]
step 100: train loss 2.7142, val loss 2.7673 [10.230346202850342 sec]
step 200: train loss 2.5638, val loss 2.6343 [16.555928468704224 sec]
step 300: train loss 2.4941, val loss 2.5700 [23.026739597320557 sec]
step 400: train loss 2.4418, val loss 2.5246 [29.457990169525146 sec]
2.4105823040008545
Total Training Time: 32.18096303939819 seconds

ad feave chy hale ar
death thid ofd ainf yed anand.
Iathicecan e a s owood balftis, yrsede nerearlor
BEGINNING (1681861555.3764427): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6420, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6628, val loss 4.6557 [1.700035572052002 sec]
step 100: train loss 2.8103, val loss 2.8429 [4.245016813278198 sec]
step 200: train loss 2.5795, val loss 2.6391 [6.857844352722168 sec]
step 300: train loss 2.4207, val loss 2.5116 [9.44882321357727 sec]
step 400: train loss 2.3001, val loss 2.3943 [12.04552412033081 sec]
2.294034481048584
Total Training Time: 12.944226264953613 seconds

the to ertt, iWeager ow
Chat iors E~lllan fon cand
hes
uanta kend ginipllw fra munot. "
haerandd Yed
BEGINNING (1681861568.6305795): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6535, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6344, val loss 4.6379 [2.288393020629883 sec]
step 100: train loss 2.7252, val loss 2.7710 [6.2848193645477295 sec]
step 200: train loss 2.5055, val loss 2.5761 [10.117289781570435 sec]
step 300: train loss 2.3487, val loss 2.4349 [14.33340048789978 sec]
step 400: train loss 2.2119, val loss 2.3225 [18.22103238105774 sec]
2.2004919052124023
Total Training Time: 19.697083473205566 seconds

any la direw mibste hlemand oiis riof towiss!4 ws he weng mall waid whus anys. I as is le 2 and sof 
BEGINNING (1681861588.8904397): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6186, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6226, val loss 4.6220 [3.009310007095337 sec]
step 100: train loss 2.6762, val loss 2.7456 [8.584913969039917 sec]
step 200: train loss 2.4763, val loss 2.5521 [13.700110912322998 sec]
step 300: train loss 2.3087, val loss 2.4068 [18.53441619873047 sec]
step 400: train loss 2.1701, val loss 2.2870 [23.3616464138031 sec]
2.0993242263793945
Total Training Time: 25.326791763305664 seconds

he Pight Gracatty.
CLKAPWond me T hadd foutild talve an
be donede tw lakacren spraesot. Thouf turent
BEGINNING (1681861614.9641): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6441, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6412, val loss 4.6485 [1.632167100906372 sec]
step 100: train loss 2.7942, val loss 2.8363 [4.196046590805054 sec]
step 200: train loss 2.6133, val loss 2.6765 [6.765725612640381 sec]
step 300: train loss 2.5191, val loss 2.5918 [9.380797147750854 sec]
step 400: train loss 2.4489, val loss 2.5305 [11.940681219100952 sec]
2.4253618717193604
Total Training Time: 12.863182306289673 seconds

at, bedinta send, oued
om. Ay of tDa veeay al Zup 5 no ftqwhalle werok mos, whiens wend tho Greres o
BEGINNING (1681861628.118275): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6169, val loss 4.6146 [2.3424410820007324 sec]
step 100: train loss 2.7217, val loss 2.7721 [6.149433135986328 sec]
step 200: train loss 2.5439, val loss 2.6184 [9.955302476882935 sec]
step 300: train loss 2.4473, val loss 2.5266 [13.783003091812134 sec]
step 400: train loss 2.3411, val loss 2.4378 [17.631086111068726 sec]
2.301450490951538
Total Training Time: 19.123067617416382 seconds

bantkahdeak the, oiy ad walled ve sey, snorer ans astr thiderthe rew uastheve dout ourrpurhrthuthety
BEGINNING (1681861647.7543006): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6637, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6649, val loss 4.6613 [3.017869472503662 sec]
step 100: train loss 2.6929, val loss 2.7447 [8.071807861328125 sec]
step 200: train loss 2.5327, val loss 2.5942 [13.13018536567688 sec]
step 300: train loss 2.4275, val loss 2.5099 [18.28458571434021 sec]
step 400: train loss 2.2984, val loss 2.3911 [23.34018325805664 sec]
2.199864625930786
Total Training Time: 25.375312328338623 seconds

om wcithas lere Er bener mon rens
waluldin PyAut cedneled hant dopsgilyaired wngher uairid
t od mf s
BEGINNING (1681861673.8805609): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6558, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6510, val loss 4.6483 [2.2633869647979736 sec]
step 100: train loss 2.7884, val loss 2.8353 [6.095685720443726 sec]
step 200: train loss 2.6157, val loss 2.6798 [9.875582218170166 sec]
step 300: train loss 2.5474, val loss 2.6169 [13.49799370765686 sec]
step 400: train loss 2.5019, val loss 2.5747 [17.066114902496338 sec]
2.5137622356414795
Total Training Time: 18.397141218185425 seconds

csme,
Ues arongeulelrarpkat hhe t.
G loy rt prooursis s f Andr blld batanghexndd orhed aest om gh a 
BEGINNING (1681861692.5832357): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.7272, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7327, val loss 4.7298 [3.4011287689208984 sec]
step 100: train loss 2.7443, val loss 2.7884 [9.015670537948608 sec]
step 200: train loss 2.5862, val loss 2.6490 [14.641832113265991 sec]
step 300: train loss 2.5096, val loss 2.5874 [20.253458499908447 sec]
step 400: train loss 2.4540, val loss 2.5380 [25.863299131393433 sec]
2.4666948318481445
Total Training Time: 28.098272800445557 seconds

on alous hem
tee ce theen fasheomak tubyondiwwaly t ls ay w an o, ss hin 1Thlurerind Fotie rorirpour
BEGINNING (1681861721.2095091): Baseline LR(0.00025) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6142, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6169, val loss 4.6041 [4.578083515167236 sec]
step 100: train loss 2.6889, val loss 2.7385 [12.884113788604736 sec]
step 200: train loss 2.5482, val loss 2.6149 [22.083069562911987 sec]
step 300: train loss 2.4825, val loss 2.5599 [30.15791344642639 sec]
step 400: train loss 2.4216, val loss 2.5040 [37.791025161743164 sec]
2.3795835971832275
Total Training Time: 40.88203454017639 seconds

?S Theat sareasithe wacatleamat a p.
’ Hhalend ond anr wes inis anel heshe ntis mofof ther wurid ben
BEGINNING (1681861762.909611): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6170, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6347, val loss 4.6389 [1.5797357559204102 sec]
step 100: train loss 2.6872, val loss 2.7402 [4.111944675445557 sec]
step 200: train loss 2.4867, val loss 2.5611 [6.67877197265625 sec]
step 300: train loss 2.3357, val loss 2.4209 [9.228604793548584 sec]
step 400: train loss 2.2365, val loss 2.3506 [11.754528760910034 sec]
2.174042224884033
Total Training Time: 12.769899368286133 seconds

aked pa nonden'l. Bars ordeirubs beller rou cunghuple to ilewo to cuation f the at "reshpuon od alka
BEGINNING (1681861776.0488193): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6858, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6607, val loss 4.6605 [2.4484407901763916 sec]
step 100: train loss 2.6420, val loss 2.6999 [6.648837566375732 sec]
step 200: train loss 2.4446, val loss 2.5330 [11.603162050247192 sec]
step 300: train loss 2.2756, val loss 2.3786 [16.21410608291626 sec]
step 400: train loss 2.1438, val loss 2.2718 [20.866095066070557 sec]
2.113309383392334
Total Training Time: 23.13385581970215 seconds

oO had I aV wives tas paThe bood an" Tuan Peret this wo in Ano gayahis he ap cous breevel blxibs tre
BEGINNING (1681861800.0603979): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6112, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6159, val loss 4.6169 [3.7400848865509033 sec]
step 100: train loss 2.6009, val loss 2.6643 [9.703448295593262 sec]
step 200: train loss 2.4134, val loss 2.5015 [15.448974132537842 sec]
step 300: train loss 2.2439, val loss 2.3552 [21.068832635879517 sec]
step 400: train loss 2.1241, val loss 2.2665 [26.652153968811035 sec]
2.07443904876709
Total Training Time: 29.211397171020508 seconds

no“ wile rosen tuon then agom to to Getta seatta itily ing the beat hest toN he Muthe theaces
THe ci
BEGINNING (1681861830.2918954): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6445, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6341, val loss 4.6366 [1.6216518878936768 sec]
step 100: train loss 2.6905, val loss 2.7549 [4.2065205574035645 sec]
step 200: train loss 2.5477, val loss 2.6162 [6.886053562164307 sec]
step 300: train loss 2.4694, val loss 2.5458 [9.409492015838623 sec]
step 400: train loss 2.3625, val loss 2.4502 [11.87259554862976 sec]
2.291459798812866
Total Training Time: 12.859792709350586 seconds

EANGqanl patahttandd ubr.
"Whe wou TE Wls drere thas Thirid omasine
he Theremplyad llleinghin hy,
f 
BEGINNING (1681861843.534257): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6165, val loss 4.6213 [2.449270486831665 sec]
step 100: train loss 2.6328, val loss 2.6989 [6.62528920173645 sec]
step 200: train loss 2.4976, val loss 2.5743 [10.70661473274231 sec]
step 300: train loss 2.3980, val loss 2.4878 [15.010046482086182 sec]
step 400: train loss 2.2702, val loss 2.3755 [19.661496877670288 sec]
2.2823312282562256
Total Training Time: 21.59031319618225 seconds

smoolt boumecentr (ing Nisroullm.
"Yed tHeeght a chewop rit I Be whe offorsumpy. The nit ournot hask
BEGINNING (1681861865.8556337): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6127, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5969, val loss 4.5843 [3.379709243774414 sec]
step 100: train loss 2.6106, val loss 2.6825 [9.233969688415527 sec]
step 200: train loss 2.4659, val loss 2.5505 [15.416856527328491 sec]
step 300: train loss 2.3553, val loss 2.4457 [21.429638147354126 sec]
step 400: train loss 2.2187, val loss 2.3353 [27.774623155593872 sec]
2.1825146675109863
Total Training Time: 30.518656253814697 seconds

that aroncke the yrookeat mell jughim. He nowns uon le an
umd
wotu jut il unodedming."
Hed gort may 
BEGINNING (1681861897.472805): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5301, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5340, val loss 4.5425 [2.015864372253418 sec]
step 100: train loss 2.6739, val loss 2.7372 [5.761354684829712 sec]
step 200: train loss 2.5532, val loss 2.6220 [9.248109579086304 sec]
step 300: train loss 2.4947, val loss 2.5751 [12.224802017211914 sec]
step 400: train loss 2.4532, val loss 2.5399 [15.155843496322632 sec]
2.462742328643799
Total Training Time: 16.357388257980347 seconds

yarey ahmed cstee
cok a tuge benmirtthe theriit bot a w, ageun ocorands thanas ad mpemand
alras wini
BEGINNING (1681861914.2013607): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6322, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6335, val loss 4.6403 [2.884549617767334 sec]
step 100: train loss 2.6349, val loss 2.7060 [7.813467979431152 sec]
step 200: train loss 2.5213, val loss 2.5901 [12.902217626571655 sec]
step 300: train loss 2.4601, val loss 2.5388 [18.24828577041626 sec]
step 400: train loss 2.3982, val loss 2.4879 [23.61985182762146 sec]
2.3540382385253906
Total Training Time: 26.065539121627808 seconds

Asars fomwandtt al ing
pian'sefor ce. Cy grulll, than
rem." aratoved of cke oweng hauren. €ea iof
" 
BEGINNING (1681861941.037557): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6480, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6475, val loss 4.6455 [4.826434373855591 sec]
step 100: train loss 2.6078, val loss 2.6676 [12.52172303199768 sec]
step 200: train loss 2.5015, val loss 2.5862 [19.615050077438354 sec]
step 300: train loss 2.4379, val loss 2.5251 [26.702194690704346 sec]
step 400: train loss 2.3458, val loss 2.4500 [34.06927442550659 sec]
2.3047924041748047
Total Training Time: 37.09238076210022 seconds

waw priard 3AY haser apids fomed bememawngings
alle w thent aly
ur, al we tunve grin, bshak s, spked
BEGINNING (1681861979.2380629): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.7043, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6887, val loss 4.6819 [1.7427802085876465 sec]
step 100: train loss 2.6801, val loss 2.7294 [4.622802019119263 sec]
step 200: train loss 2.4695, val loss 2.5408 [7.457107067108154 sec]
step 300: train loss 2.3070, val loss 2.4020 [10.294156789779663 sec]
step 400: train loss 2.1906, val loss 2.3056 [13.4638192653656 sec]
2.1707191467285156
Total Training Time: 14.61286187171936 seconds

A thad nen thcut to aly.
"I Achaid, ate mull hat did shatyah hile theat an thieghis wis at, agemme t
BEGINNING (1681861994.2360263): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6182, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6142, val loss 4.6251 [2.584862470626831 sec]
step 100: train loss 2.6198, val loss 2.6765 [7.086481332778931 sec]
step 200: train loss 2.4033, val loss 2.4898 [11.810806274414062 sec]
step 300: train loss 2.2321, val loss 2.3407 [16.347137928009033 sec]
step 400: train loss 2.1021, val loss 2.2353 [20.608616590499878 sec]
2.0709774494171143
Total Training Time: 22.396122217178345 seconds

huy sif Aors she and the lookf tenw, trigop
of bef htrised s. The rill enny hat tlld Mncorke, the al
BEGINNING (1681862017.3623064): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6554, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6900, val loss 4.6864 [3.448935031890869 sec]
step 100: train loss 2.5834, val loss 2.6488 [9.37846040725708 sec]
step 200: train loss 2.3568, val loss 2.4508 [15.354453802108765 sec]
step 300: train loss 2.1839, val loss 2.3023 [21.29951810836792 sec]
step 400: train loss 2.0467, val loss 2.2114 [27.37730646133423 sec]
2.015293598175049
Total Training Time: 29.90549874305725 seconds

Taked and bowle had eatimy." He tooky."
Tak, be Gratta faraile arteds thene way.
The saely arhey teu
BEGINNING (1681862048.2743104): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5553, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5715, val loss 4.5858 [1.8143603801727295 sec]
step 100: train loss 2.6650, val loss 2.7231 [4.591642618179321 sec]
step 200: train loss 2.5165, val loss 2.5898 [7.441787481307983 sec]
step 300: train loss 2.4097, val loss 2.4973 [10.424365758895874 sec]
step 400: train loss 2.2873, val loss 2.3939 [13.305899381637573 sec]
2.2325055599212646
Total Training Time: 14.341894626617432 seconds

I smed c7ver. G coneme younell's buddellewn
Heveatta cutauid ad d. Thirof he
sity ea thenotr.
"Bur."
BEGINNING (1681862063.0072517): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.4940, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4964, val loss 4.4999 [2.6591970920562744 sec]
step 100: train loss 2.6179, val loss 2.6755 [7.114391803741455 sec]
step 200: train loss 2.4594, val loss 2.5413 [11.613279342651367 sec]
step 300: train loss 2.3287, val loss 2.4256 [16.138253211975098 sec]
step 400: train loss 2.1741, val loss 2.3033 [20.631039142608643 sec]
2.1033878326416016
Total Training Time: 22.528890132904053 seconds

esmut Bumat ave bearne. Gratartta atluon hame su's The
oing ananJ sown't youzimedgenns.
Her cof and

BEGINNING (1681862086.2160833): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6190, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6233, val loss 4.6269 [3.6744930744171143 sec]
step 100: train loss 2.5872, val loss 2.6493 [10.59730577468872 sec]
step 200: train loss 2.4280, val loss 2.5130 [17.206316232681274 sec]
step 300: train loss 2.2714, val loss 2.3842 [23.740257740020752 sec]
step 400: train loss 2.0961, val loss 2.2418 [30.18561291694641 sec]
2.002927780151367
Total Training Time: 32.825554847717285 seconds

Aly ceist in eesmelenay?" The rratead.
Aned celow you, spver son ar, theunne Tauas
wil ar sputed.
IZ
BEGINNING (1681862120.1205206): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6632, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6620, val loss 4.6531 [2.4644527435302734 sec]
step 100: train loss 2.6655, val loss 2.7240 [6.467048406600952 sec]
step 200: train loss 2.5471, val loss 2.6142 [10.49443531036377 sec]
step 300: train loss 2.4852, val loss 2.5643 [14.420949935913086 sec]
step 400: train loss 2.4287, val loss 2.5114 [18.621793746948242 sec]
2.398578643798828
Total Training Time: 20.15830659866333 seconds

mthe moplouowe twive, ld Hid s sherey he aleah soserde. wleriuse
squr at't anooond r."
A "Aica
– mnd
BEGINNING (1681862140.7098124): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6550, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6516, val loss 4.6527 [4.393296480178833 sec]
step 100: train loss 2.6305, val loss 2.6952 [11.135772705078125 sec]
step 200: train loss 2.5096, val loss 2.5833 [18.27623176574707 sec]
step 300: train loss 2.4363, val loss 2.5175 [25.705917358398438 sec]
step 400: train loss 2.3545, val loss 2.4527 [33.103477001190186 sec]
2.2748115062713623
Total Training Time: 35.811848878860474 seconds

st aved. Ast agled anad. Non worrt f yof gho Tratsur tomirighat th. Vere
marim ma itick the. Ha'sthe
BEGINNING (1681862177.2296402): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6339, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6294, val loss 4.6274 [6.219388484954834 sec]
step 100: train loss 2.5984, val loss 2.6609 [15.616411209106445 sec]
step 200: train loss 2.4912, val loss 2.5638 [26.74008345603943 sec]
step 300: train loss 2.4121, val loss 2.4940 [38.186747312545776 sec]
step 400: train loss 2.2952, val loss 2.4095 [48.055166482925415 sec]
2.1846678256988525
Total Training Time: 51.800944805145264 seconds

soriong th fos thely anmors. " hit! Be Pe)
ssmeadd s rer thadsthe at taus atgred. akeddold gror thar
BEGINNING (1681862230.0941317): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6118, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6174, val loss 4.6162 [1.9643549919128418 sec]
step 100: train loss 2.6584, val loss 2.7085 [5.333478212356567 sec]
step 200: train loss 2.4344, val loss 2.5071 [8.383012771606445 sec]
step 300: train loss 2.2529, val loss 2.3578 [11.529417514801025 sec]
step 400: train loss 2.1420, val loss 2.2703 [14.59699034690857 sec]
2.1916470527648926
Total Training Time: 15.657731056213379 seconds

will of the or whe cowchamyou upe gos wcelly."
Crianot din hy mon thevill alyss raved hacloor solbe.
BEGINNING (1681862246.1239142): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6177, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6188, val loss 4.6064 [2.862018585205078 sec]
step 100: train loss 2.5959, val loss 2.6571 [8.516988754272461 sec]
step 200: train loss 2.3529, val loss 2.4384 [13.750633716583252 sec]
step 300: train loss 2.1671, val loss 2.2856 [18.510693788528442 sec]
step 400: train loss 2.0287, val loss 2.1790 [23.458328247070312 sec]
2.010286331176758
Total Training Time: 25.384636402130127 seconds

happof to callyse, (ore will
mante ofir had allean't ationh colly thomen, the thune8 Pre Yong boffon
BEGINNING (1681862272.2237484): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6906, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6883, val loss 4.6914 [3.7157440185546875 sec]
step 100: train loss 2.5668, val loss 2.6438 [9.844013690948486 sec]
step 200: train loss 2.3127, val loss 2.4055 [15.824823141098022 sec]
step 300: train loss 2.1300, val loss 2.2575 [21.856364965438843 sec]
step 400: train loss 1.9848, val loss 2.1495 [27.895275115966797 sec]
1.9831427335739136
Total Training Time: 30.39339780807495 seconds

ward, thely ssuld shas look, "I heauld vaty
ons wearkit with as dowlad sprived, "I cast and beact ga
BEGINNING (1681862303.5907025): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6205, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6115, val loss 4.6058 [2.0150527954101562 sec]
step 100: train loss 2.6618, val loss 2.7152 [5.248640298843384 sec]
step 200: train loss 2.5093, val loss 2.5798 [8.46487832069397 sec]
step 300: train loss 2.3671, val loss 2.4583 [11.689527988433838 sec]
step 400: train loss 2.2105, val loss 2.3332 [14.897825241088867 sec]
2.202877998352051
Total Training Time: 16.181994438171387 seconds

El wer ho sot thait muld the are pagh PThe
coubs Theats furreforeent ot prey sort yrearn'
That. a – 
BEGINNING (1681862320.1847024): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6546, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6418, val loss 4.6313 [3.2771284580230713 sec]
step 100: train loss 2.6104, val loss 2.6670 [8.443290710449219 sec]
step 200: train loss 2.4552, val loss 2.5380 [13.522500038146973 sec]
step 300: train loss 2.2943, val loss 2.3959 [18.608925819396973 sec]
step 400: train loss 2.1318, val loss 2.2646 [23.715973615646362 sec]
2.0908126831054688
Total Training Time: 25.808541297912598 seconds

alowe well fore bemavec. Tuit? Ana Toons iphaverthe qmaved sof
toon's idgh.
"He colly wit "It a his 
BEGINNING (1681862346.6922305): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6321, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6317, val loss 4.6381 [4.620684623718262 sec]
step 100: train loss 2.5798, val loss 2.6440 [11.74171257019043 sec]
step 200: train loss 2.4188, val loss 2.5023 [19.37633752822876 sec]
step 300: train loss 2.2377, val loss 2.3503 [26.753506183624268 sec]
step 400: train loss 2.0518, val loss 2.1971 [35.375389099121094 sec]
1.981726050376892
Total Training Time: 38.942511320114136 seconds

TA7T YEATE
Ok TEY
FGCHAPEKY
GPTUEEREAS
McAPTEFSE Tnekews towl the forgavaked hiser nW
Tree watuon ce
BEGINNING (1681862386.993748): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5835, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5856, val loss 4.5796 [3.366349458694458 sec]
step 100: train loss 2.6645, val loss 2.7227 [8.955101013183594 sec]
step 200: train loss 2.5307, val loss 2.6012 [13.95011305809021 sec]
step 300: train loss 2.4695, val loss 2.5477 [18.91849708557129 sec]
step 400: train loss 2.4018, val loss 2.4922 [23.76606559753418 sec]
2.386461019515991
Total Training Time: 25.85598087310791 seconds

rpont the. Thed wof gedert fieds at nois crat tackns wandecatne nto llMuneal git cat t thed withe. t
BEGINNING (1681862413.3673544): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7076, val loss 4.7057 [5.42931866645813 sec]
step 100: train loss 2.6271, val loss 2.6977 [15.011568069458008 sec]
step 200: train loss 2.5012, val loss 2.5724 [24.63503932952881 sec]
step 300: train loss 2.4282, val loss 2.5075 [33.95632982254028 sec]
step 400: train loss 2.3323, val loss 2.4273 [42.983659982681274 sec]
2.2835609912872314
Total Training Time: 46.23902940750122 seconds

cl dorepornat igl." Pyothe w!"
Batt wothe Pnteat silleroers hh
ay. Toud simerilout fore, ay ou d. An
BEGINNING (1681862460.3250175): Baseline LR(0.00025) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5855, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5908, val loss 4.6019 [6.466111660003662 sec]
step 100: train loss 2.5863, val loss 2.6512 [17.862346649169922 sec]
step 200: train loss 2.4746, val loss 2.5537 [30.32146644592285 sec]
step 300: train loss 2.3786, val loss 2.4765 [42.0702486038208 sec]
step 400: train loss 2.2349, val loss 2.3491 [54.78565216064453 sec]
2.1672744750976562
Total Training Time: 61.079455852508545 seconds

te otra smemaroun the sualme wad to an tn lot, aid
lined andern wasit." "One, to tonged."
ER Arew ma
BEGINNING (1681862522.5155468): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6837, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6643, val loss 4.6602 [1.9659276008605957 sec]
step 100: train loss 2.6130, val loss 2.6794 [5.052957534790039 sec]
step 200: train loss 2.3972, val loss 2.4813 [8.264068841934204 sec]
step 300: train loss 2.2324, val loss 2.3613 [11.391984224319458 sec]
step 400: train loss 2.1268, val loss 2.2567 [14.59452509880066 sec]
2.1498703956604004
Total Training Time: 15.712072849273682 seconds

vas milsply
himat neam, "Con Tuods, Yurdwernig. I the ned Pilg Grratta preirgat
coud pre aned." Ging
BEGINNING (1681862538.6926525): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5903, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5972, val loss 4.6069 [2.8630669116973877 sec]
step 100: train loss 2.5575, val loss 2.6291 [7.957087516784668 sec]
step 200: train loss 2.3651, val loss 2.4575 [13.234821319580078 sec]
step 300: train loss 2.1850, val loss 2.3006 [18.757988929748535 sec]
step 400: train loss 2.0543, val loss 2.2000 [24.269277572631836 sec]
1.9465190172195435
Total Training Time: 26.968275785446167 seconds

Anay reast
you ful and astonrs uts hat delam stepb, aid ead, spung, waich way.
Gratta
sta suddwild s
BEGINNING (1681862566.613022): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5392, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5802, val loss 4.5930 [4.3141725063323975 sec]
step 100: train loss 2.5423, val loss 2.6144 [12.160156726837158 sec]
step 200: train loss 2.3269, val loss 2.4223 [20.0883731842041 sec]
step 300: train loss 2.1349, val loss 2.2791 [27.36465573310852 sec]
step 400: train loss 1.9978, val loss 2.1516 [34.52362847328186 sec]
2.0699329376220703
Total Training Time: 37.65633177757263 seconds

roves and emeys
the the fats
eir. Grata thuss
on a was his yrie mir tookeats an teaky a stily es
upi
BEGINNING (1681862605.5665524): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6384, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6288, val loss 4.6372 [1.8323805332183838 sec]
step 100: train loss 2.6216, val loss 2.6799 [4.881286859512329 sec]
step 200: train loss 2.4739, val loss 2.5493 [8.134653568267822 sec]
step 300: train loss 2.3384, val loss 2.4196 [11.472010374069214 sec]
step 400: train loss 2.1876, val loss 2.3135 [14.667909622192383 sec]
2.107548236846924
Total Training Time: 15.930326461791992 seconds

cam th ce
icingay.
" themp
to ainglaitges to on 
thisst ingll tung Arnck celidd
cell
ow themalveed, 
BEGINNING (1681862622.0007615): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6564, val loss 4.6570 [3.0151615142822266 sec]
step 100: train loss 2.5810, val loss 2.6521 [8.063977241516113 sec]
step 200: train loss 2.4381, val loss 2.5288 [13.10367727279663 sec]
step 300: train loss 2.2823, val loss 2.3874 [18.640626192092896 sec]
step 400: train loss 2.1150, val loss 2.2674 [23.79667329788208 sec]
2.133424758911133
Total Training Time: 26.0263569355011 seconds

Clatew ourdrratna mise youss recoffo tur.
"Whall
se the sment cuw,
woured nrads coter Anahsubeend on
BEGINNING (1681862649.0003612): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5493, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5561, val loss 4.5525 [4.533902883529663 sec]
step 100: train loss 2.5535, val loss 2.6373 [12.486187934875488 sec]
step 200: train loss 2.4150, val loss 2.5056 [20.059087991714478 sec]
step 300: train loss 2.2581, val loss 2.3714 [27.4301815032959 sec]
step 400: train loss 2.0845, val loss 2.2304 [34.63370084762573 sec]
2.0369391441345215
Total Training Time: 37.771114349365234 seconds

pred, Ve shadis ded sacainch, and ent of the gromestin
his tan esatea– he actread dout.
Tor thi dele
BEGINNING (1681862688.0574768): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6656, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6764, val loss 4.6783 [2.1820600032806396 sec]
step 100: train loss 2.6221, val loss 2.6871 [5.803512096405029 sec]
step 200: train loss 2.5201, val loss 2.5927 [9.598859786987305 sec]
step 300: train loss 2.4625, val loss 2.5384 [13.340683937072754 sec]
step 400: train loss 2.4046, val loss 2.4878 [17.291250944137573 sec]
2.3509092330932617
Total Training Time: 18.78814125061035 seconds

pthe t eatto Prid 4hop samandad
deazed."Yende sed grmir alastor rstherin d wastutiled wad ben wed
of
BEGINNING (1681862707.31961): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5648, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5606, val loss 4.5559 [4.002503395080566 sec]
step 100: train loss 2.5833, val loss 2.6492 [10.815276861190796 sec]
step 200: train loss 2.4827, val loss 2.5640 [17.237115383148193 sec]
step 300: train loss 2.4085, val loss 2.4950 [23.7103111743927 sec]
step 400: train loss 2.3071, val loss 2.4146 [30.086445569992065 sec]
2.211883544921875
Total Training Time: 32.97267198562622 seconds

ap rrins mor fi, son ounqa's Theg ind
thbeat sis imap tuwe che
Tumaved benckerppes nome." Aid, – Pya
BEGINNING (1681862741.1437986): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6220, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5993, val loss 4.6117 [5.130987882614136 sec]
step 100: train loss 2.5581, val loss 2.6340 [14.280189752578735 sec]
step 200: train loss 2.4635, val loss 2.5469 [24.536651849746704 sec]
step 300: train loss 2.3726, val loss 2.4676 [34.717812299728394 sec]
step 400: train loss 2.2153, val loss 2.3405 [43.997347831726074 sec]
2.1191890239715576
Total Training Time: 47.94392418861389 seconds

60
SEN"G VERNRI PTullkned THEAY
O
G TE TDCES V; CH
ART
SE h
SEI1~HA AI EE
ACN MEANG PNHAN tatuimp. G
BEGINNING (1681862790.4148881): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6575, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6682, val loss 4.6707 [1.922670841217041 sec]
step 100: train loss 2.5901, val loss 2.6603 [5.277978897094727 sec]
step 200: train loss 2.3505, val loss 2.4394 [8.479734897613525 sec]
step 300: train loss 2.1831, val loss 2.3051 [11.827811002731323 sec]
step 400: train loss 2.0666, val loss 2.2212 [15.209774732589722 sec]
2.070070266723633
Total Training Time: 16.492769479751587 seconds

Mlis fill hum to wear bet notiod him thas smove teew, Pris..
"Charnagsh, hime, a ncem lenets wink he
BEGINNING (1681862807.372658): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5937, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6064, val loss 4.6001 [3.1879031658172607 sec]
step 100: train loss 2.5418, val loss 2.6071 [8.79021430015564 sec]
step 200: train loss 2.2916, val loss 2.3953 [14.555275678634644 sec]
step 300: train loss 2.1071, val loss 2.2609 [19.92880415916443 sec]
step 400: train loss 1.9546, val loss 2.1135 [25.59948968887329 sec]
1.9587312936782837
Total Training Time: 27.97573971748352 seconds

his crave aites
have ing O plroken poow Nowed. Ard Cle
domow and thoioghour be gooke's at
O prateren
BEGINNING (1681862836.2389858): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.7408, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7464, val loss 4.7351 [4.518016576766968 sec]
step 100: train loss 2.5200, val loss 2.5887 [12.755841732025146 sec]
step 200: train loss 2.2791, val loss 2.3887 [20.86625099182129 sec]
step 300: train loss 2.0730, val loss 2.2247 [28.74789810180664 sec]
step 400: train loss 1.9390, val loss 2.1267 [36.714746713638306 sec]
1.9363441467285156
Total Training Time: 40.161845684051514 seconds

SERDEWMTER TER O I
thal was you nedelt, bast thenray sull.
Arhand him. Siah the fougr
Gratta Eow his
BEGINNING (1681862877.7781389): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6083, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6160, val loss 4.6099 [2.293335437774658 sec]
step 100: train loss 2.5970, val loss 2.6659 [5.956311464309692 sec]
step 200: train loss 2.4517, val loss 2.5371 [9.61394190788269 sec]
step 300: train loss 2.3082, val loss 2.4080 [13.442826509475708 sec]
step 400: train loss 2.1481, val loss 2.2734 [17.318055868148804 sec]
2.1156857013702393
Total Training Time: 18.717665433883667 seconds

snoy to the to The wily. Nitt murne I berrow thritt se," "Me a walled qill habus w and of Youshist b
BEGINNING (1681862897.0099077): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.7233, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7055, val loss 4.7035 [3.5850515365600586 sec]
step 100: train loss 2.5616, val loss 2.6284 [9.712057828903198 sec]
step 200: train loss 2.4000, val loss 2.4890 [15.84336805343628 sec]
step 300: train loss 2.2313, val loss 2.3371 [22.170026779174805 sec]
step 400: train loss 2.0645, val loss 2.2213 [28.240203619003296 sec]
2.0086452960968018
Total Training Time: 30.80734419822693 seconds

stroo leathile a tarne, "Yiy he mo hat the ead
Taka hadirev. God. Thimsefor
thity whit foms you astu
BEGINNING (1681862928.7796934): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6339, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6257, val loss 4.6289 [4.994004011154175 sec]
step 100: train loss 2.5432, val loss 2.6148 [13.572807550430298 sec]
step 200: train loss 2.3754, val loss 2.4716 [22.021583557128906 sec]
step 300: train loss 2.1604, val loss 2.2863 [30.55321168899536 sec]
step 400: train loss 1.9822, val loss 2.1514 [39.0842981338501 sec]
1.9489482641220093
Total Training Time: 42.73109817504883 seconds

AYKED TEEGCW
CHAPTUWh AY
CAPTEN Take mall the
APEAE
HES TEN howould ashf cRAY
hare causs snid. "EAN7
BEGINNING (1681862972.8659394): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5399, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5513, val loss 4.5575 [3.245997428894043 sec]
step 100: train loss 2.6064, val loss 2.6733 [8.731976509094238 sec]
step 200: train loss 2.4966, val loss 2.5726 [14.364686489105225 sec]
step 300: train loss 2.4277, val loss 2.5129 [19.86018180847168 sec]
step 400: train loss 2.3394, val loss 2.4376 [25.42318058013916 sec]
2.2948646545410156
Total Training Time: 27.71941304206848 seconds

som ans oa bed wices bef th,
and to woreved snon brr!
"O gand tonethas monarnt bt amoma int Py, byot
BEGINNING (1681863001.0893452): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6215, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6169, val loss 4.6138 [5.770620584487915 sec]
step 100: train loss 2.5682, val loss 2.6385 [15.380172967910767 sec]
step 200: train loss 2.4739, val loss 2.5525 [25.0862295627594 sec]
step 300: train loss 2.3784, val loss 2.4667 [34.72346472740173 sec]
step 400: train loss 2.2315, val loss 2.3408 [44.35173177719116 sec]
2.105797529220581
Total Training Time: 48.79901695251465 seconds

orge se reartanderraxcim
warior pait longays, and he rent moff sthit!"
Grattatttight we ugrog an fol
BEGINNING (1681863050.87702): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6768, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6695, val loss 4.6747 [8.273114681243896 sec]
step 100: train loss 2.5454, val loss 2.6147 [22.578994512557983 sec]
step 200: train loss 2.4409, val loss 2.5202 [37.29284739494324 sec]
step 300: train loss 2.3140, val loss 2.4142 [49.34952998161316 sec]
step 400: train loss 2.1292, val loss 2.2662 [62.86669850349426 sec]
2.017411708831787
Total Training Time: 68.4189875125885 seconds

foryauch whe wkermire, the winght wo wers doouldm
waskande tuld locs, sm the tuor imst. Thererly
CHA
BEGINNING (1681863120.7666485): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6003, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5986, val loss 4.6045 [2.6630492210388184 sec]
step 100: train loss 2.5809, val loss 2.6415 [6.53239107131958 sec]
step 200: train loss 2.3250, val loss 2.4172 [10.098781108856201 sec]
step 300: train loss 2.1628, val loss 2.2758 [13.575041770935059 sec]
step 400: train loss 2.0359, val loss 2.1911 [17.051927089691162 sec]
2.027224540710449
Total Training Time: 18.3459632396698 seconds

Gratta nod scon thering atin. 
Kquing is on yoâa Aord gudneytah ha toidge thay's
to niN Gratta. Alpo
BEGINNING (1681863139.5955298): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6714, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6652, val loss 4.6728 [3.438868999481201 sec]
step 100: train loss 2.5201, val loss 2.5900 [9.54129958152771 sec]
step 200: train loss 2.2631, val loss 2.3641 [15.291464567184448 sec]
step 300: train loss 2.0617, val loss 2.2026 [20.856357097625732 sec]
step 400: train loss 1.9262, val loss 2.0834 [26.60059905052185 sec]
1.8998899459838867
Total Training Time: 29.013720989227295 seconds

gerust,
and him and to hast thhe pand anis.
Namate hir
tow, den was selled Arnayah caidt comig, rat 
BEGINNING (1681863169.4890294): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5783, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5918, val loss 4.5983 [4.657555818557739 sec]
step 100: train loss 2.4958, val loss 2.5729 [13.342637300491333 sec]
step 200: train loss 2.2275, val loss 2.3516 [21.603976726531982 sec]
step 300: train loss 2.0176, val loss 2.1868 [29.711864948272705 sec]
step 400: train loss 1.8767, val loss 2.0708 [37.425353050231934 sec]
1.8781342506408691
Total Training Time: 40.704699754714966 seconds

frate mige, he cubs. He graat smagcesters. Tike wall nant hid andssoned the wo?"
I seokle roun, flow
BEGINNING (1681863211.4537878): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6099, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6081, val loss 4.6108 [2.626613140106201 sec]
step 100: train loss 2.5931, val loss 2.6593 [6.859846353530884 sec]
step 200: train loss 2.4302, val loss 2.5110 [10.79439091682434 sec]
step 300: train loss 2.2504, val loss 2.3607 [14.617873191833496 sec]
step 400: train loss 2.0896, val loss 2.2358 [18.594282150268555 sec]
2.0693981647491455
Total Training Time: 20.147388696670532 seconds

peece warient angirnes umped had be colg, than's ward
tuoncoing ince laroithe ne seon!"
7©Gratta the
BEGINNING (1681863232.0851986): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6107, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6142, val loss 4.6154 [4.446970462799072 sec]
step 100: train loss 2.5449, val loss 2.6110 [11.70970368385315 sec]
step 200: train loss 2.3679, val loss 2.4645 [19.02417540550232 sec]
step 300: train loss 2.1569, val loss 2.2906 [26.170189142227173 sec]
step 400: train loss 1.9772, val loss 2.1502 [33.29285478591919 sec]
1.9566712379455566
Total Training Time: 36.23821806907654 seconds

PRAN M"McKII mu Thad wat?
0I EE you will prealat PEEAN"
GrESstatill weour and and
mull StiâThackMat 
BEGINNING (1681863269.2572532): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6182, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6152, val loss 4.6119 [5.742457151412964 sec]
step 100: train loss 2.5207, val loss 2.5965 [15.625782251358032 sec]
step 200: train loss 2.3421, val loss 2.4364 [25.890652894973755 sec]
step 300: train loss 2.1078, val loss 2.2513 [35.75650954246521 sec]
step 400: train loss 1.9298, val loss 2.1158 [45.70637249946594 sec]
1.865544319152832
Total Training Time: 49.887752294540405 seconds

thec, ned and nowd end as witce seul in that
Critilfem.
Ifere fring to frole."
Gratta lour wsellowav
BEGINNING (1681863320.4042783): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6827, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6656, val loss 4.6620 [3.749345302581787 sec]
step 100: train loss 2.6046, val loss 2.6692 [9.978690147399902 sec]
step 200: train loss 2.4899, val loss 2.5657 [16.21661925315857 sec]
step 300: train loss 2.4114, val loss 2.5055 [22.462863206863403 sec]
step 400: train loss 2.2946, val loss 2.3964 [28.77962017059326 sec]
2.2130446434020996
Total Training Time: 31.328097581863403 seconds

oll, – Tround milexd watuem
t feandt was. "DKs ghime. High shint fo for ren reen Turat cent
seir ane
BEGINNING (1681863352.2123744): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5735, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5767, val loss 4.5802 [6.573121786117554 sec]
step 100: train loss 2.5580, val loss 2.6257 [17.659653425216675 sec]
step 200: train loss 2.4541, val loss 2.5316 [28.626669883728027 sec]
step 300: train loss 2.3370, val loss 2.4365 [39.60676407814026 sec]
step 400: train loss 2.1526, val loss 2.2827 [50.850525856018066 sec]
2.0470476150512695
Total Training Time: 57.6962251663208 seconds

Aiste ras. Ther mued Mcis! Gored
conenty of fors and heund be sis,
thared wils syah air cagom fir an
BEGINNING (1681863410.88626): Baseline LR(0.00025) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5419, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5440, val loss 4.5453 [11.642112016677856 sec]
step 100: train loss 2.5465, val loss 2.6150 [30.13795757293701 sec]
step 200: train loss 2.4278, val loss 2.5125 [47.98569369316101 sec]
step 300: train loss 2.2862, val loss 2.3963 [60.62843132019043 sec]
step 400: train loss 2.0852, val loss 2.2300 [73.00102162361145 sec]
2.029484510421753
Total Training Time: 78.45355582237244 seconds

the with? Gordin's to mullal'turnus nitswas,
the and what hais sy therpytwith shes morien asns owari
BEGINNING (1681863490.693106): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6539, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6605, val loss 4.6546 [2.200575828552246 sec]
step 100: train loss 2.5336, val loss 2.6082 [6.040046215057373 sec]
step 200: train loss 2.2624, val loss 2.3647 [9.762215375900269 sec]
step 300: train loss 2.1025, val loss 2.2450 [13.490627527236938 sec]
step 400: train loss 1.9909, val loss 2.1613 [17.27304768562317 sec]
1.9061250686645508
Total Training Time: 18.884920358657837 seconds

outwayah pailted be wouch stuss at.
"We shat Gex fulat you fricl you hem.
Anay notented Goutzento sp
BEGINNING (1681863510.2781258): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6792, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6307, val loss 4.6194 [3.893439531326294 sec]
step 100: train loss 2.4868, val loss 2.5673 [11.007309436798096 sec]
step 200: train loss 2.2295, val loss 2.3417 [17.697929620742798 sec]
step 300: train loss 2.0557, val loss 2.2025 [24.67504072189331 sec]
step 400: train loss 1.9231, val loss 2.1136 [32.03821611404419 sec]
1.8158303499221802
Total Training Time: 35.171483755111694 seconds

mole yours to of ack. The mo wo walligh to smas, and dook the suvere
shomplights had ive king wearo 
BEGINNING (1681863546.775996): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6897, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6949, val loss 4.6969 [6.432743072509766 sec]
step 100: train loss 2.4622, val loss 2.5451 [17.46127676963806 sec]
step 200: train loss 2.2146, val loss 2.3360 [28.475780487060547 sec]
step 300: train loss 2.0060, val loss 2.1608 [39.7518413066864 sec]
step 400: train loss 1.8905, val loss 2.0803 [51.1025390625 sec]
1.8550512790679932
Total Training Time: 55.859612464904785 seconds

at woulderhers with thaps!" Arpoghips
and of in to be upen
musert. They was nell our was as to the i
BEGINNING (1681863604.5904858): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6467, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6468, val loss 4.6437 [2.5362350940704346 sec]
step 100: train loss 2.5402, val loss 2.6187 [6.80558967590332 sec]
step 200: train loss 2.3857, val loss 2.4844 [10.955995559692383 sec]
step 300: train loss 2.1892, val loss 2.3281 [15.259603500366211 sec]
step 400: train loss 2.0378, val loss 2.2179 [19.074856519699097 sec]
2.027170181274414
Total Training Time: 20.64621067047119 seconds

A III to knowding. Benwa slots, Pyou bealdide of bey nomed ot kenstrooned an be." Gratta stake
6EAIN
BEGINNING (1681863625.8608084): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5265, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5388, val loss 4.5351 [3.863718271255493 sec]
step 100: train loss 2.5139, val loss 2.5891 [11.304766178131104 sec]
step 200: train loss 2.3391, val loss 2.4423 [19.853023767471313 sec]
step 300: train loss 2.1124, val loss 2.2545 [27.916146755218506 sec]
step 400: train loss 1.9327, val loss 2.1229 [35.22654604911804 sec]
1.939440369606018
Total Training Time: 38.4737823009491 seconds

pad ouhch at mhe torst satced. This to coplenst he elrectem bons weathed
his hart." Cad Aid pard kea
BEGINNING (1681863665.6804526): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6654, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6865, val loss 4.6943 [5.824174165725708 sec]
step 100: train loss 2.5133, val loss 2.5914 [15.942187547683716 sec]
step 200: train loss 2.3246, val loss 2.4360 [26.72736644744873 sec]
step 300: train loss 2.0756, val loss 2.2246 [38.52462029457092 sec]
step 400: train loss 1.9103, val loss 2.0809 [50.28980731964111 sec]
1.9098694324493408
Total Training Time: 54.930052280426025 seconds

ver with sier heeve well tuong the dractester meled. Kriyan's
snevaly feew to turnandgill. Ssuighty 
BEGINNING (1681863722.5584269): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6100, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6470, val loss 4.6435 [2.8663442134857178 sec]
step 100: train loss 2.5527, val loss 2.6268 [7.69151759147644 sec]
step 200: train loss 2.4555, val loss 2.5406 [12.50908350944519 sec]
step 300: train loss 2.3541, val loss 2.4496 [18.215205430984497 sec]
step 400: train loss 2.1989, val loss 2.3240 [23.355093479156494 sec]
2.241008758544922
Total Training Time: 25.428130865097046 seconds

ndepeacer."S
Che sound yare Whimeys urdedell, cuaen. caun "The
hae mesisus ansard tolion the tal all
BEGINNING (1681863748.735061): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5919, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5980, val loss 4.6026 [5.095377206802368 sec]
step 100: train loss 2.5186, val loss 2.5944 [14.08169937133789 sec]
step 200: train loss 2.4151, val loss 2.5094 [23.328600645065308 sec]
step 300: train loss 2.2659, val loss 2.3818 [32.87005043029785 sec]
step 400: train loss 2.0687, val loss 2.2185 [44.15846395492554 sec]
2.0159413814544678
Total Training Time: 48.70909857749939 seconds

Afn 75 – Martie che pump.
Dingn ear to feourpel pecuchil. Namasts an maker whant with
78
COCO
TER – 
BEGINNING (1681863798.753043): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6087, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6130, val loss 4.6068 [6.6285083293914795 sec]
step 100: train loss 2.5195, val loss 2.5995 [19.140671730041504 sec]
step 200: train loss 2.4228, val loss 2.5111 [31.698663473129272 sec]
step 300: train loss 2.2393, val loss 2.3550 [44.320772886276245 sec]
step 400: train loss 2.0147, val loss 2.1791 [58.626548767089844 sec]
1.9513734579086304
Total Training Time: 65.88343143463135 seconds

why sent evell was ras watew. We heraly
land
tene him have," to Gratta noodgeaysted a leotece inin
t
BEGINNING (1681863866.9407415): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6476, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6274, val loss 4.6239 [3.0228283405303955 sec]
step 100: train loss 2.5021, val loss 2.5739 [7.799823760986328 sec]
step 200: train loss 2.2029, val loss 2.3148 [12.045918941497803 sec]
step 300: train loss 2.0374, val loss 2.2054 [16.248857975006104 sec]
step 400: train loss 1.9356, val loss 2.1175 [20.677762746810913 sec]
1.9131717681884766
Total Training Time: 22.474823713302612 seconds

seen cubs but wible thing on head˜e warion earek and riege ower. Kave Gringated had-
Ana was a as
ce
BEGINNING (1681863890.1076565): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6076, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5887, val loss 4.5897 [4.229464769363403 sec]
step 100: train loss 2.4523, val loss 2.5287 [11.734334468841553 sec]
step 200: train loss 2.1663, val loss 2.2992 [19.406261920928955 sec]
step 300: train loss 1.9576, val loss 2.1338 [26.785277128219604 sec]
step 400: train loss 1.8241, val loss 2.0252 [34.22983455657959 sec]
1.8535751104354858
Total Training Time: 37.347920656204224 seconds

winR is worsido, the ence they will and worst sold now you arturned. Anaya Notherher, "What my after
BEGINNING (1681863928.7348843): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6265, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6624, val loss 4.6689 [5.767541408538818 sec]
step 100: train loss 2.4291, val loss 2.5240 [17.09734869003296 sec]
step 200: train loss 2.1363, val loss 2.2923 [27.953002214431763 sec]
step 300: train loss 1.9335, val loss 2.0989 [38.422829389572144 sec]
step 400: train loss 1.7893, val loss 1.9901 [49.50012421607971 sec]
1.717545509338379
Total Training Time: 54.00563716888428 seconds

"Whiss not 't bloodked" then my our the , Porany
moune, and have to and you," Gratta humfed, alvoned
BEGINNING (1681863984.6878006): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6287, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6157, val loss 4.6148 [2.5884976387023926 sec]
step 100: train loss 2.5177, val loss 2.5956 [7.088901996612549 sec]
step 200: train loss 2.3165, val loss 2.4275 [11.802565097808838 sec]
step 300: train loss 2.0981, val loss 2.2416 [16.3059561252594 sec]
step 400: train loss 1.9570, val loss 2.1275 [20.871705293655396 sec]
1.962214469909668
Total Training Time: 22.61909532546997 seconds

hat he dlabs ght, "We was lech.
Aidd wortin and hy will and a foskins.
"The
Nammale mark this, ene w
BEGINNING (1681864007.9587119): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6293, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6378, val loss 4.6381 [4.265639543533325 sec]
step 100: train loss 2.4948, val loss 2.5838 [11.7114839553833 sec]
step 200: train loss 2.2738, val loss 2.3825 [20.099473476409912 sec]
step 300: train loss 2.0205, val loss 2.1746 [28.558583974838257 sec]
step 400: train loss 1.8525, val loss 2.0411 [36.728010416030884 sec]
1.8265795707702637
Total Training Time: 39.88659143447876 seconds

"You us Gratta ask My e2. “V. Arphad the ound of thaw
somenty. Arphad Anayah, buts of we mQu© wastil
BEGINNING (1681864049.097614): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5756, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5826, val loss 4.5754 [5.700204133987427 sec]
step 100: train loss 2.4704, val loss 2.5562 [16.11780285835266 sec]
step 200: train loss 2.2460, val loss 2.3569 [26.659072637557983 sec]
step 300: train loss 1.9745, val loss 2.1363 [37.23314666748047 sec]
step 400: train loss 1.8006, val loss 2.0211 [47.823127031326294 sec]
1.802042007446289
Total Training Time: 52.59036993980408 seconds

orlange formblany we
oun. "I was will fr the youl. Aidden me took walows wils.
"Now.
"There our and 
BEGINNING (1681864103.5275545): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5824, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5724, val loss 4.5704 [3.5949912071228027 sec]
step 100: train loss 2.5421, val loss 2.6248 [9.878194570541382 sec]
step 200: train loss 2.4267, val loss 2.5206 [16.360056400299072 sec]
step 300: train loss 2.3149, val loss 2.4231 [23.0328152179718 sec]
step 400: train loss 2.1360, val loss 2.2673 [29.98721694946289 sec]
2.0538170337677
Total Training Time: 33.221686363220215 seconds

Zrext ofluart worn youl ta cachorgeizetiat ibaty pe
sir6 THe Tuons that Ka lale nit warto anted mun 
BEGINNING (1681864137.4170723): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6338, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6335, val loss 4.6321 [6.877032995223999 sec]
step 100: train loss 2.5162, val loss 2.5859 [21.207792043685913 sec]
step 200: train loss 2.3936, val loss 2.4847 [33.055169105529785 sec]
step 300: train loss 2.2039, val loss 2.3273 [46.17830419540405 sec]
step 400: train loss 1.9820, val loss 2.1516 [62.40830993652344 sec]
1.9030181169509888
Total Training Time: 69.77846097946167 seconds

shad of anilf thanotin filior the a pand. Hiest nechs al
wars has
no mell andt four the sacas legand
BEGINNING (1681864208.5466676): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6715, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6672, val loss 4.6737 [12.538155794143677 sec]
step 100: train loss 2.5021, val loss 2.5766 [31.246390342712402 sec]
step 200: train loss 2.3730, val loss 2.4761 [53.064067125320435 sec]
step 300: train loss 2.1381, val loss 2.2695 [73.78583931922913 sec]
step 400: train loss 1.9219, val loss 2.1174 [94.81086921691895 sec]
1.8519116640090942
Total Training Time: 104.1372320652008 seconds

spight of out they dilly with not as cupraad. He sa way
roded deare an to oner of f0 – She A –racame
BEGINNING (1681864314.8492398): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6019, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5920, val loss 4.5780 [2.805776834487915 sec]
step 100: train loss 2.4742, val loss 2.5637 [7.5519349575042725 sec]
step 200: train loss 2.1717, val loss 2.2959 [12.463899612426758 sec]
step 300: train loss 2.0005, val loss 2.1693 [17.923248767852783 sec]
step 400: train loss 1.8826, val loss 2.0694 [23.178061962127686 sec]
1.8759732246398926
Total Training Time: 25.056559562683105 seconds

louon and to can't do. A It lagk the
cans. Gor ei
smalek the lamey
maked with ware fumer themicaus d
BEGINNING (1681864340.62779): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5919, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6143, val loss 4.6148 [4.639326333999634 sec]
step 100: train loss 2.4219, val loss 2.5101 [12.279305696487427 sec]
step 200: train loss 2.1035, val loss 2.2529 [19.97166872024536 sec]
step 300: train loss 1.9140, val loss 2.0952 [27.950873136520386 sec]
step 400: train loss 1.7874, val loss 1.9989 [37.46953797340393 sec]
1.715248465538025
Total Training Time: 41.23619365692139 seconds

"CRSGAN AN ENCENT."
DESTE
F
THE TERA9
CON
~TER GS
Y/D SESDGAN EveriLf was halk hims."
The was ingger
BEGINNING (1681864383.3024037): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5564, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5439, val loss 4.5346 [6.332730293273926 sec]
step 100: train loss 2.4179, val loss 2.5145 [18.38599920272827 sec]
step 200: train loss 2.0911, val loss 2.2303 [31.717937707901 sec]
step 300: train loss 1.8737, val loss 2.0583 [43.309504985809326 sec]
step 400: train loss 1.7525, val loss 1.9801 [56.054521560668945 sec]
1.7545710802078247
Total Training Time: 60.899532079696655 seconds

Chief%, he
fow Taka anowled."
Gla streated no4; oncilf Yah Even ap of Thes dose bratke,
and you has 
BEGINNING (1681864446.1096823): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5688, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5681, val loss 4.5656 [3.2584259510040283 sec]
step 100: train loss 2.5106, val loss 2.5889 [9.331331968307495 sec]
step 200: train loss 2.2939, val loss 2.3928 [14.522838830947876 sec]
step 300: train loss 2.0642, val loss 2.2134 [20.46193528175354 sec]
step 400: train loss 1.9256, val loss 2.1058 [26.754351139068604 sec]
1.9303703308105469
Total Training Time: 28.84661555290222 seconds

bughoued ble loone. You gains of the his oment his
wers oints.
"You I have wasce ockne of ge a an A 
BEGINNING (1681864475.6328573): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6807, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6689, val loss 4.6742 [5.08432936668396 sec]
step 100: train loss 2.4767, val loss 2.5523 [13.877624034881592 sec]
step 200: train loss 2.2411, val loss 2.3602 [23.4350905418396 sec]
step 300: train loss 1.9684, val loss 2.1451 [34.83717179298401 sec]
step 400: train loss 1.8110, val loss 2.0118 [43.61815047264099 sec]
1.752698302268982
Total Training Time: 47.43864440917969 seconds

Go3"
Gratta askeed thereated. Throed to from ameaze carma
rick the beeved.
"I haver, now as had lo˜l
BEGINNING (1681864524.3472626): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6990, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6846, val loss 4.6911 [7.1958253383636475 sec]
step 100: train loss 2.4654, val loss 2.5448 [20.724228382110596 sec]
step 200: train loss 2.2070, val loss 2.3310 [35.71759033203125 sec]
step 300: train loss 1.9128, val loss 2.0922 [50.93008732795715 sec]
step 400: train loss 1.7485, val loss 1.9789 [65.92756938934326 sec]
1.727918267250061
Total Training Time: 72.3295886516571 seconds

1A." Nena Nal, and smil, had ruached saile, It Vas
Gratta notiot a It your and any of The tisabe gap
BEGINNING (1681864598.718198): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6173, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6101, val loss 4.5994 [6.503208160400391 sec]
step 100: train loss 2.5339, val loss 2.6081 [18.858697414398193 sec]
step 200: train loss 2.4310, val loss 2.5167 [33.668972969055176 sec]
step 300: train loss 2.2974, val loss 2.4040 [51.98567485809326 sec]
step 400: train loss 2.1016, val loss 2.2457 [64.37914705276489 sec]
2.0116093158721924
Total Training Time: 69.36155414581299 seconds

hocKAnodds sarch. He owle backa sall
the was wing you-s be thaved a seminst oret .
Kigh therome bura
BEGINNING (1681864668.7765815): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5951, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5995, val loss 4.5921 [10.50905179977417 sec]
step 100: train loss 2.5076, val loss 2.5855 [38.11917304992676 sec]
step 200: train loss 2.3820, val loss 2.4836 [67.88397431373596 sec]
step 300: train loss 2.1591, val loss 2.2959 [88.62157845497131 sec]
step 400: train loss 1.9458, val loss 2.1213 [116.35709524154663 sec]
1.8751176595687866
Total Training Time: 128.20064735412598 seconds

riceste we the Perationgh
eeng saidn oriong in arthiliss. My.
"Ye, desid, halpe to hour latibouted t
BEGINNING (1681864798.3994071): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6167, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6107, val loss 4.6171 [31.78003191947937 sec]
step 100: train loss 2.4946, val loss 2.5741 [70.47552990913391 sec]
step 200: train loss 2.3558, val loss 2.4582 [99.92436623573303 sec]
step 300: train loss 2.0807, val loss 2.2278 [134.72113871574402 sec]
step 400: train loss 1.8579, val loss 2.0652 [168.27525901794434 sec]
1.8102023601531982
Total Training Time: 180.85952138900757 seconds

Gratta't shis and, cong it the smidliest.
"I cextly she not at grow, Aftur plose amels
of 8 Kris If 
BEGINNING (1681864981.2961166): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5898, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5950, val loss 4.5996 [2.9647035598754883 sec]
step 100: train loss 2.4844, val loss 2.5684 [7.973333120346069 sec]
step 200: train loss 2.1847, val loss 2.3100 [12.912463665008545 sec]
step 300: train loss 2.0183, val loss 2.1757 [17.842569828033447 sec]
step 400: train loss 1.9115, val loss 2.0948 [22.817715406417847 sec]
1.9191651344299316
Total Training Time: 24.872130155563354 seconds

Ana who mked at enos stso?
Namolvis Ses in of our!"
The he is he weshas at the to all but
oncing att
BEGINNING (1681865006.982291): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6276, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6369, val loss 4.6395 [4.903609275817871 sec]
step 100: train loss 2.4399, val loss 2.5458 [13.79146933555603 sec]
step 200: train loss 2.1467, val loss 2.2803 [23.42701530456543 sec]
step 300: train loss 1.9615, val loss 2.1524 [33.84451508522034 sec]
step 400: train loss 1.8494, val loss 2.0655 [43.35094404220581 sec]
1.8948115110397339
Total Training Time: 47.50088691711426 seconds

hild ben turens entigil se, andter and do ba( the ava's sookid to sub him and that the
by ba dibefot
BEGINNING (1681865056.1921368): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6731, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6789, val loss 4.6872 [7.164545059204102 sec]
step 100: train loss 2.4385, val loss 2.5344 [21.421459674835205 sec]
step 200: train loss 2.1293, val loss 2.2779 [35.37769174575806 sec]
step 300: train loss 1.9384, val loss 2.1238 [48.73425912857056 sec]
step 400: train loss 1.8209, val loss 2.0112 [61.768444299697876 sec]
1.6983708143234253
Total Training Time: 67.99286127090454 seconds

ampand prost not nat wort
be was." Namaled and will atters
was and would toigh the was
with!"
"Anaya
BEGINNING (1681865126.6370335): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6342, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6318, val loss 4.6277 [3.703021764755249 sec]
step 100: train loss 2.5055, val loss 2.5938 [9.612855195999146 sec]
step 200: train loss 2.3051, val loss 2.4081 [15.255593299865723 sec]
step 300: train loss 2.0736, val loss 2.2208 [20.93205165863037 sec]
step 400: train loss 1.9351, val loss 2.1213 [26.584644079208374 sec]
1.9493358135223389
Total Training Time: 29.043431758880615 seconds

Vall, hiss hereand looke
sow the wituond ore anu..
"CRAN I MuCE The pagaire. His fe coubt youl dist 
BEGINNING (1681865156.5025332): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6377, val loss 4.6414 [5.862225294113159 sec]
step 100: train loss 2.4727, val loss 2.5573 [16.309032201766968 sec]
step 200: train loss 2.2757, val loss 2.3925 [27.376376390457153 sec]
step 300: train loss 1.9961, val loss 2.1687 [39.83563184738159 sec]
step 400: train loss 1.8394, val loss 2.0427 [50.36658978462219 sec]
1.8416101932525635
Total Training Time: 55.092957973480225 seconds

pathed tood, and nicn, and con the smegater. They recte
potion nadslerged rand them cap onfith eir
P
BEGINNING (1681865213.1085715): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6488, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6387, val loss 4.6426 [8.458794355392456 sec]
step 100: train loss 2.4768, val loss 2.5798 [24.003260135650635 sec]
step 200: train loss 2.2488, val loss 2.3755 [39.311569690704346 sec]
step 300: train loss 1.9727, val loss 2.1444 [54.6763219833374 sec]
step 400: train loss 1.7971, val loss 2.0132 [70.02354192733765 sec]
1.7578867673873901
Total Training Time: 76.97189497947693 seconds

E˜D I!"
They, but vefome he we gold and shomnew, and sind but
77
CHAN CHA DEAN McKAY
NEMBSPEAY
CUMcK
BEGINNING (1681865292.4843569): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6620, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6564, val loss 4.6540 [4.86557412147522 sec]
step 100: train loss 2.5195, val loss 2.5957 [13.395892143249512 sec]
step 200: train loss 2.4121, val loss 2.5007 [21.913252592086792 sec]
step 300: train loss 2.2804, val loss 2.3957 [30.50633931159973 sec]
step 400: train loss 2.0826, val loss 2.2300 [39.02192664146423 sec]
1.9825040102005005
Total Training Time: 42.71407651901245 seconds

courened nottooed, "Mcere cut Grack the fuillown turnated dUGratta
ped the Gratta ache tat men had s
BEGINNING (1681865336.0324275): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6802, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6693, val loss 4.6637 [8.974200248718262 sec]
step 100: train loss 2.4961, val loss 2.5831 [25.01056718826294 sec]
step 200: train loss 2.3824, val loss 2.4870 [41.01106643676758 sec]
step 300: train loss 2.1685, val loss 2.3117 [57.04849171638489 sec]
step 400: train loss 1.9536, val loss 2.1313 [73.085853099823 sec]
1.8939192295074463
Total Training Time: 80.52522253990173 seconds

atto his hive havellans gee hed a sive hauad
be ye and to luort the liogh.
Gratta lland (oit lak his
BEGINNING (1681865418.0793464): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6447, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6328, val loss 4.6313 [13.17910385131836 sec]
step 100: train loss 2.4898, val loss 2.5718 [37.463956356048584 sec]
step 200: train loss 2.3762, val loss 2.4890 [60.69186496734619 sec]
step 300: train loss 2.1044, val loss 2.2546 [83.86145401000977 sec]
step 400: train loss 1.8950, val loss 2.0925 [107.07427906990051 sec]
1.8719834089279175
Total Training Time: 117.31918144226074 seconds

gait A speopp
wayan, aus and borneg are sta deat plikh hop the
chate squitier?"
"Turied0
Chef Ar! TE
BEGINNING (1681865537.6797903): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6555, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6623, val loss 4.6615 [3.097242593765259 sec]
step 100: train loss 2.4285, val loss 2.5171 [8.376394271850586 sec]
step 200: train loss 2.1251, val loss 2.2759 [13.694537162780762 sec]
step 300: train loss 1.9452, val loss 2.1236 [19.134698152542114 sec]
step 400: train loss 1.8406, val loss 2.0330 [24.610710859298706 sec]
1.8366045951843262
Total Training Time: 26.85108184814453 seconds

siding him ofuen your he the phe ko sues would is
for yor preechief 6
Tit shif Chie, a, but his athe
BEGINNING (1681865565.3094153): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6163, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6288, val loss 4.6326 [5.418111801147461 sec]
step 100: train loss 2.3884, val loss 2.4809 [15.04237961769104 sec]
step 200: train loss 2.0623, val loss 2.2080 [24.555864810943604 sec]
step 300: train loss 1.8768, val loss 2.0550 [34.061203479766846 sec]
step 400: train loss 1.7507, val loss 1.9798 [43.60149121284485 sec]
1.6933685541152954
Total Training Time: 47.815428733825684 seconds

a sill!" "Taka wille slay guardsere capto. Soure them to The were we humase the tustlen rubsought fu
BEGINNING (1681865614.6282473): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6851, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6651, val loss 4.6632 [7.651967763900757 sec]
step 100: train loss 2.3933, val loss 2.4815 [21.61635184288025 sec]
step 200: train loss 2.0599, val loss 2.2035 [35.42781209945679 sec]
step 300: train loss 1.8463, val loss 2.0350 [49.186803102493286 sec]
step 400: train loss 1.7288, val loss 1.9577 [63.171841859817505 sec]
1.7869147062301636
Total Training Time: 69.43105506896973 seconds

Thoile marst en with Arnayah's" ins. They
smilent res, him in had Anayah drided to rowack, the surnl
BEGINNING (1681865686.2711859): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6067, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6131, val loss 4.6162 [4.195749044418335 sec]
step 100: train loss 2.4772, val loss 2.5641 [11.350253343582153 sec]
step 200: train loss 2.2144, val loss 2.3442 [18.453314542770386 sec]
step 300: train loss 1.9996, val loss 2.1673 [25.547308921813965 sec]
step 400: train loss 1.8582, val loss 2.0703 [32.618560552597046 sec]
1.86772620677948
Total Training Time: 35.60294771194458 seconds

"Whess fach Pa
there mubs."
"We thone and tant would saited mo shiss in he g©e,3
Cover takmay."
Grat
BEGINNING (1681865722.6513228): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5140, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5177, val loss 4.5201 [7.322293519973755 sec]
step 100: train loss 2.4532, val loss 2.5340 [20.317509651184082 sec]
step 200: train loss 2.1787, val loss 2.2983 [33.429192543029785 sec]
step 300: train loss 1.9091, val loss 2.1065 [46.45726490020752 sec]
step 400: train loss 1.7457, val loss 1.9694 [59.47089624404907 sec]
1.718226432800293
Total Training Time: 65.14623403549194 seconds

IGE
CHAPTEI MY
sily come that incurdes."
"Divens your in celby seabrouned the upenter
he necouldmild
BEGINNING (1681865789.295153): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5721, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5724, val loss 4.5853 [10.542022466659546 sec]
step 100: train loss 2.4458, val loss 2.5316 [29.578951835632324 sec]
step 200: train loss 2.1471, val loss 2.2784 [48.61264395713806 sec]
step 300: train loss 1.8700, val loss 2.0641 [67.57161355018616 sec]
step 400: train loss 1.6993, val loss 1.9350 [86.55001521110535 sec]
1.744625449180603
Total Training Time: 94.97234201431274 seconds

 the speened them of founds to but had ad the gin
bound 27
SEAT Mchay, they ween "Thisice they maeuw
BEGINNING (1681865886.685506): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5981, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5872, val loss 4.5878 [6.355909585952759 sec]
step 100: train loss 2.5033, val loss 2.5839 [17.469385623931885 sec]
step 200: train loss 2.3715, val loss 2.4724 [28.607038497924805 sec]
step 300: train loss 2.1545, val loss 2.2873 [39.715409994125366 sec]
step 400: train loss 1.9551, val loss 2.1288 [50.82106924057007 sec]
1.9045792818069458
Total Training Time: 55.61953568458557 seconds

AY9ust held melse wat athe quiell Mafested?Wal
"Coulles. The them moleain bowgaid ford theint. The w
BEGINNING (1681865943.1251788): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6045, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6037, val loss 4.5997 [11.764178037643433 sec]
step 100: train loss 2.4870, val loss 2.5714 [32.34359836578369 sec]
step 200: train loss 2.3338, val loss 2.4325 [52.958125829696655 sec]
step 300: train loss 2.0494, val loss 2.2008 [73.59576773643494 sec]
step 400: train loss 1.8338, val loss 2.0447 [94.23746466636658 sec]
1.7611818313598633
Total Training Time: 103.12152552604675 seconds

were sourt sead sommin, withy challved thing ont nat aie
was to farls theaters toretion them of'r Th
BEGINNING (1681866047.8269029): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5619, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5515, val loss 4.5494 [17.092326641082764 sec]
step 100: train loss 2.4912, val loss 2.5774 [46.161792516708374 sec]
step 200: train loss 2.3474, val loss 2.4495 [75.25942468643188 sec]
step 300: train loss 2.0488, val loss 2.2056 [104.60329556465149 sec]
step 400: train loss 1.7996, val loss 2.0092 [133.73567390441895 sec]
1.7070680856704712
Total Training Time: 146.2732856273651 seconds

choer ir. Hes wourdre mied the uns fould be that met maters
at of the ceachin tuon en the pabs." Tak
BEGINNING (1681866196.416395): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5951, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6006, val loss 4.6029 [3.4303910732269287 sec]
step 100: train loss 2.3919, val loss 2.4774 [9.358133316040039 sec]
step 200: train loss 2.0740, val loss 2.2207 [15.283943891525269 sec]
step 300: train loss 1.9022, val loss 2.0940 [21.31479001045227 sec]
step 400: train loss 1.7866, val loss 2.0005 [27.370333909988403 sec]
1.7767492532730103
Total Training Time: 29.831099271774292 seconds

couthrelfated and. Vid mome knors. We joing arlues wilded
of the saw the fame arway Priestiessed. No
BEGINNING (1681866227.0497394): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6618, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6513, val loss 4.6525 [5.948033571243286 sec]
step 100: train loss 2.3604, val loss 2.4575 [16.49984335899353 sec]
step 200: train loss 2.0123, val loss 2.1621 [27.02977705001831 sec]
step 300: train loss 1.8202, val loss 2.0317 [37.54712700843811 sec]
step 400: train loss 1.6989, val loss 1.9294 [48.0645854473114 sec]
1.7117289304733276
Total Training Time: 52.67712092399597 seconds

fore ming fort oll. "%emen you5
thout e what reawled mothed and four
turibe nrief and have reary sou
BEGINNING (1681866281.4619005): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6697, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6725, val loss 4.6674 [8.731029272079468 sec]
step 100: train loss 2.3523, val loss 2.4423 [23.92374086380005 sec]
step 200: train loss 1.9937, val loss 2.1581 [39.10909628868103 sec]
step 300: train loss 1.7849, val loss 1.9927 [54.697185039520264 sec]
step 400: train loss 1.6712, val loss 1.9141 [69.99876165390015 sec]
1.5850470066070557
Total Training Time: 76.85703158378601 seconds

5S A passed. Nouble fille, he last und the wayll
and sniffil before Thas El ling ound safted, and si
BEGINNING (1681866360.4968212): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6779, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6777, val loss 4.6774 [4.897643327713013 sec]
step 100: train loss 2.4680, val loss 2.5524 [13.385251998901367 sec]
step 200: train loss 2.1710, val loss 2.2931 [21.860857248306274 sec]
step 300: train loss 1.9494, val loss 2.1172 [30.5207736492157 sec]
step 400: train loss 1.8089, val loss 1.9999 [39.07116365432739 sec]
1.7682017087936401
Total Training Time: 42.643234968185425 seconds

1ust rose culd him nng hat wo nifure viyase neet
this thin sure. Boti hes bores. "Cleft will sme, an
BEGINNING (1681866403.9638262): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6887, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6831, val loss 4.6821 [8.839384317398071 sec]
step 100: train loss 2.4406, val loss 2.5268 [24.45804786682129 sec]
step 200: train loss 2.1148, val loss 2.2617 [39.99306893348694 sec]
step 300: train loss 1.8518, val loss 2.0492 [55.51616621017456 sec]
step 400: train loss 1.6844, val loss 1.9320 [71.05376625061035 sec]
1.6254256963729858
Total Training Time: 77.80856084823608 seconds

moniy fracr the witheirs chaps, showle anithout was res
reatior faith. He tase hough earlus ir and G
BEGINNING (1681866483.3398564): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6756, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6662, val loss 4.6661 [12.713535070419312 sec]
step 100: train loss 2.4322, val loss 2.5221 [35.36640286445618 sec]
step 200: train loss 2.1026, val loss 2.2445 [58.02898144721985 sec]
step 300: train loss 1.8130, val loss 2.0169 [80.70170760154724 sec]
step 400: train loss 1.6389, val loss 1.8895 [103.3187780380249 sec]
1.5824730396270752
Total Training Time: 113.30403351783752 seconds

for the loone." Arphad and way of the
roor antand ssawly.
Chief Gor-ancing to bace joverty
Bu, saich
BEGINNING (1681866598.8231068): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6061, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6120, val loss 4.6131 [7.700460433959961 sec]
step 100: train loss 2.4905, val loss 2.5718 [21.266986846923828 sec]
step 200: train loss 2.3583, val loss 2.4577 [34.81151247024536 sec]
step 300: train loss 2.1499, val loss 2.2836 [48.339481830596924 sec]
step 400: train loss 1.9272, val loss 2.1077 [61.85360527038574 sec]
1.8695354461669922
Total Training Time: 67.64345097541809 seconds

the reashond be, ind harmy know matuon what Gra thens
word gaing that. Gratta su." Pyrred. "We Yah A
BEGINNING (1681866667.275615): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5894, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5898, val loss 4.5846 [14.24556016921997 sec]
step 100: train loss 2.4779, val loss 2.5649 [38.25168013572693 sec]
step 200: train loss 2.3081, val loss 2.4171 [62.130542278289795 sec]
step 300: train loss 2.0115, val loss 2.1831 [85.88674926757812 sec]
step 400: train loss 1.8003, val loss 2.0248 [109.59331774711609 sec]
1.7505775690078735
Total Training Time: 119.44627857208252 seconds

ther. We Pyries tries worif an Gratta's
turned leed epling this oner featt the could tron of
the cal
BEGINNING (1681866788.178033): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5373, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5369, val loss 4.5411 [20.71161651611328 sec]
step 100: train loss 2.4792, val loss 2.5653 [54.30148792266846 sec]
step 200: train loss 2.3053, val loss 2.4132 [88.0345606803894 sec]
step 300: train loss 1.9673, val loss 2.1460 [121.87434530258179 sec]
step 400: train loss 1.7410, val loss 1.9655 [155.63199543952942 sec]
1.6609677076339722
Total Training Time: 169.99067616462708 seconds

file. Thas one soid, and uped a spayan in eace. Thas
wilh rachospowed and againtiatere to. The douga
BEGINNING (1681866960.4448402): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6450, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6516, val loss 4.6456 [3.965252161026001 sec]
step 100: train loss 2.3779, val loss 2.4850 [10.96901273727417 sec]
step 200: train loss 2.0661, val loss 2.2427 [17.971131801605225 sec]
step 300: train loss 1.9133, val loss 2.0962 [24.967142820358276 sec]
step 400: train loss 1.8241, val loss 2.0449 [31.96989893913269 sec]
1.8122506141662598
Total Training Time: 35.063552141189575 seconds

Praw manting iscoursh..
5%
CHAPTEAP SEA9 the 4
PyZEAN
THE THERAN
CHAM• OF SEAY
NE PTEWI – BI€E
sppen
BEGINNING (1681866996.6142526): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6518, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6302, val loss 4.6400 [7.154049396514893 sec]
step 100: train loss 2.3913, val loss 2.4866 [20.356157064437866 sec]
step 200: train loss 2.0555, val loss 2.2177 [33.497292041778564 sec]
step 300: train loss 1.8655, val loss 2.0677 [46.61715054512024 sec]
step 400: train loss 1.7455, val loss 1.9856 [59.734342098236084 sec]
1.7909117937088013
Total Training Time: 65.7035014629364 seconds

"Mo Gratta nodde fating Once corminf the in you jrost. He worsh a not
mew has thists. He I'lave the 
BEGINNING (1681867064.5016487): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6630, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6737, val loss 4.6725 [10.399445056915283 sec]
step 100: train loss 2.4101, val loss 2.5188 [29.815820932388306 sec]
step 200: train loss 2.0929, val loss 2.2306 [49.16101598739624 sec]
step 300: train loss 1.8767, val loss 2.0703 [68.51385116577148 sec]
step 400: train loss 1.7622, val loss 2.0131 [87.90501570701599 sec]
1.7483361959457397
Total Training Time: 96.85706090927124 seconds

VES ALayah Gratta. Grat€ed ununedned sto and Gratta."
"We hook have his will as caee frobs from moga
BEGINNING (1681867164.6193554): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6133, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6146, val loss 4.6268 [4.951120615005493 sec]
step 100: train loss 2.4451, val loss 2.5360 [14.098227262496948 sec]
step 200: train loss 2.1375, val loss 2.2836 [23.258918285369873 sec]
step 300: train loss 1.9045, val loss 2.0972 [32.42047357559204 sec]
step 400: train loss 1.7802, val loss 1.9890 [41.5764582157135 sec]
1.7012765407562256
Total Training Time: 45.78838348388672 seconds

in compliver and to by he warrive a vackage hors.
Gratta wing the was rearrive bat plany yels." The

BEGINNING (1681867211.588591): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6269, val loss 4.6239 [9.298356771469116 sec]
step 100: train loss 2.4432, val loss 2.5232 [26.712177276611328 sec]
step 200: train loss 2.1033, val loss 2.2474 [44.202420711517334 sec]
step 300: train loss 1.8517, val loss 2.0476 [61.62682819366455 sec]
step 400: train loss 1.6904, val loss 1.9083 [79.0563793182373 sec]
1.6445828676223755
Total Training Time: 87.19403767585754 seconds

lebled and It ay treater. Gratta had uped his havprows."
shere estrudgent head, "Her smill, and maid
BEGINNING (1681867301.0060058): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5862, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5851, val loss 4.5904 [13.62980031967163 sec]
step 100: train loss 2.4667, val loss 2.5587 [39.118183612823486 sec]
step 200: train loss 2.1498, val loss 2.2884 [64.57670879364014 sec]
step 300: train loss 1.8635, val loss 2.0628 [90.03083419799805 sec]
step 400: train loss 1.6880, val loss 1.9329 [115.53593182563782 sec]
1.5631380081176758
Total Training Time: 127.35500860214233 seconds

coubs?" Gor, nodded Taka ho bough fir for." Oncenty therned
for nexp was waffeboly."
"Aidden the hen
BEGINNING (1681867431.6537106): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6521, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6580, val loss 4.6547 [8.02411961555481 sec]
step 100: train loss 2.4782, val loss 2.5694 [22.435689687728882 sec]
step 200: train loss 2.3374, val loss 2.4512 [36.833486795425415 sec]
step 300: train loss 2.0833, val loss 2.2447 [51.20871162414551 sec]
step 400: train loss 1.8695, val loss 2.0644 [65.60651278495789 sec]
1.779293179512024
Total Training Time: 71.98789429664612 seconds

I will fruling of of the riscened. Aiddeptayah
had maughte with the Tughe Vand, and Gratta'sed
the s
BEGINNING (1681867504.797056): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5575, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5654, val loss 4.5611 [15.280275583267212 sec]
step 100: train loss 2.4742, val loss 2.5599 [41.983277797698975 sec]
step 200: train loss 2.3216, val loss 2.4333 [68.7027018070221 sec]
step 300: train loss 1.9879, val loss 2.1645 [95.59477972984314 sec]
step 400: train loss 1.7840, val loss 2.0077 [122.15907430648804 sec]
1.6974947452545166
Total Training Time: 133.91617512702942 seconds

retherscon, and of the are Pyrned Clangablefthan As
stook a dingenuars. Krhans turnas Nomal's the de
BEGINNING (1681867640.9469097): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6023, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6020, val loss 4.5992 [21.812370777130127 sec]
step 100: train loss 2.4845, val loss 2.5722 [56.40977168083191 sec]
step 200: train loss 2.3421, val loss 2.4538 [90.40425443649292 sec]
step 300: train loss 2.0336, val loss 2.1993 [124.42216730117798 sec]
step 400: train loss 1.7667, val loss 1.9950 [158.737056016922 sec]
1.6881505250930786
Total Training Time: 174.2388572692871 seconds

chummint ofher wicPeril, Swe areach came throutned.
If was to wit with a rechbaty speack an tored of
BEGINNING (1681867818.5964217): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6310, val loss 4.6262 [4.429519891738892 sec]
step 100: train loss 2.3228, val loss 2.4349 [12.450698614120483 sec]
step 200: train loss 2.0181, val loss 2.1913 [20.4746413230896 sec]
step 300: train loss 1.8393, val loss 2.0425 [28.48446536064148 sec]
step 400: train loss 1.7292, val loss 1.9465 [36.51450753211975 sec]
1.7843308448791504
Total Training Time: 40.098427057266235 seconds

CD
SEDGAMBIT THAPCES ANE con to fir and
yel. He him quarestaka to fool hall to gold
backingions neve
BEGINNING (1681867859.8414323): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6475, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6546, val loss 4.6432 [8.009929180145264 sec]
step 100: train loss 2.3405, val loss 2.4433 [23.01720929145813 sec]
step 200: train loss 1.9839, val loss 2.1603 [37.97174549102783 sec]
step 300: train loss 1.7955, val loss 2.0083 [52.891828298568726 sec]
step 400: train loss 1.6704, val loss 1.9371 [67.84963750839233 sec]
1.6873582601547241
Total Training Time: 74.7798318862915 seconds

am book Thas in he fuand velooied his head to they malee, A doozent and
It the wall. He was male wit
BEGINNING (1681867936.8320518): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5904, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5844, val loss 4.5798 [11.686691999435425 sec]
step 100: train loss 2.3690, val loss 2.4645 [33.71048283576965 sec]
step 200: train loss 1.9917, val loss 2.1557 [55.72057914733887 sec]
step 300: train loss 1.7847, val loss 2.0133 [77.73774790763855 sec]
step 400: train loss 1.6586, val loss 1.9110 [99.73094177246094 sec]
1.5892976522445679
Total Training Time: 110.05887937545776 seconds

also my stroved?"
CHE HE EF9
AYWhok the he tuon dows and them. Anayah
mon the miction to him. Gratta
BEGINNING (1681868050.173979): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5420, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5463, val loss 4.5483 [6.474602222442627 sec]
step 100: train loss 2.4115, val loss 2.5093 [18.052907943725586 sec]
step 200: train loss 2.0574, val loss 2.2127 [29.65354609489441 sec]
step 300: train loss 1.8296, val loss 2.0337 [41.25563669204712 sec]
step 400: train loss 1.6923, val loss 1.9216 [52.84485864639282 sec]
1.6262993812561035
Total Training Time: 57.91674447059631 seconds

m~eve, without panYed chell sile have
wore belved aund the dowert onder of the melsoss
cast thanim h
BEGINNING (1681868109.2429428): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6317, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6368, val loss 4.6383 [12.096413612365723 sec]
step 100: train loss 2.4252, val loss 2.5179 [33.909565687179565 sec]
step 200: train loss 2.0428, val loss 2.2080 [55.71759533882141 sec]
step 300: train loss 1.7818, val loss 2.0012 [77.53450584411621 sec]
step 400: train loss 1.6226, val loss 1.9001 [99.3717532157898 sec]
1.528279423713684
Total Training Time: 109.11443662643433 seconds

Arphad the irdention. Is mily, foint were mouew bableves.
"What, a peocir in even uarded. If had not
BEGINNING (1681868220.5736732): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6174, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6213, val loss 4.6167 [17.690288543701172 sec]
step 100: train loss 2.4409, val loss 2.5305 [48.27057337760925 sec]
step 200: train loss 2.0876, val loss 2.2422 [78.8900830745697 sec]
step 300: train loss 1.8014, val loss 2.0198 [109.35340285301208 sec]
step 400: train loss 1.6159, val loss 1.8899 [140.26815390586853 sec]
1.5285645723342896
Total Training Time: 153.64875483512878 seconds

SEAT RIN – fooll half my kine snow. He faoll, septack.
"Kinn's 10
CLAPTEAN McKAY
Gornered shoshalf, 
BEGINNING (1681868377.491986): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5907, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6005, val loss 4.6024 [10.615197896957397 sec]
step 100: train loss 2.4705, val loss 2.5640 [28.964279651641846 sec]
step 200: train loss 2.2820, val loss 2.4014 [47.33001518249512 sec]
step 300: train loss 1.9834, val loss 2.1585 [65.82385468482971 sec]
step 400: train loss 1.7851, val loss 1.9965 [84.31860899925232 sec]
1.7399259805679321
Total Training Time: 92.30216836929321 seconds

PEE A rits Ely, surys gaie."
A Mayah soved, you was sainay sold not wit
to thy kew wereead noddisted
BEGINNING (1681868470.9583037): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6748, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6741, val loss 4.6773 [19.938244342803955 sec]
step 100: train loss 2.4656, val loss 2.5526 [50.893991470336914 sec]
step 200: train loss 2.2454, val loss 2.3740 [81.49931168556213 sec]
step 300: train loss 1.8963, val loss 2.0946 [112.44623827934265 sec]
step 400: train loss 1.6838, val loss 1.9310 [143.7461793422699 sec]
1.5785521268844604
Total Training Time: 157.92448019981384 seconds

ween folle. He a smilemped a norced they were Yah
Clan an I maut meuws."
Gratta felled that houghe l
BEGINNING (1681868631.141041): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6285, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6144, val loss 4.6079 [26.923893451690674 sec]
step 100: train loss 2.4760, val loss 2.5649 [70.96383571624756 sec]
step 200: train loss 2.2967, val loss 2.4173 [114.62474799156189 sec]
step 300: train loss 1.9261, val loss 2.1080 [158.3323941230774 sec]
step 400: train loss 1.6734, val loss 1.9439 [202.16443157196045 sec]
1.6380804777145386
Total Training Time: 224.2105507850647 seconds

she tem, appet and evilial ble, sparlasked. Arphat
the had's was we in cornal. Buse your mes wan wit
BEGINNING (1681868858.979915): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5760, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5826, val loss 4.5757 [5.203576564788818 sec]
step 100: train loss 2.2734, val loss 2.3940 [15.313553810119629 sec]
step 200: train loss 1.9513, val loss 2.1367 [24.83685302734375 sec]
step 300: train loss 1.7965, val loss 2.0179 [34.3096182346344 sec]
step 400: train loss 1.6673, val loss 1.9220 [44.574060678482056 sec]
1.6191688776016235
Total Training Time: 48.99289584159851 seconds

ran must
Zecapy will chard in umn opled and
likikst. But now have choy he discarve in mount and wait
BEGINNING (1681868909.3238866): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6624, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6610, val loss 4.6499 [9.768889904022217 sec]
step 100: train loss 2.2922, val loss 2.4052 [27.740609169006348 sec]
step 200: train loss 1.9079, val loss 2.1070 [46.53193235397339 sec]
step 300: train loss 1.7368, val loss 1.9681 [64.91873812675476 sec]
step 400: train loss 1.6091, val loss 1.8886 [83.14565920829773 sec]
1.6175568103790283
Total Training Time: 92.10036587715149 seconds

Pelad the persat!
Camber a jumpene alfe Gratta Namal. Bridgen,
and any we will and phat Perhaps eget
BEGINNING (1681869004.0544453): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5898, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5648, val loss 4.5610 [15.198391199111938 sec]
step 100: train loss 2.3106, val loss 2.4209 [42.407156229019165 sec]
step 200: train loss 1.9323, val loss 2.1340 [68.73591780662537 sec]
step 300: train loss 1.7339, val loss 1.9715 [95.90531301498413 sec]
step 400: train loss 1.5984, val loss 1.8843 [121.97303032875061 sec]
1.4944500923156738
Total Training Time: 134.00728940963745 seconds

No, notioris weaquy.
Gratta would you?" Taka's in lanters of this attacching, and A most wo
noted hi
BEGINNING (1681869141.880411): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6560, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6344, val loss 4.6376 [8.175666809082031 sec]
step 100: train loss 2.3832, val loss 2.4922 [22.69042134284973 sec]
step 200: train loss 2.0112, val loss 2.1962 [37.21453046798706 sec]
step 300: train loss 1.7884, val loss 2.0099 [51.78495192527771 sec]
step 400: train loss 1.6488, val loss 1.9075 [66.27414965629578 sec]
1.6350269317626953
Total Training Time: 72.62395572662354 seconds

ANBMY
skning eturne to in out onlent. Gratta could. Anyah!
Tuon I tuon what were furn istood. He hum
BEGINNING (1681869215.7140644): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.7207, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7268, val loss 4.7233 [15.204787492752075 sec]
step 100: train loss 2.3915, val loss 2.4945 [42.54261255264282 sec]
step 200: train loss 1.9700, val loss 2.1447 [69.44821953773499 sec]
step 300: train loss 1.7141, val loss 1.9532 [96.31415605545044 sec]
step 400: train loss 1.5520, val loss 1.8525 [123.16852879524231 sec]
1.4844461679458618
Total Training Time: 134.9422128200531 seconds

notiong ablowed.
Gratta showiled with at his in and ceaptered hidget and
Anayah him bach there diest
BEGINNING (1681869353.1120698): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5789, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5805, val loss 4.5749 [22.07475185394287 sec]
step 100: train loss 2.4320, val loss 2.5195 [60.55210852622986 sec]
step 200: train loss 2.0077, val loss 2.1732 [97.67117643356323 sec]
step 300: train loss 1.7292, val loss 1.9608 [135.2813446521759 sec]
step 400: train loss 1.5366, val loss 1.8339 [172.94588041305542 sec]
1.4882359504699707
Total Training Time: 189.21933126449585 seconds

nee warls not by the grow heard, sightalk
was encent. The maduews maeuw.
"Alt whicilen your kull who
BEGINNING (1681869545.9718895): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6126, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6115, val loss 4.6078 [13.42277979850769 sec]
step 100: train loss 2.4548, val loss 2.5513 [35.99629068374634 sec]
step 200: train loss 2.2414, val loss 2.3715 [58.595423221588135 sec]
step 300: train loss 1.9196, val loss 2.1054 [81.14359450340271 sec]
step 400: train loss 1.7228, val loss 1.9608 [103.74204683303833 sec]
1.6721041202545166
Total Training Time: 113.37775158882141 seconds

"Yes, but fumaed him, just be slike and Gratta Tilas
took had, pritil. Zepphayahiesure is will as wi
BEGINNING (1681869660.6362617): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5208, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5259, val loss 4.5292 [26.92650318145752 sec]
step 100: train loss 2.4455, val loss 2.5348 [68.73605680465698 sec]
step 200: train loss 2.1696, val loss 2.3114 [111.13475513458252 sec]
step 300: train loss 1.8267, val loss 2.0386 [154.02926421165466 sec]
step 400: train loss 1.6074, val loss 1.8905 [195.22640705108643 sec]
1.5210374593734741
Total Training Time: 216.3964068889618 seconds

do, you, as celming be couts had retwerely
chapt the are it signty, and guard flowly his allood
powe
BEGINNING (1681869880.2076445): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6724, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6816, val loss 4.6719 [1.1770522594451904 sec]
step 100: train loss 3.2096, val loss 3.2274 [2.9168269634246826 sec]
step 200: train loss 2.9755, val loss 3.0148 [4.708914279937744 sec]
step 300: train loss 2.8249, val loss 2.8688 [6.471918821334839 sec]
step 400: train loss 2.7286, val loss 2.7664 [8.25308346748352 sec]
2.6479647159576416
Total Training Time: 8.920511245727539 seconds

odt cBe crPer pre
rior hAma(r aabihaim"tj gfed t t Genhed osma t
y fsoouy.xhid ©t thelrevethe cATmip
BEGINNING (1681869889.3881454): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5737, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5700, val loss 4.5598 [1.6851449012756348 sec]
step 100: train loss 3.1506, val loss 3.1865 [4.38900089263916 sec]
step 200: train loss 2.8536, val loss 2.8906 [7.0186638832092285 sec]
step 300: train loss 2.7187, val loss 2.7584 [9.632659673690796 sec]
step 400: train loss 2.6299, val loss 2.6848 [12.226190328598022 sec]
2.6705734729766846
Total Training Time: 13.25818920135498 seconds

al at we whasinyofaow.
Gratetpa wil."
ams y t "Nhed "
Cwat hednedio s heuthe n wukend Pa t" Heruresl
BEGINNING (1681869903.0654774): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5095, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5000, val loss 4.5000 [2.071040630340576 sec]
step 100: train loss 3.0779, val loss 3.0987 [5.665330648422241 sec]
step 200: train loss 2.8113, val loss 2.8449 [9.135032892227173 sec]
step 300: train loss 2.6747, val loss 2.7275 [12.631943464279175 sec]
step 400: train loss 2.5954, val loss 2.6553 [16.243654251098633 sec]
2.5554027557373047
Total Training Time: 17.784462451934814 seconds

€omare hemuy.
Aat th
Nhatoplof uwetint wo t.
I tis hioo well he ne wond.."
"© pe e tiriei%rwig den A
BEGINNING (1681869921.4402304): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6259, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6154, val loss 4.6214 [1.1342766284942627 sec]
step 100: train loss 3.2034, val loss 3.2289 [2.9062557220458984 sec]
step 200: train loss 2.9611, val loss 3.0050 [4.674417972564697 sec]
step 300: train loss 2.8004, val loss 2.8527 [6.432299613952637 sec]
step 400: train loss 2.7081, val loss 2.7666 [8.20715880393982 sec]
2.681978940963745
Total Training Time: 8.863380670547485 seconds

ak phifse Akaod AY%ed –
be gatenn
thitintheled.v e ay c wars, Apahua is it Iitale wtupet genatug ata
BEGINNING (1681869930.538618): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.7012, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6791, val loss 4.6794 [1.5729963779449463 sec]
step 100: train loss 3.1779, val loss 3.2120 [4.149648427963257 sec]
step 200: train loss 2.8983, val loss 2.9357 [6.809313774108887 sec]
step 300: train loss 2.7512, val loss 2.8005 [9.49758243560791 sec]
step 400: train loss 2.6721, val loss 2.7337 [12.308843851089478 sec]
2.6524908542633057
Total Training Time: 13.459213495254517 seconds

5lelre bopa gis rt. s halouruan,."T"
is a
bird.H thd is"©He s yo uorcra3 fd toran thG
1,'tXnd y, Brh
BEGINNING (1681869944.449674): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6534, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6593, val loss 4.6521 [2.2758471965789795 sec]
step 100: train loss 3.1357, val loss 3.1576 [6.0714781284332275 sec]
step 200: train loss 2.8351, val loss 2.8734 [9.844155311584473 sec]
step 300: train loss 2.7016, val loss 2.7558 [13.501906633377075 sec]
step 400: train loss 2.6282, val loss 2.6846 [17.300819158554077 sec]
2.548931121826172
Total Training Time: 18.794892072677612 seconds

his aemesicrid "r wse
ifolour  ;sas
qs ds
t foepsthid athais  orutetelererohes sozlt cokl ny " rr c(
BEGINNING (1681869963.8969114): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6251, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6227, val loss 4.6281 [1.293522596359253 sec]
step 100: train loss 3.1457, val loss 3.1706 [3.331711530685425 sec]
step 200: train loss 2.9433, val loss 2.9762 [5.347957134246826 sec]
step 300: train loss 2.8030, val loss 2.8463 [7.3555169105529785 sec]
step 400: train loss 2.7252, val loss 2.7774 [9.416510105133057 sec]
2.7425925731658936
Total Training Time: 10.56314754486084 seconds

bt uelle I leurad Zuy ok bend,edend ldr
bt weibinos croldv's,
umsss
foutieore Ar t PRlow aven h
wtha
BEGINNING (1681869974.724442): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6528, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6519, val loss 4.6367 [1.9614088535308838 sec]
step 100: train loss 3.1347, val loss 3.1546 [5.1226770877838135 sec]
step 200: train loss 2.8703, val loss 2.9019 [8.357306241989136 sec]
step 300: train loss 2.7520, val loss 2.7955 [11.62018346786499 sec]
step 400: train loss 2.6727, val loss 2.7283 [14.87001085281372 sec]
2.6416749954223633
Total Training Time: 16.191220998764038 seconds

8ane andef
a l oasugous tooraurafue f,the. rgoti.e. "rmad hemar f
po."
"R cthon henun
otn heoon, nsn
BEGINNING (1681869991.32565): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5790, val loss 4.5779 [2.272515296936035 sec]
step 100: train loss 3.0574, val loss 3.0840 [6.364134073257446 sec]
step 200: train loss 2.8138, val loss 2.8609 [10.356119871139526 sec]
step 300: train loss 2.7067, val loss 2.7603 [14.514009475708008 sec]
step 400: train loss 2.6456, val loss 2.7021 [18.678191661834717 sec]
2.6369082927703857
Total Training Time: 20.731389045715332 seconds

us. Taeall lls. tou wed Grnd wa, caAre hKo awicund l minvohe. wuwes mon wnd tt ad och t n ale
hmes s
BEGINNING (1681870012.6895611): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6646, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6716, val loss 4.6730 [1.347303867340088 sec]
step 100: train loss 3.1943, val loss 3.2225 [3.3751018047332764 sec]
step 200: train loss 2.9331, val loss 2.9731 [5.290287494659424 sec]
step 300: train loss 2.7848, val loss 2.8333 [7.261162042617798 sec]
step 400: train loss 2.6898, val loss 2.7507 [9.272820949554443 sec]
2.6444149017333984
Total Training Time: 10.023187160491943 seconds

Q
liceo td
!5mamillerl
"adD sephecondoe uve r llloelaime , rilyatullelthel wo thuow Anay theno b my 
BEGINNING (1681870022.9718845): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6340, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6406, val loss 4.6482 [1.7837343215942383 sec]
step 100: train loss 3.1602, val loss 3.1927 [4.600966215133667 sec]
step 200: train loss 2.8521, val loss 2.8909 [7.452531814575195 sec]
step 300: train loss 2.7126, val loss 2.7579 [10.295973539352417 sec]
step 400: train loss 2.6174, val loss 2.6715 [13.122626781463623 sec]
2.584597110748291
Total Training Time: 14.262739181518555 seconds

med ", b.aniore he he athand weUnthZsthis'irge aeveturt to%e HI hamre ls I.
calr Tkis? sheoures the 
BEGINNING (1681870037.6534362): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.7174, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7121, val loss 4.7047 [2.235492706298828 sec]
step 100: train loss 3.1133, val loss 3.1277 [6.012266397476196 sec]
step 200: train loss 2.8196, val loss 2.8411 [9.89024543762207 sec]
step 300: train loss 2.6801, val loss 2.7291 [13.985317468643188 sec]
step 400: train loss 2.5860, val loss 2.6380 [18.023122549057007 sec]
2.5293612480163574
Total Training Time: 19.529380083084106 seconds

(f cin!
pais atheve anteap sanled thins tHhe oSifobe,
"Tha w
agh Aeyofemrlimy xchounk than hit tt. b
BEGINNING (1681870057.7338092): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6072, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6072, val loss 4.5985 [1.3208699226379395 sec]
step 100: train loss 3.2073, val loss 3.2356 [3.3143539428710938 sec]
step 200: train loss 2.9510, val loss 2.9802 [5.285634279251099 sec]
step 300: train loss 2.7853, val loss 2.8232 [7.272730827331543 sec]
step 400: train loss 2.6982, val loss 2.7497 [9.275809049606323 sec]
2.6766223907470703
Total Training Time: 9.992801189422607 seconds

Wras a t
hreduthed tape i Aralle CSifsou u
Ian?"er rawt rat be C• o oChitatae uth mt g halereritka
j
BEGINNING (1681870067.9646134): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6622, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6557, val loss 4.6561 [1.837066411972046 sec]
step 100: train loss 3.1450, val loss 3.1683 [4.720981597900391 sec]
step 200: train loss 2.8638, val loss 2.9013 [7.666464328765869 sec]
step 300: train loss 2.7209, val loss 2.7658 [10.642891883850098 sec]
step 400: train loss 2.6418, val loss 2.7068 [13.55175232887268 sec]
2.5976362228393555
Total Training Time: 14.694766521453857 seconds

coVend thase ona I cs– ta wefuMindise thealsa t Ohen crid aneaWg s cthero fagkhais Thed he aim d ghu
BEGINNING (1681870083.0445535): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5764, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5753, val loss 4.5738 [2.2229325771331787 sec]
step 100: train loss 3.1082, val loss 3.1393 [5.994046926498413 sec]
step 200: train loss 2.8190, val loss 2.8579 [9.722369194030762 sec]
step 300: train loss 2.6893, val loss 2.7501 [13.51241946220398 sec]
step 400: train loss 2.6057, val loss 2.6723 [17.289350032806396 sec]
2.5568296909332275
Total Training Time: 18.819661855697632 seconds

c9zat GThelk, Wkesta
BX, ind ad uam curs w pâoud
N"V€od oused tth, iskere oulikAlve c
foratrith"O g 
BEGINNING (1681870102.440265): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6072, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6106, val loss 4.6176 [1.492224931716919 sec]
step 100: train loss 3.1796, val loss 3.1982 [3.8591597080230713 sec]
step 200: train loss 2.9217, val loss 2.9537 [6.237991809844971 sec]
step 300: train loss 2.7870, val loss 2.8252 [8.612768411636353 sec]
step 400: train loss 2.7164, val loss 2.7584 [10.95413851737976 sec]
2.7101047039031982
Total Training Time: 11.846437454223633 seconds

E,0jtheArsAr t a2thorerouar ce. mary che fo Aro tat y wh E ce samaOous"
TIreh.l
Chenstof n Yismigkth
BEGINNING (1681870114.5356295): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.7242, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7312, val loss 4.7286 [2.1922757625579834 sec]
step 100: train loss 3.1414, val loss 3.1630 [5.780711650848389 sec]
step 200: train loss 2.8653, val loss 2.8949 [9.251993417739868 sec]
step 300: train loss 2.7404, val loss 2.7903 [12.728785991668701 sec]
step 400: train loss 2.6596, val loss 2.7190 [16.223430395126343 sec]
2.6771278381347656
Total Training Time: 17.60408091545105 seconds

ras Hs,
os Gormed wo!Are, ratand,"atCrt.
Deuid. a. R mngphrouwart hir G~e
HR IR"; yuhe ay qlelr ths 
BEGINNING (1681870132.552707): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.7659, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7734, val loss 4.7681 [2.8088760375976562 sec]
step 100: train loss 3.1231, val loss 3.1441 [7.800987720489502 sec]
step 200: train loss 2.8398, val loss 2.8845 [12.821797370910645 sec]
step 300: train loss 2.7174, val loss 2.7708 [17.94987177848816 sec]
step 400: train loss 2.6372, val loss 2.7014 [22.71270990371704 sec]
2.5857865810394287
Total Training Time: 24.77366614341736 seconds

so?W  ar hatheromu le dd, ws. lnepred at w
wowtr cobrh fhol aalild woraunlaty.
oursZlle ule as he te
BEGINNING (1681870157.858966): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6109, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6226, val loss 4.6192 [1.4460113048553467 sec]
step 100: train loss 3.2002, val loss 3.2220 [3.545605182647705 sec]
step 200: train loss 2.9270, val loss 2.9549 [5.770881652832031 sec]
step 300: train loss 2.7659, val loss 2.8063 [8.073384284973145 sec]
step 400: train loss 2.6668, val loss 2.7145 [10.236769914627075 sec]
2.654326915740967
Total Training Time: 11.040584087371826 seconds

winit the4tikinans. w. Grat ArM kd as aln torll, adien tus wadit athuonye t• u the t. Dcoese A rerin
BEGINNING (1681870169.1265423): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5948, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5999, val loss 4.6061 [1.9913382530212402 sec]
step 100: train loss 3.1434, val loss 3.1637 [5.246827125549316 sec]
step 200: train loss 2.8387, val loss 2.8840 [8.252856016159058 sec]
step 300: train loss 2.6940, val loss 2.7411 [11.254281759262085 sec]
step 400: train loss 2.5957, val loss 2.6575 [14.49626874923706 sec]
2.528625965118408
Total Training Time: 15.733983755111694 seconds

hily yougth
sulla5th
sy, Z
f itegoke. bnt toun waim,
AnlinMu Tle3e le
'itt bel xpica be. tarirowond 
BEGINNING (1681870185.2505732): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6445, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6553, val loss 4.6508 [2.3670337200164795 sec]
step 100: train loss 3.0996, val loss 3.1302 [6.215938329696655 sec]
step 200: train loss 2.8264, val loss 2.8633 [10.010305881500244 sec]
step 300: train loss 2.6798, val loss 2.7352 [13.807889699935913 sec]
step 400: train loss 2.5844, val loss 2.6341 [17.660887241363525 sec]
2.525059938430786
Total Training Time: 19.2792706489563 seconds

niel cy the
uraleuttineleF ad Thagaind,y d wto tingZyt ckthaom, td, redondtalQh b
af te aine. phes m
BEGINNING (1681870205.1256332): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6328, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6341, val loss 4.6308 [1.6491320133209229 sec]
step 100: train loss 3.1904, val loss 3.2062 [4.142215728759766 sec]
step 200: train loss 2.9222, val loss 2.9519 [6.6932692527771 sec]
step 300: train loss 2.7845, val loss 2.8213 [9.19974970817566 sec]
step 400: train loss 2.6961, val loss 2.7417 [11.645665168762207 sec]
2.7129085063934326
Total Training Time: 12.49280858039856 seconds

h yeso8ans w) G t.a Arepemus me bereple a P the barurertharkef I wite.  ntadhe atre mid idstong bigh
BEGINNING (1681870217.861574): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6115, val loss 4.6184 [2.0857901573181152 sec]
step 100: train loss 3.1420, val loss 3.1693 [5.437997102737427 sec]
step 200: train loss 2.8476, val loss 2.8906 [8.788625955581665 sec]
step 300: train loss 2.7104, val loss 2.7557 [12.58012843132019 sec]
step 400: train loss 2.6195, val loss 2.6810 [16.13678240776062 sec]
2.584881067276001
Total Training Time: 17.457122802734375 seconds

wrad tourokeg Ar nth s lat t soofred sw rpasboYllllhergess taf d, 8s ve d hethely t ndothiaukadrivix
BEGINNING (1681870235.8187952): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6246, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6237, val loss 4.6229 [2.6201772689819336 sec]
step 100: train loss 3.0867, val loss 3.1244 [7.05251145362854 sec]
step 200: train loss 2.8145, val loss 2.8623 [11.765548467636108 sec]
step 300: train loss 2.6894, val loss 2.7497 [16.734426736831665 sec]
step 400: train loss 2.6076, val loss 2.6759 [21.200722694396973 sec]
2.5671515464782715
Total Training Time: 23.05236840248108 seconds

the. Fe sZr fooup foke gavsherere t
Cas, me f or se % r ofofoh
atthe wouthd fe ndemamase "-osMinwT H
BEGINNING (1681870259.4860435): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6047, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5993, val loss 4.5990 [1.965291976928711 sec]
step 100: train loss 3.1763, val loss 3.2054 [4.9508397579193115 sec]
step 200: train loss 2.9152, val loss 2.9572 [7.899433612823486 sec]
step 300: train loss 2.7900, val loss 2.8368 [10.83141279220581 sec]
step 400: train loss 2.7130, val loss 2.7675 [13.802814483642578 sec]
2.6585044860839844
Total Training Time: 14.914048194885254 seconds

n"•0Ded l, o7 aPtr thed
ghiast dantred thhaty Invelk
ongealr is sthat
hesuth theo
gellythesisXid o t
BEGINNING (1681870274.638413): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6330, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6416, val loss 4.6497 [2.7314624786376953 sec]
step 100: train loss 3.1269, val loss 3.1519 [6.982428073883057 sec]
step 200: train loss 2.8491, val loss 2.8831 [11.259625434875488 sec]
step 300: train loss 2.7158, val loss 2.7562 [15.715187788009644 sec]
step 400: train loss 2.6419, val loss 2.6944 [19.845297813415527 sec]
2.602266311645508
Total Training Time: 21.548438787460327 seconds

nthed cey add w bicoue, houruaime Hatimantt rfr t Worhetoren w f echul' hautan fered fussneadt sbly 
BEGINNING (1681870296.5855234): Baseline LR(0.0002) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6758, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6755, val loss 4.6652 [3.4530909061431885 sec]
step 100: train loss 3.0554, val loss 3.0779 [9.317145347595215 sec]
step 200: train loss 2.8043, val loss 2.8415 [15.002999305725098 sec]
step 300: train loss 2.6895, val loss 2.7433 [20.895346641540527 sec]
step 400: train loss 2.6192, val loss 2.6765 [26.53658127784729 sec]
2.602670431137085
Total Training Time: 28.814799070358276 seconds

Go nsieenthed of am CHhet
G td hon Chimodahes lFwes Herovas af hos oran
Gr y. yoo thenere wowofoa te
BEGINNING (1681870325.9673283): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5971, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5905, val loss 4.5892 [1.4919414520263672 sec]
step 100: train loss 2.8765, val loss 2.9070 [3.8125522136688232 sec]
step 200: train loss 2.6776, val loss 2.7371 [6.026033163070679 sec]
step 300: train loss 2.5568, val loss 2.6228 [8.128330945968628 sec]
step 400: train loss 2.4552, val loss 2.5448 [10.283089876174927 sec]
2.479994535446167
Total Training Time: 11.126850128173828 seconds

wathil
gl0
ou. Greyest
Abe thap. Yoved
"
Ta ledis waleumpathasal " statdind
renaromilr
s thed tanang
BEGINNING (1681870337.4005804): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6986, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6631, val loss 4.6642 [2.059126853942871 sec]
step 100: train loss 2.8245, val loss 2.8687 [5.599203109741211 sec]
step 200: train loss 2.6009, val loss 2.6525 [9.1488938331604 sec]
step 300: train loss 2.4994, val loss 2.5645 [12.572904825210571 sec]
step 400: train loss 2.3875, val loss 2.4731 [15.993212938308716 sec]
2.3696086406707764
Total Training Time: 17.49579429626465 seconds

ito ared healfermin isthest haker tis led Che ae wopell. Ethele says lie d ingisted ongy tuw haid ta
BEGINNING (1681870355.4864316): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5313, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5579, val loss 4.5451 [2.741389274597168 sec]
step 100: train loss 2.7657, val loss 2.8178 [7.6446661949157715 sec]
step 200: train loss 2.5793, val loss 2.6515 [12.561832189559937 sec]
step 300: train loss 2.4572, val loss 2.5348 [17.533353567123413 sec]
step 400: train loss 2.3379, val loss 2.4260 [22.450818300247192 sec]
2.2756118774414062
Total Training Time: 24.846925497055054 seconds

hemplald a ad gomuenckree orit h, sueey tur shas bs. Phoit obna
An rof lke woichim yr, t!ven smuiter
BEGINNING (1681870381.21606): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6928, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6945, val loss 4.6978 [1.5153932571411133 sec]
step 100: train loss 2.8895, val loss 2.9403 [4.015936613082886 sec]
step 200: train loss 2.6835, val loss 2.7446 [6.314192056655884 sec]
step 300: train loss 2.5937, val loss 2.6678 [8.697447299957275 sec]
step 400: train loss 2.5314, val loss 2.6088 [11.009010314941406 sec]
2.5127949714660645
Total Training Time: 11.916431427001953 seconds

trrean
wav?xin th " nes iuoull o bs lan w mashicug buked f topew. ar, usstomimad ty
Antur toatingedi
BEGINNING (1681870393.4647837): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5603, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5501, val loss 4.5528 [2.2077994346618652 sec]
step 100: train loss 2.8058, val loss 2.8381 [5.881156921386719 sec]
step 200: train loss 2.6153, val loss 2.6782 [9.49402141571045 sec]
step 300: train loss 2.5225, val loss 2.5951 [13.15673828125 sec]
step 400: train loss 2.4489, val loss 2.5314 [16.784910202026367 sec]
2.444199800491333
Total Training Time: 18.384408712387085 seconds

sfoussow wedves f dicatte. Gof ta
sanged whe
ndf fed. Tthis, shigrah kinde, s
d DBd 5Kell ho s herro
BEGINNING (1681870412.4498756): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6635, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6559, val loss 4.6482 [2.7767159938812256 sec]
step 100: train loss 2.7567, val loss 2.8042 [7.8114142417907715 sec]
step 200: train loss 2.5924, val loss 2.6560 [12.584606885910034 sec]
step 300: train loss 2.5038, val loss 2.5699 [17.44426131248474 sec]
step 400: train loss 2.4398, val loss 2.5137 [22.12296438217163 sec]
2.453153133392334
Total Training Time: 24.115784883499146 seconds

Cvivatl smas.
CHAThp Griculy."CBis a. ABAERSEt T Mly Y
wan ha
SP!QMcEDSE’o E TE othanle d! He haig V
BEGINNING (1681870437.3905714): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6196, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6198, val loss 4.6176 [1.5898168087005615 sec]
step 100: train loss 2.8873, val loss 2.9205 [4.200159072875977 sec]
step 200: train loss 2.6851, val loss 2.7371 [6.912883520126343 sec]
step 300: train loss 2.5944, val loss 2.6669 [9.595548152923584 sec]
step 400: train loss 2.5550, val loss 2.6213 [12.245829105377197 sec]
2.5520777702331543
Total Training Time: 13.297873497009277 seconds

anourvof "
nd wishang plla nisto  ngre athit a aling, oue gede pofurererattwn
pef6capSond sis t winl
BEGINNING (1681870451.006963): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6964, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7032, val loss 4.6987 [2.564683675765991 sec]
step 100: train loss 2.8236, val loss 2.8661 [7.227717876434326 sec]
step 200: train loss 2.6403, val loss 2.6955 [11.641719341278076 sec]
step 300: train loss 2.5606, val loss 2.6238 [15.844323873519897 sec]
step 400: train loss 2.5149, val loss 2.5872 [20.39313840866089 sec]
2.5359840393066406
Total Training Time: 22.58828592300415 seconds

nasbumprck ntt. Tat aoko Ait ie hin anerdothethismed, bokes igak
pand ast t be chid, the frythe. t r
BEGINNING (1681870474.1966658): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.7189, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7093, val loss 4.7071 [3.538691520690918 sec]
step 100: train loss 2.7737, val loss 2.8311 [9.547482252120972 sec]
step 200: train loss 2.6006, val loss 2.6688 [15.898383140563965 sec]
step 300: train loss 2.5322, val loss 2.6009 [22.198333501815796 sec]
step 400: train loss 2.4898, val loss 2.5620 [28.497727394104004 sec]
2.4752726554870605
Total Training Time: 31.140958309173584 seconds

orind hedistrt? Thong
Gnalal icon,
t?""Grnokie g HEAY t od wowhisc?"
Ank."
Hed
" atas she mliney the
BEGINNING (1681870506.220234): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6510, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6360, val loss 4.6389 [1.5840439796447754 sec]
step 100: train loss 2.8631, val loss 2.8955 [4.127709865570068 sec]
step 200: train loss 2.6448, val loss 2.7078 [6.8491621017456055 sec]
step 300: train loss 2.4997, val loss 2.5696 [9.58866834640503 sec]
step 400: train loss 2.3882, val loss 2.4696 [12.12191104888916 sec]
2.408478021621704
Total Training Time: 13.060247421264648 seconds

lye sulleath. Tuofans,
Grd,N:a hid. " Jorited Note. Iitae cuok h Med attyalkeFt quw thake fase til t
BEGINNING (1681870519.599389): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5808, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6002, val loss 4.5842 [2.4425079822540283 sec]
step 100: train loss 2.7851, val loss 2.8191 [6.275134563446045 sec]
step 200: train loss 2.5764, val loss 2.6371 [9.83508563041687 sec]
step 300: train loss 2.4279, val loss 2.5045 [13.553534030914307 sec]
step 400: train loss 2.3017, val loss 2.3988 [17.28977608680725 sec]
2.266754388809204
Total Training Time: 18.864652395248413 seconds

soh Tupmor bly ake belo sof youmeed was alsna tuesthe hamean then the m med ulfos whe we splout
how 
BEGINNING (1681870539.0306354): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5597, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5642, val loss 4.5629 [3.04482102394104 sec]
step 100: train loss 2.7391, val loss 2.7862 [8.372634649276733 sec]
step 200: train loss 2.5582, val loss 2.6195 [13.629801988601685 sec]
step 300: train loss 2.4248, val loss 2.5086 [18.850531101226807 sec]
step 400: train loss 2.2913, val loss 2.4015 [23.95483899116516 sec]
2.245270252227783
Total Training Time: 26.125803232192993 seconds

EiY
A8
CERâR Thes TPElageik them. Ta thin sie thar ta sindarat
hed, bubed,"Wrily ? That hid.
Grast w
BEGINNING (1681870565.9687827): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6406, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6579, val loss 4.6534 [1.5999858379364014 sec]
step 100: train loss 2.8741, val loss 2.9067 [4.55323338508606 sec]
step 200: train loss 2.6617, val loss 2.7145 [7.219781160354614 sec]
step 300: train loss 2.5642, val loss 2.6374 [9.912773609161377 sec]
step 400: train loss 2.5030, val loss 2.5735 [12.794951438903809 sec]
2.4614923000335693
Total Training Time: 13.840893745422363 seconds

atZan. smat wa is hut gYea s aradaibe menay miplis. neuat h hatomotfourine spaund ted ahee, the Aref
BEGINNING (1681870580.149607): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6169, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6275, val loss 4.6293 [2.660653829574585 sec]
step 100: train loss 2.7938, val loss 2.8385 [6.866881370544434 sec]
step 200: train loss 2.5996, val loss 2.6633 [11.108226537704468 sec]
step 300: train loss 2.5200, val loss 2.5929 [15.422767162322998 sec]
step 400: train loss 2.4409, val loss 2.5236 [19.921924352645874 sec]
2.4189982414245605
Total Training Time: 21.67410111427307 seconds

wil maBnl couve nply hin'g Thed stont.
" walic. INEjvasqhalan, bu wak Thid ha ce rndndpye fot II tou
BEGINNING (1681870602.4219148): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6748, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6783, val loss 4.6698 [3.270369529724121 sec]
step 100: train loss 2.7561, val loss 2.8028 [9.078505516052246 sec]
step 200: train loss 2.5732, val loss 2.6350 [14.652024984359741 sec]
step 300: train loss 2.4799, val loss 2.5508 [20.047378301620483 sec]
step 400: train loss 2.3888, val loss 2.4734 [25.31605100631714 sec]
2.3059446811676025
Total Training Time: 27.67776846885681 seconds

ackiyot in hans aplyo pofg war, a amp ing thaourile tnd ps s."
X thelicul we sthe uve Vy llyerorilea
BEGINNING (1681870630.9218748): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5915, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5804, val loss 4.5925 [2.0177907943725586 sec]
step 100: train loss 2.8511, val loss 2.8923 [5.3138720989227295 sec]
step 200: train loss 2.6754, val loss 2.7307 [8.51965594291687 sec]
step 300: train loss 2.5933, val loss 2.6574 [11.740089178085327 sec]
step 400: train loss 2.5449, val loss 2.6229 [15.080593585968018 sec]
2.5182266235351562
Total Training Time: 16.380260944366455 seconds

"HAike "Yah isodin.
"B"
ACTHEDFoubACE5 "CHSEGr b
is ha ms
sth Gup CEAio Tsowon ra.R ad G!"Yowok, hak
BEGINNING (1681870647.6077998): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6971, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7001, val loss 4.7021 [3.0917301177978516 sec]
step 100: train loss 2.8058, val loss 2.8435 [8.402881860733032 sec]
step 200: train loss 2.6258, val loss 2.6876 [13.757988929748535 sec]
step 300: train loss 2.5481, val loss 2.6174 [18.982595205307007 sec]
step 400: train loss 2.5041, val loss 2.5782 [24.444921255111694 sec]
2.5090625286102295
Total Training Time: 26.58080554008484 seconds

(iny hirs, wourt,
Iwheon rmota the n theil s Gr8hafrwen ourpato Sily
h.
Oothathll
Taten d s warpond 
BEGINNING (1681870674.7387085): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5705, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5649, val loss 4.5673 [4.167989492416382 sec]
step 100: train loss 2.7667, val loss 2.8084 [11.31763243675232 sec]
step 200: train loss 2.6015, val loss 2.6592 [18.752875566482544 sec]
step 300: train loss 2.5254, val loss 2.5931 [26.204845666885376 sec]
step 400: train loss 2.4762, val loss 2.5556 [33.624409675598145 sec]
2.4688329696655273
Total Training Time: 36.79704022407532 seconds

red
cuan a! UCe popa the tilding." gentin ce thek, a bod mers gruuone che ssers tilinthe
thesm, Buit
BEGINNING (1681870712.39451): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5837, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5933, val loss 4.5913 [1.7451157569885254 sec]
step 100: train loss 2.8770, val loss 2.9141 [4.526245355606079 sec]
step 200: train loss 2.6342, val loss 2.7018 [7.351553916931152 sec]
step 300: train loss 2.4989, val loss 2.5627 [10.211820363998413 sec]
step 400: train loss 2.3876, val loss 2.4645 [13.023164510726929 sec]
2.38352632522583
Total Training Time: 14.037678480148315 seconds

ruadd thate non, than th a tos I
thes arD.
TER Kp pivist Tan A th."
Chet."
Gracurtoknt ar twemad uts
BEGINNING (1681870726.7731876): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.7001, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6935, val loss 4.6858 [2.680568218231201 sec]
step 100: train loss 2.7781, val loss 2.8170 [6.904346227645874 sec]
step 200: train loss 2.5541, val loss 2.6156 [10.693605899810791 sec]
step 300: train loss 2.4067, val loss 2.4838 [14.725762367248535 sec]
step 400: train loss 2.2836, val loss 2.3776 [18.780506372451782 sec]
2.2555947303771973
Total Training Time: 20.389197826385498 seconds

om ta treofid. Hoon wile all wand wackipl orat ha:d on, ar theme. It aeoted or bel4ew
sifef sar wal’
BEGINNING (1681870747.7654877): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6119, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5916, val loss 4.5952 [3.1749141216278076 sec]
step 100: train loss 2.7309, val loss 2.7821 [8.397729873657227 sec]
step 200: train loss 2.5203, val loss 2.5909 [13.574033737182617 sec]
step 300: train loss 2.3701, val loss 2.4576 [18.85318899154663 sec]
step 400: train loss 2.2425, val loss 2.3508 [23.927486181259155 sec]
2.1765594482421875
Total Training Time: 26.307775020599365 seconds

staut." Ta led. Thuw ullans a Tureaclyand the peefoned alt or Anah Grata
gu." Tan, "UWher caiL P3as 
BEGINNING (1681870774.8799646): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7039, val loss 4.6989 [1.908496618270874 sec]
step 100: train loss 2.9116, val loss 2.9477 [4.949417591094971 sec]
step 200: train loss 2.6646, val loss 2.7150 [7.8849170207977295 sec]
step 300: train loss 2.5635, val loss 2.6285 [10.796858787536621 sec]
step 400: train loss 2.4936, val loss 2.5662 [13.695337533950806 sec]
2.45841121673584
Total Training Time: 14.813596725463867 seconds

E•
anorhi tyored ced he avedd iednefim bo t shead fatin cupalinov, a
bande sofosthasarco re, wrot io
BEGINNING (1681870790.0222783): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5748, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5832, val loss 4.5766 [2.6217870712280273 sec]
step 100: train loss 2.7826, val loss 2.8262 [6.993675470352173 sec]
step 200: train loss 2.5866, val loss 2.6477 [11.473598718643188 sec]
step 300: train loss 2.4900, val loss 2.5607 [15.920026540756226 sec]
step 400: train loss 2.4064, val loss 2.4883 [20.37666630744934 sec]
2.362293243408203
Total Training Time: 22.15479588508606 seconds

RKY souo4 fyitt sthim in heldem ab. ditharnd theinsttChe 5
Grome ced aloro d. tuealist t lhthe ish m
BEGINNING (1681870812.7646663): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5850, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5654, val loss 4.5671 [3.7018768787384033 sec]
step 100: train loss 2.7338, val loss 2.7806 [9.810292959213257 sec]
step 200: train loss 2.5522, val loss 2.6092 [15.809866666793823 sec]
step 300: train loss 2.4544, val loss 2.5302 [21.797719717025757 sec]
step 400: train loss 2.3452, val loss 2.4354 [27.87329936027527 sec]
2.263425588607788
Total Training Time: 30.504265308380127 seconds

plG9uttte she Pfor matemad.
Aie we Ond macand hiy Cheskiney Grata m suwsheas, yoke thte,
Tht toure a
BEGINNING (1681870844.1981032): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6442, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6516, val loss 4.6501 [2.633639335632324 sec]
step 100: train loss 2.8607, val loss 2.8911 [6.677810430526733 sec]
step 200: train loss 2.6648, val loss 2.7130 [10.7573082447052 sec]
step 300: train loss 2.5843, val loss 2.6443 [14.856533527374268 sec]
step 400: train loss 2.5329, val loss 2.6038 [18.81862759590149 sec]
2.5395843982696533
Total Training Time: 20.418985843658447 seconds

HA4I oECH˜yoramape wit
ttalundaly t ald idie ta allls. APJtak ciat ta Py h un
bet frtht me ye
Are br
BEGINNING (1681870864.9572477): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6330, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6212, val loss 4.6243 [4.104406118392944 sec]
step 100: train loss 2.7759, val loss 2.8149 [10.601006746292114 sec]
step 200: train loss 2.6076, val loss 2.6754 [17.37940740585327 sec]
step 300: train loss 2.5334, val loss 2.6061 [23.771297931671143 sec]
step 400: train loss 2.4845, val loss 2.5669 [30.27315855026245 sec]
2.469738006591797
Total Training Time: 32.85649013519287 seconds

tha, Gravelfed me is matissn he e werimanomorie d
a n taml onsthe therouound
h y ce pomen
an tls4 tt
BEGINNING (1681870898.393949): Baseline LR(0.0002) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5383, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5275, val loss 4.5338 [5.337610244750977 sec]
step 100: train loss 2.7536, val loss 2.7996 [13.958127975463867 sec]
step 200: train loss 2.5856, val loss 2.6492 [22.482196807861328 sec]
step 300: train loss 2.5071, val loss 2.5791 [30.9659161567688 sec]
step 400: train loss 2.4529, val loss 2.5327 [39.423792362213135 sec]
2.4396743774414062
Total Training Time: 42.970757246017456 seconds

ghithe tube wes thercr live s her saton, he sefeMwinede malit. Hiteotang nesiy t gred S
f fthed mara
BEGINNING (1681870942.1745832): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6599, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6547, val loss 4.6706 [1.6833016872406006 sec]
step 100: train loss 2.7274, val loss 2.7835 [4.377889633178711 sec]
step 200: train loss 2.5469, val loss 2.6102 [7.00018310546875 sec]
step 300: train loss 2.4026, val loss 2.4805 [9.585172891616821 sec]
step 400: train loss 2.2827, val loss 2.3911 [12.186265468597412 sec]
2.2612154483795166
Total Training Time: 13.24115538597107 seconds

ple dol inked Oil
ungagn, momergh wallt dars, no câkeabged'n urat ra wes."
tent helf
Arrfoull hIf at
BEGINNING (1681870955.823409): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6303, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6188, val loss 4.6141 [2.5188381671905518 sec]
step 100: train loss 2.6755, val loss 2.7177 [6.886455774307251 sec]
step 200: train loss 2.4933, val loss 2.5691 [11.216581106185913 sec]
step 300: train loss 2.3459, val loss 2.4290 [15.674175024032593 sec]
step 400: train loss 2.2265, val loss 2.3240 [20.162904500961304 sec]
2.1021065711975098
Total Training Time: 22.13107204437256 seconds

wof the grathenis~ad, sik lil thim to leser the beerraf
prrimin han kaiet hes. Gratta fatta gatle.
B
BEGINNING (1681870978.756368): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5793, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5691, val loss 4.5661 [3.693434000015259 sec]
step 100: train loss 2.6504, val loss 2.7097 [9.973186731338501 sec]
step 200: train loss 2.4715, val loss 2.5569 [16.170995473861694 sec]
step 300: train loss 2.3112, val loss 2.4044 [22.41530728340149 sec]
step 400: train loss 2.1931, val loss 2.3312 [28.641127824783325 sec]
2.2003469467163086
Total Training Time: 31.40096139907837 seconds

us cout omengres ofir stef sicell the meend apt To tians wa"'. We, Iurs It of
S the
megead loaty
We 
BEGINNING (1681871011.2413478): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5510, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5762, val loss 4.5803 [1.617917776107788 sec]
step 100: train loss 2.7246, val loss 2.7946 [4.38668155670166 sec]
step 200: train loss 2.5717, val loss 2.6509 [7.161311388015747 sec]
step 300: train loss 2.4816, val loss 2.5693 [10.177818536758423 sec]
step 400: train loss 2.3985, val loss 2.5003 [12.987687826156616 sec]
2.357591152191162
Total Training Time: 14.140239953994751 seconds

Ti""
Greatti. tasp Tarrour bah uwE herrug thert ht t sheade athe ato umo monrs athaseenod mimn T,
Pa
BEGINNING (1681871025.8070366): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5911, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5968, val loss 4.5902 [2.5330426692962646 sec]
step 100: train loss 2.6727, val loss 2.7246 [7.24152946472168 sec]
step 200: train loss 2.5294, val loss 2.6056 [11.940386772155762 sec]
step 300: train loss 2.4383, val loss 2.5287 [16.734169244766235 sec]
step 400: train loss 2.3432, val loss 2.4408 [21.503875970840454 sec]
2.264230728149414
Total Training Time: 23.47923231124878 seconds

TTEKArent twaloot th7 ake tuo bbiemte stiok.
Ar If conither moutFe ad mphet teookes n."Wede mof se h
BEGINNING (1681871050.04142): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6467, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6386, val loss 4.6428 [3.4588451385498047 sec]
step 100: train loss 2.6410, val loss 2.6916 [9.669505834579468 sec]
step 200: train loss 2.4996, val loss 2.5641 [15.724509000778198 sec]
step 300: train loss 2.4007, val loss 2.4806 [21.87709927558899 sec]
step 400: train loss 2.2963, val loss 2.3971 [28.083685398101807 sec]
2.2294087409973145
Total Training Time: 30.85624074935913 seconds

the fa wind. Mna mar as Natt Tis this fuette rruls id. Oice pheay
te sa rtomandird the)ham. be Grame
BEGINNING (1681871082.0147874): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.7098, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7012, val loss 4.6938 [2.1136646270751953 sec]
step 100: train loss 2.7319, val loss 2.7809 [5.532161712646484 sec]
step 200: train loss 2.5810, val loss 2.6477 [8.921207666397095 sec]
step 300: train loss 2.5223, val loss 2.5985 [12.287966251373291 sec]
step 400: train loss 2.4817, val loss 2.5616 [15.603837490081787 sec]
2.442357301712036
Total Training Time: 16.988447904586792 seconds

ANas hand drat t youpthe n lyon arers satlinofathicoonauss yn he t n f htous out f
f taren. ahind
C

BEGINNING (1681871099.411275): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6104, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6069, val loss 4.6139 [3.2565181255340576 sec]
step 100: train loss 2.6721, val loss 2.7278 [9.260295152664185 sec]
step 200: train loss 2.5464, val loss 2.6155 [15.130407810211182 sec]
step 300: train loss 2.4845, val loss 2.5618 [21.122496843338013 sec]
step 400: train loss 2.4379, val loss 2.5224 [26.906781673431396 sec]
2.4287467002868652
Total Training Time: 29.429754972457886 seconds

abrint then phorard overon to thangidnd tonca
the the, Nado wowwio ted yokisand th t, yallle k whers
BEGINNING (1681871129.6098647): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6507, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6554, val loss 4.6503 [4.768222093582153 sec]
step 100: train loss 2.6521, val loss 2.7096 [12.725099563598633 sec]
step 200: train loss 2.5313, val loss 2.6068 [20.818793773651123 sec]
step 300: train loss 2.4669, val loss 2.5545 [29.08669424057007 sec]
step 400: train loss 2.4134, val loss 2.5054 [37.107624530792236 sec]
2.4122397899627686
Total Training Time: 40.6287100315094 seconds

lt ing trostht wie towhe.
M€o He
"I "ER IIsred, pra t incKAPThers CEKi
cr de t, mullor aeoowald thit
BEGINNING (1681871171.39022): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6530, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6316, val loss 4.6356 [1.8314628601074219 sec]
step 100: train loss 2.7345, val loss 2.7799 [5.006710052490234 sec]
step 200: train loss 2.5257, val loss 2.5875 [8.03477692604065 sec]
step 300: train loss 2.3604, val loss 2.4477 [10.84581470489502 sec]
step 400: train loss 2.2510, val loss 2.3531 [13.64284062385559 sec]
2.2263052463531494
Total Training Time: 14.889894008636475 seconds

Grenjtta?"Q his. Grameswastt
q2
arainftor ow."go the wele mowst bmarsne. Grat ttthe the
miezeve Grat
BEGINNING (1681871186.6807568): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5639, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5775, val loss 4.5910 [2.6739089488983154 sec]
step 100: train loss 2.6480, val loss 2.7168 [7.145969867706299 sec]
step 200: train loss 2.4689, val loss 2.5397 [11.684982538223267 sec]
step 300: train loss 2.2973, val loss 2.4059 [16.266748428344727 sec]
step 400: train loss 2.1726, val loss 2.2937 [20.934029817581177 sec]
2.129234790802002
Total Training Time: 23.015214920043945 seconds

pum, won harmasars as
cs?Yougs Anher bed bates
oror Matin a
corpud cans and lownand cuttind wars eer
BEGINNING (1681871210.4689565): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6480, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6666, val loss 4.6668 [3.709451913833618 sec]
step 100: train loss 2.6273, val loss 2.6936 [10.267404317855835 sec]
step 200: train loss 2.4294, val loss 2.5104 [16.743370056152344 sec]
step 300: train loss 2.2637, val loss 2.3665 [23.07695198059082 sec]
step 400: train loss 2.1396, val loss 2.2732 [29.23288917541504 sec]
2.136268138885498
Total Training Time: 32.08172869682312 seconds

athe wal tatte'ty wap cros ans, wit;a" Cs Arplen A anded.
"HAPy was of worche to Thim fe
haly silk."
BEGINNING (1681871243.6710517): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6767, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6554, val loss 4.6515 [1.9202821254730225 sec]
step 100: train loss 2.7220, val loss 2.7645 [4.94910192489624 sec]
step 200: train loss 2.5519, val loss 2.6199 [7.972720146179199 sec]
step 300: train loss 2.4571, val loss 2.5372 [11.047780990600586 sec]
step 400: train loss 2.3551, val loss 2.4418 [14.1306631565094 sec]
2.3160765171051025
Total Training Time: 15.367100954055786 seconds

Ancamansiy
CHig retknd he sperrathe
oung ff then fullll tous, anla oo we torrwa on yomnarghed onotep
BEGINNING (1681871259.4321632): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6581, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6659, val loss 4.6704 [2.956916332244873 sec]
step 100: train loss 2.6729, val loss 2.7280 [8.0135498046875 sec]
step 200: train loss 2.5094, val loss 2.5813 [13.008352756500244 sec]
step 300: train loss 2.4008, val loss 2.4806 [18.0538067817688 sec]
step 400: train loss 2.2648, val loss 2.3711 [23.28413200378418 sec]
2.2252016067504883
Total Training Time: 25.446330070495605 seconds

REANTEACEER OFSKAG
l9 HAT– IHS Y2HE
"CE1B WUSESAT
AY
" J–
"WlE UD Thayarrrea t Too baroay yend rinu 
BEGINNING (1681871285.6055195): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6357, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6359, val loss 4.6334 [4.135995864868164 sec]
step 100: train loss 2.6316, val loss 2.6855 [11.250251770019531 sec]
step 200: train loss 2.4892, val loss 2.5609 [18.32172417640686 sec]
step 300: train loss 2.3705, val loss 2.4633 [25.83843994140625 sec]
step 400: train loss 2.2276, val loss 2.3406 [33.05170655250549 sec]
2.130175828933716
Total Training Time: 36.0984947681427 seconds

tume anddd thing wis tenes in, mall ul
and ing
ti6
LD2
CHE Yine to colaR H My tht comexf wat ck as e
BEGINNING (1681871322.8682015): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6537, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6501, val loss 4.6610 [2.6031062602996826 sec]
step 100: train loss 2.7089, val loss 2.7576 [7.046767711639404 sec]
step 200: train loss 2.5706, val loss 2.6416 [11.607000589370728 sec]
step 300: train loss 2.5099, val loss 2.5819 [16.073689460754395 sec]
step 400: train loss 2.4738, val loss 2.5529 [20.341322422027588 sec]
2.4526572227478027
Total Training Time: 22.09000253677368 seconds

Sleund the thandenhee. Vong we thrithin, orhenceft poumu as ha f thist.."Thio t wakens
tin " ht
s I 
BEGINNING (1681871345.3889709): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6027, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6049, val loss 4.5965 [4.1178741455078125 sec]
step 100: train loss 2.6551, val loss 2.7206 [11.180249214172363 sec]
step 200: train loss 2.5319, val loss 2.6101 [18.693219423294067 sec]
step 300: train loss 2.4696, val loss 2.5538 [26.280099391937256 sec]
step 400: train loss 2.4088, val loss 2.4991 [33.73719120025635 sec]
2.3612940311431885
Total Training Time: 36.70812630653381 seconds

to ncy Ta but aroust thingh tor haf chae foul klcie l Sas
Pahisowit, bowa th Th tod lag ilyonowhere 
BEGINNING (1681871382.8541026): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6106, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6126, val loss 4.6159 [5.769674777984619 sec]
step 100: train loss 2.6428, val loss 2.7060 [16.197561025619507 sec]
step 200: train loss 2.5166, val loss 2.5835 [26.839171171188354 sec]
step 300: train loss 2.4522, val loss 2.5310 [37.73067092895508 sec]
step 400: train loss 2.3883, val loss 2.4764 [48.61691498756409 sec]
2.34387469291687
Total Training Time: 53.1158242225647 seconds

NAChck torour, cup red agarcinto snou. tugin los bee sist iaba 75
Chin PTadere ba andenm tis nor t f
BEGINNING (1681871437.1067896): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6596, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6321, val loss 4.6351 [2.1149823665618896 sec]
step 100: train loss 2.7089, val loss 2.7570 [5.505175590515137 sec]
step 200: train loss 2.5164, val loss 2.5908 [8.849126100540161 sec]
step 300: train loss 2.3510, val loss 2.4361 [12.546118974685669 sec]
step 400: train loss 2.2321, val loss 2.3393 [15.968328714370728 sec]
2.16976261138916
Total Training Time: 17.299140453338623 seconds

sapowerth the,, Clled
ved alliand ver ot ine Thaskoreiten. "Nanat. Whased he spehe "Y
ceve worsant l
BEGINNING (1681871454.8501449): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5235, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5424, val loss 4.5445 [3.384357452392578 sec]
step 100: train loss 2.6383, val loss 2.6997 [8.526268482208252 sec]
step 200: train loss 2.4295, val loss 2.5119 [13.713117837905884 sec]
step 300: train loss 2.2497, val loss 2.3589 [18.8251314163208 sec]
step 400: train loss 2.1142, val loss 2.2451 [23.86505365371704 sec]
2.0237860679626465
Total Training Time: 25.997443437576294 seconds

what met
ron antown, ssponed GriAva waidraying
and win the bad Coin telly mang. Mthit smelyor!" quin
BEGINNING (1681871481.5661368): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6462, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6271, val loss 4.6340 [4.15474796295166 sec]
step 100: train loss 2.6158, val loss 2.6689 [11.203808307647705 sec]
step 200: train loss 2.3837, val loss 2.4691 [18.130202293395996 sec]
step 300: train loss 2.2119, val loss 2.3251 [25.112412929534912 sec]
step 400: train loss 2.0786, val loss 2.2214 [32.120660066604614 sec]
2.0504424571990967
Total Training Time: 35.08621859550476 seconds

Grattunked nastaked Gonchcen dees wit had?" Ve be. Gratturantth sneythe thenter tiobes the
cat Plead
BEGINNING (1681871517.7566414): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6066, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6121, val loss 4.6172 [2.3052945137023926 sec]
step 100: train loss 2.7080, val loss 2.7691 [6.3888490200042725 sec]
step 200: train loss 2.5495, val loss 2.6188 [10.273037672042847 sec]
step 300: train loss 2.4457, val loss 2.5299 [14.118016004562378 sec]
step 400: train loss 2.3335, val loss 2.4305 [17.954670667648315 sec]
2.2599220275878906
Total Training Time: 19.43003225326538 seconds

ita f'llo, added
dow y“gugheremithe maid dognchion te en
ur tred "Yo ffan, lieaven wtt th weritha bo
BEGINNING (1681871537.6202757): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6907, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6753, val loss 4.6767 [3.675950288772583 sec]
step 100: train loss 2.6655, val loss 2.7269 [9.815545320510864 sec]
step 200: train loss 2.5012, val loss 2.5817 [16.10616946220398 sec]
step 300: train loss 2.3777, val loss 2.4745 [22.185099601745605 sec]
step 400: train loss 2.2323, val loss 2.3484 [28.34421968460083 sec]
2.1669211387634277
Total Training Time: 30.811264276504517 seconds

onieam the him, thepnigves anet and tne murs unt netor hr the chire,
Chaqueast wy aicas hasis torest
BEGINNING (1681871569.2745407): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.7039, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7090, val loss 4.6988 [5.009542226791382 sec]
step 100: train loss 2.6158, val loss 2.6742 [13.490655899047852 sec]
step 200: train loss 2.4596, val loss 2.5386 [22.25753092765808 sec]
step 300: train loss 2.3246, val loss 2.4222 [31.22138214111328 sec]
step 400: train loss 2.1859, val loss 2.3116 [39.870667457580566 sec]
2.102407693862915
Total Training Time: 43.51739454269409 seconds

cood the yiu
mus ared cut, "I meaKrean the oftervowgid beke fo’ wash. The tus and wne
ardat?"
"Di mB
BEGINNING (1681871614.0414927): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5629, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5668, val loss 4.5667 [3.4869425296783447 sec]
step 100: train loss 2.7128, val loss 2.7593 [8.721842288970947 sec]
step 200: train loss 2.5688, val loss 2.6377 [14.152751922607422 sec]
step 300: train loss 2.5045, val loss 2.5811 [19.843586206436157 sec]
step 400: train loss 2.4583, val loss 2.5376 [25.69339942932129 sec]
2.4014639854431152
Total Training Time: 28.051034212112427 seconds

necorand theeas emacous sos." th. o Oovere su HAnt tonenen tho wainas himed sof a d sththeur, a SGr 
BEGINNING (1681871642.5348873): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6869, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6887, val loss 4.6883 [5.532274007797241 sec]
step 100: train loss 2.6571, val loss 2.7131 [15.630623817443848 sec]
step 200: train loss 2.5288, val loss 2.6041 [24.77790379524231 sec]
step 300: train loss 2.4644, val loss 2.5456 [33.414302587509155 sec]
step 400: train loss 2.4018, val loss 2.4882 [42.0162787437439 sec]
2.375882625579834
Total Training Time: 45.58621954917908 seconds

Pyre any hamad the was epie hongtuster he d g. "Wr! cescoublar cedgustr cu
veads t aid hel as theati
BEGINNING (1681871688.916222): Baseline LR(0.0002) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6202, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6123, val loss 4.6118 [6.96979022026062 sec]
step 100: train loss 2.6352, val loss 2.6972 [19.006491899490356 sec]
step 200: train loss 2.5140, val loss 2.5825 [32.216089487075806 sec]
step 300: train loss 2.4386, val loss 2.5155 [45.28803539276123 sec]
step 400: train loss 2.3513, val loss 2.4417 [56.36799478530884 sec]
2.2822296619415283
Total Training Time: 63.164311170578 seconds

and and," oiz, hay.L Gratgha
Grtata auo fuld have lanehe tho
be alstth. end o sthieselyond thy us hu
BEGINNING (1681871753.2731853): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5477, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5709, val loss 4.5795 [1.9202725887298584 sec]
step 100: train loss 2.6655, val loss 2.7211 [5.082245826721191 sec]
step 200: train loss 2.4768, val loss 2.5460 [8.342912673950195 sec]
step 300: train loss 2.3053, val loss 2.4119 [11.412744522094727 sec]
step 400: train loss 2.1973, val loss 2.3123 [14.43491506576538 sec]
2.0671472549438477
Total Training Time: 15.632062196731567 seconds

jokll had fo gur
che, aout. Mas frothad pat at elomokich's ta jondd lot cott ancer.
Astock wrad ta h
BEGINNING (1681871769.38524): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.7040, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6968, val loss 4.6929 [2.9752371311187744 sec]
step 100: train loss 2.6105, val loss 2.6646 [8.098443269729614 sec]
step 200: train loss 2.4246, val loss 2.5108 [13.13066029548645 sec]
step 300: train loss 2.2506, val loss 2.3589 [18.17076349258423 sec]
step 400: train loss 2.1233, val loss 2.2533 [23.2268705368042 sec]
2.1842079162597656
Total Training Time: 25.41032075881958 seconds

a muryout ou wareard bow Quss iler, ath na."
"2
"Youly wat, a to loorkicte arive hers alL
not mat?"Y
BEGINNING (1681871795.6665845): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6234, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6012, val loss 4.5973 [3.9711787700653076 sec]
step 100: train loss 2.5720, val loss 2.6401 [11.105428457260132 sec]
step 200: train loss 2.3970, val loss 2.4847 [18.270119667053223 sec]
step 300: train loss 2.2173, val loss 2.3376 [25.476893663406372 sec]
step 400: train loss 2.0880, val loss 2.2181 [32.82160925865173 sec]
2.0829763412475586
Total Training Time: 36.15082597732544 seconds

vereme layseed sien ing migh. On waturpe thalled
Anad wagh agrand a lut moniss ofty has ive mef, Mci
BEGINNING (1681871833.1208937): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5721, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5745, val loss 4.5775 [1.885133981704712 sec]
step 100: train loss 2.6481, val loss 2.7051 [5.03909707069397 sec]
step 200: train loss 2.5127, val loss 2.5867 [8.23731780052185 sec]
step 300: train loss 2.4052, val loss 2.4903 [11.32751202583313 sec]
step 400: train loss 2.2800, val loss 2.3844 [14.463716983795166 sec]
2.206923723220825
Total Training Time: 15.700831174850464 seconds

and they tAimarewaedins call I5
wagny cozed bpougantt hard, oncked yame his widdd thake arest
Hinged
BEGINNING (1681871849.3028395): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5606, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5764, val loss 4.5759 [2.9991936683654785 sec]
step 100: train loss 2.6022, val loss 2.6821 [8.470722913742065 sec]
step 200: train loss 2.4714, val loss 2.5524 [13.891784191131592 sec]
step 300: train loss 2.3554, val loss 2.4551 [19.285615921020508 sec]
step 400: train loss 2.2270, val loss 2.3424 [24.783034563064575 sec]
2.128082275390625
Total Training Time: 27.371557235717773 seconds

loun fir of and's shis
sped the mereurle tubs mesternet fued re©. Krla,
tait out ther to lat, siblor
BEGINNING (1681871877.6118984): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5511, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5700, val loss 4.5703 [4.293361663818359 sec]
step 100: train loss 2.5788, val loss 2.6418 [11.933529138565063 sec]
step 200: train loss 2.4489, val loss 2.5277 [19.927148818969727 sec]
step 300: train loss 2.3317, val loss 2.4304 [27.91390872001648 sec]
step 400: train loss 2.1812, val loss 2.2989 [35.71576261520386 sec]
2.1717123985290527
Total Training Time: 39.10996985435486 seconds

he be mureer to ca filo thareasame an'tere mies.
Grattay nomed. Not w uoul Grttad sland orrd lat ssk
BEGINNING (1681871918.1273448): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6311, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6228, val loss 4.6229 [2.7944374084472656 sec]
step 100: train loss 2.6478, val loss 2.7132 [6.8981451988220215 sec]
step 200: train loss 2.5344, val loss 2.6040 [10.987052917480469 sec]
step 300: train loss 2.4845, val loss 2.5644 [15.082066297531128 sec]
step 400: train loss 2.4319, val loss 2.5191 [19.179105043411255 sec]
2.484502077102661
Total Training Time: 20.87145709991455 seconds

pat foed. Npardond the tres, prin lrrenictom.."I lir. Founa cked woplas wlithicokemsked Goup. Gno Py
BEGINNING (1681871939.4938426): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5924, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5999, val loss 4.6000 [4.075831890106201 sec]
step 100: train loss 2.6046, val loss 2.6720 [11.490260601043701 sec]
step 200: train loss 2.5034, val loss 2.5777 [18.988500595092773 sec]
step 300: train loss 2.4376, val loss 2.5267 [26.493252277374268 sec]
step 400: train loss 2.3678, val loss 2.4654 [33.9574453830719 sec]
2.304389476776123
Total Training Time: 37.0651741027832 seconds

husn." Bes sat wanw teave brs
tuto of bio fighuch r wigxcon ers apkecces chit and, whabe eut
wis.
Do
BEGINNING (1681871977.476654): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5540, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5609, val loss 4.5493 [6.004251003265381 sec]
step 100: train loss 2.5760, val loss 2.6438 [16.47923493385315 sec]
step 200: train loss 2.4888, val loss 2.5682 [26.825072765350342 sec]
step 300: train loss 2.4230, val loss 2.5099 [36.880855321884155 sec]
step 400: train loss 2.3273, val loss 2.4336 [47.45958113670349 sec]
2.2573578357696533
Total Training Time: 51.954002380371094 seconds

wheieros in. Thah aierrey had parod thid stul."RUS Gonttat thas wo
cteicheon aknd ceronfthedbem thin
BEGINNING (1681872030.8941026): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6574, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6476, val loss 4.6444 [2.0814049243927 sec]
step 100: train loss 2.6379, val loss 2.6986 [5.945553302764893 sec]
step 200: train loss 2.4424, val loss 2.5226 [9.749757766723633 sec]
step 300: train loss 2.2673, val loss 2.3593 [13.297363042831421 sec]
step 400: train loss 2.1466, val loss 2.2724 [16.677164793014526 sec]
2.117701292037964
Total Training Time: 18.068755865097046 seconds

th's sin."
"ONHA waPden, 5HAPut name warimple. "NTuod ing spe, you his dow a( the Pitre wat blah
Cri
BEGINNING (1681872049.422804): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6319, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6534, val loss 4.6525 [3.1712396144866943 sec]
step 100: train loss 2.5776, val loss 2.6420 [8.643480062484741 sec]
step 200: train loss 2.3695, val loss 2.4558 [14.077983617782593 sec]
step 300: train loss 2.1958, val loss 2.3168 [19.702996253967285 sec]
step 400: train loss 2.0631, val loss 2.2007 [25.259048223495483 sec]
2.1088707447052
Total Training Time: 27.685444116592407 seconds

mall the sto wie and hourd, pusn they we iove. Anallke Thawka his hith
the oned walu."
B't wim the t
BEGINNING (1681872078.043095): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6378, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6503, val loss 4.6446 [4.411988973617554 sec]
step 100: train loss 2.5669, val loss 2.6397 [12.157532691955566 sec]
step 200: train loss 2.3592, val loss 2.4503 [19.93220567703247 sec]
step 300: train loss 2.1550, val loss 2.2797 [28.420804738998413 sec]
step 400: train loss 1.9898, val loss 2.1558 [36.2717981338501 sec]
2.0615789890289307
Total Training Time: 39.82145571708679 seconds

his y.. "Areppesis the tonw all whe culd bareshiy.
"1ef Arphan you pesss greop, me tru. Tuon he
cong
BEGINNING (1681872119.2895408): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5926, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5902, val loss 4.5985 [2.3001444339752197 sec]
step 100: train loss 2.6307, val loss 2.7052 [6.211938142776489 sec]
step 200: train loss 2.4899, val loss 2.5638 [10.183944940567017 sec]
step 300: train loss 2.3600, val loss 2.4545 [14.119619846343994 sec]
step 400: train loss 2.2207, val loss 2.3369 [18.20120120048523 sec]
2.164433717727661
Total Training Time: 19.762694120407104 seconds

Anavefta th jof wicind benter – we
amothe weratis forege! The Gremulatightheake thed.
Here he draxph
BEGINNING (1681872139.6069605): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6365, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6384, val loss 4.6341 [3.9482507705688477 sec]
step 100: train loss 2.5802, val loss 2.6410 [10.515146732330322 sec]
step 200: train loss 2.4483, val loss 2.5195 [16.988472938537598 sec]
step 300: train loss 2.3162, val loss 2.4065 [23.246403694152832 sec]
step 400: train loss 2.1500, val loss 2.2829 [29.690133094787598 sec]
2.0886950492858887
Total Training Time: 32.33966851234436 seconds

crp" eakaily. "Me sty ulsuin wa Anaddir
gat buldeds t
an to withe dond ungon incem be. He sellg up. 
BEGINNING (1681872172.8550029): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5830, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5736, val loss 4.5765 [5.039319276809692 sec]
step 100: train loss 2.5711, val loss 2.6476 [14.055320024490356 sec]
step 200: train loss 2.4206, val loss 2.5056 [23.23063063621521 sec]
step 300: train loss 2.2678, val loss 2.3764 [32.54803919792175 sec]
step 400: train loss 2.0970, val loss 2.2304 [41.86498141288757 sec]
1.9975987672805786
Total Training Time: 45.83445930480957 seconds

wampt qusentigh't. Thery He migh nopll gated
of urren lopeangentir asSfer, toollik hil his wilve
war
BEGINNING (1681872220.279801): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5818, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5785, val loss 4.5817 [3.1456966400146484 sec]
step 100: train loss 2.6492, val loss 2.7076 [8.330500602722168 sec]
step 200: train loss 2.5282, val loss 2.5983 [13.543036460876465 sec]
step 300: train loss 2.4648, val loss 2.5435 [18.744081497192383 sec]
step 400: train loss 2.4031, val loss 2.4959 [23.911919116973877 sec]
2.337838649749756
Total Training Time: 26.0035560131073 seconds

gauss wahtield hap ple cull nead
"It his deyout pamgaamedghever skasthigry. Prtt t sindes
-h t satid
BEGINNING (1681872246.7751467): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5932, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5934, val loss 4.5846 [5.2373223304748535 sec]
step 100: train loss 2.5954, val loss 2.6641 [14.279335498809814 sec]
step 200: train loss 2.4873, val loss 2.5614 [23.311750650405884 sec]
step 300: train loss 2.4151, val loss 2.4986 [32.3529589176178 sec]
step 400: train loss 2.3359, val loss 2.4434 [41.431694984436035 sec]
2.255021095275879
Total Training Time: 45.25547385215759 seconds

it. ino the thim anddegass Pyou foutw h. Os Sstoren, yerou
tie arnt wat the at whepay to. Gratta
med
BEGINNING (1681872292.9152796): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6545, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6575, val loss 4.6545 [7.445069313049316 sec]
step 100: train loss 2.5756, val loss 2.6483 [20.385157108306885 sec]
step 200: train loss 2.4724, val loss 2.5541 [33.28155875205994 sec]
step 300: train loss 2.3898, val loss 2.4806 [46.175522327423096 sec]
step 400: train loss 2.2754, val loss 2.3854 [59.078691244125366 sec]
2.1961543560028076
Total Training Time: 64.58385992050171 seconds

Slis dee gat was.
2. DWe dow solf, thi He wa wen cull hemit tor
of the tungot torells ’non wamaindgh
BEGINNING (1681872358.8470402): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6183, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5975, val loss 4.5953 [2.390833616256714 sec]
step 100: train loss 2.6218, val loss 2.6818 [5.925674200057983 sec]
step 200: train loss 2.4048, val loss 2.4853 [9.539708375930786 sec]
step 300: train loss 2.2265, val loss 2.3368 [13.09848952293396 sec]
step 400: train loss 2.1061, val loss 2.2424 [16.54038977622986 sec]
2.1200900077819824
Total Training Time: 17.846455335617065 seconds

wator sordeded towe
ramofned."
but!E Mc4 Gratta
PHAMcked."
CHATEA NS
Any.
"Nid the sbistacXncoked an
BEGINNING (1681872377.1536841): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5511, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5525, val loss 4.5452 [3.327930212020874 sec]
step 100: train loss 2.5663, val loss 2.6311 [8.970567464828491 sec]
step 200: train loss 2.3155, val loss 2.4183 [14.54360294342041 sec]
step 300: train loss 2.1275, val loss 2.2639 [20.069911241531372 sec]
step 400: train loss 1.9898, val loss 2.1571 [25.66744828224182 sec]
2.002290725708008
Total Training Time: 27.981130599975586 seconds

As had mois a fuphed of thims doung. Hetuon. Mon un he
comme!" Anay ad peidd, rould spoonded. I had 
BEGINNING (1681872406.0434847): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5997, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6244, val loss 4.6275 [4.422728776931763 sec]
step 100: train loss 2.5347, val loss 2.6056 [12.194868087768555 sec]
step 200: train loss 2.3047, val loss 2.4064 [20.002805948257446 sec]
step 300: train loss 2.1031, val loss 2.2361 [28.02356719970703 sec]
step 400: train loss 1.9460, val loss 2.1102 [35.79055857658386 sec]
1.9439165592193604
Total Training Time: 39.121400594711304 seconds

warlild cant the himp." Takaes ubs reecreib speem fomp
the you noity beack an! Arephay of I the
will
BEGINNING (1681872446.4840438): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6176, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6187, val loss 4.6210 [2.5613291263580322 sec]
step 100: train loss 2.6375, val loss 2.6987 [7.51389217376709 sec]
step 200: train loss 2.4766, val loss 2.5540 [12.329538345336914 sec]
step 300: train loss 2.3330, val loss 2.4307 [16.798866748809814 sec]
step 400: train loss 2.1932, val loss 2.3128 [21.28392243385315 sec]
2.1565134525299072
Total Training Time: 23.10185694694519 seconds

uc Tand Grackin thing hemee ontead ding tho his of histe, ad nowstine had – Arag
CH
leneakat th rale
BEGINNING (1681872470.1224937): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5935, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5946, val loss 4.5901 [4.800024509429932 sec]
step 100: train loss 2.5862, val loss 2.6526 [12.396320819854736 sec]
step 200: train loss 2.4314, val loss 2.5182 [19.604955434799194 sec]
step 300: train loss 2.2664, val loss 2.3746 [26.793699979782104 sec]
step 400: train loss 2.0950, val loss 2.2286 [34.08611226081848 sec]
2.0204010009765625
Total Training Time: 37.18276071548462 seconds

Arpad batderad
cKillar ing al theve upplmag they knos eat inn
doned yrhin anas Gratta
la the his car
BEGINNING (1681872508.2435715): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6037, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5994, val loss 4.6072 [5.9516730308532715 sec]
step 100: train loss 2.5599, val loss 2.6295 [16.399759531021118 sec]
step 200: train loss 2.3982, val loss 2.4778 [26.96461296081543 sec]
step 300: train loss 2.2171, val loss 2.3390 [37.506906270980835 sec]
step 400: train loss 2.0368, val loss 2.1865 [48.07171320915222 sec]
1.986741542816162
Total Training Time: 52.404813289642334 seconds

coutien areatin amayse he man, he to nidSo com s haw
3
CHS
Scris weftell ceave." Gontta to asBo Grat
BEGINNING (1681872562.0336118): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6310, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6424, val loss 4.6386 [4.0151801109313965 sec]
step 100: train loss 2.6340, val loss 2.6981 [11.63106107711792 sec]
step 200: train loss 2.5187, val loss 2.5953 [18.616389513015747 sec]
step 300: train loss 2.4541, val loss 2.5320 [24.998703241348267 sec]
step 400: train loss 2.3824, val loss 2.4816 [31.35020422935486 sec]
2.3455777168273926
Total Training Time: 33.932499170303345 seconds

foul llin
Graly wharan Xereyou feustisk
Grang
t.
"Eanc wahin me Y
bok som ato tatre Dodratokind Oith
BEGINNING (1681872596.4462872): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6663, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6687, val loss 4.6718 [6.4526448249816895 sec]
step 100: train loss 2.5944, val loss 2.6591 [17.754523515701294 sec]
step 200: train loss 2.4837, val loss 2.5598 [29.917008638381958 sec]
step 300: train loss 2.3982, val loss 2.4847 [42.15720891952515 sec]
step 400: train loss 2.2753, val loss 2.3796 [54.85934925079346 sec]
2.1878316402435303
Total Training Time: 59.9880096912384 seconds

sprop ly arsto milon the tu
no culd disean, t Coman xbrm ata?"Cleried Acef
Wherpad ahied Anond ve ca
BEGINNING (1681872657.4640923): Baseline LR(0.0002) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6328, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6322, val loss 4.6332 [10.167637586593628 sec]
step 100: train loss 2.5678, val loss 2.6387 [28.363596200942993 sec]
step 200: train loss 2.4584, val loss 2.5407 [45.70382213592529 sec]
step 300: train loss 2.3675, val loss 2.4598 [62.33041262626648 sec]
step 400: train loss 2.2257, val loss 2.3433 [80.03472661972046 sec]
2.132917642593384
Total Training Time: 87.69113874435425 seconds

wolf re dilled wight. I wive a theid to le the ma
Datintwere, was, nir cubst owlh we s!"
Tualeding b
BEGINNING (1681872746.7520063): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6906, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6793, val loss 4.6824 [2.6894006729125977 sec]
step 100: train loss 2.5709, val loss 2.6524 [7.251145362854004 sec]
step 200: train loss 2.3463, val loss 2.4456 [11.564083814620972 sec]
step 300: train loss 2.1852, val loss 2.3081 [16.200144290924072 sec]
step 400: train loss 2.0803, val loss 2.2459 [20.758291006088257 sec]
1.992919921875
Total Training Time: 22.661981344223022 seconds

We the sprmellgation, nelll ans werancicelp sTis "EA ploeur saed the srould pect raf. Graatta penodd
BEGINNING (1681872770.1261613): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6570, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6542, val loss 4.6644 [4.164551258087158 sec]
step 100: train loss 2.5344, val loss 2.6066 [11.842409610748291 sec]
step 200: train loss 2.3072, val loss 2.4069 [19.294580698013306 sec]
step 300: train loss 2.1098, val loss 2.2667 [26.86778974533081 sec]
step 400: train loss 1.9771, val loss 2.1629 [34.17789649963379 sec]
2.0079896450042725
Total Training Time: 37.256282567977905 seconds

whed thros? Trom all eawfith'"
Gratta a hen yene feroall cugho't. Ked you weood as ot the be affrey 
BEGINNING (1681872808.6477873): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5745, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5860, val loss 4.5864 [5.695120573043823 sec]
step 100: train loss 2.5084, val loss 2.5784 [15.989331007003784 sec]
step 200: train loss 2.2951, val loss 2.3904 [26.923083782196045 sec]
step 300: train loss 2.0851, val loss 2.2304 [38.36233830451965 sec]
step 400: train loss 1.9505, val loss 2.1235 [48.684483766555786 sec]
1.8816635608673096
Total Training Time: 53.14904189109802 seconds

the cubs fort afteg; El. "I Then mone
to am mithe quing gast." Gorttedned peht arnteroped this take 
BEGINNING (1681872863.710905): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6643, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6413, val loss 4.6493 [2.659241199493408 sec]
step 100: train loss 2.5745, val loss 2.6527 [7.407272577285767 sec]
step 200: train loss 2.4325, val loss 2.5060 [12.142008781433105 sec]
step 300: train loss 2.2759, val loss 2.3858 [16.784469604492188 sec]
step 400: train loss 2.1239, val loss 2.2651 [21.419188261032104 sec]
2.073815107345581
Total Training Time: 23.25786304473877 seconds

he kaddep. Pcreare, is Nomolf sen Mecef an't aired to turn to his
to younave we thacuopls and lin fu
BEGINNING (1681872887.6389835): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6034, val loss 4.6028 [4.601528167724609 sec]
step 100: train loss 2.5290, val loss 2.6203 [12.969121217727661 sec]
step 200: train loss 2.3849, val loss 2.4744 [21.19792914390564 sec]
step 300: train loss 2.2093, val loss 2.3411 [29.51551842689514 sec]
step 400: train loss 2.0314, val loss 2.1905 [38.00067329406738 sec]
1.9860776662826538
Total Training Time: 41.626678228378296 seconds

Ber this with manich eamesw. The adut torrejur." ked the le moun hem tornd thour
jundgickly and at a
BEGINNING (1681872930.6055858): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6451, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6564, val loss 4.6567 [6.541387557983398 sec]
step 100: train loss 2.5177, val loss 2.5938 [18.842071294784546 sec]
step 200: train loss 2.3727, val loss 2.4648 [30.834816932678223 sec]
step 300: train loss 2.1907, val loss 2.3213 [43.12041926383972 sec]
step 400: train loss 2.0016, val loss 2.1672 [55.0189425945282 sec]
1.9566709995269775
Total Training Time: 60.152888774871826 seconds

vey hights fer ound sivil wack and ambet ightes our. "Whoult with
was reingeredt on was sais gre, yo
BEGINNING (1681872992.7250679): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6547, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6512, val loss 4.6466 [3.574934244155884 sec]
step 100: train loss 2.5775, val loss 2.6443 [9.63143515586853 sec]
step 200: train loss 2.4863, val loss 2.5657 [15.734618902206421 sec]
step 300: train loss 2.4161, val loss 2.5097 [22.115238189697266 sec]
step 400: train loss 2.3371, val loss 2.4305 [28.628347158432007 sec]
2.2617745399475098
Total Training Time: 31.29317307472229 seconds

EE SITCHE MY
SE1
BV TEAN
"HANE4
"CN
ZEKdenen HE– I
"Y
anener bomor cel ber amin stell er, toot thand
BEGINNING (1681873024.7465193): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.7061, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7020, val loss 4.7043 [6.5931737422943115 sec]
step 100: train loss 2.5435, val loss 2.6094 [18.031723260879517 sec]
step 200: train loss 2.4449, val loss 2.5315 [29.89738917350769 sec]
step 300: train loss 2.3454, val loss 2.4465 [41.39111828804016 sec]
step 400: train loss 2.1808, val loss 2.2995 [52.889015436172485 sec]
2.1223626136779785
Total Training Time: 57.80435132980347 seconds

has but chupt aly hilenet, you condrickah ins in trreneCoing ain.
Gratt walp cowhist wotllits Yegh w
BEGINNING (1681873083.9915507): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6870, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6766, val loss 4.6647 [9.714507102966309 sec]
step 100: train loss 2.5281, val loss 2.6067 [26.846884965896606 sec]
step 200: train loss 2.4458, val loss 2.5288 [44.13949418067932 sec]
step 300: train loss 2.3411, val loss 2.4500 [61.03985619544983 sec]
step 400: train loss 2.1464, val loss 2.2782 [77.46698904037476 sec]
2.0184645652770996
Total Training Time: 84.70751953125 seconds

un smely notaurer hor had of trat warto tuok to.
Grattia werled thiss te a may, "Ysaceugh al?"
Araya
BEGINNING (1681873170.6528769): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6371, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6385, val loss 4.6308 [2.555476427078247 sec]
step 100: train loss 2.5370, val loss 2.6046 [6.852294445037842 sec]
step 200: train loss 2.2717, val loss 2.3750 [11.249377727508545 sec]
step 300: train loss 2.0873, val loss 2.2354 [15.89068055152893 sec]
step 400: train loss 1.9939, val loss 2.1558 [20.7815945148468 sec]
1.8949508666992188
Total Training Time: 22.604297399520874 seconds

athe waraly seall
oves and the fromaefor and saow. Wher maitre the oing fur leach. "IfI for cout bro
BEGINNING (1681873193.9962263): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5718, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5869, val loss 4.5871 [4.385493278503418 sec]
step 100: train loss 2.4967, val loss 2.5759 [12.712453126907349 sec]
step 200: train loss 2.2388, val loss 2.3496 [20.71068286895752 sec]
step 300: train loss 2.0501, val loss 2.1987 [28.94198775291443 sec]
step 400: train loss 1.9050, val loss 2.0784 [37.13455629348755 sec]
1.8649988174438477
Total Training Time: 40.613176584243774 seconds

smight 1 of the praply had two hell wos, then! Ze Kried mance was
take was thim we sto rever have Ma
BEGINNING (1681873235.944387): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5986, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6278, val loss 4.6254 [6.160372018814087 sec]
step 100: train loss 2.4674, val loss 2.5372 [17.459181308746338 sec]
step 200: train loss 2.1999, val loss 2.3313 [28.58804488182068 sec]
step 300: train loss 1.9984, val loss 2.1595 [39.666563749313354 sec]
step 400: train loss 1.8685, val loss 2.0515 [50.76117491722107 sec]
1.8328012228012085
Total Training Time: 55.46620845794678 seconds

2 A Eaps, be cuntimmes they to den, ce
and – fittim ta will thinithforce cownl abut 47
SEAY
se toons
BEGINNING (1681873293.9026592): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5995, val loss 4.5929 [3.4911749362945557 sec]
step 100: train loss 2.5669, val loss 2.6457 [9.031966209411621 sec]
step 200: train loss 2.3990, val loss 2.4939 [14.429485559463501 sec]
step 300: train loss 2.2015, val loss 2.3247 [19.93267059326172 sec]
step 400: train loss 2.0430, val loss 2.1892 [25.555564403533936 sec]
1.9690375328063965
Total Training Time: 27.70234751701355 seconds

wi nod. He hen. Namel Loous af a sailed the had
scl notess if det an attew Thiur aventa of tirs the 
BEGINNING (1681873322.310654): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6125, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6165, val loss 4.6142 [5.667876720428467 sec]
step 100: train loss 2.5250, val loss 2.6100 [15.722636938095093 sec]
step 200: train loss 2.3639, val loss 2.4656 [25.620806217193604 sec]
step 300: train loss 2.1508, val loss 2.2874 [35.161725759506226 sec]
step 400: train loss 1.9677, val loss 2.1433 [44.74791193008423 sec]
1.8485280275344849
Total Training Time: 48.71465516090393 seconds

Tilasighe wo this Gratta dn to pached had.
And lay ot tuood cat the lad to thi© He cave
ant, "You th
BEGINNING (1681873372.3107035): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6028, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5919, val loss 4.5924 [7.713612794876099 sec]
step 100: train loss 2.4926, val loss 2.5753 [21.68586301803589 sec]
step 200: train loss 2.3211, val loss 2.4172 [35.32941031455994 sec]
step 300: train loss 2.0854, val loss 2.2167 [49.556509256362915 sec]
step 400: train loss 1.8924, val loss 2.0779 [63.38797736167908 sec]
1.8370641469955444
Total Training Time: 69.39358425140381 seconds

lal taka, a ase him the firdess to squigh Lumient whos ar."
Nief wen Prearnapped and and low
sidge o
BEGINNING (1681873443.7402544): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6561, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6613, val loss 4.6562 [5.233954191207886 sec]
step 100: train loss 2.5673, val loss 2.6406 [15.081538200378418 sec]
step 200: train loss 2.4600, val loss 2.5407 [23.336305618286133 sec]
step 300: train loss 2.3699, val loss 2.4599 [32.35439920425415 sec]
step 400: train loss 2.2439, val loss 2.3587 [40.59907102584839 sec]
2.153202772140503
Total Training Time: 44.82615303993225 seconds

AVE o goes,"
thas tube lied ily tirpoon tererco bict, then!"
Dou?"
"Yop I are the sheroow chir t the
BEGINNING (1681873489.336427): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5840, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5816, val loss 4.5851 [8.723936319351196 sec]
step 100: train loss 2.5343, val loss 2.6123 [23.507856369018555 sec]
step 200: train loss 2.4244, val loss 2.5135 [38.745311975479126 sec]
step 300: train loss 2.2956, val loss 2.4062 [54.00004291534424 sec]
step 400: train loss 2.1029, val loss 2.2433 [70.20001459121704 sec]
1.995035171508789
Total Training Time: 81.44131445884705 seconds

omet or andeponed a had walll. Tha wisthee
and tard and smaterood bent. Inilden dit Cint Gratta
by y
BEGINNING (1681873572.1722834): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5245, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5123, val loss 4.5199 [12.173916816711426 sec]
step 100: train loss 2.5113, val loss 2.5909 [40.39885711669922 sec]
step 200: train loss 2.4113, val loss 2.5024 [73.72907614707947 sec]
step 300: train loss 2.2559, val loss 2.3718 [97.93246102333069 sec]
step 400: train loss 2.0296, val loss 2.1877 [120.09426140785217 sec]
1.9942296743392944
Total Training Time: 129.6975793838501 seconds

areated, sin thestienght the tathe oss cwued tine
to smenioaly feur aman lo asterme norse esas larer
BEGINNING (1681873703.786414): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6652, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6570, val loss 4.6511 [2.8748085498809814 sec]
step 100: train loss 2.5154, val loss 2.5869 [7.618749141693115 sec]
step 200: train loss 2.2309, val loss 2.3545 [12.403785705566406 sec]
step 300: train loss 2.0568, val loss 2.2072 [17.095350980758667 sec]
step 400: train loss 1.9446, val loss 2.1350 [21.751468420028687 sec]
1.9925835132598877
Total Training Time: 23.562041997909546 seconds

sturn or prastin, belen!" wo have
Kack cuttenthe
loom artiontion. 1ng to ust th shad gno fow urh cay
BEGINNING (1681873728.001483): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6177, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6092, val loss 4.6153 [4.841304540634155 sec]
step 100: train loss 2.4656, val loss 2.5497 [13.216723442077637 sec]
step 200: train loss 2.1911, val loss 2.3058 [21.48743486404419 sec]
step 300: train loss 1.9801, val loss 2.1445 [29.64046311378479 sec]
step 400: train loss 1.8506, val loss 2.0398 [37.92092800140381 sec]
1.822185754776001
Total Training Time: 41.25218439102173 seconds

uparchanurred as relt That We quamer for wits, borief bits saled an vale's wilmeroor his as fould an
BEGINNING (1681873770.5108166): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6316, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6304, val loss 4.6388 [6.5267229080200195 sec]
step 100: train loss 2.4501, val loss 2.5339 [17.995169162750244 sec]
step 200: train loss 2.1603, val loss 2.2893 [29.647842168807983 sec]
step 300: train loss 1.9559, val loss 2.1303 [41.269407987594604 sec]
step 400: train loss 1.8129, val loss 2.0202 [53.048866748809814 sec]
1.7533038854599
Total Training Time: 58.24969983100891 seconds

xeard
to ground thumblight. I meake warge
that dany me and han art
and saw aw to the sarrougin older
BEGINNING (1681873830.6804235): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5800, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5856, val loss 4.5868 [4.133426189422607 sec]
step 100: train loss 2.5438, val loss 2.6181 [11.231427669525146 sec]
step 200: train loss 2.3599, val loss 2.4539 [20.186606407165527 sec]
step 300: train loss 2.1498, val loss 2.2804 [26.94417953491211 sec]
step 400: train loss 1.9893, val loss 2.1538 [33.21401071548462 sec]
1.970877766609192
Total Training Time: 36.18640947341919 seconds

Sepreceht thauF wath tred wely
cel
mur. "Ya ining, hand smeraud soll. The
whe be uone hist thave agu
BEGINNING (1681873867.5278258): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6456, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6543, val loss 4.6492 [6.951991081237793 sec]
step 100: train loss 2.5044, val loss 2.5812 [18.466713666915894 sec]
step 200: train loss 2.3189, val loss 2.4211 [30.03060746192932 sec]
step 300: train loss 2.0680, val loss 2.2111 [41.432581663131714 sec]
step 400: train loss 1.8826, val loss 2.0676 [61.42067766189575 sec]
1.8395196199417114
Total Training Time: 66.15960669517517 seconds

sheany the goads, and sesome sment for prespousseclespowar neg as homin
le ma( Taelast me ous him. A
BEGINNING (1681873934.987677): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5968, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6014, val loss 4.6054 [8.939569234848022 sec]
step 100: train loss 2.4895, val loss 2.5608 [25.146585702896118 sec]
step 200: train loss 2.2835, val loss 2.3909 [41.451136350631714 sec]
step 300: train loss 2.0363, val loss 2.1881 [60.305973052978516 sec]
step 400: train loss 1.8571, val loss 2.0540 [77.50464916229248 sec]
1.8248510360717773
Total Training Time: 84.86993074417114 seconds

Prriag th Burr be as conss soanced?"
Gratta looked, you were and smollaght, the Grim al stuation. "T
BEGINNING (1681874022.0001905): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6024, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6030, val loss 4.5939 [6.521825313568115 sec]
step 100: train loss 2.5617, val loss 2.6379 [17.225241899490356 sec]
step 200: train loss 2.4550, val loss 2.5404 [28.043192148208618 sec]
step 300: train loss 2.3535, val loss 2.4539 [38.752535820007324 sec]
step 400: train loss 2.2174, val loss 2.3392 [51.89976644515991 sec]
2.145663022994995
Total Training Time: 56.107993602752686 seconds

uws
fole sto hemplueugh
teriene. Gradey the enot um chot hermon tumit hana hacell gautes
fores lomng
BEGINNING (1681874078.7928185): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6171, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6218, val loss 4.6258 [10.363354682922363 sec]
step 100: train loss 2.5257, val loss 2.5981 [29.04970407485962 sec]
step 200: train loss 2.4176, val loss 2.5088 [47.15675711631775 sec]
step 300: train loss 2.2800, val loss 2.3882 [65.36051177978516 sec]
step 400: train loss 2.0807, val loss 2.2294 [83.6963541507721 sec]
2.0072696208953857
Total Training Time: 91.36453795433044 seconds

hy hor is to face."
Gratta of the daindednge. And the clly ke mistt of yeand
veros ther cacke inard 
BEGINNING (1681874171.3905594): Baseline LR(0.0002) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5407, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5401, val loss 4.5388 [14.790142059326172 sec]
step 100: train loss 2.5099, val loss 2.5885 [40.47335195541382 sec]
step 200: train loss 2.3972, val loss 2.4868 [66.08019733428955 sec]
step 300: train loss 2.2028, val loss 2.3235 [91.68393898010254 sec]
step 400: train loss 1.9667, val loss 2.1449 [117.27346849441528 sec]
1.9029401540756226
Total Training Time: 128.2047312259674 seconds

mees cot hist the gat tur ourn his ind prat the was of
a sty the a he lokeme a yrer ou upperwly'ther
BEGINNING (1681874301.4552515): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6950, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6815, val loss 4.6760 [2.795978546142578 sec]
step 100: train loss 2.5018, val loss 2.5823 [7.608775854110718 sec]
step 200: train loss 2.2444, val loss 2.3457 [12.399407625198364 sec]
step 300: train loss 2.0884, val loss 2.2351 [17.18460249900818 sec]
step 400: train loss 1.9650, val loss 2.1366 [21.966370105743408 sec]
2.076608896255493
Total Training Time: 23.9628586769104 seconds

Gratta.ta stulfored a stoved, saved
to th patiolivey, soon nod jus saced tur your yon under to istre
BEGINNING (1681874326.2127092): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6872, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6984, val loss 4.6953 [4.802157163619995 sec]
step 100: train loss 2.4723, val loss 2.5609 [13.44820785522461 sec]
step 200: train loss 2.2006, val loss 2.3325 [22.10005521774292 sec]
step 300: train loss 2.0074, val loss 2.1914 [30.99601125717163 sec]
step 400: train loss 1.8865, val loss 2.0825 [39.596901655197144 sec]
1.852522850036621
Total Training Time: 43.565372943878174 seconds

to ation. Hell for he rihtide( to pathen0we reacere naded reavey but a
goad tent hus twas. Grotta ba
BEGINNING (1681874371.2960284): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6464, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6062, val loss 4.6082 [6.731154203414917 sec]
step 100: train loss 2.4595, val loss 2.5410 [19.326621055603027 sec]
step 200: train loss 2.2019, val loss 2.3295 [31.790757656097412 sec]
step 300: train loss 1.9997, val loss 2.1669 [44.453675985336304 sec]
step 400: train loss 1.8740, val loss 2.0526 [56.98556995391846 sec]
1.9123014211654663
Total Training Time: 62.74449014663696 seconds

orts the the farrier roddsjed best countlin. "De, ve and as nows lat fround ware nord lops camply si
BEGINNING (1681874436.3060358): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.7061, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7025, val loss 4.7057 [3.221038818359375 sec]
step 100: train loss 2.5318, val loss 2.6217 [8.942167043685913 sec]
step 200: train loss 2.3656, val loss 2.4763 [14.654595851898193 sec]
step 300: train loss 2.1643, val loss 2.2894 [20.302672386169434 sec]
step 400: train loss 2.0053, val loss 2.1708 [25.951342582702637 sec]
1.9225364923477173
Total Training Time: 28.399495363235474 seconds

BKA,"The and dea was kn tonerd cunce oners vether are selly dow
snacked
reaganners theen. Aany
fel o
BEGINNING (1681874465.5055873): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.7065, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7024, val loss 4.6981 [5.765788555145264 sec]
step 100: train loss 2.5014, val loss 2.5879 [16.235758304595947 sec]
step 200: train loss 2.3136, val loss 2.4190 [26.689236879348755 sec]
step 300: train loss 2.0846, val loss 2.2207 [37.116347789764404 sec]
step 400: train loss 1.9086, val loss 2.0899 [47.56322503089905 sec]
1.8334330320358276
Total Training Time: 52.25041604042053 seconds

Anay geation's of of now. Gra comfold themarey is ounst
roos comon, and nowop ine Deis. Ihade oure f
BEGINNING (1681874519.3037977): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6472, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6389, val loss 4.6430 [8.307192087173462 sec]
step 100: train loss 2.4939, val loss 2.5707 [23.559783458709717 sec]
step 200: train loss 2.3077, val loss 2.4143 [38.88205432891846 sec]
step 300: train loss 2.0648, val loss 2.2139 [54.147052526474 sec]
step 400: train loss 1.8829, val loss 2.0754 [69.79741835594177 sec]
1.7687804698944092
Total Training Time: 76.78317451477051 seconds

hed hat heire ks id bentsled dowly as and The bradets toziarron)
That Ep tom is iniy the men, Gentta
BEGINNING (1681874598.3578165): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6045, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6184, val loss 4.6282 [4.829286336898804 sec]
step 100: train loss 2.5368, val loss 2.6051 [13.339557886123657 sec]
step 200: train loss 2.4435, val loss 2.5266 [21.875062227249146 sec]
step 300: train loss 2.3579, val loss 2.4623 [30.625240564346313 sec]
step 400: train loss 2.2219, val loss 2.3445 [39.380842208862305 sec]
2.16548228263855
Total Training Time: 43.18862843513489 seconds

cEANI MYMY
cou tene the tho you IV thean cuweagr. Gety Ktherab stellio
onced the cly. Zesios airath 
BEGINNING (1681874642.367439): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5869, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5931, val loss 4.5929 [9.058518409729004 sec]
step 100: train loss 2.5082, val loss 2.5819 [25.384716749191284 sec]
step 200: train loss 2.4084, val loss 2.5128 [41.73930287361145 sec]
step 300: train loss 2.2626, val loss 2.3873 [58.0748336315155 sec]
step 400: train loss 2.0260, val loss 2.1942 [74.40623641014099 sec]
1.9587446451187134
Total Training Time: 81.60809755325317 seconds

paizs. Scorid. Gratta
att nod atin thay you to calligh oveate as inubleet croveromes.
"Thae lataked 
BEGINNING (1681874725.5515237): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5790, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5697, val loss 4.5600 [13.235921144485474 sec]
step 100: train loss 2.4988, val loss 2.5849 [37.13013434410095 sec]
step 200: train loss 2.4100, val loss 2.5020 [60.976266622543335 sec]
step 300: train loss 2.2182, val loss 2.3441 [84.79353094100952 sec]
step 400: train loss 1.9860, val loss 2.1645 [108.68414235115051 sec]
1.9079686403274536
Total Training Time: 119.24714040756226 seconds

to the coom.
"Ner of mand enrad, to foold A yah we
well cumby licle. He fut lens omem. Pyrahie nood

BEGINNING (1681874847.1347322): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6992, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6931, val loss 4.6922 [3.1197991371154785 sec]
step 100: train loss 2.4858, val loss 2.5634 [8.40276575088501 sec]
step 200: train loss 2.2159, val loss 2.3453 [13.702132225036621 sec]
step 300: train loss 2.0335, val loss 2.1937 [18.983083724975586 sec]
step 400: train loss 1.9114, val loss 2.1030 [24.30547261238098 sec]
1.869117259979248
Total Training Time: 26.511805772781372 seconds

then PeraCchand withil the way if Elyows losk,
dem. I wave humans Everich Hure als here oxced the
pa
BEGINNING (1681874874.4595442): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6998, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6757, val loss 4.6825 [7.825318813323975 sec]
step 100: train loss 2.4362, val loss 2.5320 [24.346586227416992 sec]
step 200: train loss 2.1424, val loss 2.2830 [33.917425870895386 sec]
step 300: train loss 1.9511, val loss 2.1178 [43.49939298629761 sec]
step 400: train loss 1.8243, val loss 2.0203 [53.05615830421448 sec]
1.8262853622436523
Total Training Time: 57.30247616767883 seconds

phy Elsses mi72 Vasely. Tuon wfore the wall gard barraws exeps. The waskeor
roucgold! "Comperowith Y
BEGINNING (1681874933.2582474): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6712, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6966, val loss 4.6915 [7.62373685836792 sec]
step 100: train loss 2.4234, val loss 2.5133 [21.528417110443115 sec]
step 200: train loss 2.1335, val loss 2.2776 [35.315423011779785 sec]
step 300: train loss 1.9291, val loss 2.1151 [49.18133521080017 sec]
step 400: train loss 1.7819, val loss 1.9840 [62.99843430519104 sec]
1.6483254432678223
Total Training Time: 69.25405287742615 seconds

in this he gor just pack en fescorst and
sunjouly gite
to hal befroun and the sid: Ron him,
tonneysh
BEGINNING (1681875004.7911787): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5996, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6056, val loss 4.6071 [4.107593059539795 sec]
step 100: train loss 2.5078, val loss 2.5862 [11.198872804641724 sec]
step 200: train loss 2.3148, val loss 2.4094 [18.25957942008972 sec]
step 300: train loss 2.0847, val loss 2.2336 [25.339937925338745 sec]
step 400: train loss 1.9269, val loss 2.1073 [32.4094181060791 sec]
1.9209376573562622
Total Training Time: 35.40552806854248 seconds

my for mikes onca We well qua!"
dige qued diriare alms. Golfor the werethe
dailecound, "Sh's wine tu
BEGINNING (1681875040.9737923): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6365, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6495, val loss 4.6496 [7.342673301696777 sec]
step 100: train loss 2.4805, val loss 2.5648 [20.399823904037476 sec]
step 200: train loss 2.2517, val loss 2.3693 [33.450037479400635 sec]
step 300: train loss 1.9999, val loss 2.1670 [46.479087114334106 sec]
step 400: train loss 1.8362, val loss 2.0303 [59.519461154937744 sec]
1.779396891593933
Total Training Time: 65.25490093231201 seconds

cub)
doug fundim enjor stirs, melut he amment
vells!"
sing lightedgen. What wals your nelyit his the
BEGINNING (1681875107.7682934): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6390, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6272, val loss 4.6171 [10.622430801391602 sec]
step 100: train loss 2.4578, val loss 2.5534 [29.688122510910034 sec]
step 200: train loss 2.2463, val loss 2.3660 [48.698426246643066 sec]
step 300: train loss 1.9572, val loss 2.1323 [67.73361420631409 sec]
step 400: train loss 1.7883, val loss 2.0157 [86.77010703086853 sec]
1.6945247650146484
Total Training Time: 95.1960666179657 seconds

watvers tooch thre un twe dudrood bot by OF Gon PirFa
to Lasmer to up Gor be turned hat had caman hi
BEGINNING (1681875205.4079585): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6029, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6061, val loss 4.5963 [6.370321035385132 sec]
step 100: train loss 2.5256, val loss 2.6078 [17.871370553970337 sec]
step 200: train loss 2.4194, val loss 2.5114 [29.468361139297485 sec]
step 300: train loss 2.2907, val loss 2.4069 [41.030495166778564 sec]
step 400: train loss 2.0913, val loss 2.2361 [52.56838607788086 sec]
2.0048110485076904
Total Training Time: 57.57132697105408 seconds

haneroigh tatlan a deen preatte
coughas. Blyonin hey an rea cll wall. He
Arphand sackarg ats wn will
BEGINNING (1681875263.8345366): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6619, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6560, val loss 4.6658 [12.05613088607788 sec]
step 100: train loss 2.4984, val loss 2.5860 [33.54319357872009 sec]
step 200: train loss 2.3911, val loss 2.4913 [55.01860547065735 sec]
step 300: train loss 2.2006, val loss 2.3288 [76.37087273597717 sec]
step 400: train loss 1.9821, val loss 2.1658 [97.73269510269165 sec]
1.8825751543045044
Total Training Time: 107.07559585571289 seconds

A w; chayaver wh, magorwntied and the at
avoolle. Thars in ay boardion thim for ent wall an your.
"D
BEGINNING (1681875372.4992154): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5760, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5677, val loss 4.5617 [17.612353086471558 sec]
step 100: train loss 2.4955, val loss 2.5829 [47.79321360588074 sec]
step 200: train loss 2.3623, val loss 2.4582 [77.4814293384552 sec]
step 300: train loss 2.0982, val loss 2.2400 [107.15093421936035 sec]
step 400: train loss 1.8679, val loss 2.0693 [136.70136189460754 sec]
1.8381675481796265
Total Training Time: 149.42578721046448 seconds

wis cabe muon rights iplecty.
Gratta ganed wered tlooke they the Perear.
Chied him dold mieuest thes
BEGINNING (1681875524.3229434): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6741, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6838, val loss 4.6824 [3.473595380783081 sec]
step 100: train loss 2.4631, val loss 2.5409 [9.546650886535645 sec]
step 200: train loss 2.1435, val loss 2.2899 [15.627891778945923 sec]
step 300: train loss 1.9782, val loss 2.1425 [21.650962829589844 sec]
step 400: train loss 1.8542, val loss 2.0374 [27.65416431427002 sec]
1.8779592514038086
Total Training Time: 30.17224359512329 seconds

untes neer why are wild wvong o a coured diblano that thed sound gYes wis thele ent. Goraty saind wi
BEGINNING (1681875555.282318): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5360, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5305, val loss 4.5257 [6.001328945159912 sec]
step 100: train loss 2.4138, val loss 2.4965 [16.737576961517334 sec]
step 200: train loss 2.0866, val loss 2.2370 [27.501771211624146 sec]
step 300: train loss 1.8892, val loss 2.0746 [38.17652606964111 sec]
step 400: train loss 1.7550, val loss 1.9722 [48.77316856384277 sec]
1.711153268814087
Total Training Time: 53.39060163497925 seconds

dow – Tham and enter, etungels and he brosid, this at
the puremplies be he horeards. I hism Gone
his
BEGINNING (1681875610.223616): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.4834, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4941, val loss 4.4880 [8.41437578201294 sec]
step 100: train loss 2.3916, val loss 2.4935 [23.6828510761261 sec]
step 200: train loss 2.0709, val loss 2.2253 [38.96873188018799 sec]
step 300: train loss 1.8530, val loss 2.0582 [54.221359968185425 sec]
step 400: train loss 1.7264, val loss 1.9675 [69.52827525138855 sec]
1.6045883893966675
Total Training Time: 76.35293793678284 seconds

Jyonsides. They comfe." We quirrived hat felo at alemply.
We buth" mave squeckl! They cart by to tha
BEGINNING (1681875688.8617258): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6443, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6581, val loss 4.6463 [4.94579553604126 sec]
step 100: train loss 2.4962, val loss 2.5759 [13.502863883972168 sec]
step 200: train loss 2.2679, val loss 2.3784 [22.07498025894165 sec]
step 300: train loss 2.0230, val loss 2.1820 [30.634111642837524 sec]
step 400: train loss 1.8787, val loss 2.0592 [39.220622301101685 sec]
1.8609529733657837
Total Training Time: 42.82973790168762 seconds

nout and alloues for didriffom slijs pockn's. Tily wosas frow
lhetring the didsht canstooked, sairt!
BEGINNING (1681875732.5066838): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6233, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6220, val loss 4.6142 [8.875133752822876 sec]
step 100: train loss 2.4670, val loss 2.5510 [24.953158140182495 sec]
step 200: train loss 2.2243, val loss 2.3434 [41.026678800582886 sec]
step 300: train loss 1.9444, val loss 2.1189 [57.11141872406006 sec]
step 400: train loss 1.7743, val loss 1.9861 [73.36420321464539 sec]
1.755195140838623
Total Training Time: 80.31661820411682 seconds

Arphe maeuw the in the our in tas Yeu be yout, solde the ovir
laugh. We shar with a to clonthis my m
BEGINNING (1681875814.3708813): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5865, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5971, val loss 4.5874 [12.900167465209961 sec]
step 100: train loss 2.4508, val loss 2.5390 [36.149041175842285 sec]
step 200: train loss 2.2024, val loss 2.3172 [59.57205820083618 sec]
step 300: train loss 1.9123, val loss 2.0993 [82.78421568870544 sec]
step 400: train loss 1.7215, val loss 1.9541 [105.98963904380798 sec]
1.6335793733596802
Total Training Time: 116.20974898338318 seconds

to shost their Pyalan aland booking a Gratta look ast Arphand
shold But quith Pyrhaps! Onjon of Perh
BEGINNING (1681875932.8895178): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6281, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6328, val loss 4.6328 [7.835224390029907 sec]
step 100: train loss 2.5213, val loss 2.5972 [21.807554244995117 sec]
step 200: train loss 2.4066, val loss 2.4932 [35.78558659553528 sec]
step 300: train loss 2.2592, val loss 2.3751 [49.762521505355835 sec]
step 400: train loss 2.0496, val loss 2.1986 [63.53567051887512 sec]
1.9814058542251587
Total Training Time: 69.4428038597107 seconds

with soireed Aidg lam with theed. "I st meat and in to
sithe hin Trowa sad wele isto hed This eapns.
BEGINNING (1681876003.1301398): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5482, val loss 4.5535 [14.25841212272644 sec]
step 100: train loss 2.4909, val loss 2.5691 [38.6448860168457 sec]
step 200: train loss 2.3539, val loss 2.4532 [62.86116409301758 sec]
step 300: train loss 2.1214, val loss 2.2658 [87.43181347846985 sec]
step 400: train loss 1.9053, val loss 2.0926 [111.97788119316101 sec]
1.837712287902832
Total Training Time: 122.33932900428772 seconds

Kadear ood him s the sim. Arwayah noyas waling an a
tuon nowe gaidgab." Arstayah a afimes the cals
c
BEGINNING (1681876126.972828): Baseline LR(0.0002) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5711, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5748, val loss 4.5762 [20.62667942047119 sec]
step 100: train loss 2.4788, val loss 2.5595 [54.08074450492859 sec]
step 200: train loss 2.3297, val loss 2.4461 [87.7699773311615 sec]
step 300: train loss 2.0542, val loss 2.2162 [121.35715842247009 sec]
step 400: train loss 1.8369, val loss 2.0471 [154.85597920417786 sec]
1.7373106479644775
Total Training Time: 169.12960743904114 seconds

flrodes."
"Yes, woy saguse, she cust the cexpliven. "Bere to peconcing
may now! The our catiter yor 
BEGINNING (1681876298.3695388): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6337, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6223, val loss 4.6202 [3.929020404815674 sec]
step 100: train loss 2.4395, val loss 2.5365 [10.958844900131226 sec]
step 200: train loss 2.1302, val loss 2.2718 [17.949800968170166 sec]
step 300: train loss 1.9594, val loss 2.1431 [24.92854380607605 sec]
step 400: train loss 1.8568, val loss 2.0620 [31.981817483901978 sec]
1.794317603111267
Total Training Time: 35.050288915634155 seconds

took 1 lose." The he hom was
Gratta dans cor in dered in he nod
as ditherlfors weed befor
the maeuw.
BEGINNING (1681876334.5768495): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6057, val loss 4.6064 [7.172006845474243 sec]
step 100: train loss 2.4099, val loss 2.5022 [20.257014751434326 sec]
step 200: train loss 2.0975, val loss 2.2423 [33.36452054977417 sec]
step 300: train loss 1.8981, val loss 2.0816 [46.46692776679993 sec]
step 400: train loss 1.7927, val loss 2.0119 [59.59049940109253 sec]
1.710748553276062
Total Training Time: 65.58098244667053 seconds

talking what as shy legatword feling oth a sheet are trragd. We
sugome towe held gon had below melk 
BEGINNING (1681876402.3119817): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6393, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6665, val loss 4.6658 [10.372190713882446 sec]
step 100: train loss 2.4481, val loss 2.5352 [29.731643199920654 sec]
step 200: train loss 2.1212, val loss 2.2638 [49.041638135910034 sec]
step 300: train loss 1.9310, val loss 2.1152 [68.40515971183777 sec]
step 400: train loss 1.7960, val loss 2.0301 [87.74688768386841 sec]
1.6753880977630615
Total Training Time: 96.66896843910217 seconds

turned and longation with.
He a would him brong with neall must." The of
Knoon Sorn't rehechent whin
BEGINNING (1681876502.2860622): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6161, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6148, val loss 4.6104 [4.95276665687561 sec]
step 100: train loss 2.4715, val loss 2.5564 [14.107394218444824 sec]
step 200: train loss 2.2574, val loss 2.3781 [23.249328136444092 sec]
step 300: train loss 2.0124, val loss 2.1764 [32.378395080566406 sec]
step 400: train loss 1.8627, val loss 2.0666 [41.51386117935181 sec]
1.8775358200073242
Total Training Time: 45.68578481674194 seconds

taked the Pir fim, "We there rive spaiefore of office
hish umarellving
tone atin?
Shod int smile%ko 
BEGINNING (1681876549.1210423): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6368, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6385, val loss 4.6277 [9.25720763206482 sec]
step 100: train loss 2.4618, val loss 2.5643 [26.642244815826416 sec]
step 200: train loss 2.2188, val loss 2.3405 [43.99507522583008 sec]
step 300: train loss 1.9329, val loss 2.1142 [61.4014778137207 sec]
step 400: train loss 1.7759, val loss 2.0006 [78.80142521858215 sec]
1.6291728019714355
Total Training Time: 86.85592460632324 seconds

Kve:, Spenee cnodd will not arow hirrim. Gratta led ward
fee to in him suuped Chief.
"Gratta doimed 
BEGINNING (1681876638.136302): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6082, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6208, val loss 4.6227 [13.582731246948242 sec]
step 100: train loss 2.4751, val loss 2.5689 [39.0621542930603 sec]
step 200: train loss 2.2491, val loss 2.3749 [64.4200267791748 sec]
step 300: train loss 1.9447, val loss 2.1212 [89.864266872406 sec]
step 400: train loss 1.7683, val loss 1.9760 [118.019846200943 sec]
1.7250224351882935
Total Training Time: 129.84501695632935 seconds

HAPTER IVESPEAMEACE
KAYN THAPTE VI – TON PEERIN
Aften;Y."
"Yes your! Theere took sice, wholdince sou
BEGINNING (1681876771.3236227): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5725, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5835, val loss 4.5682 [8.032092809677124 sec]
step 100: train loss 2.4949, val loss 2.5802 [22.45717740058899 sec]
step 200: train loss 2.3837, val loss 2.4894 [36.882866621017456 sec]
step 300: train loss 2.2148, val loss 2.3473 [51.28205943107605 sec]
step 400: train loss 2.0155, val loss 2.1841 [65.69902300834656 sec]
1.9028570652008057
Total Training Time: 72.0912733078003 seconds

L6ADSE LG
tan this cele."
Peverepled and youen, suw be lied feared
of Nad how theerearest arey this 
BEGINNING (1681876844.5941224): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6484, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6419, val loss 4.6393 [15.297319412231445 sec]
step 100: train loss 2.4874, val loss 2.5672 [42.27716398239136 sec]
step 200: train loss 2.3604, val loss 2.4629 [68.99068307876587 sec]
step 300: train loss 2.0883, val loss 2.2439 [95.76819562911987 sec]
step 400: train loss 1.8614, val loss 2.0701 [122.6496114730835 sec]
1.8135817050933838
Total Training Time: 134.41439771652222 seconds

NNomal allaing at othed
book pactiopipest his mart hiss Countief Arphad.
Beriod hellaption seaing th
BEGINNING (1681876981.2859094): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6617, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6519, val loss 4.6561 [21.83450698852539 sec]
step 100: train loss 2.4891, val loss 2.5703 [56.43800139427185 sec]
step 200: train loss 2.3752, val loss 2.4827 [90.21707272529602 sec]
step 300: train loss 2.1040, val loss 2.2560 [124.00593423843384 sec]
step 400: train loss 1.8530, val loss 2.0627 [157.55430626869202 sec]
1.7637132406234741
Total Training Time: 172.7715141773224 seconds

xmen anood at he feeellong themp in ca. Grattra
turn Beriyalizing the voore feeare, and but thate fr
BEGINNING (1681877157.4622395): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6171, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6178, val loss 4.6095 [4.3902246952056885 sec]
step 100: train loss 2.3834, val loss 2.4885 [12.369004011154175 sec]
step 200: train loss 2.0465, val loss 2.2094 [20.36565351486206 sec]
step 300: train loss 1.8810, val loss 2.0633 [28.37790822982788 sec]
step 400: train loss 1.7725, val loss 1.9848 [36.3604199886322 sec]
1.7572392225265503
Total Training Time: 39.93791317939758 seconds

HE McKAY
Z0ANSEATE McKibe Penas and a mid;. We was onfer the numaros arow the puret could and him, "
BEGINNING (1681877198.5125632): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6129, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6122, val loss 4.6259 [7.994046926498413 sec]
step 100: train loss 2.3713, val loss 2.4684 [22.917105436325073 sec]
step 200: train loss 2.0211, val loss 2.1800 [37.826900243759155 sec]
step 300: train loss 1.8237, val loss 2.0255 [52.739872455596924 sec]
step 400: train loss 1.7111, val loss 1.9369 [67.66978335380554 sec]
1.671425461769104
Total Training Time: 74.577228307724 seconds

stion, for with ho a lets his dornow to like the but and giznered moy with me hid
squarel over prist
BEGINNING (1681877275.2965024): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6737, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6748, val loss 4.6741 [11.644725561141968 sec]
step 100: train loss 2.4060, val loss 2.5023 [33.63119840621948 sec]
step 200: train loss 2.0276, val loss 2.1993 [55.62877869606018 sec]
step 300: train loss 1.8307, val loss 2.0314 [77.58813810348511 sec]
step 400: train loss 1.7000, val loss 1.9540 [99.5274269580841 sec]
1.6654319763183594
Total Training Time: 109.85441017150879 seconds

had everned at mom-led dressed." Gratta to look had
be happriaved. "2
quicke, the cave as grudg- be 
BEGINNING (1681877388.3602743): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5559, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5596, val loss 4.5605 [6.435558319091797 sec]
step 100: train loss 2.4456, val loss 2.5320 [17.96972393989563 sec]
step 200: train loss 2.1716, val loss 2.2989 [29.506417512893677 sec]
step 300: train loss 1.9175, val loss 2.1030 [41.04028296470642 sec]
step 400: train loss 1.7714, val loss 1.9921 [52.56993532180786 sec]
1.739414095878601
Total Training Time: 57.65792918205261 seconds

Uevind in trodiencirs at neesso the had ight plowed to befe
humagat, to ands. Take tuon's well the m
BEGINNING (1681877447.164308): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6368, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6573, val loss 4.6620 [12.07603931427002 sec]
step 100: train loss 2.4447, val loss 2.5424 [33.89176869392395 sec]
step 200: train loss 2.1129, val loss 2.2629 [55.86557626724243 sec]
step 300: train loss 1.8386, val loss 2.0407 [77.67463898658752 sec]
step 400: train loss 1.6816, val loss 1.9269 [99.43454074859619 sec]
1.6273717880249023
Total Training Time: 109.15750503540039 seconds

GAOMB% idest wet wr topent. Thas ma?"
Zidyey and cought was the caull finamear maued
the cupoued mel
BEGINNING (1681877558.5334759): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6134, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6146, val loss 4.6123 [17.65952467918396 sec]
step 100: train loss 2.4442, val loss 2.5508 [48.631863832473755 sec]
step 200: train loss 2.1351, val loss 2.2852 [79.0003399848938 sec]
step 300: train loss 1.8325, val loss 2.0408 [109.29497265815735 sec]
step 400: train loss 1.6579, val loss 1.9260 [140.03516221046448 sec]
1.6482173204421997
Total Training Time: 153.2588677406311 seconds

muswarrs stood armbart the
73
Arphad seappy your uppectoon theon to shoncess, Anayah, hold dow
the a
BEGINNING (1681877715.2135036): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6192, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6261, val loss 4.6370 [10.611866474151611 sec]
step 100: train loss 2.4900, val loss 2.5785 [28.921313047409058 sec]
step 200: train loss 2.3450, val loss 2.4605 [47.26170325279236 sec]
step 300: train loss 2.1122, val loss 2.2589 [65.58467435836792 sec]
step 400: train loss 1.9051, val loss 2.1066 [83.95520687103271 sec]
1.842745304107666
Total Training Time: 91.90398240089417 seconds

verpead yre.
Gratta maed torked was, the noul neare be of knothed of
rehe brovered. Hime of smme tha
BEGINNING (1681877808.2522936): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5828, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5783, val loss 4.5741 [19.9660165309906 sec]
step 100: train loss 2.4733, val loss 2.5577 [51.449968576431274 sec]
step 200: train loss 2.3007, val loss 2.4166 [82.22986960411072 sec]
step 300: train loss 1.9907, val loss 2.1611 [113.61962532997131 sec]
step 400: train loss 1.7643, val loss 1.9875 [144.72413063049316 sec]
1.7404776811599731
Total Training Time: 158.49156975746155 seconds

TARD
SEAN A NOMcKAY
try, sill fier, with that ectinn siffe morls. Ever
the his for claus to thels br
BEGINNING (1681877969.0401013): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5890, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5859, val loss 4.5858 [24.744571208953857 sec]
step 100: train loss 2.4839, val loss 2.5670 [68.36960053443909 sec]
step 200: train loss 2.3394, val loss 2.4483 [112.03569769859314 sec]
step 300: train loss 2.0255, val loss 2.1851 [160.41167426109314 sec]
step 400: train loss 1.7690, val loss 2.0018 [204.84372234344482 sec]
1.6745562553405762
Total Training Time: 225.4543776512146 seconds

nail
other eachtiongter with that me notiouPYes! Fisul marew criede. Fory
muewse, the crould of they
BEGINNING (1681878198.0659225): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6455, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6466, val loss 4.6333 [5.195410966873169 sec]
step 100: train loss 2.3653, val loss 2.4569 [14.545708179473877 sec]
step 200: train loss 2.0204, val loss 2.1905 [23.88444232940674 sec]
step 300: train loss 1.8393, val loss 2.0382 [33.23079562187195 sec]
step 400: train loss 1.7284, val loss 1.9595 [42.593798875808716 sec]
1.7971248626708984
Total Training Time: 46.77204990386963 seconds

sif or the mould as gate treee aster
fach. Onhere loskely 88
CHAPTER V15
SEAN HE TUON TUEF OF TERRIT
BEGINNING (1681878245.981971): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6972, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7104, val loss 4.7199 [9.430469989776611 sec]
step 100: train loss 2.3425, val loss 2.4402 [26.925768613815308 sec]
step 200: train loss 1.9759, val loss 2.1518 [44.41160988807678 sec]
step 300: train loss 1.7874, val loss 2.0094 [61.903852701187134 sec]
step 400: train loss 1.6559, val loss 1.9135 [79.41890025138855 sec]
1.6393918991088867
Total Training Time: 87.48948979377747 seconds

felt. Torial up triver of cither
4
Ar in to at trough browireldn't one thiner smiled. Namal to by
ab
BEGINNING (1681878335.726912): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5198, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5266, val loss 4.5292 [13.745750427246094 sec]
step 100: train loss 2.3627, val loss 2.4612 [39.2508339881897 sec]
step 200: train loss 1.9944, val loss 2.1666 [64.62637996673584 sec]
step 300: train loss 1.7757, val loss 2.0146 [90.18629860877991 sec]
step 400: train loss 1.6356, val loss 1.9146 [115.57760429382324 sec]
1.6275323629379272
Total Training Time: 127.29840612411499 seconds

BE be amounhind wills, compl are
thelso. Anoyah last beful a sompling
shistly. He his
wacks the twe 
BEGINNING (1681878466.3659258): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6046, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6145, val loss 4.6170 [8.08214545249939 sec]
step 100: train loss 2.4345, val loss 2.5332 [22.44189763069153 sec]
step 200: train loss 2.1063, val loss 2.2583 [36.77961349487305 sec]
step 300: train loss 1.8645, val loss 2.0644 [51.13923525810242 sec]
step 400: train loss 1.7177, val loss 1.9520 [65.50081253051758 sec]
1.700380563735962
Total Training Time: 71.78665924072266 seconds

terhom have and not to grot enthinced oners and
ring bber their, and most the out they grouniens all
BEGINNING (1681878539.311946): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6199, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6162, val loss 4.6091 [15.10735011100769 sec]
step 100: train loss 2.4152, val loss 2.5032 [42.113983392715454 sec]
step 200: train loss 2.0536, val loss 2.2222 [68.94044804573059 sec]
step 300: train loss 1.7813, val loss 2.0103 [95.67861676216125 sec]
step 400: train loss 1.6288, val loss 1.8971 [122.42970585823059 sec]
1.5716274976730347
Total Training Time: 134.10809111595154 seconds

wes about for be stood al did antlisut in
it withith the woulds with thishis elese our him. The
goua
BEGINNING (1681878675.694922): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5226, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5280, val loss 4.5253 [22.10450553894043 sec]
step 100: train loss 2.4348, val loss 2.5298 [58.89726424217224 sec]
step 200: train loss 2.1066, val loss 2.2452 [95.72230243682861 sec]
step 300: train loss 1.7923, val loss 2.0019 [132.46034145355225 sec]
step 400: train loss 1.6092, val loss 1.8756 [169.21187925338745 sec]
1.610769271850586
Total Training Time: 184.91792964935303 seconds

2 – – P(RESPUONGUONG THE OF SEATEUOFE THE
CHAPTERD WISTER ICE
APTERDIIITE UIIN9
SEES SEEMY
"ONG BRAN
BEGINNING (1681878864.1012242): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6413, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6436, val loss 4.6499 [13.335317850112915 sec]
step 100: train loss 2.4806, val loss 2.5663 [35.69860029220581 sec]
step 200: train loss 2.3116, val loss 2.4301 [58.063000202178955 sec]
step 300: train loss 2.0607, val loss 2.2192 [80.53836011886597 sec]
step 400: train loss 1.8528, val loss 2.0531 [102.98862195014954 sec]
1.7638908624649048
Total Training Time: 112.51699495315552 seconds

to but with surde. He scowly honare conly ox wit wour
ach have lowns if tacked." How Ot Goopped weri
BEGINNING (1681878977.823799): Baseline LR(0.0002) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6388, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6363, val loss 4.6284 [24.37272024154663 sec]
step 100: train loss 2.4694, val loss 2.5626 [63.37806987762451 sec]
step 200: train loss 2.2844, val loss 2.3993 [103.22386074066162 sec]
step 300: train loss 1.9279, val loss 2.1249 [141.75307250022888 sec]
step 400: train loss 1.7133, val loss 1.9568 [179.6624939441681 sec]
1.6850789785385132
Total Training Time: 197.01392364501953 seconds

Fs. Slo pewert of he felaci-1d?"
Aidden sood, Arphad was speleyarted with
and Namal knew had not thi
BEGINNING (1681879177.2703383): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6115, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6348, val loss 4.6318 [1.0267117023468018 sec]
step 100: train loss 3.2737, val loss 3.3094 [2.6678848266601562 sec]
step 200: train loss 3.0610, val loss 3.0899 [4.284109115600586 sec]
step 300: train loss 2.9190, val loss 2.9532 [5.936199426651001 sec]
step 400: train loss 2.8089, val loss 2.8458 [7.580976486206055 sec]
2.7277188301086426
Total Training Time: 8.196908712387085 seconds

hed uVe wageyhodtsk.id Hant saDAnoLeauge eu wectre ng cis as tipottromouredal be t kead med hiband
D
BEGINNING (1681879185.694709): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6134, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6383, val loss 4.6374 [1.4829645156860352 sec]
step 100: train loss 3.2401, val loss 3.2715 [3.973489999771118 sec]
step 200: train loss 2.9603, val loss 2.9824 [6.4263691902160645 sec]
step 300: train loss 2.8034, val loss 2.8434 [8.869752168655396 sec]
step 400: train loss 2.7009, val loss 2.7555 [11.290214776992798 sec]
2.704418182373047
Total Training Time: 12.287352800369263 seconds

cd."lele"G KE, Y.icTE"Q wane hebend,"O
-3©ankamamainco.
qund pemdDhed Gm,s o f anord.arri hd
an fy f
BEGINNING (1681879198.3541844): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5240, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5312, val loss 4.5315 [1.948117971420288 sec]
step 100: train loss 3.2029, val loss 3.2247 [5.160170555114746 sec]
step 200: train loss 2.9120, val loss 2.9422 [8.422191619873047 sec]
step 300: train loss 2.7561, val loss 2.7934 [11.71001410484314 sec]
step 400: train loss 2.6489, val loss 2.6967 [15.027109861373901 sec]
2.6175637245178223
Total Training Time: 16.42808437347412 seconds

Arad wy m Ias ses ham
te war?llemyend arakdlegke un ongoAve wanparr be, tn to 9ktl t Na.qhaking wk a
BEGINNING (1681879215.2961645): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5873, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6101, val loss 4.6135 [1.0460059642791748 sec]
step 100: train loss 3.2577, val loss 3.2827 [2.69547700881958 sec]
step 200: train loss 3.0424, val loss 3.0826 [4.358130693435669 sec]
step 300: train loss 2.8887, val loss 2.9325 [5.998947620391846 sec]
step 400: train loss 2.7953, val loss 2.8430 [7.6744225025177 sec]
2.722960948944092
Total Training Time: 8.301581859588623 seconds

s,w"
n wionmy hd
 KdF'inete amut thenene m fhil pe asaiomorIql heatanne g Toule r aEouthat,rald t Ar
BEGINNING (1681879223.8254747): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.7068, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6996, val loss 4.6947 [1.4990601539611816 sec]
step 100: train loss 3.2476, val loss 3.2659 [3.9594523906707764 sec]
step 200: train loss 2.9822, val loss 3.0216 [6.64815878868103 sec]
step 300: train loss 2.8139, val loss 2.8565 [9.21275281906128 sec]
step 400: train loss 2.7225, val loss 2.7654 [11.781974077224731 sec]
2.765901565551758
Total Training Time: 12.755874633789062 seconds

as thinm fakemat okid. s Ee ouagob-oy t he wh?
Kaitatrd Gr%1t'tha.Qsole tad onmPd in?
HaraHHeue d on
BEGINNING (1681879236.9426193): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5678, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6010, val loss 4.5975 [1.9176621437072754 sec]
step 100: train loss 3.1624, val loss 3.1835 [5.183255195617676 sec]
step 200: train loss 2.8911, val loss 2.9204 [8.490249872207642 sec]
step 300: train loss 2.7578, val loss 2.8049 [11.800721883773804 sec]
step 400: train loss 2.6815, val loss 2.7402 [15.11943244934082 sec]
2.613840103149414
Total Training Time: 16.465815782546997 seconds

y2" tratthe wes limeyind 1 anst hss, to
tafIas! ath tal, ca55 lllbP atan.ksyopaneio banou riergame f
BEGINNING (1681879253.9507241): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5980, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6140, val loss 4.6144 [1.1942288875579834 sec]
step 100: train loss 3.2436, val loss 3.2707 [3.0766611099243164 sec]
step 200: train loss 3.0158, val loss 3.0551 [4.984296083450317 sec]
step 300: train loss 2.8683, val loss 2.9131 [6.856440544128418 sec]
step 400: train loss 2.7836, val loss 2.8340 [8.73068356513977 sec]
2.694554328918457
Total Training Time: 9.480714559555054 seconds

y teito tue tin yXXr 9 haptettillshe ufuuand s i'Tin siz aytod rhary thend " fiy bs Yt lYdoms sll Ty
BEGINNING (1681879263.7081785): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6362, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6208, val loss 4.6127 [1.6839635372161865 sec]
step 100: train loss 3.2230, val loss 3.2387 [4.504997491836548 sec]
step 200: train loss 2.9357, val loss 2.9677 [7.355786323547363 sec]
step 300: train loss 2.7898, val loss 2.8369 [10.175546646118164 sec]
step 400: train loss 2.7157, val loss 2.7622 [12.997090101242065 sec]
2.6675639152526855
Total Training Time: 14.143393993377686 seconds

whe , t" Alarouram ls hng ltug
od.
Datins d thiwan st thike ad wiI2le wantumiPokalsupeny t wis.e we 
BEGINNING (1681879278.2195742): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5731, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5608, val loss 4.5571 [2.218820571899414 sec]
step 100: train loss 3.1671, val loss 3.1894 [5.976095914840698 sec]
step 200: train loss 2.8946, val loss 2.9293 [9.751358985900879 sec]
step 300: train loss 2.7611, val loss 2.8041 [13.524735689163208 sec]
step 400: train loss 2.6908, val loss 2.7332 [17.415889263153076 sec]
2.6401493549346924
Total Training Time: 19.099628686904907 seconds

ath."
3airind tisuoferabled hisene fmaputattiyarl c I 
P outomeng tersakYois fitirsin t y
Hed t hati
BEGINNING (1681879297.8959165): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6162, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6193, val loss 4.6240 [1.2277390956878662 sec]
step 100: train loss 3.2992, val loss 3.3207 [3.0105788707733154 sec]
step 200: train loss 3.0356, val loss 3.0722 [4.757531642913818 sec]
step 300: train loss 2.8763, val loss 2.9125 [6.53068995475769 sec]
step 400: train loss 2.7612, val loss 2.8060 [8.300071001052856 sec]
2.701616048812866
Total Training Time: 8.94717001914978 seconds

loke2y teimpaom."d. inBbrelde mosas mesrahaor Pthereae cziAYv, ftngit fWoâthineuourmaland ay s casme
BEGINNING (1681879307.0581539): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6424, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6598, val loss 4.6604 [1.6346487998962402 sec]
step 100: train loss 3.2171, val loss 3.2416 [4.218973159790039 sec]
step 200: train loss 2.9377, val loss 2.9690 [6.784554481506348 sec]
step 300: train loss 2.7892, val loss 2.8273 [9.345302820205688 sec]
step 400: train loss 2.6925, val loss 2.7486 [11.924483299255371 sec]
2.608663558959961
Total Training Time: 12.957014083862305 seconds

tieLeLpoothart sets atr T C3oo, a hed fraS ow wice harnTad wthe , f usrobre wenlin.e orre hasrowored
BEGINNING (1681879320.3982527): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5061, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5210, val loss 4.5246 [1.99031400680542 sec]
step 100: train loss 3.1468, val loss 3.1701 [5.35106635093689 sec]
step 200: train loss 2.8702, val loss 2.9002 [8.725649356842041 sec]
step 300: train loss 2.7252, val loss 2.7802 [12.045359134674072 sec]
step 400: train loss 2.6438, val loss 2.6926 [15.401634216308594 sec]
2.5771734714508057
Total Training Time: 16.790394067764282 seconds

We ek shecokoven. t tof t
aure tt than beders. u urend a ran harssg cowetaltoy“trcoun f I I ans t an
BEGINNING (1681879337.6939468): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5707, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5677, val loss 4.5690 [1.2305233478546143 sec]
step 100: train loss 3.2691, val loss 3.2936 [3.1290032863616943 sec]
step 200: train loss 2.9914, val loss 3.0179 [5.024541139602661 sec]
step 300: train loss 2.8371, val loss 2.8841 [6.92587423324585 sec]
step 400: train loss 2.7376, val loss 2.7956 [8.808548212051392 sec]
2.7253530025482178
Total Training Time: 9.495115041732788 seconds

a iE tlns thisaked mom ditAs hhelf wone sesged. im
tedre els. e ero
e JdJan Ee the, s thers esYhslsi
BEGINNING (1681879347.4293284): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5118, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4995, val loss 4.5058 [1.6903407573699951 sec]
step 100: train loss 3.1773, val loss 3.2022 [4.424426317214966 sec]
step 200: train loss 2.9094, val loss 2.9358 [7.14572811126709 sec]
step 300: train loss 2.7647, val loss 2.8101 [9.866110801696777 sec]
step 400: train loss 2.6844, val loss 2.7322 [12.646229028701782 sec]
2.625239372253418
Total Training Time: 13.811407327651978 seconds

wow tunle n:r s ace nd 5co•d
k meda Gr tiare se, a6ery.
f l t."tnnoth
omtaseerk h. sule wye oThee bh
BEGINNING (1681879361.652738): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6167, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6128, val loss 4.6080 [2.285128355026245 sec]
step 100: train loss 3.2008, val loss 3.2206 [5.898916006088257 sec]
step 200: train loss 2.9014, val loss 2.9255 [9.519641637802124 sec]
step 300: train loss 2.7441, val loss 2.7926 [13.119996070861816 sec]
step 400: train loss 2.6643, val loss 2.7150 [16.645081996917725 sec]
2.618651866912842
Total Training Time: 18.15025806427002 seconds

TIbes msag tiverHing mee wirejhe anhe nyadHem ths1e Vaouend Gro ered, raylZra9 usamis A, wint~os thi
BEGINNING (1681879380.3122578): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6792, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6839, val loss 4.6869 [1.4493403434753418 sec]
step 100: train loss 3.2860, val loss 3.3116 [3.7267823219299316 sec]
step 200: train loss 3.0131, val loss 3.0527 [6.007532596588135 sec]
step 300: train loss 2.8577, val loss 2.9040 [8.292500734329224 sec]
step 400: train loss 2.7742, val loss 2.8277 [10.576478958129883 sec]
2.777111053466797
Total Training Time: 11.415814876556396 seconds

GisIe sOhihe wd wIarrireathi avaf. ci n t Ko as Mgrand tol topumiefar thoita m fhe th cunKss hhey, t
BEGINNING (1681879391.9643338): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6680, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6613, val loss 4.6606 [2.0735421180725098 sec]
step 100: train loss 3.1969, val loss 3.2243 [5.433012962341309 sec]
step 200: train loss 2.8983, val loss 2.9367 [8.811965942382812 sec]
step 300: train loss 2.7759, val loss 2.8208 [12.181233644485474 sec]
step 400: train loss 2.7062, val loss 2.7584 [15.559860229492188 sec]
2.6211395263671875
Total Training Time: 16.90903115272522 seconds

athed ydeemas3eree t t peFiwa! hed ow
toallo f l I tererd
"er Mab(e odhing,ulloooras Rr atDradt pid 
BEGINNING (1681879409.2346826): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5970, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5877, val loss 4.5919 [2.6506009101867676 sec]
step 100: train loss 3.2030, val loss 3.2313 [7.148049592971802 sec]
step 200: train loss 2.8965, val loss 2.9320 [11.6285240650177 sec]
step 300: train loss 2.7519, val loss 2.8035 [16.12172293663025 sec]
step 400: train loss 2.6727, val loss 2.7315 [20.81478524208069 sec]
2.6288678646087646
Total Training Time: 22.704099655151367 seconds

Sat, con tust rine triz;
anrand tatogreanetrar at ary stham tompmavatofomgeNoe bemourkbe rled t a, p
BEGINNING (1681879432.4616761): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6631, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6783, val loss 4.6820 [1.3381717205047607 sec]
step 100: train loss 3.2927, val loss 3.3117 [3.3349790573120117 sec]
step 200: train loss 2.9928, val loss 3.0249 [5.337199449539185 sec]
step 300: train loss 2.8313, val loss 2.8730 [7.3528947830200195 sec]
step 400: train loss 2.7351, val loss 2.7944 [9.317396402359009 sec]
2.7218058109283447
Total Training Time: 9.990419387817383 seconds

?– Xhasanghandeng5tay woI ghiGnd os eames rs, s kpedehoncuttheatonesell mo m olpu
brot Hheoura sswif
BEGINNING (1681879442.6851156): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6170, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6147, val loss 4.6089 [1.7554264068603516 sec]
step 100: train loss 3.2369, val loss 3.2634 [4.573013067245483 sec]
step 200: train loss 2.9510, val loss 2.9759 [7.397064208984375 sec]
step 300: train loss 2.7766, val loss 2.8170 [10.142152786254883 sec]
step 400: train loss 2.6828, val loss 2.7279 [12.96310043334961 sec]
2.6445798873901367
Total Training Time: 14.006242036819458 seconds

Tgouwe bes ored we
thileralcadsmeshe theAous heroth wan Thesiso h he de sa tawreelyae tev
Chiuat the
BEGINNING (1681879457.0543659): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6702, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6566, val loss 4.6463 [2.198410749435425 sec]
step 100: train loss 3.1723, val loss 3.2025 [5.777202606201172 sec]
step 200: train loss 2.8775, val loss 2.9069 [9.349388837814331 sec]
step 300: train loss 2.7281, val loss 2.7735 [12.925414323806763 sec]
step 400: train loss 2.6179, val loss 2.6768 [16.44375228881836 sec]
2.5576236248016357
Total Training Time: 17.902854204177856 seconds

re©reaM4 soThecelegu anenils
ase~Zl at s shanand, lttbe and h sias frechaiacim cam b! obus h tarrdos
BEGINNING (1681879475.4618833): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5873, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5913, val loss 4.5818 [1.4229168891906738 sec]
step 100: train loss 3.2315, val loss 3.2540 [3.6169674396514893 sec]
step 200: train loss 2.9962, val loss 3.0295 [5.800340175628662 sec]
step 300: train loss 2.8512, val loss 2.8795 [7.985594272613525 sec]
step 400: train loss 2.7560, val loss 2.8012 [10.185580015182495 sec]
2.7204883098602295
Total Training Time: 10.98323392868042 seconds

be h vewe athe candkoris Ausneo"yatn
d€einS thenegrigs sthak t s, WarBiteng Gaviglon at hicatho
t Ta
BEGINNING (1681879486.6680481): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6627, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6537, val loss 4.6473 [1.935286283493042 sec]
step 100: train loss 3.2348, val loss 3.2570 [5.019318580627441 sec]
step 200: train loss 2.9496, val loss 2.9787 [8.237023115158081 sec]
step 300: train loss 2.7893, val loss 2.8276 [11.384602546691895 sec]
step 400: train loss 2.7026, val loss 2.7534 [14.504031658172607 sec]
2.67647385597229
Total Training Time: 15.691013813018799 seconds

wUiHibt llecigRh wathe E he
mero acH whethey Ichenl T.
chirefrra t taE
"d Iu3r t• aranesphel(7 "
Cn 
BEGINNING (1681879502.7307916): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6130, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6176, val loss 4.6123 [2.419217109680176 sec]
step 100: train loss 3.1891, val loss 3.2140 [6.921284198760986 sec]
step 200: train loss 2.8794, val loss 2.9150 [11.446834802627563 sec]
step 300: train loss 2.7433, val loss 2.7879 [15.613370656967163 sec]
step 400: train loss 2.6672, val loss 2.7271 [20.304771184921265 sec]
2.590315103530884
Total Training Time: 22.130014657974243 seconds

catty turmegiy. s aneOtast bappo- at.
st
thed'ithAnd t es ioe woons. o
rempirpr
a th s hed, Prithera
BEGINNING (1681879525.3945532): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6191, val loss 4.6193 [1.7778959274291992 sec]
step 100: train loss 3.2570, val loss 3.2793 [4.5447609424591064 sec]
step 200: train loss 2.9871, val loss 3.0178 [7.3445823192596436 sec]
step 300: train loss 2.8409, val loss 2.8760 [10.134726524353027 sec]
step 400: train loss 2.7541, val loss 2.8043 [12.893030643463135 sec]
2.710475206375122
Total Training Time: 13.897372007369995 seconds

He pt ar an ciranged niofanuld e J ithef nore arentan core If ss6 se t atled e rkochea wple cald, he
BEGINNING (1681879539.513917): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5648, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5742, val loss 4.5656 [2.527590274810791 sec]
step 100: train loss 3.2153, val loss 3.2355 [6.613192319869995 sec]
step 200: train loss 2.9369, val loss 2.9752 [10.709057569503784 sec]
step 300: train loss 2.7951, val loss 2.8353 [14.834667205810547 sec]
step 400: train loss 2.7033, val loss 2.7542 [20.06496000289917 sec]
2.6983401775360107
Total Training Time: 21.712385654449463 seconds

Arof ale hser
thans Ct•
Thedicam" 
kvaldd cos a owoulranesor, t sE
Grn ql t oiil bta
Thed, sthiah
ut
BEGINNING (1681879561.63829): Baseline LR(0.00016) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6646, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6584, val loss 4.6606 [3.4899117946624756 sec]
step 100: train loss 3.2187, val loss 3.2479 [9.186283826828003 sec]
step 200: train loss 2.9095, val loss 2.9444 [14.608123540878296 sec]
step 300: train loss 2.7699, val loss 2.8124 [19.981022357940674 sec]
step 400: train loss 2.6894, val loss 2.7435 [25.370189666748047 sec]
2.6254327297210693
Total Training Time: 27.518723726272583 seconds

qcof oCle aI
ed Pes
rudild t" Ts g’okeow.© hed Thenro beend hiucernen Pe cee. be t panere gha pletes
BEGINNING (1681879589.7083604): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6102, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6213, val loss 4.6176 [1.2665398120880127 sec]
step 100: train loss 2.9745, val loss 3.0006 [3.275076389312744 sec]
step 200: train loss 2.7269, val loss 2.7756 [5.3035805225372314 sec]
step 300: train loss 2.6129, val loss 2.6694 [7.311920166015625 sec]
step 400: train loss 2.5343, val loss 2.5982 [9.29781174659729 sec]
2.508472442626953
Total Training Time: 10.099326610565186 seconds

pASerithale s rl wedie ad othind sY u, Pathal has ayhen,
se waibelle wimay shinedd nacimed ng dabuma
BEGINNING (1681879600.1018417): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6476, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6151, val loss 4.6081 [1.9269607067108154 sec]
step 100: train loss 2.9041, val loss 2.9437 [5.137584924697876 sec]
step 200: train loss 2.6598, val loss 2.7154 [8.327515125274658 sec]
step 300: train loss 2.5426, val loss 2.6122 [11.621276378631592 sec]
step 400: train loss 2.4598, val loss 2.5261 [14.815600395202637 sec]
2.411956787109375
Total Training Time: 16.171478748321533 seconds

"AH©A– mW.Ym, TA cand ThubtothedeWow ar cre wollend re beld andeans, dewinci remspid me Ohe phe tak 
BEGINNING (1681879616.783857): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6522, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6027, val loss 4.6087 [2.5751914978027344 sec]
step 100: train loss 2.8386, val loss 2.8716 [7.061832904815674 sec]
step 200: train loss 2.6192, val loss 2.6828 [11.605581045150757 sec]
step 300: train loss 2.5040, val loss 2.5764 [16.113864421844482 sec]
step 400: train loss 2.4147, val loss 2.4923 [20.599355697631836 sec]
2.3570384979248047
Total Training Time: 22.53159761428833 seconds

rcep ruaaits, ouriaTh apnddeallaw thelld heven tr ore toked cuW lurerend ond. in, atttey. Tht e laps
BEGINNING (1681879640.0672388): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5946, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6132, val loss 4.6119 [1.3317127227783203 sec]
step 100: train loss 2.9905, val loss 3.0221 [3.412504196166992 sec]
step 200: train loss 2.7375, val loss 2.8030 [5.507161378860474 sec]
step 300: train loss 2.6406, val loss 2.7104 [7.555495262145996 sec]
step 400: train loss 2.5831, val loss 2.6549 [9.632494688034058 sec]
2.5899226665496826
Total Training Time: 10.43268632888794 seconds

GAmezC– hoththio€ Dug B chins 4 Tay marisus an thmis is tha cllll orofp s t mrscaekOaledatcrepicered
BEGINNING (1681879650.7996466): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6120, val loss 4.6171 [1.942016363143921 sec]
step 100: train loss 2.8634, val loss 2.9044 [5.197735071182251 sec]
step 200: train loss 2.6728, val loss 2.7263 [8.44727897644043 sec]
step 300: train loss 2.5723, val loss 2.6484 [11.736202239990234 sec]
step 400: train loss 2.5138, val loss 2.5890 [15.00664734840393 sec]
2.467294692993164
Total Training Time: 16.370884895324707 seconds

ClarR, lourd iy. Cugang carul hes " coms, ghre marighe ak othev
reachatheskeeared ogo!" arane at r, 
BEGINNING (1681879667.692559): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5932, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5897, val loss 4.5900 [2.5287985801696777 sec]
step 100: train loss 2.8146, val loss 2.8492 [7.0264036655426025 sec]
step 200: train loss 2.6380, val loss 2.6872 [11.447671175003052 sec]
step 300: train loss 2.5405, val loss 2.6083 [15.931440830230713 sec]
step 400: train loss 2.4780, val loss 2.5484 [20.35547971725464 sec]
2.4416589736938477
Total Training Time: 22.314464330673218 seconds

hilea ledins is eant me olepthek. GraI8 Gasshe aca:atosthe hins shef as. hie 9.
Gotha's Fanthanto bi
BEGINNING (1681879690.8056033): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5955, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6002, val loss 4.6054 [1.5277063846588135 sec]
step 100: train loss 2.9353, val loss 2.9627 [4.070122480392456 sec]
step 200: train loss 2.7248, val loss 2.7746 [6.564031600952148 sec]
step 300: train loss 2.6398, val loss 2.7064 [9.074048280715942 sec]
step 400: train loss 2.5867, val loss 2.6609 [11.566358804702759 sec]
2.56205415725708
Total Training Time: 12.5697181224823 seconds

AAraras le w cun abogheonraillathe.
"Horang hearand dgomigyre g che p
f
av, is we."sthesc f an tTas,
BEGINNING (1681879703.6963594): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6200, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6058, val loss 4.5960 [2.3668055534362793 sec]
step 100: train loss 2.8682, val loss 2.8913 [6.415498733520508 sec]
step 200: train loss 2.6692, val loss 2.7277 [10.471728801727295 sec]
step 300: train loss 2.5949, val loss 2.6579 [14.513017892837524 sec]
step 400: train loss 2.5422, val loss 2.6120 [18.584484577178955 sec]
2.544255256652832
Total Training Time: 20.277427911758423 seconds

he on hen O rtesofme camignld ls se blllst e,
ovtriberas mpan tugacersemanou ara a atins, atthiyoun,
BEGINNING (1681879724.50531): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.7026, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7022, val loss 4.7061 [3.2247982025146484 sec]
step 100: train loss 2.8297, val loss 2.8647 [8.823531150817871 sec]
step 200: train loss 2.6488, val loss 2.7033 [14.397811889648438 sec]
step 300: train loss 2.5670, val loss 2.6331 [19.994134664535522 sec]
step 400: train loss 2.5171, val loss 2.5893 [25.602777242660522 sec]
2.4734086990356445
Total Training Time: 27.95074701309204 seconds

Gad sont
atthaiy ghoming oya r n wadiraeastyis we plchat houd ha ung hambs ch Axsmess ilastu t s
uts
BEGINNING (1681879753.2115657): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5792, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5948, val loss 4.5901 [1.4342386722564697 sec]
step 100: train loss 2.9614, val loss 2.9778 [3.7128820419311523 sec]
step 200: train loss 2.7089, val loss 2.7550 [5.984659671783447 sec]
step 300: train loss 2.5993, val loss 2.6655 [8.283599138259888 sec]
step 400: train loss 2.4948, val loss 2.5641 [10.52443528175354 sec]
2.419098377227783
Total Training Time: 11.354478120803833 seconds

hatt ands.
"TEom oothe Z
I ke Gromill, agoul st, lis he woonmeld beis sesurveir aty s foorlnel a,
we
BEGINNING (1681879764.8672626): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6689, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6596, val loss 4.6662 [2.0365848541259766 sec]
step 100: train loss 2.8705, val loss 2.9067 [5.473927021026611 sec]
step 200: train loss 2.6347, val loss 2.6981 [9.02907419204712 sec]
step 300: train loss 2.5165, val loss 2.5885 [12.459538698196411 sec]
step 400: train loss 2.4048, val loss 2.4891 [15.889145612716675 sec]
2.411067008972168
Total Training Time: 17.30459213256836 seconds

facsent hay as avel yone lobs uont
fo
cuntrkeofg wis, aret adi fof had mard alyor be Thas sp ard
iab
BEGINNING (1681879782.6848483): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5035, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5080, val loss 4.5158 [2.717536211013794 sec]
step 100: train loss 2.8137, val loss 2.8643 [7.376467704772949 sec]
step 200: train loss 2.6017, val loss 2.6605 [12.048710107803345 sec]
step 300: train loss 2.4786, val loss 2.5493 [16.69901752471924 sec]
step 400: train loss 2.3665, val loss 2.4554 [21.37504291534424 sec]
2.288759708404541
Total Training Time: 23.337615966796875 seconds

 –jN
SO"AVIN R THESATHE
SEAY MAYMN
ATHBTEY
Gro 68nf  LEin Tidoukng.
G3averouug~ s,
off be no, t thas
BEGINNING (1681879806.7635424): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6282, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6374, val loss 4.6327 [1.4855563640594482 sec]
step 100: train loss 2.9330, val loss 2.9657 [3.8720030784606934 sec]
step 200: train loss 2.7178, val loss 2.7587 [6.263564586639404 sec]
step 300: train loss 2.6198, val loss 2.6768 [8.651077508926392 sec]
step 400: train loss 2.5485, val loss 2.6195 [11.015018224716187 sec]
2.545353889465332
Total Training Time: 11.918046236038208 seconds

S˜cotheang wee Coke an omaptt ff, hie ofrp heng arin't
lve neoull !anicong corn s ande wide beb werw
BEGINNING (1681879818.9775488): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6419, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6372, val loss 4.6416 [2.317365884780884 sec]
step 100: train loss 2.8544, val loss 2.8888 [6.098365306854248 sec]
step 200: train loss 2.6594, val loss 2.7190 [9.768194437026978 sec]
step 300: train loss 2.5564, val loss 2.6265 [13.451262950897217 sec]
step 400: train loss 2.4850, val loss 2.5589 [17.260003328323364 sec]
2.5045926570892334
Total Training Time: 18.746481895446777 seconds

Aistat, rad ragh
’ER fin denilyeamerind now bhin mBe s as ptanl f
ousad
shee meinarcef 
ted he lue'n
BEGINNING (1681879838.2630372): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6325, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6290, val loss 4.6375 [2.9171268939971924 sec]
step 100: train loss 2.8087, val loss 2.8503 [7.912390470504761 sec]
step 200: train loss 2.6162, val loss 2.6800 [12.91412091255188 sec]
step 300: train loss 2.5286, val loss 2.5927 [17.907843828201294 sec]
step 400: train loss 2.4498, val loss 2.5378 [22.911757707595825 sec]
2.400736093521118
Total Training Time: 24.974483966827393 seconds

ipalt nen at that ooSigeek
ghe6 ahe phingrnd grof ftthirt min urthind al tteroris aps." –., b s
ahe 
BEGINNING (1681879864.0360434): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5796, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5829, val loss 4.5782 [1.9301891326904297 sec]
step 100: train loss 2.9344, val loss 2.9708 [5.09791111946106 sec]
step 200: train loss 2.7153, val loss 2.7712 [8.28029465675354 sec]
step 300: train loss 2.6157, val loss 2.6822 [11.457054376602173 sec]
step 400: train loss 2.5711, val loss 2.6327 [14.623785495758057 sec]
2.55723237991333
Total Training Time: 15.865033388137817 seconds

ramcway islouid be mo wof t yr. omicf
athars Ph% want y lyed.
bf wiothHp an'zenges tellies tifin. t 
BEGINNING (1681879880.1960764): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6066, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6102, val loss 4.6043 [3.01957631111145 sec]
step 100: train loss 2.8544, val loss 2.8960 [8.139533996582031 sec]
step 200: train loss 2.6606, val loss 2.7079 [13.268985509872437 sec]
step 300: train loss 2.5702, val loss 2.6353 [18.39341425895691 sec]
step 400: train loss 2.5298, val loss 2.5962 [23.52182126045227 sec]
2.5474770069122314
Total Training Time: 25.617855310440063 seconds

MEze lyof woke
hethantuor hane en t be w. d and hakepakeis aaftobronthire fscor to warouavenand
nbki
BEGINNING (1681879906.3500469): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6283, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6142, val loss 4.6143 [4.11893105506897 sec]
step 100: train loss 2.8105, val loss 2.8519 [11.145343542098999 sec]
step 200: train loss 2.6353, val loss 2.6933 [18.169809341430664 sec]
step 300: train loss 2.5515, val loss 2.6229 [25.213695764541626 sec]
step 400: train loss 2.5079, val loss 2.5824 [32.23652243614197 sec]
2.5042436122894287
Total Training Time: 35.14616417884827 seconds

owQass ashssawelibe n5
r s cowima Nt fofus Grchah sis athigad leYom dieangd Pen ke cavetthivediwand 
BEGINNING (1681879942.2872093): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5741, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5673, val loss 4.5808 [1.6181318759918213 sec]
step 100: train loss 2.9204, val loss 2.9603 [4.070744037628174 sec]
step 200: train loss 2.6979, val loss 2.7454 [6.596272230148315 sec]
step 300: train loss 2.5772, val loss 2.6320 [9.050813674926758 sec]
step 400: train loss 2.4622, val loss 2.5296 [11.524683237075806 sec]
2.4074690341949463
Total Training Time: 12.44748568534851 seconds

wstoo rint t el ta walye TR ont as thilirasE mon Yane – we wat.
vowh asmyreref fab'ttto. Goo "hinq
s
BEGINNING (1681879955.0266957): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5901, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5731, val loss 4.5707 [2.2497353553771973 sec]
step 100: train loss 2.8667, val loss 2.8970 [5.953212738037109 sec]
step 200: train loss 2.6331, val loss 2.6939 [9.629400253295898 sec]
step 300: train loss 2.5026, val loss 2.5784 [13.243084192276001 sec]
step 400: train loss 2.3907, val loss 2.4729 [16.96745538711548 sec]
2.355807065963745
Total Training Time: 18.38962173461914 seconds

-abam." I Af
Mthi3 TI
Are4 CEmoftrns Gras.1 ’wellastorgaker. Sintly El youghier ito th hed thagap to
BEGINNING (1681879973.942586): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6404, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6368, val loss 4.6414 [2.8754239082336426 sec]
step 100: train loss 2.8061, val loss 2.8406 [7.774900436401367 sec]
step 200: train loss 2.5933, val loss 2.6507 [12.59743595123291 sec]
step 300: train loss 2.4559, val loss 2.5249 [17.503017902374268 sec]
step 400: train loss 2.3423, val loss 2.4264 [22.420267343521118 sec]
2.310417413711548
Total Training Time: 24.396756649017334 seconds

an to dang then, wirtheersed.'s ar yah welom bne yard thoo sned."
"Roiste – mattrinathe an the
hy le
BEGINNING (1681879999.1099918): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6126, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6095, val loss 4.6001 [1.7616496086120605 sec]
step 100: train loss 2.9223, val loss 2.9659 [4.553310394287109 sec]
step 200: train loss 2.7064, val loss 2.7570 [7.326423406600952 sec]
step 300: train loss 2.6134, val loss 2.6757 [10.128889322280884 sec]
step 400: train loss 2.5400, val loss 2.6080 [12.93047046661377 sec]
2.5188279151916504
Total Training Time: 13.973647594451904 seconds

Aert. 1an?"Yt.
WCâ?Taneyf?ph
The on
h
Zof b;?"
"AN
akasd po s 9anlYed thi'bind 6 ne aiens, fayaeres 
BEGINNING (1681880013.387346): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6187, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6219, val loss 4.6098 [2.597795009613037 sec]
step 100: train loss 2.8457, val loss 2.8844 [6.851759910583496 sec]
step 200: train loss 2.6427, val loss 2.6971 [11.18215036392212 sec]
step 300: train loss 2.5384, val loss 2.6065 [15.485864400863647 sec]
step 400: train loss 2.4630, val loss 2.5455 [19.74580407142639 sec]
2.467954397201538
Total Training Time: 21.451761722564697 seconds

Gulwsteour atied, teds bed tharn 8ont ctaly d, dorilst te
coucie rsan salewife blll, win rt
Che sthi
BEGINNING (1681880035.3661978): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6181, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6125, val loss 4.6082 [3.379725933074951 sec]
step 100: train loss 2.7955, val loss 2.8393 [9.097804307937622 sec]
step 200: train loss 2.6107, val loss 2.6682 [14.810431957244873 sec]
step 300: train loss 2.5133, val loss 2.5816 [20.53593111038208 sec]
step 400: train loss 2.4255, val loss 2.5079 [26.239744663238525 sec]
2.3715667724609375
Total Training Time: 28.573586225509644 seconds

kraveon pomeongit thiv rur cat ak."
Lon"Chan he he hal "
a wasuambed st."˜ st Che epea˜
sl
foc
blle 
BEGINNING (1681880064.7163324): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6236, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6283, val loss 4.6332 [2.337013006210327 sec]
step 100: train loss 2.9577, val loss 2.9904 [6.121448993682861 sec]
step 200: train loss 2.7234, val loss 2.7650 [9.90145492553711 sec]
step 300: train loss 2.6242, val loss 2.6847 [13.665069818496704 sec]
step 400: train loss 2.5697, val loss 2.6367 [17.475056409835815 sec]
2.5415406227111816
Total Training Time: 18.926517248153687 seconds

A"Mh HANAv€e AP
OTy casobe
A“sth.k as, pirakroully therns ut.
gt the C"Yederis."ttalls munce b munal
BEGINNING (1681880083.9445243): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6061, val loss 4.6132 [3.6338417530059814 sec]
step 100: train loss 2.8328, val loss 2.8586 [9.74207091331482 sec]
step 200: train loss 2.6511, val loss 2.7074 [15.796775102615356 sec]
step 300: train loss 2.5704, val loss 2.6281 [21.83479332923889 sec]
step 400: train loss 2.5217, val loss 2.5896 [27.890480756759644 sec]
2.546368360519409
Total Training Time: 30.328860998153687 seconds

bo nee dorforald crame%. wer eas Thead. lte Va ave Har. s Chathed Thed uum asuligasrd hefol ofowango
BEGINNING (1681880114.8281057): Baseline LR(0.00016) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6165, val loss 4.6185 [4.899430751800537 sec]
step 100: train loss 2.7892, val loss 2.8231 [13.208537817001343 sec]
step 200: train loss 2.6148, val loss 2.6767 [21.51518940925598 sec]
step 300: train loss 2.5410, val loss 2.6104 [29.822089910507202 sec]
step 400: train loss 2.4846, val loss 2.5566 [38.133710622787476 sec]
2.4517436027526855
Total Training Time: 41.566370725631714 seconds

wristachsmean
we brd
bomeed taroume d !"Wask yonyae. ou I meato a thedded at eanthize whe tgherourid
BEGINNING (1681880157.1639454): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6259, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6123, val loss 4.6202 [1.5232250690460205 sec]
step 100: train loss 2.8026, val loss 2.8425 [4.030577182769775 sec]
step 200: train loss 2.6008, val loss 2.6573 [6.482754468917847 sec]
step 300: train loss 2.4823, val loss 2.5504 [8.955313205718994 sec]
step 400: train loss 2.3647, val loss 2.4663 [11.41722846031189 sec]
2.4234225749969482
Total Training Time: 12.413810729980469 seconds

alerinwe tuothaltrta ing k margX hans tha. doof woured inghem mave. yoay ann. whe
Graowk, "Zlatadod 
BEGINNING (1681880169.9595242): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6639, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6806, val loss 4.6765 [2.3471293449401855 sec]
step 100: train loss 2.7332, val loss 2.7815 [6.643122911453247 sec]
step 200: train loss 2.5415, val loss 2.6058 [10.751332521438599 sec]
step 300: train loss 2.4139, val loss 2.4993 [14.825808048248291 sec]
step 400: train loss 2.3089, val loss 2.3984 [18.896469116210938 sec]
2.302612066268921
Total Training Time: 20.639115810394287 seconds

EGSNNy ailed onok aus. Gr 1ullowe an Gr mad ou ther Rah fes sen thewahd ne to molliod, Ait hon!
HeAr
BEGINNING (1681880191.2942562): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5529, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5561, val loss 4.5571 [3.261223077774048 sec]
step 100: train loss 2.6825, val loss 2.7343 [9.01829218864441 sec]
step 200: train loss 2.5084, val loss 2.5785 [14.71706223487854 sec]
step 300: train loss 2.3894, val loss 2.4708 [20.441278219223022 sec]
step 400: train loss 2.2639, val loss 2.3608 [26.202378273010254 sec]
2.239802122116089
Total Training Time: 28.814950704574585 seconds

nent." quo eshiddso;d as
wng fores sicox ire sring astil ipon paws gomer as. Hiy a)d and
ta wiwhey c
BEGINNING (1681880221.1439598): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5531, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5921, val loss 4.5888 [1.5502593517303467 sec]
step 100: train loss 2.7789, val loss 2.8164 [4.129889965057373 sec]
step 200: train loss 2.6243, val loss 2.6832 [6.63350510597229 sec]
step 300: train loss 2.5371, val loss 2.6038 [9.191134214401245 sec]
step 400: train loss 2.4691, val loss 2.5541 [11.709406614303589 sec]
2.4511361122131348
Total Training Time: 12.716614723205566 seconds

his, the iyoon, toug le bormoredd att, waBe irama t cKTha Fanomwh lllt walis urth s taknolthe I n br
BEGINNING (1681880234.2382593): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6160, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6296, val loss 4.6220 [2.401491165161133 sec]
step 100: train loss 2.7327, val loss 2.7760 [6.558181047439575 sec]
step 200: train loss 2.5738, val loss 2.6332 [10.71145224571228 sec]
step 300: train loss 2.4857, val loss 2.5599 [14.864774703979492 sec]
step 400: train loss 2.4137, val loss 2.4965 [19.02730107307434 sec]
2.4299697875976562
Total Training Time: 20.80319094657898 seconds

Nislop,Ser he hack hor and. hewin GrryGred on tofokedf ande ttof ssa c– he tbas
wataandghesamanaf
tc
BEGINNING (1681880255.7034693): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5679, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5737, val loss 4.5587 [3.3003079891204834 sec]
step 100: train loss 2.6769, val loss 2.7288 [9.14349627494812 sec]
step 200: train loss 2.5303, val loss 2.6026 [14.962517023086548 sec]
step 300: train loss 2.4464, val loss 2.5297 [20.795475721359253 sec]
step 400: train loss 2.3574, val loss 2.4412 [26.647777318954468 sec]
2.3557116985321045
Total Training Time: 29.256285667419434 seconds

berueced tras, ven eredesehe att ant sitoup, hendd
be 1
Ithed. Aianpha mic tagas rro ougit,. Weyodec
BEGINNING (1681880286.011899): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5764, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5787, val loss 4.5773 [1.923518419265747 sec]
step 100: train loss 2.7585, val loss 2.8006 [5.18555212020874 sec]
step 200: train loss 2.6157, val loss 2.6820 [8.442856550216675 sec]
step 300: train loss 2.5514, val loss 2.6252 [11.693158388137817 sec]
step 400: train loss 2.5157, val loss 2.5838 [14.952072143554688 sec]
2.5531272888183594
Total Training Time: 16.28606677055359 seconds

pratid lnche ttobantht, vel indo hof torf s. Nasethas thiary u. po boof 0e sthugo-
Gonchinllf 9 he W
BEGINNING (1681880302.6920302): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6642, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6754, val loss 4.6654 [3.1800951957702637 sec]
step 100: train loss 2.7166, val loss 2.7625 [8.69011926651001 sec]
step 200: train loss 2.5801, val loss 2.6527 [14.187800645828247 sec]
step 300: train loss 2.5152, val loss 2.5883 [19.68801975250244 sec]
step 400: train loss 2.4717, val loss 2.5408 [25.183948516845703 sec]
2.4467594623565674
Total Training Time: 27.52982997894287 seconds

h nupe d onfoid amid, uric3
ckew.
chelat "S ane n,"W Tee ches. rowne reard Thie Yonce mincV bo ad, m
BEGINNING (1681880330.923739): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5702, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5682, val loss 4.5663 [4.411627531051636 sec]
step 100: train loss 2.6801, val loss 2.7360 [12.112182378768921 sec]
step 200: train loss 2.5484, val loss 2.6180 [19.827958345413208 sec]
step 300: train loss 2.4902, val loss 2.5624 [27.586300134658813 sec]
step 400: train loss 2.4417, val loss 2.5271 [35.28130531311035 sec]
2.424215078353882
Total Training Time: 38.57543396949768 seconds

Vavtthed the. Then  uamaved me sse thiloughen ary
rinacute moty. wid " "An Thais ow ridet, at y dost
BEGINNING (1681880370.5381706): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6428, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6254, val loss 4.6219 [1.6756935119628906 sec]
step 100: train loss 2.7716, val loss 2.8160 [4.340091228485107 sec]
step 200: train loss 2.5921, val loss 2.6547 [7.007611989974976 sec]
step 300: train loss 2.4642, val loss 2.5466 [9.649369716644287 sec]
step 400: train loss 2.3240, val loss 2.4224 [12.316378116607666 sec]
2.2456178665161133
Total Training Time: 13.340928792953491 seconds

Nirl Nat gh
tGok and res ilith
hatata lE I wot –
"Mhomnaw wat
"Hin y thache hapefureathe i;g sosk os
BEGINNING (1681880384.2516217): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6889, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6873, val loss 4.6938 [2.5259995460510254 sec]
step 100: train loss 2.7116, val loss 2.7499 [6.794398784637451 sec]
step 200: train loss 2.5213, val loss 2.5973 [11.050015687942505 sec]
step 300: train loss 2.3744, val loss 2.4609 [15.32496190071106 sec]
step 400: train loss 2.2545, val loss 2.3543 [19.59554624557495 sec]
2.194777250289917
Total Training Time: 21.351127862930298 seconds

Gran amathe thror prta wis Grxtawey Andisccant, ro dir Gratlinod instta um."
Anand win ge lbaght ani
BEGINNING (1681880406.2828116): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6108, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6146, val loss 4.6196 [3.4610607624053955 sec]
step 100: train loss 2.6625, val loss 2.7191 [9.374556541442871 sec]
step 200: train loss 2.4755, val loss 2.5516 [15.416361570358276 sec]
step 300: train loss 2.3427, val loss 2.4311 [21.31508779525757 sec]
step 400: train loss 2.2114, val loss 2.3291 [27.212778329849243 sec]
2.1383941173553467
Total Training Time: 29.727723360061646 seconds

lemas and freo thepknop at murbed laint ta
led yan, lleagon go. "I and owe, h all buor pokil ma smow
BEGINNING (1681880437.055212): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6547, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6403, val loss 4.6432 [1.786790370941162 sec]
step 100: train loss 2.7754, val loss 2.8093 [4.700706243515015 sec]
step 200: train loss 2.5997, val loss 2.6503 [7.61641526222229 sec]
step 300: train loss 2.5102, val loss 2.5722 [10.5238516330719 sec]
step 400: train loss 2.4335, val loss 2.5129 [13.475078344345093 sec]
2.3946750164031982
Total Training Time: 14.598102807998657 seconds

hal!"B
"Weand ore atwielerake mul.
"Apat pesimaed stheratthe
Vat ss atckefo tat Che fthallay theleya
BEGINNING (1681880452.0334218): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5532, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5436, val loss 4.5501 [2.814910650253296 sec]
step 100: train loss 2.6939, val loss 2.7564 [7.544553518295288 sec]
step 200: train loss 2.5434, val loss 2.6150 [12.304391860961914 sec]
step 300: train loss 2.4465, val loss 2.5304 [17.038084506988525 sec]
step 400: train loss 2.3462, val loss 2.4451 [21.791682720184326 sec]
2.2401962280273438
Total Training Time: 23.750946044921875 seconds

0Grathit."
"I yof pothils alyen ted. "Tre to trede
merie erake tsios sinate.
"Leng waro I tahin odll
BEGINNING (1681880476.4749348): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5149, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5261, val loss 4.5390 [3.802215099334717 sec]
step 100: train loss 2.6653, val loss 2.7231 [10.478976726531982 sec]
step 200: train loss 2.5132, val loss 2.5921 [17.091692686080933 sec]
step 300: train loss 2.4139, val loss 2.5010 [25.027642250061035 sec]
step 400: train loss 2.3196, val loss 2.4139 [31.68964648246765 sec]
2.266047716140747
Total Training Time: 34.53061032295227 seconds

apokextered the opon spad.
"I sanims enf pulledd." Nadan fruowo ses thirna's for the bllornckif on'l
BEGINNING (1681880512.0284784): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6573, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6470, val loss 4.6468 [2.4822022914886475 sec]
step 100: train loss 2.7831, val loss 2.8235 [6.617040157318115 sec]
step 200: train loss 2.6214, val loss 2.6897 [10.74678087234497 sec]
step 300: train loss 2.5435, val loss 2.6165 [14.885501384735107 sec]
step 400: train loss 2.5074, val loss 2.5793 [19.02809190750122 sec]
2.4778125286102295
Total Training Time: 20.682958126068115 seconds

8ug ahrd sm tuty watillll mfa as be."8
D The arangedisalp, Griompaane Thed br aqounaloug it.
caid ck
BEGINNING (1681880533.121941): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6463, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6373, val loss 4.6371 [4.083416700363159 sec]
step 100: train loss 2.7069, val loss 2.7573 [11.021079778671265 sec]
step 200: train loss 2.5636, val loss 2.6249 [18.0577974319458 sec]
step 300: train loss 2.5012, val loss 2.5724 [25.197559595108032 sec]
step 400: train loss 2.4581, val loss 2.5408 [32.2376606464386 sec]
2.4402310848236084
Total Training Time: 35.12320280075073 seconds

ca's moze faddgsthar't smeminst ll dom.
Afat ut, ous, fioupeahed ppe uhee s he winguyou as."
sp uwed
BEGINNING (1681880569.0201316): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6211, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6212, val loss 4.6218 [5.592683553695679 sec]
step 100: train loss 2.6779, val loss 2.7346 [15.313650608062744 sec]
step 200: train loss 2.5444, val loss 2.6145 [25.050214052200317 sec]
step 300: train loss 2.4819, val loss 2.5575 [34.81654667854309 sec]
step 400: train loss 2.4238, val loss 2.5135 [44.562034606933594 sec]
2.3674376010894775
Total Training Time: 48.76631832122803 seconds

tits aryouaed sas t unak, hile Grerkerrrat oditite t h thengean
tKop oved berem.
Gra apr s athan der
BEGINNING (1681880618.936507): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5761, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5927, val loss 4.5919 [1.9023125171661377 sec]
step 100: train loss 2.7521, val loss 2.8050 [4.863736629486084 sec]
step 200: train loss 2.5690, val loss 2.6383 [7.879706621170044 sec]
step 300: train loss 2.4109, val loss 2.5021 [10.965620040893555 sec]
step 400: train loss 2.2889, val loss 2.3945 [14.008354187011719 sec]
2.2298378944396973
Total Training Time: 15.133195161819458 seconds

wours, we. In wat a ta find
chatce fut himit an tse whe ayorhat to "I bKAsean's dins.
Gra
Tno ouinge
BEGINNING (1681880634.4386888): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6472, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6092, val loss 4.6144 [2.7804622650146484 sec]
step 100: train loss 2.6912, val loss 2.7444 [7.4799723625183105 sec]
step 200: train loss 2.4941, val loss 2.5701 [12.168556451797485 sec]
step 300: train loss 2.3502, val loss 2.4374 [16.82700490951538 sec]
step 400: train loss 2.2182, val loss 2.3253 [21.60965323448181 sec]
2.180610179901123
Total Training Time: 23.50963521003723 seconds

thi's fenr ay an mreld cke this hN
geimen hean
sil"s ly I"TouicK, ad CHL. Yes This p cakerour the bu
BEGINNING (1681880658.7288811): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6658, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6264, val loss 4.6266 [3.7354400157928467 sec]
step 100: train loss 2.6504, val loss 2.7003 [10.083157539367676 sec]
step 200: train loss 2.4556, val loss 2.5282 [16.385316610336304 sec]
step 300: train loss 2.2897, val loss 2.3919 [22.859517812728882 sec]
step 400: train loss 2.1615, val loss 2.2838 [29.543782234191895 sec]
2.161412239074707
Total Training Time: 32.23821449279785 seconds

turso tos of furtaint."?"
"Ees as fer
GracZenge bereiorgagen
tuont on dess allis, thel!"
Cof Pillong
BEGINNING (1681880691.9325376): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6513, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6587, val loss 4.6519 [2.188523769378662 sec]
step 100: train loss 2.7594, val loss 2.8155 [5.719759702682495 sec]
step 200: train loss 2.5861, val loss 2.6510 [9.269080877304077 sec]
step 300: train loss 2.4978, val loss 2.5687 [12.79162311553955 sec]
step 400: train loss 2.4103, val loss 2.4982 [16.316997528076172 sec]
2.3684349060058594
Total Training Time: 17.6896071434021 seconds

s eatha tar or Prayoul
shof ceNoist tove tendilell. g d oo o call forseds lin stham aste nod." luoun
BEGINNING (1681880710.0203037): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6415, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6509, val loss 4.6475 [3.3326051235198975 sec]
step 100: train loss 2.7123, val loss 2.7594 [8.994967222213745 sec]
step 200: train loss 2.5382, val loss 2.6115 [14.663207054138184 sec]
step 300: train loss 2.4382, val loss 2.5159 [20.293721675872803 sec]
step 400: train loss 2.3355, val loss 2.4319 [25.970585346221924 sec]
2.266267776489258
Total Training Time: 28.269245624542236 seconds

nonad jugles lon im tow nalencand lloued haldthemadeul.
"Hengesmand, ut. Grand wajo
"Gr Yros aid at 
BEGINNING (1681880739.0072362): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6252, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6263, val loss 4.6152 [4.517595529556274 sec]
step 100: train loss 2.6661, val loss 2.7206 [12.491375923156738 sec]
step 200: train loss 2.5090, val loss 2.5811 [20.223626613616943 sec]
step 300: train loss 2.4073, val loss 2.4896 [28.032475471496582 sec]
step 400: train loss 2.2916, val loss 2.3988 [35.770591259002686 sec]
2.169421672821045
Total Training Time: 38.99847078323364 seconds

too ntter atuont cer the as, Ccuis rhis sugest, he hras
co a heny he cupll gutl camal rrerhaid atp p
BEGINNING (1681880779.0683012): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.7387, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7506, val loss 4.7409 [3.0208771228790283 sec]
step 100: train loss 2.7844, val loss 2.8223 [7.990880012512207 sec]
step 200: train loss 2.6172, val loss 2.6762 [12.973684310913086 sec]
step 300: train loss 2.5410, val loss 2.6089 [17.920345306396484 sec]
step 400: train loss 2.4953, val loss 2.5642 [22.881362438201904 sec]
2.4813711643218994
Total Training Time: 24.86038875579834 seconds

chio larrsakentthe th lplanghestr."
Grthe athas clmprand omat Pye ou minono– ce tt " " thend hupruts
BEGINNING (1681880804.3246002): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6033, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5892, val loss 4.5877 [4.932977914810181 sec]
step 100: train loss 2.7041, val loss 2.7469 [13.437400102615356 sec]
step 200: train loss 2.5562, val loss 2.6227 [21.809298515319824 sec]
step 300: train loss 2.4884, val loss 2.5630 [30.165743112564087 sec]
step 400: train loss 2.4357, val loss 2.5145 [38.513670206069946 sec]
2.4154229164123535
Total Training Time: 41.989203214645386 seconds

win st er. Arormeatted.. Tho alse Ta
h nd thacl a' t
thhat kain pano d yon aned t laspefond ifuro sh
BEGINNING (1681880847.021914): Baseline LR(0.00016) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5746, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5808, val loss 4.5920 [6.826623439788818 sec]
step 100: train loss 2.6654, val loss 2.7319 [18.595141887664795 sec]
step 200: train loss 2.5343, val loss 2.6061 [30.35152792930603 sec]
step 300: train loss 2.4703, val loss 2.5474 [42.16844201087952 sec]
step 400: train loss 2.4100, val loss 2.4954 [53.942365646362305 sec]
2.397296190261841
Total Training Time: 58.88366484642029 seconds

Yeees bo bulile lee hubled ting ghaat ha ghe
cod coutl lomen head ac1O
svo I's.
Whe Nirricke " aif A
BEGINNING (1681880906.9488955): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5788, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5979, val loss 4.5960 [1.8022494316101074 sec]
step 100: train loss 2.6980, val loss 2.7486 [4.699684381484985 sec]
step 200: train loss 2.5269, val loss 2.5959 [7.680495500564575 sec]
step 300: train loss 2.3748, val loss 2.4709 [10.471309185028076 sec]
step 400: train loss 2.2743, val loss 2.3869 [13.422625541687012 sec]
2.1849775314331055
Total Training Time: 14.54250431060791 seconds

wHER
~erraild soidd yausssmens watlis)e Then ansecren geawiy.
"I Arthe ofed broag pentsaes lenteds, 
BEGINNING (1681880921.951593): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6246, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6214, val loss 4.6287 [2.968309164047241 sec]
step 100: train loss 2.6367, val loss 2.6918 [7.94258713722229 sec]
step 200: train loss 2.4611, val loss 2.5512 [12.97847867012024 sec]
step 300: train loss 2.3157, val loss 2.4151 [18.0214581489563 sec]
step 400: train loss 2.1983, val loss 2.3189 [23.075968742370605 sec]
2.0820441246032715
Total Training Time: 25.343268156051636 seconds

of catt. – adRo the nos poud falled snot mand ut the qugs ime noof Tagw turn sve suaber.6, "Soul
!–"
BEGINNING (1681880948.15815): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6080, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5799, val loss 4.5796 [4.0685460567474365 sec]
step 100: train loss 2.6194, val loss 2.6860 [11.067306518554688 sec]
step 200: train loss 2.4488, val loss 2.5375 [18.01732039451599 sec]
step 300: train loss 2.2953, val loss 2.3981 [24.93461322784424 sec]
step 400: train loss 2.1705, val loss 2.2923 [31.924487590789795 sec]
2.1088314056396484
Total Training Time: 34.970391273498535 seconds

SE-R2W AZRR GrO Gratta wackgoid latdens A0
zoulyard nahe so pes ond tiga anctoit?" Grattubs bided if
BEGINNING (1681880984.401532): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5865, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5896, val loss 4.5926 [1.8353819847106934 sec]
step 100: train loss 2.6975, val loss 2.7432 [4.906851291656494 sec]
step 200: train loss 2.5531, val loss 2.6229 [7.950457334518433 sec]
step 300: train loss 2.4600, val loss 2.5413 [10.991269826889038 sec]
step 400: train loss 2.3713, val loss 2.4595 [14.02350664138794 sec]
2.332146644592285
Total Training Time: 15.254692792892456 seconds

has ameflozeChl tiere. Gratttatattattttattay war ha
s so wed bed d tngoo ckenomango t. at he oqu,
Py
BEGINNING (1681881000.1110172): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6491, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6653, val loss 4.6720 [2.9657294750213623 sec]
step 100: train loss 2.6373, val loss 2.7018 [8.124284982681274 sec]
step 200: train loss 2.5057, val loss 2.5861 [13.604237794876099 sec]
step 300: train loss 2.4029, val loss 2.4898 [19.027534008026123 sec]
step 400: train loss 2.3056, val loss 2.4114 [24.255438566207886 sec]
2.2477736473083496
Total Training Time: 26.454623460769653 seconds

onouny. kno the himpely and mouby thint." Yer
He, thed abuamy yonedest. Ail is tuthedsha cok thaceo 
BEGINNING (1681881027.4221966): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5795, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5955, val loss 4.5974 [4.1128551959991455 sec]
step 100: train loss 2.6110, val loss 2.6686 [11.432402849197388 sec]
step 200: train loss 2.4841, val loss 2.5604 [18.72195839881897 sec]
step 300: train loss 2.3897, val loss 2.4730 [26.09897518157959 sec]
step 400: train loss 2.2738, val loss 2.3816 [33.45634603500366 sec]
2.2469053268432617
Total Training Time: 36.67167043685913 seconds

hyily. Ath.
Kghe stes he usta to ily nete we throt coft's!
Alllas Aifwh. Tme wed wlorow huwly whe na
BEGINNING (1681881065.3739023): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6370, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6430, val loss 4.6390 [2.337130308151245 sec]
step 100: train loss 2.6979, val loss 2.7495 [6.28700852394104 sec]
step 200: train loss 2.5662, val loss 2.6406 [10.242971420288086 sec]
step 300: train loss 2.5091, val loss 2.5843 [14.277450799942017 sec]
step 400: train loss 2.4734, val loss 2.5592 [18.358064651489258 sec]
2.4483325481414795
Total Training Time: 19.973937273025513 seconds

rwordsum wor;im ous3âene, st.
r beily mor. thalsekokef ts ""Wethefenowigamungutr w Therengar ayou we
BEGINNING (1681881085.7979395): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6338, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6362, val loss 4.6379 [4.006982326507568 sec]
step 100: train loss 2.6419, val loss 2.7048 [10.932693243026733 sec]
step 200: train loss 2.5298, val loss 2.6015 [17.838260650634766 sec]
step 300: train loss 2.4728, val loss 2.5471 [24.801294803619385 sec]
step 400: train loss 2.4186, val loss 2.5137 [31.739258289337158 sec]
2.3583824634552
Total Training Time: 34.70030331611633 seconds

Fo de se fearpahy.
"Hee babed mblyouton t t ooof w. "
"II I hacroorscl ddThe – pe
whathit?Kpe thele 
BEGINNING (1681881121.363339): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6172, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6207, val loss 4.6257 [5.62434720993042 sec]
step 100: train loss 2.6152, val loss 2.6847 [15.501155614852905 sec]
step 200: train loss 2.5065, val loss 2.5759 [25.44490098953247 sec]
step 300: train loss 2.4470, val loss 2.5244 [35.330421447753906 sec]
step 400: train loss 2.3808, val loss 2.4700 [45.1995735168457 sec]
2.3558006286621094
Total Training Time: 49.48566555976868 seconds

Gratckarwing o“
gof OfhKAis Whe thil the arelly. w – lutith – ave Profuond in ave
s Chnanereled powe
BEGINNING (1681881172.068716): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5700, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5781, val loss 4.5747 [1.9382951259613037 sec]
step 100: train loss 2.6782, val loss 2.7329 [5.072149991989136 sec]
step 200: train loss 2.4945, val loss 2.5734 [8.1339693069458 sec]
step 300: train loss 2.3320, val loss 2.4319 [11.171436071395874 sec]
step 400: train loss 2.2111, val loss 2.3264 [14.336815118789673 sec]
2.255944013595581
Total Training Time: 15.519866228103638 seconds

copances, athe whe to ano he ore cucid hemopreg deackile led co0 Ye chomkas ald enarout atto the sod
BEGINNING (1681881188.050013): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.7218, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6955, val loss 4.6965 [3.058250904083252 sec]
step 100: train loss 2.6266, val loss 2.6877 [8.156383752822876 sec]
step 200: train loss 2.4277, val loss 2.5111 [13.257838726043701 sec]
step 300: train loss 2.2663, val loss 2.3747 [18.34859561920166 sec]
step 400: train loss 2.1393, val loss 2.2671 [23.683300495147705 sec]
2.011887550354004
Total Training Time: 25.96881890296936 seconds

"Kig a puods ins notto ar
hey nstile bloous huows. Wid Som lik alk outhe had bet
eas the/"
Youns cro
BEGINNING (1681881214.9345338): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6087, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6158, val loss 4.6145 [4.24014139175415 sec]
step 100: train loss 2.5978, val loss 2.6639 [11.480905055999756 sec]
step 200: train loss 2.4078, val loss 2.4819 [18.73538374900818 sec]
step 300: train loss 2.2368, val loss 2.3451 [25.94241166114807 sec]
step 400: train loss 2.1045, val loss 2.2434 [32.98791146278381 sec]
2.0055415630340576
Total Training Time: 36.16584801673889 seconds

cimilgs warrroh toacu."
haced's eseume wasercanta leex
ckely
him
opout
puyounayas. "Ye maince
wa, se
BEGINNING (1681881252.360473): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6528, val loss 4.6466 [2.210461139678955 sec]
step 100: train loss 2.6726, val loss 2.7323 [5.734109401702881 sec]
step 200: train loss 2.5299, val loss 2.5982 [9.265960931777954 sec]
step 300: train loss 2.4283, val loss 2.5145 [12.82673192024231 sec]
step 400: train loss 2.3184, val loss 2.4227 [16.373759746551514 sec]
2.2755115032196045
Total Training Time: 17.76116108894348 seconds

lighe wilas qupray." Grtand sattt hiced iah
ple was thess d, An ak.
he he he.
"˜ ad."'s sceabereds i
BEGINNING (1681881270.5805726): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5837, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5788, val loss 4.5895 [3.5495104789733887 sec]
step 100: train loss 2.6280, val loss 2.6793 [9.796844005584717 sec]
step 200: train loss 2.4835, val loss 2.5615 [15.871715068817139 sec]
step 300: train loss 2.3815, val loss 2.4737 [21.9614474773407 sec]
step 400: train loss 2.2457, val loss 2.3507 [28.042616605758667 sec]
2.2092034816741943
Total Training Time: 30.550708532333374 seconds

fart them a raind forrenteflimackista noind thallws
faf brrainca!"
Arotleakat: cokuled phed enat Af 
BEGINNING (1681881301.9878767): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5699, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5707, val loss 4.5729 [4.949353933334351 sec]
step 100: train loss 2.5906, val loss 2.6547 [13.485243320465088 sec]
step 200: train loss 2.4552, val loss 2.5269 [21.977116584777832 sec]
step 300: train loss 2.3291, val loss 2.4197 [30.487332582473755 sec]
step 400: train loss 2.1901, val loss 2.3061 [38.97835993766785 sec]
2.1690762042999268
Total Training Time: 42.77378487586975 seconds

ong in?" Ta ba ive – Gof sta pree. Hed ort the thef nur ofr'n sarin hit of thar for
the hal tyou oy,
BEGINNING (1681881346.0965827): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6193, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6135, val loss 4.6215 [3.100879192352295 sec]
step 100: train loss 2.6807, val loss 2.7377 [8.226893186569214 sec]
step 200: train loss 2.5546, val loss 2.6271 [13.353048086166382 sec]
step 300: train loss 2.4999, val loss 2.5700 [18.485897302627563 sec]
step 400: train loss 2.4600, val loss 2.5442 [23.609696865081787 sec]
2.435370683670044
Total Training Time: 25.68293333053589 seconds

re te he tenow
eat rthesin y
ra
igeforyck,
Anathichilsh tef fe ave t lithes le oug, r tha Areis Thai
BEGINNING (1681881372.2319362): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6065, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6165, val loss 4.6213 [5.205029010772705 sec]
step 100: train loss 2.6358, val loss 2.6909 [14.189981937408447 sec]
step 200: train loss 2.5140, val loss 2.5922 [23.17683982849121 sec]
step 300: train loss 2.4521, val loss 2.5339 [32.16295671463013 sec]
step 400: train loss 2.3753, val loss 2.4634 [41.239173412323 sec]
2.334752082824707
Total Training Time: 45.03157162666321 seconds

ptis alfettion ah ylaNehin t we tlos
Peand fermei9
"E toted an. T• wonoxcer she lassthearea'so.
wknd
BEGINNING (1681881418.0793052): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6314, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6265, val loss 4.6275 [7.375463962554932 sec]
step 100: train loss 2.5998, val loss 2.6673 [20.192203998565674 sec]
step 200: train loss 2.4939, val loss 2.5687 [33.00895047187805 sec]
step 300: train loss 2.4315, val loss 2.5197 [45.84380054473877 sec]
step 400: train loss 2.3598, val loss 2.4575 [58.70192527770996 sec]
2.3060078620910645
Total Training Time: 64.20987319946289 seconds

Prs cre slad enauns ck hang ayedind the sno ariream s
oung anch ous ts heit at ofe hionkse net aive 
BEGINNING (1681881483.6113188): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5509, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5478, val loss 4.5550 [2.173781156539917 sec]
step 100: train loss 2.6673, val loss 2.7283 [5.58065390586853 sec]
step 200: train loss 2.4750, val loss 2.5561 [8.990790128707886 sec]
step 300: train loss 2.2934, val loss 2.3919 [12.414827585220337 sec]
step 400: train loss 2.1735, val loss 2.3033 [15.850329637527466 sec]
2.2054848670959473
Total Training Time: 17.125062942504883 seconds

to wo aith. The trreratta tGow for to sowift, and, milet ave fo he wole.
5ecromushell cat 1re, we fr
BEGINNING (1681881501.1994283): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6233, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6028, val loss 4.6087 [3.2360167503356934 sec]
step 100: train loss 2.6070, val loss 2.6782 [8.772406816482544 sec]
step 200: train loss 2.3951, val loss 2.4771 [14.30541443824768 sec]
step 300: train loss 2.2137, val loss 2.3337 [19.802552938461304 sec]
step 400: train loss 2.0813, val loss 2.2250 [25.184802532196045 sec]
2.060105562210083
Total Training Time: 27.474797010421753 seconds

frolt now puld Wisk reat saithes wed alo gut thnty ain! Too
his Goodd tra." Grragood ans coust; hid 
BEGINNING (1681881529.5211627): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6127, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6259, val loss 4.6234 [4.3561155796051025 sec]
step 100: train loss 2.5796, val loss 2.6504 [11.993377208709717 sec]
step 200: train loss 2.3950, val loss 2.4838 [19.9272141456604 sec]
step 300: train loss 2.2066, val loss 2.3226 [27.486953258514404 sec]
step 400: train loss 2.0538, val loss 2.2043 [34.96359705924988 sec]
1.999814748764038
Total Training Time: 38.40717363357544 seconds

Gratturds hadd!" cjounichs ould chexl coun troms wake op tho Pyoun hienet ave me somushe
toop Eve hi
BEGINNING (1681881569.176877): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.7107, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6990, val loss 4.6914 [2.5667684078216553 sec]
step 100: train loss 2.6715, val loss 2.7355 [6.779251575469971 sec]
step 200: train loss 2.5231, val loss 2.5990 [10.96744990348816 sec]
step 300: train loss 2.4170, val loss 2.5040 [15.267175674438477 sec]
step 400: train loss 2.2968, val loss 2.4025 [19.485443830490112 sec]
2.223407030105591
Total Training Time: 21.109114408493042 seconds

t caver alle. SER Preled."
"Yevata the ang cerecoutsed hosid wa ty. d, Gras matukettrinneng.
taplly,
BEGINNING (1681881590.7552152): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6343, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6359, val loss 4.6261 [4.108349800109863 sec]
step 100: train loss 2.6137, val loss 2.6877 [11.111294984817505 sec]
step 200: train loss 2.4714, val loss 2.5440 [18.326421976089478 sec]
step 300: train loss 2.3487, val loss 2.4410 [25.459861278533936 sec]
step 400: train loss 2.2092, val loss 2.3278 [32.48017501831055 sec]
2.1146702766418457
Total Training Time: 35.36909008026123 seconds

Goddill ens afa tear norhe fus the sol. The âoupt."
Theiroubl hy Grilo the nne wont, aingens paer al
BEGINNING (1681881626.9791675): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6296, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6243, val loss 4.6258 [5.636476516723633 sec]
step 100: train loss 2.5891, val loss 2.6518 [15.53668737411499 sec]
step 200: train loss 2.4399, val loss 2.5209 [25.441846132278442 sec]
step 300: train loss 2.3113, val loss 2.4139 [35.299558877944946 sec]
step 400: train loss 2.1550, val loss 2.2782 [45.32409977912903 sec]
2.104154109954834
Total Training Time: 49.626102685928345 seconds

quand to gowle sBil in, Cled the sto thee t˜eaed bair fred do
vefiteng, warrait, st?" I Tkif to his,
BEGINNING (1681881677.9463248): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6094, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6096, val loss 4.6135 [3.6946494579315186 sec]
step 100: train loss 2.6752, val loss 2.7291 [9.88459587097168 sec]
step 200: train loss 2.5396, val loss 2.6127 [16.073906421661377 sec]
step 300: train loss 2.4773, val loss 2.5578 [22.274393558502197 sec]
step 400: train loss 2.4261, val loss 2.5113 [28.441105842590332 sec]
2.422703981399536
Total Training Time: 30.961976528167725 seconds

rpomlsigatanverned to ou“ed
hithe. Grathe tay f d fratowal gited. wigh
han bewhed a s the ouopinccKa
BEGINNING (1681881709.3563755): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6031, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6137, val loss 4.6111 [6.285306930541992 sec]
step 100: train loss 2.6284, val loss 2.6949 [17.180784225463867 sec]
step 200: train loss 2.5141, val loss 2.5886 [28.04160213470459 sec]
step 300: train loss 2.4458, val loss 2.5284 [38.959765672683716 sec]
step 400: train loss 2.3734, val loss 2.4624 [49.778695583343506 sec]
2.2795891761779785
Total Training Time: 54.37107229232788 seconds

chen cyahistcke fne to arily milh
tnethah she thharst sme1
toyand I ug the re aulld. Arstice sckalld
BEGINNING (1681881764.5927393): Baseline LR(0.00016) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6428, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6384, val loss 4.6333 [8.86825680732727 sec]
step 100: train loss 2.5894, val loss 2.6619 [24.36811327934265 sec]
step 200: train loss 2.4843, val loss 2.5605 [39.8484423160553 sec]
step 300: train loss 2.4027, val loss 2.4932 [55.37339520454407 sec]
step 400: train loss 2.2858, val loss 2.3888 [70.91266202926636 sec]
2.1799793243408203
Total Training Time: 77.55587697029114 seconds

rritr a sced. Gr, weattok,
ce wa pator cen anke iburer ouped te po thish
angath teied wncepporraiat.
BEGINNING (1681881843.424941): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6680, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6462, val loss 4.6413 [2.2861673831939697 sec]
step 100: train loss 2.5974, val loss 2.6772 [6.085533618927002 sec]
step 200: train loss 2.4179, val loss 2.4974 [9.851369380950928 sec]
step 300: train loss 2.2405, val loss 2.3607 [13.609777450561523 sec]
step 400: train loss 2.1335, val loss 2.2721 [17.400218725204468 sec]
2.0706262588500977
Total Training Time: 18.92323660850525 seconds

hfir,s The thes heaw out the
for prowk. Athe them.""
Ro as a tried alast. He will oich hus
hin gust 
BEGINNING (1681881862.9773972): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5978, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6163, val loss 4.6271 [3.811790704727173 sec]
step 100: train loss 2.5647, val loss 2.6395 [10.761977195739746 sec]
step 200: train loss 2.3635, val loss 2.4586 [17.54176640510559 sec]
step 300: train loss 2.1932, val loss 2.3106 [24.229963302612305 sec]
step 400: train loss 2.0511, val loss 2.2050 [30.871337890625 sec]
2.062260150909424
Total Training Time: 33.76343250274658 seconds

to fore dremen.
A muth. Theser ald of fuwfor that ther or SlKoter snech baranf of. That the Nap s th
BEGINNING (1681881897.9417117): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6801, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6738, val loss 4.6622 [5.311817169189453 sec]
step 100: train loss 2.5372, val loss 2.6088 [14.95497465133667 sec]
step 200: train loss 2.3492, val loss 2.4502 [24.556641817092896 sec]
step 300: train loss 2.1620, val loss 2.3126 [34.06919002532959 sec]
step 400: train loss 2.0246, val loss 2.1871 [43.98945617675781 sec]
1.9196122884750366
Total Training Time: 48.348366260528564 seconds

ollance but Pexa I is Gratta fistinta in hown lon the be godres messme hils cors hodne and ta sookla
BEGINNING (1681881948.0644457): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5527, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5612, val loss 4.5640 [2.579113245010376 sec]
step 100: train loss 2.6107, val loss 2.6752 [6.90263295173645 sec]
step 200: train loss 2.4694, val loss 2.5569 [11.179618120193481 sec]
step 300: train loss 2.3577, val loss 2.4564 [15.443093299865723 sec]
step 400: train loss 2.2217, val loss 2.3458 [19.696422338485718 sec]
2.1933274269104004
Total Training Time: 21.465861082077026 seconds

Kince fare no thole won. Hid savellled ow datd hons wor
chadoor hea4
he thad arikatt
nin't was on th
BEGINNING (1681881970.1779969): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6413, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6423, val loss 4.6391 [4.366232872009277 sec]
step 100: train loss 2.5581, val loss 2.6301 [12.08613920211792 sec]
step 200: train loss 2.4326, val loss 2.5235 [19.79249334335327 sec]
step 300: train loss 2.3074, val loss 2.4073 [27.452980041503906 sec]
step 400: train loss 2.1591, val loss 2.2997 [35.221269607543945 sec]
2.0064401626586914
Total Training Time: 38.761287689208984 seconds

walyahurd ou encKing.
62
SEY
5
CSQ APTHORS – LAle?"qF keaiyah t, youme aref the min quone.
The herim
BEGINNING (1681882010.1379514): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5915, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5889, val loss 4.5797 [6.24131441116333 sec]
step 100: train loss 2.5330, val loss 2.6173 [17.338585138320923 sec]
step 200: train loss 2.4287, val loss 2.5165 [28.35994029045105 sec]
step 300: train loss 2.2671, val loss 2.3821 [39.46628475189209 sec]
step 400: train loss 2.1015, val loss 2.2511 [50.5277214050293 sec]
2.0343804359436035
Total Training Time: 55.362865686416626 seconds

hremy." mien duld as sein a7at The
En Theve fit, Anta sand." "Chis ad dof lo dooked diad
• tree, Kan
BEGINNING (1681882067.2211883): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6320, val loss 4.6441 [3.474036931991577 sec]
step 100: train loss 2.6032, val loss 2.6716 [9.44547986984253 sec]
step 200: train loss 2.5036, val loss 2.5842 [15.347575426101685 sec]
step 300: train loss 2.4563, val loss 2.5363 [21.23993444442749 sec]
step 400: train loss 2.3979, val loss 2.4965 [27.149420499801636 sec]
2.378819704055786
Total Training Time: 29.624690532684326 seconds

lil sokle hatcoo wily rar hilyoncelyo ag m saman the the fo
stof shithe fopreringhen ake ale. witoo 
BEGINNING (1681882097.4812524): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5928, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5918, val loss 4.5971 [6.111585855484009 sec]
step 100: train loss 2.5668, val loss 2.6377 [16.897684812545776 sec]
step 200: train loss 2.4731, val loss 2.5566 [27.69327211380005 sec]
step 300: train loss 2.4018, val loss 2.4942 [38.72344183921814 sec]
step 400: train loss 2.3103, val loss 2.4201 [50.09836483001709 sec]
2.2549633979797363
Total Training Time: 54.769567012786865 seconds

ERAY, N8
APTET
APTER FERACHY
ACSITHAPRCES PTOSEN TRE
MPCHACEAN, I PTE
HSEAMEAY1~NK56
5
Grakns NG Thi
BEGINNING (1681882153.3855839): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5765, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5836, val loss 4.5871 [8.813799858093262 sec]
step 100: train loss 2.5462, val loss 2.6176 [24.49033284187317 sec]
step 200: train loss 2.4621, val loss 2.5424 [40.17791390419006 sec]
step 300: train loss 2.3733, val loss 2.4703 [55.89449191093445 sec]
step 400: train loss 2.2436, val loss 2.3570 [71.5635416507721 sec]
2.129176378250122
Total Training Time: 78.42444586753845 seconds

clbll mobe the. grat ellivexched thistion yo waph quon this
gre the frit putl, age mallege forst cki
BEGINNING (1681882233.66133): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6595, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6496, val loss 4.6535 [2.4659574031829834 sec]
step 100: train loss 2.5805, val loss 2.6584 [6.573706150054932 sec]
step 200: train loss 2.3618, val loss 2.4479 [10.621597051620483 sec]
step 300: train loss 2.1912, val loss 2.3129 [14.693564414978027 sec]
step 400: train loss 2.0627, val loss 2.2186 [18.784464836120605 sec]
2.026376962661743
Total Training Time: 20.422590970993042 seconds

at had bake wadd gobing, wet, thous ther cout pepids, "I jepeer on of the reacaz at was ruatcill the
BEGINNING (1681882254.6929774): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5710, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5764, val loss 4.5815 [3.9960134029388428 sec]
step 100: train loss 2.5333, val loss 2.6199 [11.285066843032837 sec]
step 200: train loss 2.3117, val loss 2.4193 [18.750887155532837 sec]
step 300: train loss 2.1208, val loss 2.2572 [25.67516565322876 sec]
step 400: train loss 1.9695, val loss 2.1336 [32.65877366065979 sec]
2.04736328125
Total Training Time: 35.64864110946655 seconds

werong of bout noy ese por't. Zention ingenter, and im he for?"
ques aralsedies. That!
Hick ary to a
BEGINNING (1681882291.5430915): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5552, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5495, val loss 4.5608 [5.50929594039917 sec]
step 100: train loss 2.5097, val loss 2.5872 [15.336669683456421 sec]
step 200: train loss 2.2806, val loss 2.3889 [25.139925718307495 sec]
step 300: train loss 2.0730, val loss 2.2242 [34.97909116744995 sec]
step 400: train loss 1.9386, val loss 2.1182 [45.20531392097473 sec]
1.8702874183654785
Total Training Time: 49.76401424407959 seconds

Vent's mack.
The hazour quan's,
Aidd bidge tooddo slittace inst
to uld is atre to besent.
Arpheright
BEGINNING (1681882343.074097): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5864, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5783, val loss 4.5793 [2.97808575630188 sec]
step 100: train loss 2.5883, val loss 2.6514 [8.004886388778687 sec]
step 200: train loss 2.4523, val loss 2.5380 [13.023819923400879 sec]
step 300: train loss 2.3106, val loss 2.4147 [18.041284322738647 sec]
step 400: train loss 2.1522, val loss 2.2860 [23.088855028152466 sec]
2.0863866806030273
Total Training Time: 25.134701251983643 seconds

siting. Tsond theing a ws. Arabus forend eur
bed cham asay fised."
3 Youre. And hat Nened anes, alp.
BEGINNING (1681882368.881097): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5686, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5818, val loss 4.5698 [5.13903546333313 sec]
step 100: train loss 2.5509, val loss 2.6232 [14.0739266872406 sec]
step 200: train loss 2.4001, val loss 2.4925 [23.043365955352783 sec]
step 300: train loss 2.2272, val loss 2.3404 [32.02626276016235 sec]
step 400: train loss 2.0486, val loss 2.1973 [41.06636595726013 sec]
2.0556130409240723
Total Training Time: 44.864832639694214 seconds

dout. I swe ann thersuond scrthibroon's
ts."
Anatyah woure ing tum, os thime. "I tle ateds you ourt!
BEGINNING (1681882414.9540236): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5635, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5525, val loss 4.5683 [7.307494878768921 sec]
step 100: train loss 2.5212, val loss 2.5980 [20.20099925994873 sec]
step 200: train loss 2.3789, val loss 2.4719 [33.05851936340332 sec]
step 300: train loss 2.1909, val loss 2.3187 [45.922149896621704 sec]
step 400: train loss 1.9980, val loss 2.1679 [58.95058226585388 sec]
1.903393268585205
Total Training Time: 64.51004528999329 seconds

could and it the up dondderop ast wis th troong and scope
taek for and We coungs, the megattes the c
BEGINNING (1681882481.2339756): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6515, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6495, val loss 4.6474 [4.508602619171143 sec]
step 100: train loss 2.5965, val loss 2.6602 [12.28575086593628 sec]
step 200: train loss 2.4944, val loss 2.5751 [20.06664538383484 sec]
step 300: train loss 2.4298, val loss 2.5098 [27.84626793861389 sec]
step 400: train loss 2.3480, val loss 2.4492 [35.57579731941223 sec]
2.2997562885284424
Total Training Time: 38.83116388320923 seconds

thes fe hut bes. "Thisn faveir
quece win Grartac frarive topexcy ed ftreng of iebrirs4"
Ba COCETChat
BEGINNING (1681882520.6953912): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6526, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6668, val loss 4.6726 [8.070956230163574 sec]
step 100: train loss 2.5669, val loss 2.6475 [22.3080313205719 sec]
step 200: train loss 2.4574, val loss 2.5438 [36.43902087211609 sec]
step 300: train loss 2.3597, val loss 2.4548 [50.62407302856445 sec]
step 400: train loss 2.2204, val loss 2.3455 [64.76205563545227 sec]
2.1252593994140625
Total Training Time: 70.81747579574585 seconds

hessety, and alleme thace feris no or he ay
com spreal head the asuredrinel, wash ago tmar henlforsk
BEGINNING (1681882592.7067332): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5927, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5826, val loss 4.5812 [11.697592735290527 sec]
step 100: train loss 2.5293, val loss 2.6050 [32.139225482940674 sec]
step 200: train loss 2.4378, val loss 2.5206 [52.630147218704224 sec]
step 300: train loss 2.3234, val loss 2.4398 [73.07245635986328 sec]
step 400: train loss 2.1372, val loss 2.2720 [93.57509922981262 sec]
2.0383241176605225
Total Training Time: 102.39683794975281 seconds

hed Pringhatling anied wiresk wing
larte mat, gellll sot far and theyy couris cking list, ame. Armea
BEGINNING (1681882696.9183073): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6367, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6359, val loss 4.6306 [2.852804660797119 sec]
step 100: train loss 2.5604, val loss 2.6381 [7.6522228717803955 sec]
step 200: train loss 2.3238, val loss 2.4190 [12.256657600402832 sec]
step 300: train loss 2.1522, val loss 2.2754 [16.871368646621704 sec]
step 400: train loss 2.0293, val loss 2.1819 [21.492928743362427 sec]
2.0004796981811523
Total Training Time: 23.344690561294556 seconds

Thacked aleg, is nubly as osk the
thrould newe as hund Mn more geld tok! OI A Thumane tuone of mou a
BEGINNING (1681882720.906286): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6613, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6822, val loss 4.6747 [4.661941289901733 sec]
step 100: train loss 2.5252, val loss 2.5964 [12.881280183792114 sec]
step 200: train loss 2.2662, val loss 2.3755 [20.91572880744934 sec]
step 300: train loss 2.0654, val loss 2.2264 [28.836782693862915 sec]
step 400: train loss 1.9274, val loss 2.0949 [36.71880221366882 sec]
1.9243141412734985
Total Training Time: 40.005593061447144 seconds

Claieind.
Gratta ot toon Elyou could on noot decubs
ted, NoChief tace he to Pyvo tuon tremuaned Pbem
BEGINNING (1681882762.0898902): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5824, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5638, val loss 4.5510 [6.4878411293029785 sec]
step 100: train loss 2.4842, val loss 2.5587 [17.742987155914307 sec]
step 200: train loss 2.2496, val loss 2.3606 [29.001292943954468 sec]
step 300: train loss 2.0418, val loss 2.2010 [40.55845046043396 sec]
step 400: train loss 1.8930, val loss 2.0821 [51.85622715950012 sec]
1.848900556564331
Total Training Time: 56.62051177024841 seconds

Zeriant-riy's the comper
and fiarllied. This mester aur." Namal
ably werpowed oter had woulds and ha
BEGINNING (1681882820.4903197): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6653, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6625, val loss 4.6638 [3.5828893184661865 sec]
step 100: train loss 2.5852, val loss 2.6460 [9.677623748779297 sec]
step 200: train loss 2.4185, val loss 2.5102 [15.742002487182617 sec]
step 300: train loss 2.2576, val loss 2.3681 [21.80688214302063 sec]
step 400: train loss 2.0956, val loss 2.2327 [27.857940673828125 sec]
2.080983877182007
Total Training Time: 30.347532987594604 seconds

the fee canes, merpl the hiew, your fis ange gmatiow
Couens int spoer hil gups to
Chie, ksila!" and 
BEGINNING (1681882851.4682148): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5432, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5474, val loss 4.5365 [6.1489479541778564 sec]
step 100: train loss 2.5319, val loss 2.6051 [16.931598663330078 sec]
step 200: train loss 2.3881, val loss 2.4795 [27.605050563812256 sec]
step 300: train loss 2.1767, val loss 2.3018 [38.28221893310547 sec]
step 400: train loss 1.9882, val loss 2.1557 [48.94642400741577 sec]
1.9164695739746094
Total Training Time: 53.44902563095093 seconds

Gratta rabr."
lo seer aveo rarked we coveree compled whly of there.
Akidd whas kidders andis snatus 
BEGINNING (1681882906.1405416): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5928, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6059, val loss 4.6121 [8.755600452423096 sec]
step 100: train loss 2.5132, val loss 2.5845 [24.08900737762451 sec]
step 200: train loss 2.3500, val loss 2.4473 [39.48878836631775 sec]
step 300: train loss 2.1348, val loss 2.2631 [54.78572702407837 sec]
step 400: train loss 1.9285, val loss 2.1024 [70.11495614051819 sec]
1.8620994091033936
Total Training Time: 76.70623064041138 seconds

the hat ight fis." Gratta daby warled
and site do to otheirssns. The culds for to cansid.
"I held we
BEGINNING (1681882984.648265): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5942, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5806, val loss 4.5818 [5.517697811126709 sec]
step 100: train loss 2.5834, val loss 2.6538 [15.191760301589966 sec]
step 200: train loss 2.4795, val loss 2.5608 [24.682329177856445 sec]
step 300: train loss 2.4053, val loss 2.4962 [34.19074106216431 sec]
step 400: train loss 2.3241, val loss 2.4284 [43.68041801452637 sec]
2.2448203563690186
Total Training Time: 47.673096895217896 seconds

lons dard jUllad we histhimarten we terowns cid, and pond cke
th waguped." d me derenaty thaly heant
BEGINNING (1681883032.9632435): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5862, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5971, val loss 4.5941 [9.886146545410156 sec]
step 100: train loss 2.5559, val loss 2.6324 [27.05818247795105 sec]
step 200: train loss 2.4501, val loss 2.5314 [44.34588956832886 sec]
step 300: train loss 2.3480, val loss 2.4547 [61.54553174972534 sec]
step 400: train loss 2.2087, val loss 2.3326 [78.74854516983032 sec]
2.0567309856414795
Total Training Time: 86.04601788520813 seconds

coumppucaut on the fo bubs, beor wou "Yay us cinsqurert."
"Onot well in iesued fitterseced maf the t
BEGINNING (1681883120.2593508): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6775, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6804, val loss 4.6764 [14.259338617324829 sec]
step 100: train loss 2.5307, val loss 2.6000 [38.92439103126526 sec]
step 200: train loss 2.4339, val loss 2.5151 [63.50169658660889 sec]
step 300: train loss 2.3183, val loss 2.4196 [88.11635136604309 sec]
step 400: train loss 2.1130, val loss 2.2541 [112.80799794197083 sec]
2.001450538635254
Total Training Time: 123.31557106971741 seconds

the farem."
And tur bene quniknyaed wivo a de to the mere
Colden." Nal haren wn anoy castus tozit no
BEGINNING (1681883245.4233856): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5492, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5583, val loss 4.5486 [2.846029758453369 sec]
step 100: train loss 2.5457, val loss 2.6233 [7.6740336418151855 sec]
step 200: train loss 2.3110, val loss 2.4238 [12.420265197753906 sec]
step 300: train loss 2.1443, val loss 2.2811 [17.428643226623535 sec]
step 400: train loss 2.0293, val loss 2.1847 [22.43385934829712 sec]
1.9961059093475342
Total Training Time: 24.51596760749817 seconds

stone cverem.
"Van ssa voil ist tromm to the camue be maesge, ing en if an with iffel ate
them wfe t
BEGINNING (1681883270.7443118): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5617, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5526, val loss 4.5590 [4.867217779159546 sec]
step 100: train loss 2.5098, val loss 2.5875 [13.304076671600342 sec]
step 200: train loss 2.2921, val loss 2.3985 [21.714383602142334 sec]
step 300: train loss 2.0998, val loss 2.2528 [30.254637241363525 sec]
step 400: train loss 1.9571, val loss 2.1353 [38.62744760513306 sec]
1.8671495914459229
Total Training Time: 42.41119432449341 seconds

bughts feemle. The to maut we him the
bat dood Torta pund doo for comaled, if sto andent Ather
ey.
G
BEGINNING (1681883314.632306): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6086, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5869, val loss 4.5815 [6.648940563201904 sec]
step 100: train loss 2.4810, val loss 2.5638 [19.623822450637817 sec]
step 200: train loss 2.2761, val loss 2.3853 [32.15815043449402 sec]
step 300: train loss 2.0664, val loss 2.2160 [45.03748655319214 sec]
step 400: train loss 1.9355, val loss 2.1044 [57.43037438392639 sec]
1.9104082584381104
Total Training Time: 63.0853853225708 seconds

acang our his to cemitintens you."
CHAPTEAF wERR.
"This is him seembes was intion haris
coppheriout 
BEGINNING (1681883379.9590955): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6257, val loss 4.6165 [3.1743407249450684 sec]
step 100: train loss 2.5530, val loss 2.6284 [8.816343069076538 sec]
step 200: train loss 2.4147, val loss 2.5008 [14.580684661865234 sec]
step 300: train loss 2.2567, val loss 2.3604 [20.364076137542725 sec]
step 400: train loss 2.0848, val loss 2.2245 [25.963029384613037 sec]
2.0346336364746094
Total Training Time: 28.37826895713806 seconds

ACE EE
CCEAY
CEve and you seve rine beckibs, wall wop bet an oncer se we
ateed Grollomaly ridded.
Th
BEGINNING (1681883409.1816883): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5926, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6121, val loss 4.6059 [5.696988582611084 sec]
step 100: train loss 2.5150, val loss 2.6088 [16.208944082260132 sec]
step 200: train loss 2.3756, val loss 2.4644 [26.598783254623413 sec]
step 300: train loss 2.1911, val loss 2.3254 [36.93690490722656 sec]
step 400: train loss 2.0110, val loss 2.1677 [47.317371129989624 sec]
1.925654411315918
Total Training Time: 52.11033010482788 seconds

Bileght."
weresoonced. Wen tooke the and grastaing of thrirar
mae."
Grat, starened, and ast fellly s
BEGINNING (1681883462.9143784): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6322, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6552, val loss 4.6434 [8.275342226028442 sec]
step 100: train loss 2.5062, val loss 2.5826 [23.37828803062439 sec]
step 200: train loss 2.3731, val loss 2.4647 [38.42545127868652 sec]
step 300: train loss 2.1841, val loss 2.3151 [53.47731614112854 sec]
step 400: train loss 1.9715, val loss 2.1456 [68.8188087940216 sec]
1.9068670272827148
Total Training Time: 75.62633538246155 seconds

fatled To quarkie his tubta's in frimed, "Chisfy
Pyrras, sturnow thath thim prea sarulg, in and you 
BEGINNING (1681883540.742147): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5715, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5681, val loss 4.5702 [4.817775249481201 sec]
step 100: train loss 2.5622, val loss 2.6396 [13.245022773742676 sec]
step 200: train loss 2.4677, val loss 2.5530 [21.66491460800171 sec]
step 300: train loss 2.3879, val loss 2.4789 [30.112850666046143 sec]
step 400: train loss 2.2840, val loss 2.3959 [38.53181195259094 sec]
2.3035974502563477
Total Training Time: 42.16832971572876 seconds

or. Whe
s Ium.7
Ze Mch cans mEais wout yen buar apkep co som.Thiche
Se op het mpen de ed tre be smui
BEGINNING (1681883583.7705648): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6276, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6315, val loss 4.6274 [8.858692407608032 sec]
step 100: train loss 2.5254, val loss 2.6024 [24.62510585784912 sec]
step 200: train loss 2.4382, val loss 2.5279 [40.37659287452698 sec]
step 300: train loss 2.3371, val loss 2.4393 [56.167808532714844 sec]
step 400: train loss 2.1431, val loss 2.2812 [71.89792370796204 sec]
2.0691330432891846
Total Training Time: 78.88897752761841 seconds

ge labl hive weg?"
Ks, Malyou hinicenars a to the elst Aincen lotace thay to.
"Yay snore troweererin
BEGINNING (1681883664.2269368): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5823, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5850, val loss 4.5816 [12.862157583236694 sec]
step 100: train loss 2.5129, val loss 2.5957 [36.02480983734131 sec]
step 200: train loss 2.4336, val loss 2.5194 [59.02723741531372 sec]
step 300: train loss 2.3113, val loss 2.4184 [82.11197757720947 sec]
step 400: train loss 2.0853, val loss 2.2325 [105.16383266448975 sec]
2.0356409549713135
Total Training Time: 115.32831811904907 seconds

buch me disen centy.-
177e Sist Vell Cden, 'fired a and wisshitll ther to and the ar
frmfers. Soy wh
BEGINNING (1681883781.7837265): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5532, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5681, val loss 4.5812 [3.128114938735962 sec]
step 100: train loss 2.5333, val loss 2.6024 [8.585171937942505 sec]
step 200: train loss 2.2593, val loss 2.3688 [14.10697054862976 sec]
step 300: train loss 2.0735, val loss 2.2353 [19.43283748626709 sec]
step 400: train loss 1.9485, val loss 2.1264 [24.78950262069702 sec]
2.0021655559539795
Total Training Time: 27.033093214035034 seconds

Anahour and his is. A shin,
grilight s. The havien. Till was youith sorGratta theAewed nost fur!
he 
BEGINNING (1681883809.5836532): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5572, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5467, val loss 4.5593 [5.328685283660889 sec]
step 100: train loss 2.4730, val loss 2.5576 [14.827022314071655 sec]
step 200: train loss 2.2181, val loss 2.3352 [24.367093563079834 sec]
step 300: train loss 2.0204, val loss 2.1844 [33.831560134887695 sec]
step 400: train loss 1.8756, val loss 2.0648 [43.375022649765015 sec]
1.82939612865448
Total Training Time: 47.77272868156433 seconds

sale putcinbiftes,
we nay are nod to is
out pame." Gratta noddods ald gead and aliked illis as have 
BEGINNING (1681883858.9433525): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5942, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5813, val loss 4.5787 [7.573296785354614 sec]
step 100: train loss 2.4529, val loss 2.5450 [21.412147045135498 sec]
step 200: train loss 2.1749, val loss 2.3055 [35.092222452163696 sec]
step 300: train loss 1.9705, val loss 2.1345 [48.809062480926514 sec]
step 400: train loss 1.8381, val loss 2.0411 [62.667893171310425 sec]
1.8791078329086304
Total Training Time: 69.11898493766785 seconds

I shout degonting. That sagen of tuon werrioss and
terrigh to ins."
"Bet knew in be suon's ree with 
BEGINNING (1681883930.3669524): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6075, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6098, val loss 4.6115 [4.071296691894531 sec]
step 100: train loss 2.5435, val loss 2.6251 [11.102852582931519 sec]
step 200: train loss 2.3729, val loss 2.4667 [18.12851905822754 sec]
step 300: train loss 2.1856, val loss 2.3096 [25.17196822166443 sec]
step 400: train loss 2.0192, val loss 2.1899 [32.212993144989014 sec]
1.981643557548523
Total Training Time: 35.18230438232422 seconds

hae spigh.
Teaknots and wal has coms a ce, anuid then% sa'Takeded ofled
ad. GaSEA deerand brshup. Hi
BEGINNING (1681883966.3545303): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5979, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5954, val loss 4.5902 [7.273558139801025 sec]
step 100: train loss 2.5119, val loss 2.5961 [20.227482318878174 sec]
step 200: train loss 2.3440, val loss 2.4432 [33.26781415939331 sec]
step 300: train loss 2.1257, val loss 2.2569 [46.18744945526123 sec]
step 400: train loss 1.9359, val loss 2.1176 [59.10525608062744 sec]
1.8327406644821167
Total Training Time: 64.72969818115234 seconds

theis magaced tur they, far my haves pewing.
Ves, Gratted and wight I wat a of
tre shime to it the w
BEGINNING (1681884032.5690863): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6716, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6610, val loss 4.6536 [10.53913140296936 sec]
step 100: train loss 2.4801, val loss 2.5620 [29.41845989227295 sec]
step 200: train loss 2.3060, val loss 2.4204 [48.22600221633911 sec]
step 300: train loss 2.0638, val loss 2.2239 [67.02992296218872 sec]
step 400: train loss 1.8895, val loss 2.0650 [85.85576438903809 sec]
1.8409970998764038
Total Training Time: 94.2408378124237 seconds

intDrowe and beforeculed the I maven-. "He cold poy all sleftackmined on
to the camp, affay the guac
BEGINNING (1681884129.0725915): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6145, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6243, val loss 4.6225 [6.287346124649048 sec]
step 100: train loss 2.5513, val loss 2.6222 [17.34376883506775 sec]
step 200: train loss 2.4440, val loss 2.5374 [28.404081344604492 sec]
step 300: train loss 2.3576, val loss 2.4664 [39.47933506965637 sec]
step 400: train loss 2.2425, val loss 2.3656 [50.54985690116882 sec]
2.174611806869507
Total Training Time: 55.322561740875244 seconds

cutaned thom whey youd be the nene citk
ausmert ae tthe Whe Y
bour her, ee al nourd hed hince durey,
BEGINNING (1681884185.3243008): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6878, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6778, val loss 4.6684 [11.659438371658325 sec]
step 100: train loss 2.5144, val loss 2.5969 [32.04119157791138 sec]
step 200: train loss 2.4270, val loss 2.5184 [52.49269723892212 sec]
step 300: train loss 2.3011, val loss 2.4093 [72.77726674079895 sec]
step 400: train loss 2.0997, val loss 2.2470 [93.08632254600525 sec]
1.9867558479309082
Total Training Time: 101.90585899353027 seconds

home thooked agon PI – Mither aw sconesenec
tughe be fowh the Pyrofirintars inter was. Gratta sunked
BEGINNING (1681884288.865577): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5723, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5673, val loss 4.5678 [16.972554445266724 sec]
step 100: train loss 2.5018, val loss 2.5817 [45.90243983268738 sec]
step 200: train loss 2.3986, val loss 2.4935 [74.605712890625 sec]
step 300: train loss 2.2066, val loss 2.3304 [103.50067567825317 sec]
step 400: train loss 1.9887, val loss 2.1589 [132.52353858947754 sec]
1.9067277908325195
Total Training Time: 144.9194803237915 seconds

quwalef breste sto lawame so gok. He
moubrwouth en lsowack forre body yought and, andf isht kipsin
g
BEGINNING (1681884436.1470578): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6317, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6262, val loss 4.6249 [3.4956254959106445 sec]
step 100: train loss 2.5034, val loss 2.5832 [9.428792238235474 sec]
step 200: train loss 2.2178, val loss 2.3356 [15.503043174743652 sec]
step 300: train loss 2.0329, val loss 2.1921 [21.530130863189697 sec]
step 400: train loss 1.9158, val loss 2.0957 [27.473552227020264 sec]
1.9755768775939941
Total Training Time: 29.961188077926636 seconds

wer bre den, the wares sompor oun ward from the athe
had in the of huand not and woupy but This sie 
BEGINNING (1681884466.9004982): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5243, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5158, val loss 4.5153 [5.963163137435913 sec]
step 100: train loss 2.4488, val loss 2.5353 [16.465864419937134 sec]
step 200: train loss 2.1646, val loss 2.3061 [26.9770348072052 sec]
step 300: train loss 1.9569, val loss 2.1345 [37.533355951309204 sec]
step 400: train loss 1.8206, val loss 2.0151 [48.01017427444458 sec]
1.7189126014709473
Total Training Time: 52.764591217041016 seconds

CHAPTE7
CONL KIxI Triesquares in iftes out for
at one Thad deet. Whe costripsing for and We fave the
BEGINNING (1681884521.224692): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5451, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5542, val loss 4.5597 [8.46007513999939 sec]
step 100: train loss 2.4423, val loss 2.5264 [23.609890460968018 sec]
step 200: train loss 2.1607, val loss 2.2975 [38.72835326194763 sec]
step 300: train loss 1.9354, val loss 2.1161 [53.83558630943298 sec]
step 400: train loss 1.7948, val loss 1.9916 [69.44696354866028 sec]
1.6983397006988525
Total Training Time: 76.22919368743896 seconds

crpleby.'1 Take the scafter an to at Tweith Elyon of a dongarrangesed, "Taka 't rembeach
shack at ai
BEGINNING (1681884599.772105): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5665, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5452, val loss 4.5460 [4.925645589828491 sec]
step 100: train loss 2.5232, val loss 2.6048 [13.433360815048218 sec]
step 200: train loss 2.3423, val loss 2.4394 [21.896332263946533 sec]
step 300: train loss 2.1194, val loss 2.2611 [30.36286425590515 sec]
step 400: train loss 1.9570, val loss 2.1352 [38.91615390777588 sec]
1.9252419471740723
Total Training Time: 42.485528230667114 seconds

iricty to furnet Gration tatlatta/
Aidd.
Hn the scightas, had a lookk heard
his him. The dee Turng a
BEGINNING (1681884643.0555565): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5914, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5939, val loss 4.5899 [8.88766360282898 sec]
step 100: train loss 2.4946, val loss 2.5714 [24.429452896118164 sec]
step 200: train loss 2.2987, val loss 2.4037 [39.956884145736694 sec]
step 300: train loss 2.0533, val loss 2.1995 [55.49555778503418 sec]
step 400: train loss 1.8591, val loss 2.0531 [71.06543803215027 sec]
1.8101434707641602
Total Training Time: 77.87085175514221 seconds

with th usened too sold." That so
rel the dep Gratta a's sa dopled hos any carne to his the
ChieMcAR
BEGINNING (1681884722.47497): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5542, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5597, val loss 4.5556 [12.667423009872437 sec]
step 100: train loss 2.4787, val loss 2.5652 [35.28322792053223 sec]
step 200: train loss 2.2861, val loss 2.4012 [57.89924669265747 sec]
step 300: train loss 2.0041, val loss 2.1765 [80.54751181602478 sec]
step 400: train loss 1.8218, val loss 2.0127 [103.17773270606995 sec]
1.7074499130249023
Total Training Time: 113.1394271850586 seconds

the als knot amp on Mu theaNaY
Ev CTOR A twe coulded Gor the juves Glansta mete
fored him deated. "W
BEGINNING (1681884837.9221911): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6359, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6327, val loss 4.6318 [7.72922420501709 sec]
step 100: train loss 2.5393, val loss 2.6116 [21.3633930683136 sec]
step 200: train loss 2.4307, val loss 2.5158 [34.89313530921936 sec]
step 300: train loss 2.3176, val loss 2.4253 [48.494943380355835 sec]
step 400: train loss 2.1570, val loss 2.2891 [62.063108682632446 sec]
2.0938475131988525
Total Training Time: 67.88983941078186 seconds

ut fit age with. Yo hup moulleing he warter cre lote
soons futogh forreard empt amy. Thes sughe sati
BEGINNING (1681884906.609558): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5931, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5937, val loss 4.5858 [14.266981601715088 sec]
step 100: train loss 2.5129, val loss 2.5901 [37.85796785354614 sec]
step 200: train loss 2.3999, val loss 2.4972 [61.471959829330444 sec]
step 300: train loss 2.2251, val loss 2.3469 [85.40590524673462 sec]
step 400: train loss 2.0038, val loss 2.1794 [109.1525206565857 sec]
1.9127739667892456
Total Training Time: 119.3112723827362 seconds

cont im. Thef his waurgs. I ubare shalch the noorsed.
Nareimeding. "Shat maed wats saick to to my a 
BEGINNING (1681885027.736444): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6391, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6413, val loss 4.6492 [21.168102502822876 sec]
step 100: train loss 2.5012, val loss 2.5844 [55.078835248947144 sec]
step 200: train loss 2.3805, val loss 2.4748 [88.84923958778381 sec]
step 300: train loss 2.1549, val loss 2.2920 [122.62171339988708 sec]
step 400: train loss 1.9384, val loss 2.1267 [156.31638836860657 sec]
1.8520604372024536
Total Training Time: 170.71474933624268 seconds

fCCER – VII She HES McKLACLakneed to range Efly to
sme, 9He 1
Senut, the citis blet the ristre farle
BEGINNING (1681885200.7787368): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5629, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5596, val loss 4.5553 [3.9547698497772217 sec]
step 100: train loss 2.4800, val loss 2.5605 [11.437759160995483 sec]
step 200: train loss 2.2063, val loss 2.3325 [18.52797818183899 sec]
step 300: train loss 2.0174, val loss 2.1830 [25.586734294891357 sec]
step 400: train loss 1.9173, val loss 2.1226 [32.70494031906128 sec]
1.8735922574996948
Total Training Time: 35.892070293426514 seconds

reals be willook turn hist forden erubs. Andzer home yonging –
hiper downgrhapprationtiong y had
the
BEGINNING (1681885237.8658874): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5950, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5939, val loss 4.5905 [7.468441724777222 sec]
step 100: train loss 2.4583, val loss 2.5463 [20.771989583969116 sec]
step 200: train loss 2.1893, val loss 2.3190 [34.001649618148804 sec]
step 300: train loss 1.9695, val loss 2.1403 [47.19155526161194 sec]
step 400: train loss 1.8408, val loss 2.0380 [60.31367492675781 sec]
1.8614948987960815
Total Training Time: 66.32777404785156 seconds

beskwhed, "Beraikin,"
Gratta salvood his by he oundning. In canse
with encubs the rawaigh. HE – VI –
BEGINNING (1681885306.3507042): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5938, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5779, val loss 4.5825 [10.785232543945312 sec]
step 100: train loss 2.4652, val loss 2.5514 [30.148409366607666 sec]
step 200: train loss 2.1731, val loss 2.3189 [49.447012424468994 sec]
step 300: train loss 1.9826, val loss 2.1733 [68.86082100868225 sec]
step 400: train loss 1.8251, val loss 2.0411 [88.30067729949951 sec]
1.8433895111083984
Total Training Time: 97.2318639755249 seconds

evong that milines. Arphad.". Thes will quon Na
way bat bexchentuons to lat. "I two mew, anthiss Gua
BEGINNING (1681885406.7736564): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5808, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5897, val loss 4.5863 [4.950960159301758 sec]
step 100: train loss 2.5006, val loss 2.5875 [14.085217714309692 sec]
step 200: train loss 2.3305, val loss 2.4387 [23.233279705047607 sec]
step 300: train loss 2.0887, val loss 2.2358 [32.3531551361084 sec]
step 400: train loss 1.9260, val loss 2.1028 [41.610262870788574 sec]
1.8927340507507324
Total Training Time: 45.79554271697998 seconds

stoned. If trat wing tuon mand arly donk
bing rifge, "HiV "I SE TERAPTE Kinat I THAPTHE OHAP22
OR• O
BEGINNING (1681885453.7157907): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6867, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6808, val loss 4.6745 [9.282041549682617 sec]
step 100: train loss 2.4752, val loss 2.5684 [26.638690948486328 sec]
step 200: train loss 2.2917, val loss 2.4165 [44.0701162815094 sec]
step 300: train loss 2.0303, val loss 2.1937 [61.50871443748474 sec]
step 400: train loss 1.8471, val loss 2.0329 [78.87610006332397 sec]
1.786658763885498
Total Training Time: 86.94366550445557 seconds

the pece a his was be the donciFlooned his
callste and had meke troach a fasents ofurd ly Pyrecha am
BEGINNING (1681885542.8032014): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6469, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6518, val loss 4.6573 [13.59010362625122 sec]
step 100: train loss 2.4876, val loss 2.5770 [39.110098123550415 sec]
step 200: train loss 2.2939, val loss 2.4086 [64.56647181510925 sec]
step 300: train loss 2.0451, val loss 2.2108 [90.04907584190369 sec]
step 400: train loss 1.8405, val loss 2.0617 [115.7723343372345 sec]
1.8498984575271606
Total Training Time: 127.67958283424377 seconds

lagated at him sat was shenerped.
He paw had cribt. I
Smell!" Gratta mokeded and athe daty gand smil
BEGINNING (1681885673.7060094): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6384, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6390, val loss 4.6465 [8.018235206604004 sec]
step 100: train loss 2.5149, val loss 2.6054 [22.357527017593384 sec]
step 200: train loss 2.4102, val loss 2.5073 [36.712116956710815 sec]
step 300: train loss 2.2893, val loss 2.4014 [51.04180932044983 sec]
step 400: train loss 2.0939, val loss 2.2561 [65.3859510421753 sec]
2.0206661224365234
Total Training Time: 71.71421265602112 seconds

ased ithta, "Whe Gratta theyan spray whed sold houn the
caups, ing wart tha co fo; fiesh! Namad
pafo
BEGINNING (1681885746.5408258): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5917, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5935, val loss 4.5816 [15.273309230804443 sec]
step 100: train loss 2.4962, val loss 2.5779 [42.18150806427002 sec]
step 200: train loss 2.3832, val loss 2.4875 [68.71889114379883 sec]
step 300: train loss 2.1615, val loss 2.2981 [95.38673806190491 sec]
step 400: train loss 1.9459, val loss 2.1240 [121.77428865432739 sec]
1.850056529045105
Total Training Time: 133.2970221042633 seconds

Arperne adend to to the tunne com."
Grutta farroubs ist, urne jun Twake spitiore.
"The saulllied Tak
BEGINNING (1681885882.0228074): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6014, val loss 4.5967 [21.850664138793945 sec]
step 100: train loss 2.4976, val loss 2.5786 [56.55272126197815 sec]
step 200: train loss 2.3927, val loss 2.4998 [90.45979356765747 sec]
step 300: train loss 2.1763, val loss 2.3060 [124.57138347625732 sec]
step 400: train loss 1.9165, val loss 2.1066 [158.625337600708 sec]
1.8377783298492432
Total Training Time: 174.17462253570557 seconds

willanqy had slooune to will whe ouldso." Gratta ligg bou himse
suidghorf il-I but well arritn'track
BEGINNING (1681886059.6440535): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6198, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6020, val loss 4.5990 [4.411972284317017 sec]
step 100: train loss 2.4392, val loss 2.5255 [12.434565544128418 sec]
step 200: train loss 2.1422, val loss 2.2750 [20.476367235183716 sec]
step 300: train loss 1.9602, val loss 2.1362 [28.519944429397583 sec]
step 400: train loss 1.8379, val loss 2.0331 [36.55883598327637 sec]
1.7907105684280396
Total Training Time: 40.158952951431274 seconds

ang grib, "Yroun." Anah and
Chan yelled Gratta to the mounnir
justan, Fgo hill guth the moged is sjo
BEGINNING (1681886100.9433243): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6264, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6374, val loss 4.6385 [8.217581272125244 sec]
step 100: train loss 2.4074, val loss 2.4969 [23.18026113510132 sec]
step 200: train loss 2.0780, val loss 2.2335 [38.106560945510864 sec]
step 300: train loss 1.8866, val loss 2.0964 [53.07537055015564 sec]
step 400: train loss 1.7623, val loss 1.9802 [68.08013796806335 sec]
1.7176812887191772
Total Training Time: 75.1156120300293 seconds

that bethis intrey he were. God tOme otho ten the can turplief. The garmil Gratta know. Le scart as 
BEGINNING (1681886178.171536): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5633, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5423, val loss 4.5368 [11.686245441436768 sec]
step 100: train loss 2.4254, val loss 2.5241 [33.74101781845093 sec]
step 200: train loss 2.1189, val loss 2.2548 [55.72823977470398 sec]
step 300: train loss 1.8882, val loss 2.0759 [77.82583856582642 sec]
step 400: train loss 1.7604, val loss 1.9848 [99.84437561035156 sec]
1.7273870706558228
Total Training Time: 110.16691708564758 seconds

Aida. "Did And backed you be a do thertarels aftiets. Baw
Councer it Hig to the gright thors.
Zestra
BEGINNING (1681886291.573031): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6306, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6334, val loss 4.6281 [6.443780422210693 sec]
step 100: train loss 2.4814, val loss 2.5599 [18.535855770111084 sec]
step 200: train loss 2.2633, val loss 2.3728 [31.11325430870056 sec]
step 300: train loss 2.0092, val loss 2.1718 [42.94815158843994 sec]
step 400: train loss 1.8631, val loss 2.0750 [54.62364864349365 sec]
1.8147317171096802
Total Training Time: 59.75923752784729 seconds

therof Grata sack.
He care nodd pecursew, ago tain fend hemyluat
to – Sous supe rigets preas trepwed
BEGINNING (1681886352.491032): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6613, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6572, val loss 4.6619 [12.152997493743896 sec]
step 100: train loss 2.4542, val loss 2.5434 [34.13646483421326 sec]
step 200: train loss 2.1920, val loss 2.3239 [56.11461567878723 sec]
step 300: train loss 1.9251, val loss 2.1156 [78.02772355079651 sec]
step 400: train loss 1.7616, val loss 1.9900 [100.00622749328613 sec]
1.721246361732483
Total Training Time: 109.79568195343018 seconds

"Yes." Ally to up non the way's clanly torthere fror
had hinge, and returang tillath miletiach his m
BEGINNING (1681886464.5285795): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6082, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6081, val loss 4.6089 [17.797044277191162 sec]
step 100: train loss 2.4623, val loss 2.5485 [48.65404415130615 sec]
step 200: train loss 2.2272, val loss 2.3541 [79.16293931007385 sec]
step 300: train loss 1.9179, val loss 2.1097 [110.03727912902832 sec]
step 400: train loss 1.7332, val loss 1.9708 [140.62991666793823 sec]
1.6683586835861206
Total Training Time: 154.25866961479187 seconds

agaifE pplied saed, neare to und him Grattled. We farta
the knows feoo the Tuon "
Te oncoubit the qu
BEGINNING (1681886622.1766982): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6664, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6773, val loss 4.6713 [10.704884052276611 sec]
step 100: train loss 2.5037, val loss 2.5837 [29.21820569038391 sec]
step 200: train loss 2.3929, val loss 2.4911 [47.763928174972534 sec]
step 300: train loss 2.2248, val loss 2.3489 [66.42519664764404 sec]
step 400: train loss 2.0142, val loss 2.1786 [84.99323654174805 sec]
1.9287272691726685
Total Training Time: 93.01539063453674 seconds

zus to I macly the arparay pling towaiery for non. Elull
the pew derenoown the lle thimyans illont f
BEGINNING (1681886716.3576682): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6398, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6396, val loss 4.6312 [20.029428482055664 sec]
step 100: train loss 2.4807, val loss 2.5690 [52.139944314956665 sec]
step 200: train loss 2.3379, val loss 2.4475 [84.40016055107117 sec]
step 300: train loss 2.0476, val loss 2.2157 [116.88325667381287 sec]
step 400: train loss 1.8371, val loss 2.0422 [149.28076672554016 sec]
1.8499431610107422
Total Training Time: 163.7728977203369 seconds

pereb; wariy, ang Yah margesyelf the mus not but. We agre sway
orre wit care op ae fr. I milefulrn t
BEGINNING (1681886882.465139): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5536, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5563, val loss 4.5540 [27.070622444152832 sec]
step 100: train loss 2.4799, val loss 2.5632 [71.11906504631042 sec]
step 200: train loss 2.3590, val loss 2.4554 [115.21946144104004 sec]
step 300: train loss 2.0772, val loss 2.2293 [159.2090344429016 sec]
step 400: train loss 1.8313, val loss 2.0331 [203.25756406784058 sec]
1.786211371421814
Total Training Time: 223.82774806022644 seconds

NAY
Scubn withe.' AQ melo evers, to und aray pentings a padst
goldsalle coment, of a sneair there ra
BEGINNING (1681887109.9009347): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6006, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6249, val loss 4.6184 [5.296110153198242 sec]
step 100: train loss 2.4080, val loss 2.4951 [14.989163160324097 sec]
step 200: train loss 2.0933, val loss 2.2359 [24.396286487579346 sec]
step 300: train loss 1.9131, val loss 2.0968 [38.12000846862793 sec]
step 400: train loss 1.7852, val loss 1.9851 [47.46086764335632 sec]
1.7370338439941406
Total Training Time: 51.65804982185364 seconds

with Pyrran
Kav Anayah evight, hor cabut dired siver iffent has cor of
the even compicris, the ens y
BEGINNING (1681887162.723496): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5780, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5800, val loss 4.5783 [9.444086074829102 sec]
step 100: train loss 2.3788, val loss 2.4791 [26.903460264205933 sec]
step 200: train loss 2.0300, val loss 2.2009 [44.36240100860596 sec]
step 300: train loss 1.8257, val loss 2.0293 [61.87650465965271 sec]
step 400: train loss 1.6922, val loss 1.9358 [79.36918234825134 sec]
1.7452850341796875
Total Training Time: 87.41085886955261 seconds

thems hat ant wounds him partaine
lards?"
50
CEN PTEAPTEKj RR McKAY
gales did stook, humans's Pyrrac
BEGINNING (1681887252.4113593): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6464, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6503, val loss 4.6559 [13.658555030822754 sec]
step 100: train loss 2.3903, val loss 2.4858 [39.138904094696045 sec]
step 200: train loss 2.0392, val loss 2.1876 [64.521897315979 sec]
step 300: train loss 1.8389, val loss 2.0328 [89.87759280204773 sec]
step 400: train loss 1.6905, val loss 1.9253 [115.28829097747803 sec]
1.5552791357040405
Total Training Time: 127.017587184906 seconds

irturneser."
SANN as scoked and look."
"Cou, a the men in had slennged then sigice,
"Sir, this samy 
BEGINNING (1681887382.790251): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6078, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5996, val loss 4.6059 [8.05352234840393 sec]
step 100: train loss 2.4579, val loss 2.5557 [22.392292022705078 sec]
step 200: train loss 2.1969, val loss 2.3291 [36.78604078292847 sec]
step 300: train loss 1.9398, val loss 2.1203 [51.1941704750061 sec]
step 400: train loss 1.7911, val loss 2.0000 [65.5642569065094 sec]
1.7712751626968384
Total Training Time: 71.85109448432922 seconds

eDf mea; agate he could, nome but ase whorsen frould
hivishumon were aTn tharns. Gratta stif the sil
BEGINNING (1681887455.785146): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5796, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5814, val loss 4.5899 [15.076706886291504 sec]
step 100: train loss 2.4538, val loss 2.5495 [41.873650550842285 sec]
step 200: train loss 2.1775, val loss 2.3047 [68.57316064834595 sec]
step 300: train loss 1.8871, val loss 2.0827 [95.54899024963379 sec]
step 400: train loss 1.7025, val loss 1.9368 [122.27212166786194 sec]
1.6220972537994385
Total Training Time: 134.01023960113525 seconds

be anzinled his you.
"Cou?" Gratta afrom The ase laoked arta and.
"Kand you look to you flar the Pyr
BEGINNING (1681887592.0885313): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5791, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5732, val loss 4.5701 [22.082791566848755 sec]
step 100: train loss 2.4506, val loss 2.5413 [59.629454135894775 sec]
step 200: train loss 2.1686, val loss 2.3048 [96.18682026863098 sec]
step 300: train loss 1.8553, val loss 2.0512 [132.9450705051422 sec]
step 400: train loss 1.6684, val loss 1.9154 [169.6386914253235 sec]
1.568477988243103
Total Training Time: 185.6349527835846 seconds

PEEACE
CUD THE HHE SEAY
76
SEANS AMaKAY
Arphad was weere artighom ath. He Tras
11
71. IIN
THON McKAi
BEGINNING (1681887781.219487): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6629, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6594, val loss 4.6481 [13.326196908950806 sec]
step 100: train loss 2.4941, val loss 2.5831 [35.752976417541504 sec]
step 200: train loss 2.3644, val loss 2.4634 [58.11599922180176 sec]
step 300: train loss 2.1780, val loss 2.3201 [80.52526140213013 sec]
step 400: train loss 1.9345, val loss 2.1215 [102.90981149673462 sec]
1.8954604864120483
Total Training Time: 112.4521906375885 seconds

try shem. Ha lookiff hed the as thir!Airstied. The
750
CHAPTHE Y
CHAS. Elyah. I the was ace waos lis
BEGINNING (1681887894.8512201): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5928, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5938, val loss 4.6018 [24.307096481323242 sec]
step 100: train loss 2.4734, val loss 2.5551 [62.767849922180176 sec]
step 200: train loss 2.3234, val loss 2.4364 [100.81347680091858 sec]
step 300: train loss 2.0225, val loss 2.1879 [138.97345638275146 sec]
step 400: train loss 1.8011, val loss 2.0094 [177.13778233528137 sec]
1.7182841300964355
Total Training Time: 194.59299182891846 seconds

IVI ArEl onon spsho couter unthing at;
patted. "Yor ushes. jumps relled by to doke the hat as
necoS 
BEGINNING (1681888091.9322524): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6222, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5904, val loss 4.5917 [1.0042150020599365 sec]
step 100: train loss 3.5218, val loss 3.5383 [2.629784107208252 sec]
step 200: train loss 3.2048, val loss 3.2227 [4.2663421630859375 sec]
step 300: train loss 3.0765, val loss 3.1030 [5.863402843475342 sec]
step 400: train loss 2.9609, val loss 3.0094 [7.48634934425354 sec]
3.041511058807373
Total Training Time: 8.099068403244019 seconds

Ile
s wosh pa6io
uiuzoranp t h nikis“ t Fpe t
inarngEsg
ind F 8otYnr%init nYndaX.se nhegyNo
a
tamift
BEGINNING (1681888100.2653284): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6044, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6148, val loss 4.6139 [1.5285191535949707 sec]
step 100: train loss 3.4117, val loss 3.4289 [3.993687868118286 sec]
step 200: train loss 3.1437, val loss 3.1678 [6.44044041633606 sec]
step 300: train loss 2.9844, val loss 3.0145 [8.899635791778564 sec]
step 400: train loss 2.8737, val loss 2.9056 [11.367406368255615 sec]
2.871844530105591
Total Training Time: 12.340407371520996 seconds

hR.emaata mkof oewrOgilyp ALr  
E.4 I Tdis h anelh.efo  to
"abandee hite w a he, I vreuothcooU hsttd
BEGINNING (1681888112.9857268): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5917, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5886, val loss 4.5827 [2.0058176517486572 sec]
step 100: train loss 3.3650, val loss 3.3815 [5.289907932281494 sec]
step 200: train loss 3.1029, val loss 3.1313 [8.543559074401855 sec]
step 300: train loss 2.9259, val loss 2.9734 [11.852183103561401 sec]
step 400: train loss 2.8176, val loss 2.8615 [15.057409286499023 sec]
2.730090856552124
Total Training Time: 16.45516800880432 seconds

Gras f g. wtureor ToTisrof-owants thl y tEurbonUM !es aZ s aE?wluie TT
th.thaniga B tha"fe DL ;andvd
BEGINNING (1681888129.9780781): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5880, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5842, val loss 4.5887 [1.0346319675445557 sec]
step 100: train loss 3.4843, val loss 3.5048 [2.666625499725342 sec]
step 200: train loss 3.1872, val loss 3.2166 [4.333545684814453 sec]
step 300: train loss 3.0524, val loss 3.0804 [5.9823901653289795 sec]
step 400: train loss 2.9255, val loss 2.9688 [7.630958080291748 sec]
2.8494086265563965
Total Training Time: 8.237509489059448 seconds

oosti ea ttSs ttt ohehni9td nond m lwa (otheof
m s xerae s ods atolovamouhen co cey0k
h m.osIt a tiG
BEGINNING (1681888138.4452221): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6115, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6303, val loss 4.6316 [1.539135217666626 sec]
step 100: train loss 3.4852, val loss 3.5064 [4.022346496582031 sec]
step 200: train loss 3.1652, val loss 3.1958 [6.519584894180298 sec]
step 300: train loss 2.9759, val loss 3.0071 [8.989941358566284 sec]
step 400: train loss 2.8576, val loss 2.9081 [11.525261640548706 sec]
2.854386568069458
Total Training Time: 12.518220901489258 seconds

riXd%hO . pitoilelPou yus hed,bpim f bat
z: tuthinz/t tad thaL? ’uwathma8tahi t 8thad Gthe me Wo, ""
BEGINNING (1681888151.3454435): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5853, val loss 4.5880 [1.9492039680480957 sec]
step 100: train loss 3.3367, val loss 3.3577 [5.30808424949646 sec]
step 200: train loss 3.0513, val loss 3.0822 [8.578195810317993 sec]
step 300: train loss 2.9018, val loss 2.9333 [11.853182792663574 sec]
step 400: train loss 2.8002, val loss 2.8434 [15.156785488128662 sec]
2.778796672821045
Total Training Time: 16.51777696609497 seconds

 OkhrYaon t Gmoone ce G1phein, niles hearethd,usenpoga6ele; ?yycruslondinledonaga' heyod iun bll s",
BEGINNING (1681888168.3953867): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6703, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6804, val loss 4.6760 [1.1739082336425781 sec]
step 100: train loss 3.5487, val loss 3.5636 [3.113332986831665 sec]
step 200: train loss 3.2093, val loss 3.2272 [5.00203537940979 sec]
step 300: train loss 3.0629, val loss 3.0962 [6.909475564956665 sec]
step 400: train loss 2.9460, val loss 2.9855 [8.799789905548096 sec]
2.8699533939361572
Total Training Time: 9.542333364486694 seconds

  Eo"t
anrennth? t. hasag wcaawnld peUne"dinOryuefleGweg
Bir esdonseresreanar mm;or ;ae
lica.
vletca
BEGINNING (1681888178.15972): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6141, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6159, val loss 4.6141 [1.69024658203125 sec]
step 100: train loss 3.4393, val loss 3.4549 [4.509552955627441 sec]
step 200: train loss 3.1203, val loss 3.1470 [7.338542699813843 sec]
step 300: train loss 2.9577, val loss 2.9872 [10.154207944869995 sec]
step 400: train loss 2.8601, val loss 2.8985 [13.025983095169067 sec]
2.7440030574798584
Total Training Time: 14.183164358139038 seconds

m Uus ranlef h trare Hfen R olginod
hey iminthand 7hrethcro abs eCalonton:g 7ofrkad aco Tis d cmu. c
BEGINNING (1681888192.7188919): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6318, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6378, val loss 4.6456 [2.20243501663208 sec]
step 100: train loss 3.3324, val loss 3.3529 [5.975532531738281 sec]
step 200: train loss 3.0457, val loss 3.0722 [9.763567924499512 sec]
step 300: train loss 2.8878, val loss 2.9288 [13.559805870056152 sec]
step 400: train loss 2.8063, val loss 2.8483 [17.357452154159546 sec]
2.792630672454834
Total Training Time: 18.971614599227905 seconds

Saneag ct%encuuulll toiatas "~PpsnfI ad he.une f n wsfoun an" w-ib6ded.ibs t Me bat Cbec
Rin omire V
BEGINNING (1681888212.2151): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5528, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5445, val loss 4.5463 [1.158660650253296 sec]
step 100: train loss 3.4776, val loss 3.4994 [2.9789843559265137 sec]
step 200: train loss 3.1876, val loss 3.2084 [4.752984046936035 sec]
step 300: train loss 3.0658, val loss 3.0920 [6.570169448852539 sec]
step 400: train loss 2.9462, val loss 2.9788 [8.364977598190308 sec]
2.8847944736480713
Total Training Time: 8.998018026351929 seconds

YE r de wi,4 ugD Chiunegra©athldwse ontEtungrel died om unt)tueaOrfurte."tl Heh TyedhimcnEO –rh oDgr
BEGINNING (1681888221.435046): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5767, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5671, val loss 4.5729 [1.6198022365570068 sec]
step 100: train loss 3.3913, val loss 3.4114 [4.224663496017456 sec]
step 200: train loss 3.1444, val loss 3.1683 [6.8138251304626465 sec]
step 300: train loss 2.9670, val loss 2.9972 [9.417785406112671 sec]
step 400: train loss 2.8444, val loss 2.8820 [11.987943887710571 sec]
2.7179365158081055
Total Training Time: 13.030157804489136 seconds

GpTan apttjo nGselm thites adio d ate t !tyatore Tl" 1ithe rere˜ wir
wiMd wan sa haswn s. Gord sSvaO
BEGINNING (1681888234.8423052): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6935, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7057, val loss 4.7011 [2.022721529006958 sec]
step 100: train loss 3.3823, val loss 3.4061 [5.465761184692383 sec]
step 200: train loss 3.1134, val loss 3.1421 [8.853116512298584 sec]
step 300: train loss 2.9078, val loss 2.9571 [12.23287844657898 sec]
step 400: train loss 2.7876, val loss 2.8319 [15.674617290496826 sec]
2.717644453048706
Total Training Time: 17.052141666412354 seconds

Pwtderigurt f fH/ wou8rdd wer ssont o tuged 7, ole n L
dey afoailll/ assch h9 n. ba"r B un˜od Tlgn
b
BEGINNING (1681888252.4149814): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6226, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6267, val loss 4.6348 [1.2364118099212646 sec]
step 100: train loss 3.5625, val loss 3.5739 [3.1185998916625977 sec]
step 200: train loss 3.1930, val loss 3.2128 [4.995121002197266 sec]
step 300: train loss 3.0357, val loss 3.0627 [6.946542024612427 sec]
step 400: train loss 2.9214, val loss 2.9575 [8.833908319473267 sec]
2.856574296951294
Total Training Time: 9.513848543167114 seconds

aTo ofe s Granalida ulusif mO
he nEhar awa. khe ynf onens
lsac2
TiW an"qaupuu0J
rHsp W“JThedaed whar
BEGINNING (1681888262.1508293): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5848, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6029, val loss 4.5987 [1.6842703819274902 sec]
step 100: train loss 3.4081, val loss 3.4173 [4.41631007194519 sec]
step 200: train loss 3.1169, val loss 3.1334 [7.13051438331604 sec]
step 300: train loss 2.9407, val loss 2.9709 [9.857691049575806 sec]
step 400: train loss 2.8336, val loss 2.8828 [12.581050395965576 sec]
2.759263277053833
Total Training Time: 13.639033794403076 seconds

o1 ti fd aonand. orenatt oad hayurlfme n'-ld, cingiinet veC a He tokinlamkn thall a2
kwenalond tut"-
BEGINNING (1681888276.1719291): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5790, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5737, val loss 4.5785 [2.106631278991699 sec]
step 100: train loss 3.3651, val loss 3.3903 [5.683145523071289 sec]
step 200: train loss 3.0692, val loss 3.1039 [9.278858184814453 sec]
step 300: train loss 2.8979, val loss 2.9373 [12.848600149154663 sec]
step 400: train loss 2.7949, val loss 2.8435 [16.391499519348145 sec]
2.762169599533081
Total Training Time: 17.862804889678955 seconds

CtMebed lo a touillHeld, wdethoumEnis ttanca 5datui s
ZtzN een Iiltae
rZece
Gouthe€trpe tine,o liler
BEGINNING (1681888294.5457158): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.7127, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7221, val loss 4.7279 [1.4363608360290527 sec]
step 100: train loss 3.5553, val loss 3.5815 [3.7237539291381836 sec]
step 200: train loss 3.2038, val loss 3.2356 [6.0301408767700195 sec]
step 300: train loss 3.0601, val loss 3.0926 [8.319920301437378 sec]
step 400: train loss 2.9371, val loss 2.9727 [10.624046802520752 sec]
2.9103477001190186
Total Training Time: 11.485556602478027 seconds

aep s.inmUoa T€nfn ore che Ltejjricd, f cmhe d t metE
l antaC
sg)im
ro~ge G ouinaseronitubomht©d mhh
BEGINNING (1681888306.2562773): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6100, val loss 4.6071 [2.0525436401367188 sec]
step 100: train loss 3.3847, val loss 3.4008 [5.500057697296143 sec]
step 200: train loss 3.0970, val loss 3.1215 [8.88961410522461 sec]
step 300: train loss 2.9370, val loss 2.9663 [12.277647018432617 sec]
step 400: train loss 2.8370, val loss 2.8757 [15.68568730354309 sec]
2.7666966915130615
Total Training Time: 17.033902406692505 seconds

ndeg warsr."H.. tou cwa,inducwe n5d :"
"en
the aail G.. Gonlin ast bam""T,n%ctatralerpnc: wivn/ he "
BEGINNING (1681888323.6768272): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6316, val loss 4.6348 [2.6571004390716553 sec]
step 100: train loss 3.4394, val loss 3.4533 [7.179653167724609 sec]
step 200: train loss 3.0927, val loss 3.1187 [11.709537506103516 sec]
step 300: train loss 2.9203, val loss 2.9501 [16.222022771835327 sec]
step 400: train loss 2.8111, val loss 2.8502 [20.7519268989563 sec]
2.7647244930267334
Total Training Time: 22.59431219100952 seconds

A,LVhe t bdtDhi
il fes.Iu7 s o'f thye id re h aleoratindE
n thabsgiile iE
5o ce
AelC
Au Ua.5
Heu!ean
BEGINNING (1681888346.7941363): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5991, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5938, val loss 4.5897 [1.3689985275268555 sec]
step 100: train loss 3.5391, val loss 3.5543 [3.412416458129883 sec]
step 200: train loss 3.1685, val loss 3.1968 [5.375974178314209 sec]
step 300: train loss 3.0175, val loss 3.0446 [7.34267258644104 sec]
step 400: train loss 2.8788, val loss 2.9225 [9.32428789138794 sec]
2.826310396194458
Total Training Time: 10.001381635665894 seconds

nd miaLamedb auovonvhamke te l gpwad .
. ;en 5sas f acnl˜athof. b" om s NanhoygaaiadjTondBg0s rooZ x
BEGINNING (1681888357.017166): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5668, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5674, val loss 4.5696 [1.7564284801483154 sec]
step 100: train loss 3.3796, val loss 3.4153 [4.542952299118042 sec]
step 200: train loss 3.1250, val loss 3.1630 [7.336152076721191 sec]
step 300: train loss 2.9314, val loss 2.9650 [10.121071815490723 sec]
step 400: train loss 2.8056, val loss 2.8574 [12.942065715789795 sec]
2.7308874130249023
Total Training Time: 13.98756456375122 seconds

s f
s
HrlmakedlmP, wia !thaasutasld hiniAn  utta s
oNpo6oX0he s" miclas wo tuntbeopkan thid hi• Jre.
BEGINNING (1681888371.37126): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6051, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5921, val loss 4.6042 [2.1636927127838135 sec]
step 100: train loss 3.3351, val loss 3.3591 [5.7655720710754395 sec]
step 200: train loss 3.0477, val loss 3.0689 [9.375688552856445 sec]
step 300: train loss 2.8818, val loss 2.9139 [12.95415210723877 sec]
step 400: train loss 2.7694, val loss 2.8129 [16.51862597465515 sec]
2.6698098182678223
Total Training Time: 17.96049165725708 seconds

AbacY h1y HofY
llonghh B%mey r tt, stheeassiy he. 7henanr 7a amiod, a ' ao
AwttisenCay gAle Cudire
E
BEGINNING (1681888389.8618906): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.7036, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7056, val loss 4.7071 [1.4096715450286865 sec]
step 100: train loss 3.5783, val loss 3.5987 [3.597900867462158 sec]
step 200: train loss 3.2107, val loss 3.2334 [5.818983554840088 sec]
step 300: train loss 3.0386, val loss 3.0615 [8.014490127563477 sec]
step 400: train loss 2.9182, val loss 2.9417 [10.228209972381592 sec]
2.855658531188965
Total Training Time: 11.031356811523438 seconds

la zj lusen ytI cs %bitMe taC©ou ak
tonBavW glae bo’ fae sin ttaomonnc G.Zd m wiwaioneuon âmta p w m
BEGINNING (1681888401.1242542): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6058, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6181, val loss 4.6100 [1.8951873779296875 sec]
step 100: train loss 3.3632, val loss 3.3817 [4.99405837059021 sec]
step 200: train loss 3.1015, val loss 3.1234 [8.099153518676758 sec]
step 300: train loss 2.9448, val loss 2.9626 [11.265925884246826 sec]
step 400: train loss 2.8403, val loss 2.8698 [14.70569372177124 sec]
2.784578323364258
Total Training Time: 15.899226903915405 seconds

need harzy taond
ed cuy
cs "th3 he eayrwrutiabns. aledeehan2w(
o"ed se obameve n waEe~ kanos cenade7
BEGINNING (1681888417.4233787): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6440, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6319, val loss 4.6375 [2.3776183128356934 sec]
step 100: train loss 3.4087, val loss 3.4309 [6.340575695037842 sec]
step 200: train loss 3.0879, val loss 3.1194 [10.319688081741333 sec]
step 300: train loss 2.9057, val loss 2.9459 [14.306543111801147 sec]
step 400: train loss 2.7954, val loss 2.8463 [18.279500722885132 sec]
2.7141337394714355
Total Training Time: 19.869619607925415 seconds

Rndan;la I
"
Ar he thesig3t7 Two4retad;ar uof wdon hes
sofd~ hitE"the t ad.
elln lere Taqs ald tu".o
BEGINNING (1681888437.8355172): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6034, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5992, val loss 4.5977 [1.7433509826660156 sec]
step 100: train loss 3.5197, val loss 3.5469 [4.450811147689819 sec]
step 200: train loss 3.1680, val loss 3.2042 [7.173425674438477 sec]
step 300: train loss 3.0058, val loss 3.0391 [9.895918846130371 sec]
step 400: train loss 2.8952, val loss 2.9326 [12.606083154678345 sec]
2.851444959640503
Total Training Time: 13.59567403793335 seconds

isontrinoory op ty, bis i"Ged–at tolban. rny tsthdz serhelas p
"upawau d Ad;l t nrs N s sEaAewd riac
BEGINNING (1681888451.6505868): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5773, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5565, val loss 4.5591 [2.458240032196045 sec]
step 100: train loss 3.3496, val loss 3.3715 [6.445610523223877 sec]
step 200: train loss 3.1198, val loss 3.1463 [10.439725399017334 sec]
step 300: train loss 2.9528, val loss 2.9884 [14.4288809299469 sec]
step 400: train loss 2.8482, val loss 2.8852 [18.43033766746521 sec]
2.7763123512268066
Total Training Time: 20.079222440719604 seconds

tlenr,r sine d thenSksFti utheodsovinR rs, G areerpe vstouyalm, her cdhIlneolillp8hen.se l9.5 cedal 
BEGINNING (1681888472.0898092): Baseline LR(0.0001) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6256, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6309, val loss 4.6378 [3.1950836181640625 sec]
step 100: train loss 3.3522, val loss 3.3734 [8.542179822921753 sec]
step 200: train loss 3.0572, val loss 3.0838 [13.87663722038269 sec]
step 300: train loss 2.9021, val loss 2.9377 [19.194945573806763 sec]
step 400: train loss 2.8054, val loss 2.8456 [24.512107610702515 sec]
2.744471311569214
Total Training Time: 26.634528160095215 seconds

w rl,7y se adewy ma
me. m y ss
w t sutayoQ Gke. yt Gnalvly or.miseld vj toz "umomM Gthne lid achene 
BEGINNING (1681888499.2572582): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6775, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6826, val loss 4.6833 [1.243255615234375 sec]
step 100: train loss 3.1434, val loss 3.1830 [3.2271909713745117 sec]
step 200: train loss 2.9029, val loss 2.9267 [5.245980978012085 sec]
step 300: train loss 2.7491, val loss 2.7989 [7.2469658851623535 sec]
step 400: train loss 2.6671, val loss 2.7169 [9.228321552276611 sec]
2.65744686126709
Total Training Time: 10.032163858413696 seconds

"•dlles. tge isero ndghsterwam."s coanpowC cosduier "n hen s tuf. wed Wed"EowieughinEee maytuQy bfr 
BEGINNING (1681888509.588891): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6742, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6739, val loss 4.6658 [1.9113447666168213 sec]
step 100: train loss 3.0886, val loss 3.1115 [5.115207195281982 sec]
step 200: train loss 2.7964, val loss 2.8389 [8.210333108901978 sec]
step 300: train loss 2.6788, val loss 2.7301 [11.420578956604004 sec]
step 400: train loss 2.5902, val loss 2.6583 [14.570610523223877 sec]
2.518418550491333
Total Training Time: 15.900490522384644 seconds

ksyaghKodag Tthon s cthjUr inglitouiveoumel a! lolF thend it. bed bfo •
0r4atak his othin amter watk
BEGINNING (1681888525.9861755): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6855, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6823, val loss 4.6833 [2.576833963394165 sec]
step 100: train loss 3.0021, val loss 3.0267 [6.945404767990112 sec]
step 200: train loss 2.7492, val loss 2.7864 [11.360295295715332 sec]
step 300: train loss 2.6283, val loss 2.6837 [15.798219919204712 sec]
step 400: train loss 2.5552, val loss 2.6189 [20.232805967330933 sec]
2.5275511741638184
Total Training Time: 22.144365310668945 seconds

ant
sin youltarehec-" bed yow. atd d ce me"rnche.
Gr’oulison be shiscg Wed sTesmameL, I woneanoud at
BEGINNING (1681888548.9265244): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6152, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6343, val loss 4.6373 [1.2620849609375 sec]
step 100: train loss 3.1221, val loss 3.1558 [3.3097827434539795 sec]
step 200: train loss 2.8561, val loss 2.9028 [5.365274667739868 sec]
step 300: train loss 2.7383, val loss 2.7876 [7.441063642501831 sec]
step 400: train loss 2.6705, val loss 2.7259 [9.511830806732178 sec]
2.5346224308013916
Total Training Time: 10.326300859451294 seconds

wheact9rm toerd Hot rw.u1 er
Ale torlly TuucfV m
om an igu, cm bo bunathLt nganen or athauy co'Acam 
BEGINNING (1681888559.5449295): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6469, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6318, val loss 4.6325 [1.9118473529815674 sec]
step 100: train loss 3.0200, val loss 3.0501 [5.120456218719482 sec]
step 200: train loss 2.7827, val loss 2.8205 [8.388237476348877 sec]
step 300: train loss 2.6732, val loss 2.7388 [11.647754907608032 sec]
step 400: train loss 2.6191, val loss 2.6817 [14.862329006195068 sec]
2.658756971359253
Total Training Time: 16.211056232452393 seconds

gone hin bauta wat TramugheI s mehilly lisuze blof aC cos3st t "nd meds r con' Ty
aringhe le iasd
me
BEGINNING (1681888576.3032658): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5942, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6155, val loss 4.6155 [2.5613276958465576 sec]
step 100: train loss 2.9776, val loss 3.0057 [7.010817766189575 sec]
step 200: train loss 2.7421, val loss 2.7777 [11.47625994682312 sec]
step 300: train loss 2.6352, val loss 2.6973 [15.933369398117065 sec]
step 400: train loss 2.5739, val loss 2.6415 [20.391992568969727 sec]
2.4900734424591064
Total Training Time: 22.262142419815063 seconds

lin. Nrted g'to alutl y  cedelase, ronded, donartey tqele tis."I inoue m
Merayatuton ys. f e Thaw h

BEGINNING (1681888599.3053155): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6399, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6537, val loss 4.6478 [1.4894063472747803 sec]
step 100: train loss 3.1007, val loss 3.1274 [3.9744226932525635 sec]
step 200: train loss 2.8358, val loss 2.8820 [6.4718053340911865 sec]
step 300: train loss 2.7349, val loss 2.7854 [8.945667266845703 sec]
step 400: train loss 2.6615, val loss 2.7179 [11.4614999294281 sec]
2.6733970642089844
Total Training Time: 12.463448762893677 seconds

cthr heston–unKuced tanitNin. actiivngao
TKWer ig henguleso t t Eel Yanape aCficid th
ooootheouus a 
BEGINNING (1681888612.1084244): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6606, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6589, val loss 4.6574 [2.365164041519165 sec]
step 100: train loss 3.0126, val loss 3.0430 [6.384049415588379 sec]
step 200: train loss 2.7849, val loss 2.8312 [10.413918495178223 sec]
step 300: train loss 2.6791, val loss 2.7382 [14.448734998703003 sec]
step 400: train loss 2.6198, val loss 2.6791 [18.47614550590515 sec]
2.6003506183624268
Total Training Time: 20.177980661392212 seconds

Akead hanI tpaind
 Aoallemilask
T˜ s ddor ss te st ish Mpan afis O.
" taeke, iserad ss hislithin˜o y
BEGINNING (1681888632.8025126): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6105, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6108, val loss 4.6089 [3.227080821990967 sec]
step 100: train loss 2.9461, val loss 2.9742 [8.776907444000244 sec]
step 200: train loss 2.7422, val loss 2.7835 [14.308302640914917 sec]
step 300: train loss 2.6461, val loss 2.7036 [19.830891132354736 sec]
step 400: train loss 2.5916, val loss 2.6494 [25.35705828666687 sec]
2.5728490352630615
Total Training Time: 27.68330717086792 seconds

ome ad
JANE ad p"Xu is hprsthaf stany ce ce JuEI t, na, s wapthe sle prepr Urol s nd arf Gr ed a ons
BEGINNING (1681888661.2296126): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6452, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6505, val loss 4.6497 [1.3866822719573975 sec]
step 100: train loss 3.1124, val loss 3.1372 [3.575129270553589 sec]
step 200: train loss 2.8610, val loss 2.8949 [5.7543628215789795 sec]
step 300: train loss 2.7299, val loss 2.7752 [7.967835187911987 sec]
step 400: train loss 2.6396, val loss 2.6906 [10.162834167480469 sec]
2.657646656036377
Total Training Time: 10.98789119720459 seconds

PKed, ouris
;n.
"utuNe rimavaed thas a  hedyougIpe ted. antGd meditre bise toneath a ldra red Theime
BEGINNING (1681888672.5137658): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6290, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6193, val loss 4.6253 [2.0097460746765137 sec]
step 100: train loss 3.0348, val loss 3.0777 [5.395047903060913 sec]
step 200: train loss 2.7582, val loss 2.8094 [8.838310956954956 sec]
step 300: train loss 2.6401, val loss 2.7065 [12.216917991638184 sec]
step 400: train loss 2.5579, val loss 2.6367 [15.630181074142456 sec]
2.5386409759521484
Total Training Time: 17.02788209915161 seconds

cwimbor."(wimatt t hen warrslewid t thimen ngout toue heaU
" le Thoupare tawiFourrr, war med ilcomas
BEGINNING (1681888690.0594056): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5480, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5476, val loss 4.5496 [2.743475914001465 sec]
step 100: train loss 2.9904, val loss 3.0141 [7.37079381942749 sec]
step 200: train loss 2.7254, val loss 2.7666 [11.944904088973999 sec]
step 300: train loss 2.6015, val loss 2.6599 [16.561638832092285 sec]
step 400: train loss 2.5070, val loss 2.5745 [21.12713098526001 sec]
2.5372371673583984
Total Training Time: 23.04185461997986 seconds

ped
zo tbon Gs“tc– corewayWsld igh0 stThiug t6"
inoll as
iful'lers lyoncurr Tghe tumkh
ussSithin pe 
BEGINNING (1681888713.8447752): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6195, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6076, val loss 4.6105 [1.4787631034851074 sec]
step 100: train loss 3.0951, val loss 3.1195 [3.837653398513794 sec]
step 200: train loss 2.8353, val loss 2.8760 [6.203591823577881 sec]
step 300: train loss 2.7138, val loss 2.7588 [8.548163652420044 sec]
step 400: train loss 2.6362, val loss 2.6973 [10.908767700195312 sec]
2.565225839614868
Total Training Time: 11.802937269210815 seconds

sof lo st mpe catexadis 
Gat T~€alorishe anthige sthiullturageve tulleg Uanl e gutan
t ho wune Che f
BEGINNING (1681888725.9379041): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5855, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5733, val loss 4.5775 [2.164578676223755 sec]
step 100: train loss 3.0311, val loss 3.0698 [5.824581861495972 sec]
step 200: train loss 2.7723, val loss 2.8287 [9.453553199768066 sec]
step 300: train loss 2.6645, val loss 2.7277 [13.083266258239746 sec]
step 400: train loss 2.5990, val loss 2.6757 [16.715068578720093 sec]
2.5809271335601807
Total Training Time: 18.199399709701538 seconds

at wan sss.
Tas inlyind awarertredhesunqeli s orr t thesasky weon8in car so oyoo tudke wer rss
belle
BEGINNING (1681888744.643338): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6209, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6135, val loss 4.6034 [2.885692596435547 sec]
step 100: train loss 2.9501, val loss 2.9758 [7.82543683052063 sec]
step 200: train loss 2.7274, val loss 2.7789 [12.741166591644287 sec]
step 300: train loss 2.6278, val loss 2.6907 [17.674073219299316 sec]
step 400: train loss 2.5590, val loss 2.6258 [22.652812480926514 sec]
2.5195209980010986
Total Training Time: 24.700095415115356 seconds

U%. Anthil Aeawy. owhe tukal hi,llead THe f AThant sins werr th wh ld. -e Imico."
"I%pereres sn
Ayo,
BEGINNING (1681888770.1034126): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5695, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5629, val loss 4.5679 [1.9151332378387451 sec]
step 100: train loss 3.0987, val loss 3.1317 [5.069262504577637 sec]
step 200: train loss 2.8315, val loss 2.8808 [8.204713106155396 sec]
step 300: train loss 2.7215, val loss 2.7804 [11.352494478225708 sec]
step 400: train loss 2.6580, val loss 2.7209 [14.496292352676392 sec]
2.62917160987854
Total Training Time: 15.725096225738525 seconds

eEono me coyhe JI at wor tythemalleins HewAne me thit Grme isHen pthid wafseraa. d cred d thoo led a
BEGINNING (1681888786.1366284): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6980, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7016, val loss 4.7038 [3.011727809906006 sec]
step 100: train loss 3.0137, val loss 3.0453 [8.090985774993896 sec]
step 200: train loss 2.7745, val loss 2.8205 [13.167891025543213 sec]
step 300: train loss 2.6695, val loss 2.7255 [18.237308025360107 sec]
step 400: train loss 2.6096, val loss 2.6734 [23.325486183166504 sec]
2.559746742248535
Total Training Time: 25.405277490615845 seconds

ECAF bed
andour topangherr Grat touiled the he nraâook,
f ifoun s rple f."
tiner mefeai˜Hs W."
tstou
BEGINNING (1681888812.084562): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6056, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6067, val loss 4.6105 [4.100558042526245 sec]
step 100: train loss 2.9702, val loss 2.9960 [11.079286575317383 sec]
step 200: train loss 2.7392, val loss 2.7879 [18.05609655380249 sec]
step 300: train loss 2.6431, val loss 2.7008 [25.033499002456665 sec]
step 400: train loss 2.5800, val loss 2.6415 [32.02162003517151 sec]
2.5852959156036377
Total Training Time: 34.90731906890869 seconds

worke, w“ogho carow hasd? lls Thal.
"'r w s tucoor tal pril
A, we Shand.
atofuade red, thilet toowr 
BEGINNING (1681888847.7557564): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6340, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6396, val loss 4.6382 [1.593982219696045 sec]
step 100: train loss 3.1222, val loss 3.1574 [4.007755517959595 sec]
step 200: train loss 2.8564, val loss 2.8932 [6.4089674949646 sec]
step 300: train loss 2.7124, val loss 2.7589 [8.807178497314453 sec]
step 400: train loss 2.6216, val loss 2.6747 [11.2227623462677 sec]
2.6683764457702637
Total Training Time: 12.097195148468018 seconds

ecin, asd c1ra'oud
bl tes
tmise Cherelshita Trorerot cton are torirping tane yor1 A Taeste ond arain
BEGINNING (1681888860.159575): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5817, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5837, val loss 4.5770 [2.1926190853118896 sec]
step 100: train loss 3.0338, val loss 3.0642 [5.851829528808594 sec]
step 200: train loss 2.7549, val loss 2.7964 [9.440930843353271 sec]
step 300: train loss 2.6247, val loss 2.6811 [13.05384874343872 sec]
step 400: train loss 2.5332, val loss 2.5905 [16.78300642967224 sec]
2.438066244125366
Total Training Time: 18.210184574127197 seconds

owarn pen d."ERnenorlofl?
S ceped Thek "˜er iralareright U"I c, Olitu© ao 6en heastheIMn)s“ Tat
pme

BEGINNING (1681888878.9247854): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6878, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6798, val loss 4.6684 [2.9051403999328613 sec]
step 100: train loss 2.9876, val loss 3.0198 [7.726259708404541 sec]
step 200: train loss 2.7178, val loss 2.7625 [12.550648927688599 sec]
step 300: train loss 2.5836, val loss 2.6476 [17.340088844299316 sec]
step 400: train loss 2.4967, val loss 2.5655 [22.164204597473145 sec]
2.44303297996521
Total Training Time: 24.10029649734497 seconds

sN/ sivam,
TM
g1"AMDNr BkilacI E miter andevelz(
I wokeagrs thacan d, ct
ra ""!Ued aenof– woratherl

BEGINNING (1681888903.8058095): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6187, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6190, val loss 4.6163 [1.735668659210205 sec]
step 100: train loss 3.1044, val loss 3.1323 [4.5060858726501465 sec]
step 200: train loss 2.8554, val loss 2.8973 [7.306964159011841 sec]
step 300: train loss 2.7129, val loss 2.7615 [10.086554050445557 sec]
step 400: train loss 2.6340, val loss 2.6937 [12.873421907424927 sec]
2.6173110008239746
Total Training Time: 13.90108847618103 seconds

a trbsasitost tnc, oEyatesls tthie.
cordt laam /t The t lllla t h a oka waEthey iim.
sevinad.EWAnd
b
BEGINNING (1681888917.9983265): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6660, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6545, val loss 4.6379 [2.5631818771362305 sec]
step 100: train loss 3.0215, val loss 3.0451 [6.781830072402954 sec]
step 200: train loss 2.7725, val loss 2.8239 [10.997167587280273 sec]
step 300: train loss 2.6596, val loss 2.7231 [15.211505889892578 sec]
step 400: train loss 2.5871, val loss 2.6544 [19.451775789260864 sec]
2.5603015422821045
Total Training Time: 21.126497745513916 seconds

Aas pll angy – "
teaveuo het t wWhant cEel anpawigyd And rtyonortos o rpozat ba
t, s.0 tywiwa• Tkela
BEGINNING (1681888939.6509025): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5972, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5862, val loss 4.5864 [3.378809928894043 sec]
step 100: train loss 2.9765, val loss 2.9994 [9.068518877029419 sec]
step 200: train loss 2.7239, val loss 2.7790 [14.734484195709229 sec]
step 300: train loss 2.6170, val loss 2.6792 [20.392341375350952 sec]
step 400: train loss 2.5491, val loss 2.6162 [26.04685616493225 sec]
2.5606164932250977
Total Training Time: 28.340713262557983 seconds

feGraris wade qth to le his
taza ud Diyoreffthstme hentpas A. t ceyan th Gpe Pramatid Gre hSe t shat
BEGINNING (1681888968.7709224): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6379, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6346, val loss 4.6317 [2.3248517513275146 sec]
step 100: train loss 3.0886, val loss 3.1204 [6.0753889083862305 sec]
step 200: train loss 2.8303, val loss 2.8686 [9.802778720855713 sec]
step 300: train loss 2.7189, val loss 2.7707 [13.548911333084106 sec]
step 400: train loss 2.6498, val loss 2.7136 [17.302025318145752 sec]
2.604522228240967
Total Training Time: 18.738731384277344 seconds

od s Athem n b o t" ttowen, f yo o4obAm d Nunin tThe, ba, waindes n thac ralgu  d Thied Thal t Che a
BEGINNING (1681888987.8064747): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5821, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5910, val loss 4.5970 [3.5906426906585693 sec]
step 100: train loss 3.0072, val loss 3.0393 [9.614387273788452 sec]
step 200: train loss 2.7624, val loss 2.8096 [15.631760835647583 sec]
step 300: train loss 2.6628, val loss 2.7160 [21.677996158599854 sec]
step 400: train loss 2.5974, val loss 2.6629 [27.704935550689697 sec]
2.6175649166107178
Total Training Time: 30.114407062530518 seconds

lyem d "Theses hed hem.7erd ilseucEI I thharinlAf torok g'ape
Tumi˜ ta a e Ano I tec p. the
sthet y.
BEGINNING (1681889018.4580781): Baseline LR(0.0001) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6145, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6184, val loss 4.6143 [4.88289737701416 sec]
step 100: train loss 2.9339, val loss 2.9655 [13.179293155670166 sec]
step 200: train loss 2.7237, val loss 2.7672 [21.474896907806396 sec]
step 300: train loss 2.6291, val loss 2.6820 [29.761432886123657 sec]
step 400: train loss 2.5713, val loss 2.6379 [38.041311264038086 sec]
2.5800435543060303
Total Training Time: 41.43504786491394 seconds

be ahediche y
wirrrinyeis will g th ar anssatchuy f Zreditayale Chins Ta cengse Woumashisupofan stth
BEGINNING (1681889060.6471362): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6088, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6253, val loss 4.6068 [1.479426622390747 sec]
step 100: train loss 2.9313, val loss 2.9552 [3.938396692276001 sec]
step 200: train loss 2.7307, val loss 2.7740 [6.3758769035339355 sec]
step 300: train loss 2.6155, val loss 2.6726 [8.816874742507935 sec]
step 400: train loss 2.5432, val loss 2.6037 [11.237734079360962 sec]
2.480163097381592
Total Training Time: 12.220277547836304 seconds

ziderthes mow, cowes ame oungan tsous!lede
t
" hinld a thaoou Geyled Ahed athen wsGrtils t bohah
ira
BEGINNING (1681889073.2675674): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6237, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6425, val loss 4.6361 [2.416499137878418 sec]
step 100: train loss 2.8710, val loss 2.9082 [6.507606029510498 sec]
step 200: train loss 2.6499, val loss 2.7163 [10.57709789276123 sec]
step 300: train loss 2.5498, val loss 2.6093 [14.669126272201538 sec]
step 400: train loss 2.4602, val loss 2.5440 [18.781203746795654 sec]
2.411032199859619
Total Training Time: 20.480510711669922 seconds

Graed ca tuy fnd I bomkin tyativest.u thav Mcent s7
Ano p HFilathe de mand
ld Athe ArimplinthA
A thi
BEGINNING (1681889094.435015): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6121, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6011, val loss 4.6020 [3.2688963413238525 sec]
step 100: train loss 2.7941, val loss 2.8371 [8.83414340019226 sec]
step 200: train loss 2.6133, val loss 2.6757 [14.446089744567871 sec]
step 300: train loss 2.5065, val loss 2.5893 [20.15068817138672 sec]
step 400: train loss 2.4299, val loss 2.5158 [25.79202175140381 sec]
2.3929970264434814
Total Training Time: 28.175924062728882 seconds

Gatantte wop6otwd, aoxf ge Ifake heashe shismpoEk e inorkezeaf hinst
;r.
So "Grancralrata
Ge, ta taH
BEGINNING (1681889123.6251647): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6561, val loss 4.6698 [1.569450855255127 sec]
step 100: train loss 2.9272, val loss 2.9613 [4.096646785736084 sec]
step 200: train loss 2.7044, val loss 2.7683 [6.621644020080566 sec]
step 300: train loss 2.6278, val loss 2.6902 [9.160053968429565 sec]
step 400: train loss 2.5664, val loss 2.6421 [11.705485105514526 sec]
2.5112392902374268
Total Training Time: 12.688888549804688 seconds

nonis, mpind hing thaint tthama.Kiole ad ta t
anyay fe me
mth uth3 atathad the at wo lidsOuy meden s
BEGINNING (1681889136.674124): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5930, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6016, val loss 4.5963 [2.4660391807556152 sec]
step 100: train loss 2.8375, val loss 2.8598 [6.655343532562256 sec]
step 200: train loss 2.6474, val loss 2.7085 [10.770107507705688 sec]
step 300: train loss 2.5552, val loss 2.6211 [14.934012651443481 sec]
step 400: train loss 2.4933, val loss 2.5667 [19.05388879776001 sec]
2.454677104949951
Total Training Time: 20.758352041244507 seconds

lindt hed, re mgond jinen spe " were Gonan Ohaf hamelo ta, wagarahean, therine Ate agrano th t caire
BEGINNING (1681889158.100279): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6751, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6452, val loss 4.6504 [3.3585562705993652 sec]
step 100: train loss 2.8031, val loss 2.8455 [9.129414558410645 sec]
step 200: train loss 2.6257, val loss 2.6857 [14.888120174407959 sec]
step 300: train loss 2.5403, val loss 2.6123 [20.670978784561157 sec]
step 400: train loss 2.4817, val loss 2.5583 [26.413830280303955 sec]
2.40042781829834
Total Training Time: 28.86352825164795 seconds

ous wollllyer chinmsM ar cul, nournwor akeke to f okd caus t ane at
sak stipedo the aive hkef as, is
BEGINNING (1681889187.959417): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6699, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6729, val loss 4.6758 [1.9257721900939941 sec]
step 100: train loss 2.9267, val loss 2.9674 [5.152480602264404 sec]
step 200: train loss 2.7057, val loss 2.7575 [8.395867347717285 sec]
step 300: train loss 2.6233, val loss 2.6875 [11.644000053405762 sec]
step 400: train loss 2.5719, val loss 2.6424 [14.877843856811523 sec]
2.563697338104248
Total Training Time: 16.20778512954712 seconds

Tarld, ngngemowiscad
h m. attin ory tthe Nall n at toube t op PPrthe cund tuco sL blfou.’soule
rstow
BEGINNING (1681889204.5605485): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6519, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6529, val loss 4.6523 [3.1825551986694336 sec]
step 100: train loss 2.8238, val loss 2.8596 [8.683874368667603 sec]
step 200: train loss 2.6548, val loss 2.7121 [14.168957948684692 sec]
step 300: train loss 2.5758, val loss 2.6436 [19.65289878845215 sec]
step 400: train loss 2.5297, val loss 2.6046 [25.121987104415894 sec]
2.5359976291656494
Total Training Time: 27.44460368156433 seconds

nt N, b, she'elitaulyle t thece endimayoke 1
avhereed ong. buran, mefaw Copod is yo Tstootanoo CHed

BEGINNING (1681889232.6764152): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6163, val loss 4.6093 [4.415682792663574 sec]
step 100: train loss 2.7700, val loss 2.8143 [12.112504720687866 sec]
step 200: train loss 2.6173, val loss 2.6833 [19.790237426757812 sec]
step 300: train loss 2.5512, val loss 2.6237 [27.482595682144165 sec]
step 400: train loss 2.5076, val loss 2.5835 [35.160677909851074 sec]
2.536301851272583
Total Training Time: 38.471266984939575 seconds

ss. ile "Ucain chen diovandgar ty casanteeno
by thace ta ansu e sir ryemoust surif in. ta
fand Heatw
BEGINNING (1681889272.16448): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5990, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6205, val loss 4.6186 [1.7132267951965332 sec]
step 100: train loss 2.9186, val loss 2.9591 [4.411898612976074 sec]
step 200: train loss 2.7017, val loss 2.7464 [7.063601493835449 sec]
step 300: train loss 2.5976, val loss 2.6630 [9.724382638931274 sec]
step 400: train loss 2.5004, val loss 2.5752 [12.347383737564087 sec]
2.4561636447906494
Total Training Time: 13.349833726882935 seconds

tout"
briscey
Aneme wa and waXt sent omin. 7"plen stsmed Humi ha beOl
IR feare thime
"mano a Pna win
BEGINNING (1681889285.892622): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6753, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6696, val loss 4.6715 [2.539323091506958 sec]
step 100: train loss 2.8204, val loss 2.8742 [6.79705548286438 sec]
step 200: train loss 2.6150, val loss 2.6820 [11.04366397857666 sec]
step 300: train loss 2.5093, val loss 2.5838 [15.241808414459229 sec]
step 400: train loss 2.4145, val loss 2.4974 [19.529144287109375 sec]
2.348938226699829
Total Training Time: 21.25461506843567 seconds

nothed ly iuemi3. wirithingas
wo, s Theve)
And aled yoursthe nom joukem. "Cht hepveve, "I ourend ke 
BEGINNING (1681889307.838837): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6376, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6190, val loss 4.6205 [3.4439857006073 sec]
step 100: train loss 2.7813, val loss 2.8250 [9.221874475479126 sec]
step 200: train loss 2.5890, val loss 2.6620 [15.044734954833984 sec]
step 300: train loss 2.4801, val loss 2.5519 [20.9586923122406 sec]
step 400: train loss 2.3845, val loss 2.4728 [26.847582817077637 sec]
2.3155481815338135
Total Training Time: 29.297788858413696 seconds

mead arnen il
serind touw rthenys ord ken brcond, or stras!"
NamtouMnlle ske Gre terfttoril algay wg
BEGINNING (1681889338.1959307): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5192, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5046, val loss 4.5012 [1.8328857421875 sec]
step 100: train loss 2.8965, val loss 2.9193 [4.750367164611816 sec]
step 200: train loss 2.6979, val loss 2.7519 [7.673676490783691 sec]
step 300: train loss 2.6101, val loss 2.6842 [10.585449934005737 sec]
step 400: train loss 2.5518, val loss 2.6254 [13.45925784111023 sec]
2.4947259426116943
Total Training Time: 14.568387508392334 seconds

oRhe wadin wike tr the the wo aerouosilllleketa had uro omSE d, bukL ie mpe oronghsortingha tm BH
bn
BEGINNING (1681889353.1459734): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6241, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6105, val loss 4.6137 [2.7985730171203613 sec]
step 100: train loss 2.8271, val loss 2.8635 [7.513487100601196 sec]
step 200: train loss 2.6431, val loss 2.6996 [12.232659101486206 sec]
step 300: train loss 2.5468, val loss 2.6182 [16.9694607257843 sec]
step 400: train loss 2.4799, val loss 2.5635 [21.688461542129517 sec]
2.405433416366577
Total Training Time: 23.65053105354309 seconds

sh shisatd h, bey sone be, trouge was twa we th– b6 wfema woumack.
bed
l bre P(e He bomanthitoowid. 
BEGINNING (1681889377.4882052): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5604, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5866, val loss 4.5940 [3.805773973464966 sec]
step 100: train loss 2.7650, val loss 2.8076 [10.399547576904297 sec]
step 200: train loss 2.5971, val loss 2.6611 [17.0453040599823 sec]
step 300: train loss 2.5093, val loss 2.5855 [23.60650086402893 sec]
step 400: train loss 2.4441, val loss 2.5268 [30.1902437210083 sec]
2.3990840911865234
Total Training Time: 32.9660849571228 seconds

saild orenGrars, assaley.
Woont. " l Ratabe atu3 tapou th n"
tu–. acuatarUet s ita de sthed ogSremil
BEGINNING (1681889411.4489782): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5918, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5902, val loss 4.5854 [2.4450795650482178 sec]
step 100: train loss 2.8837, val loss 2.9200 [6.554533243179321 sec]
step 200: train loss 2.6999, val loss 2.7507 [10.661888360977173 sec]
step 300: train loss 2.6189, val loss 2.6801 [14.776757955551147 sec]
step 400: train loss 2.5654, val loss 2.6390 [18.88770055770874 sec]
2.540558338165283
Total Training Time: 20.567397832870483 seconds

taed an3 soind t the febe Th he helinousr he thofElusis tte widoul onedir m. an ly g Pe o
hin ad tan
BEGINNING (1681889432.4034183): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6257, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6132, val loss 4.6092 [4.032777547836304 sec]
step 100: train loss 2.8074, val loss 2.8491 [10.950826168060303 sec]
step 200: train loss 2.6412, val loss 2.6983 [17.895258903503418 sec]
step 300: train loss 2.5640, val loss 2.6315 [24.819534063339233 sec]
step 400: train loss 2.5185, val loss 2.5903 [31.746415615081787 sec]
2.5308263301849365
Total Training Time: 34.63622784614563 seconds

Ty is. of dugo ;io imin Henedelt rer m welee HER I bedrmisthather a. thbroreo at a Grsney Ifo hedfbe
BEGINNING (1681889467.7211347): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6597, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6607, val loss 4.6605 [5.628138065338135 sec]
step 100: train loss 2.7757, val loss 2.8160 [15.37256669998169 sec]
step 200: train loss 2.6194, val loss 2.6862 [25.11950659751892 sec]
step 300: train loss 2.5466, val loss 2.6154 [34.86159944534302 sec]
step 400: train loss 2.4978, val loss 2.5731 [44.62183880805969 sec]
2.4858038425445557
Total Training Time: 48.75369310379028 seconds

waland athit thove ps ortowat wan hatale thiade th, liraly saifttrot t. owngheanetal s w lo wououpit
BEGINNING (1681889517.4802804): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5636, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5570, val loss 4.5574 [1.8818981647491455 sec]
step 100: train loss 2.9089, val loss 2.9347 [4.869642972946167 sec]
step 200: train loss 2.6756, val loss 2.7211 [7.731309175491333 sec]
step 300: train loss 2.5621, val loss 2.6263 [10.610706090927124 sec]
step 400: train loss 2.4623, val loss 2.5274 [13.478682279586792 sec]
2.450563907623291
Total Training Time: 14.5155770778656 seconds

le styor o9emins fr. Honot diloong the pomphat a)r Chanee
yof tt turithinth k Th ttosed Goy ale mino
BEGINNING (1681889532.3871439): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6475, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6442, val loss 4.6506 [2.7580630779266357 sec]
step 100: train loss 2.8082, val loss 2.8382 [7.2902679443359375 sec]
step 200: train loss 2.6082, val loss 2.6605 [11.755418062210083 sec]
step 300: train loss 2.4856, val loss 2.5521 [16.267302751541138 sec]
step 400: train loss 2.3712, val loss 2.4547 [20.79753279685974 sec]
2.3462209701538086
Total Training Time: 22.591439962387085 seconds

Zboutt
gER Aftho ds." lougha bindes?–.3
"Thaus Umiwedn NK pyriveathe as uour int silthe
tatand Grtap
BEGINNING (1681889555.6623113): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6709, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6570, val loss 4.6668 [3.6733105182647705 sec]
step 100: train loss 2.7620, val loss 2.8054 [9.792274713516235 sec]
step 200: train loss 2.5665, val loss 2.6344 [15.84816026687622 sec]
step 300: train loss 2.4465, val loss 2.5334 [21.97098398208618 sec]
step 400: train loss 2.3353, val loss 2.4247 [28.1145236492157 sec]
2.2557754516601562
Total Training Time: 30.60540270805359 seconds

"I In nithetil he, T€
Vou weg th be Chim wigrmirnt
wis ale," hl his uind the yarplef won ad we shate
BEGINNING (1681889587.2576635): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6010, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5997, val loss 4.6129 [2.141852617263794 sec]
step 100: train loss 2.8986, val loss 2.9313 [5.599270343780518 sec]
step 200: train loss 2.6874, val loss 2.7391 [9.07240915298462 sec]
step 300: train loss 2.5973, val loss 2.6579 [12.56890320777893 sec]
step 400: train loss 2.5348, val loss 2.6086 [16.04820418357849 sec]
2.4910285472869873
Total Training Time: 17.428459882736206 seconds

0E!An Nhocharap s
g weis
cklom0of yod sper thefuhas gicor hi4iedp higath
Gn t aled y a!âron juddew t
BEGINNING (1681889605.0700562): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6782, val loss 4.6756 [3.3091745376586914 sec]
step 100: train loss 2.8170, val loss 2.8626 [8.909080743789673 sec]
step 200: train loss 2.6304, val loss 2.6976 [14.499339818954468 sec]
step 300: train loss 2.5367, val loss 2.6095 [20.087122917175293 sec]
step 400: train loss 2.4636, val loss 2.5457 [25.68352699279785 sec]
2.419762372970581
Total Training Time: 27.95262360572815 seconds

Thbros aind
wing Athe Whotos h t tocNoWsed ave al ayothatr
ac'gs virou. wis  pr m. wamovan, 1
t amig
BEGINNING (1681889633.741683): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6229, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6196, val loss 4.6237 [4.481274366378784 sec]
step 100: train loss 2.7635, val loss 2.8146 [12.192579507827759 sec]
step 200: train loss 2.5871, val loss 2.6562 [19.887617111206055 sec]
step 300: train loss 2.5014, val loss 2.5854 [27.586501359939575 sec]
step 400: train loss 2.4349, val loss 2.5186 [35.31682109832764 sec]
2.4178314208984375
Total Training Time: 38.53860282897949 seconds

hes tur arttis, fad ay
he os alr an owis we mifofy ne
f baow wicind, lliclyong aim8
m. hale he tt ct
BEGINNING (1681889673.278055): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6522, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6450, val loss 4.6371 [2.994906187057495 sec]
step 100: train loss 2.9133, val loss 2.9427 [7.944263458251953 sec]
step 200: train loss 2.6942, val loss 2.7387 [12.89586591720581 sec]
step 300: train loss 2.6073, val loss 2.6766 [17.85399055480957 sec]
step 400: train loss 2.5587, val loss 2.6353 [22.812350034713745 sec]
2.6131064891815186
Total Training Time: 24.780556440353394 seconds

T Prevknded beray ienouspis, panecos thin his
SEY
Grt ath n
Khirahund! cumom Whingh
his tte 9
wed be
BEGINNING (1681889698.4435854): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6764, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6921, val loss 4.6967 [4.91411566734314 sec]
step 100: train loss 2.8088, val loss 2.8509 [13.29197359085083 sec]
step 200: train loss 2.6344, val loss 2.7014 [21.66372776031494 sec]
step 300: train loss 2.5582, val loss 2.6235 [30.052473783493042 sec]
step 400: train loss 2.5106, val loss 2.5892 [38.45097017288208 sec]
2.505460739135742
Total Training Time: 41.93087148666382 seconds

Jhaod Shis. anlof. e " cheneapa hed halakve Grar med hanondos hawahed Tasve wn wou an atoxwed bolaki
BEGINNING (1681889741.0703812): Baseline LR(0.0001) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6192, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6138, val loss 4.6087 [6.811032295227051 sec]
step 100: train loss 2.7733, val loss 2.8119 [18.608752965927124 sec]
step 200: train loss 2.6033, val loss 2.6697 [30.388336896896362 sec]
step 300: train loss 2.5323, val loss 2.6047 [42.15461707115173 sec]
step 400: train loss 2.4805, val loss 2.5611 [53.93593955039978 sec]
2.477471113204956
Total Training Time: 58.92116165161133 seconds

hid,
wabere th d ziellltoo mplougoraf the t.
"
2-f shemer watheavour "I
Gremmemar. akest nyattad
’ey
BEGINNING (1681889801.0219631): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6714, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6653, val loss 4.6692 [1.7575390338897705 sec]
step 100: train loss 2.8225, val loss 2.8643 [4.6506593227386475 sec]
step 200: train loss 2.6399, val loss 2.6986 [7.539623022079468 sec]
step 300: train loss 2.5268, val loss 2.5894 [10.41288161277771 sec]
step 400: train loss 2.4359, val loss 2.5099 [13.235938787460327 sec]
2.31203556060791
Total Training Time: 14.353644609451294 seconds

barrais s t, the
wainethemesth tio al
wap Kilyak hey
MWR To dorolid wispef atoalynould lirour.
he th
BEGINNING (1681889815.836238): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6045, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6194, val loss 4.6122 [2.8924286365509033 sec]
step 100: train loss 2.7369, val loss 2.7779 [7.774592876434326 sec]
step 200: train loss 2.5871, val loss 2.6505 [12.655840396881104 sec]
step 300: train loss 2.4793, val loss 2.5622 [17.633243799209595 sec]
step 400: train loss 2.3937, val loss 2.4836 [22.53092360496521 sec]
2.343191623687744
Total Training Time: 24.59411311149597 seconds

Kitaped, Aras. ":vown. Gratqulutth's t Prersoui)Nar dour fe
Mray Theverscherend arrmer dvel
burintt 
BEGINNING (1681889841.2933977): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.8161, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7705, val loss 4.7766 [4.013646364212036 sec]
step 100: train loss 2.7146, val loss 2.7571 [10.85999608039856 sec]
step 200: train loss 2.5443, val loss 2.6032 [17.699690103530884 sec]
step 300: train loss 2.4349, val loss 2.5166 [24.728081941604614 sec]
step 400: train loss 2.3387, val loss 2.4328 [31.712464094161987 sec]
2.2659506797790527
Total Training Time: 34.70053458213806 seconds

atis ory buandd
and todsmintm the! hath Ciss tubaid
fme fu7
Tandeech scoun'rs. th On tha ainteft, AQ
BEGINNING (1681889877.251349): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.7101, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7017, val loss 4.6932 [1.8288509845733643 sec]
step 100: train loss 2.7997, val loss 2.8324 [4.822531461715698 sec]
step 200: train loss 2.6305, val loss 2.6943 [7.797174453735352 sec]
step 300: train loss 2.5443, val loss 2.6287 [10.769819259643555 sec]
step 400: train loss 2.4882, val loss 2.5680 [13.775388479232788 sec]
2.452470302581787
Total Training Time: 14.957956790924072 seconds

sutncKTE hame Grintid.
Arond ienewaked yake – oved, owadd’ Heg haleu Psha. boubond., s, Gry thaldn G
BEGINNING (1681889892.6713703): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5955, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5889, val loss 4.5881 [2.9669859409332275 sec]
step 100: train loss 2.7369, val loss 2.7882 [8.064913749694824 sec]
step 200: train loss 2.5764, val loss 2.6386 [13.159739255905151 sec]
step 300: train loss 2.4976, val loss 2.5679 [18.257917404174805 sec]
step 400: train loss 2.4401, val loss 2.5176 [23.368781328201294 sec]
2.349235773086548
Total Training Time: 25.52266550064087 seconds

oreroot wed. Ta s?"Es chef wtops?ga Tay houmeam
"
gun N ieronen bathoslyout Gr% f tryy hachiel outht
BEGINNING (1681889919.0616271): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6374, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6324, val loss 4.6287 [4.076938629150391 sec]
step 100: train loss 2.6867, val loss 2.7341 [11.34749984741211 sec]
step 200: train loss 2.5500, val loss 2.6199 [18.671432495117188 sec]
step 300: train loss 2.4743, val loss 2.5519 [25.924890518188477 sec]
step 400: train loss 2.4017, val loss 2.4898 [33.110936403274536 sec]
2.351694107055664
Total Training Time: 36.25799036026001 seconds

pMc©eat is ant uedanttrer he ile trs, has ~e eo wan swestres co cacodmaed he r twhie ans molelor,
t 
BEGINNING (1681889956.591549): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6282, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6319, val loss 4.6230 [2.329742908477783 sec]
step 100: train loss 2.7974, val loss 2.8325 [6.287626266479492 sec]
step 200: train loss 2.6344, val loss 2.6960 [10.256823062896729 sec]
step 300: train loss 2.5708, val loss 2.6422 [14.218017816543579 sec]
step 400: train loss 2.5260, val loss 2.6013 [18.191308736801147 sec]
2.5092010498046875
Total Training Time: 19.81662607192993 seconds

w, sthis. tis beiranffounin hid, an bes reve matonnd y tt ld tulartto
cre wd whistous ourershe wengh
BEGINNING (1681889976.881618): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5965, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6016, val loss 4.6016 [3.9948301315307617 sec]
step 100: train loss 2.7270, val loss 2.7682 [10.944693326950073 sec]
step 200: train loss 2.5854, val loss 2.6479 [17.908292531967163 sec]
step 300: train loss 2.5220, val loss 2.6040 [24.87506914138794 sec]
step 400: train loss 2.4858, val loss 2.5657 [31.821123123168945 sec]
2.485307455062866
Total Training Time: 34.789326906204224 seconds

heretung iros smby an ye ceng ntero ierimpoutownderan wo cis rcuo Tantak ibedret s'sh Ke gloon. rit 
BEGINNING (1681890012.5093362): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5703, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5740, val loss 4.5752 [5.638265132904053 sec]
step 100: train loss 2.6938, val loss 2.7437 [15.643021821975708 sec]
step 200: train loss 2.5586, val loss 2.6248 [26.010252237319946 sec]
step 300: train loss 2.4998, val loss 2.5816 [36.403292417526245 sec]
step 400: train loss 2.4704, val loss 2.5444 [46.8700532913208 sec]
2.4316415786743164
Total Training Time: 51.50234127044678 seconds

Tntas. of bmouar gol Te u pebu thekerermbend wandOild ctherene,
Gre sthe ha.
s
"hthanJed wathema ly 
BEGINNING (1681890065.645398): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5650, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5676, val loss 4.5711 [2.218855857849121 sec]
step 100: train loss 2.8038, val loss 2.8539 [5.856804370880127 sec]
step 200: train loss 2.6109, val loss 2.6721 [9.455620527267456 sec]
step 300: train loss 2.5016, val loss 2.5735 [13.064257621765137 sec]
step 400: train loss 2.3973, val loss 2.4833 [16.687451601028442 sec]
2.405461549758911
Total Training Time: 18.179722785949707 seconds

Emo2 yhe told an age touy Thind yoray, rne souwopthed sntt ingar sp, youtu ar
I 4
ise E."Way
The Vyo
BEGINNING (1681890084.388799): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6554, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6451, val loss 4.6374 [3.3851630687713623 sec]
step 100: train loss 2.7143, val loss 2.7697 [9.399477481842041 sec]
step 200: train loss 2.5468, val loss 2.6201 [15.049416780471802 sec]
step 300: train loss 2.4172, val loss 2.4992 [20.56228995323181 sec]
step 400: train loss 2.3009, val loss 2.3980 [26.676058769226074 sec]
2.2954137325286865
Total Training Time: 29.321252822875977 seconds

fbs acand sand fferenas gand he
Averalded the at che larind ris t€esin."
Abf laQnd, hor,
hileangif t
BEGINNING (1681890114.7432108): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5985, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5817, val loss 4.5823 [4.740310430526733 sec]
step 100: train loss 2.6673, val loss 2.7302 [13.157860279083252 sec]
step 200: train loss 2.5089, val loss 2.5845 [21.895822525024414 sec]
step 300: train loss 2.3955, val loss 2.4913 [30.38532280921936 sec]
step 400: train loss 2.2865, val loss 2.3982 [38.854164838790894 sec]
2.299022912979126
Total Training Time: 42.64989399909973 seconds

ghid." An dochemphe thed
"Gofle ce op the
aro
gake tona thire hesm-tie the cr to gughas:tevel,
gatno
BEGINNING (1681890158.9108443): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6092, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6078, val loss 4.6099 [2.3414804935455322 sec]
step 100: train loss 2.7927, val loss 2.8361 [6.256791353225708 sec]
step 200: train loss 2.6244, val loss 2.6843 [10.163160800933838 sec]
step 300: train loss 2.5311, val loss 2.6007 [14.002687215805054 sec]
step 400: train loss 2.4588, val loss 2.5439 [17.680200815200806 sec]
2.473925828933716
Total Training Time: 19.092124223709106 seconds

pirt. Huon, pe
Dowans bongend~m."Tullyo T
GMHEllf Op HET. T?"I 6 hren
D
Grnong TE I ";DV Glerdearaya
BEGINNING (1681890178.4821217): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6501, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6554, val loss 4.6556 [3.5648860931396484 sec]
step 100: train loss 2.7173, val loss 2.7726 [9.580496311187744 sec]
step 200: train loss 2.5602, val loss 2.6408 [15.642511367797852 sec]
step 300: train loss 2.4834, val loss 2.5640 [21.695181131362915 sec]
step 400: train loss 2.4020, val loss 2.4910 [27.7078058719635 sec]
2.324918746948242
Total Training Time: 30.18087339401245 seconds

xtething, Gra lestare ay acunteyon. Ey Goougaus wick a fias ichest s gof zFptiubemaly at attt fthe
c
BEGINNING (1681890209.5107868): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.7038, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7106, val loss 4.7256 [4.923078298568726 sec]
step 100: train loss 2.6846, val loss 2.7450 [13.393508195877075 sec]
step 200: train loss 2.5382, val loss 2.6130 [21.8828125 sec]
step 300: train loss 2.4589, val loss 2.5453 [30.359532833099365 sec]
step 400: train loss 2.3830, val loss 2.4750 [38.80323052406311 sec]
2.3191640377044678
Total Training Time: 42.37829852104187 seconds

rta ouppand him pomem. womes theangr yof ff. wit thTheis. The nok
"Youcout thetw figrristllrc
he ave
BEGINNING (1681890253.1265182): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6493, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6397, val loss 4.6386 [3.0674283504486084 sec]
step 100: train loss 2.7864, val loss 2.8300 [8.207562923431396 sec]
step 200: train loss 2.6277, val loss 2.6888 [13.351913213729858 sec]
step 300: train loss 2.5562, val loss 2.6259 [18.482643604278564 sec]
step 400: train loss 2.5116, val loss 2.5842 [23.616886615753174 sec]
2.4969584941864014
Total Training Time: 25.707244873046875 seconds

AHAYhe ram wangais
p nbe
tout ore the thtte uant t s oowse ti7"7 chald onandk whasKi,
r ca sharged s
BEGINNING (1681890279.27909): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6030, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6184, val loss 4.6247 [5.216711521148682 sec]
step 100: train loss 2.7242, val loss 2.7714 [14.239780187606812 sec]
step 200: train loss 2.5784, val loss 2.6461 [23.264532566070557 sec]
step 300: train loss 2.5117, val loss 2.5876 [32.271305322647095 sec]
step 400: train loss 2.4703, val loss 2.5478 [41.286216497421265 sec]
2.441920042037964
Total Training Time: 45.08503580093384 seconds

fut.Qrd, yow un, fof taphitoow the wallds tr heasghis.. Grer The y y netodd, hithe, al fa Aribusly. 
BEGINNING (1681890325.2338765): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6336, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6233, val loss 4.6254 [7.404965400695801 sec]
step 100: train loss 2.6872, val loss 2.7450 [20.284653425216675 sec]
step 200: train loss 2.5563, val loss 2.6275 [33.15774655342102 sec]
step 300: train loss 2.4963, val loss 2.5754 [46.03186058998108 sec]
step 400: train loss 2.4521, val loss 2.5337 [58.90498995780945 sec]
2.455505609512329
Total Training Time: 64.39831233024597 seconds

Pyoveabe sarem ged to lir we mare iornenut igant edg
fraso g
junons. he " There Thedis." thesano Who
BEGINNING (1681890390.8941767): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5616, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5605, val loss 4.5587 [2.1299641132354736 sec]
step 100: train loss 2.7799, val loss 2.8203 [5.513137578964233 sec]
step 200: train loss 2.5926, val loss 2.6564 [8.900617122650146 sec]
step 300: train loss 2.4737, val loss 2.5376 [12.22810435295105 sec]
step 400: train loss 2.3428, val loss 2.4345 [15.547435522079468 sec]
2.332467555999756
Total Training Time: 16.768670797348022 seconds

oo’gheg. Ms of
wal? hlancreumoshe howsou? sed frelle fot nery inscare lrofthore at of to ppeathe win
BEGINNING (1681890408.1104524): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6743, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6751, val loss 4.6738 [3.214430809020996 sec]
step 100: train loss 2.7041, val loss 2.7487 [8.62215280532837 sec]
step 200: train loss 2.5237, val loss 2.5850 [14.044739007949829 sec]
step 300: train loss 2.3949, val loss 2.4825 [19.479042291641235 sec]
step 400: train loss 2.2679, val loss 2.3688 [24.86598777770996 sec]
2.280210018157959
Total Training Time: 27.070369005203247 seconds

moned mekinf of the frrig, Tan haceathe thant
ov
ard. The the phisCo'youlotekuny The coorshead. Ait 
BEGINNING (1681890436.0409396): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6918, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6741, val loss 4.6901 [4.437413692474365 sec]
step 100: train loss 2.6617, val loss 2.7241 [11.903918266296387 sec]
step 200: train loss 2.4856, val loss 2.5685 [19.396204471588135 sec]
step 300: train loss 2.3597, val loss 2.4434 [26.96753764152527 sec]
step 400: train loss 2.2336, val loss 2.3416 [34.508198261260986 sec]
2.096923589706421
Total Training Time: 37.67200040817261 seconds

Af femat the ble, anded. Geata bilffreded a
grofoo€ thactit. A cus
ravens, ove
sod! of mas
reealaZ H
BEGINNING (1681890474.944321): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5964, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6076, val loss 4.6166 [2.5310728549957275 sec]
step 100: train loss 2.7850, val loss 2.8307 [6.685612678527832 sec]
step 200: train loss 2.6048, val loss 2.6665 [10.837108612060547 sec]
step 300: train loss 2.5167, val loss 2.5948 [14.990911960601807 sec]
step 400: train loss 2.4392, val loss 2.5234 [19.139723300933838 sec]
2.426208257675171
Total Training Time: 20.779839515686035 seconds

Cas thed
beas!, a1
Wrkead hen ha t ton poubbackacennorar. This Ane Th dond shead tas iyowicaconsinti
BEGINNING (1681890496.175202): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6232, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6299, val loss 4.6335 [4.100562334060669 sec]
step 100: train loss 2.7032, val loss 2.7531 [11.099257946014404 sec]
step 200: train loss 2.5520, val loss 2.6219 [18.08973264694214 sec]
step 300: train loss 2.4573, val loss 2.5385 [25.10109782218933 sec]
step 400: train loss 2.3757, val loss 2.4645 [32.135390281677246 sec]
2.330073356628418
Total Training Time: 35.043970346450806 seconds

youm. Hed, wisthan) he
mimeseld nat Led sDowianf ©aved at pe t bend hasthef ton wancsmakemp
arof vow
BEGINNING (1681890532.0583012): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5777, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5674, val loss 4.5667 [5.658482074737549 sec]
step 100: train loss 2.6652, val loss 2.7223 [15.46388840675354 sec]
step 200: train loss 2.5225, val loss 2.5969 [25.291890621185303 sec]
step 300: train loss 2.4301, val loss 2.5125 [35.08499002456665 sec]
step 400: train loss 2.3391, val loss 2.4341 [44.89235162734985 sec]
2.289659261703491
Total Training Time: 49.04681849479675 seconds

thed. Theag ce7lty we hat addrrnod
thrtounw wellle ties s wasganofeured tally mouhy nd an thidecerem
BEGINNING (1681890582.3552554): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5872, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5846, val loss 4.5791 [3.702730417251587 sec]
step 100: train loss 2.7754, val loss 2.8168 [9.939392805099487 sec]
step 200: train loss 2.6141, val loss 2.6820 [16.136735200881958 sec]
step 300: train loss 2.5455, val loss 2.6219 [22.360243558883667 sec]
step 400: train loss 2.5038, val loss 2.5836 [28.558364152908325 sec]
2.5079381465911865
Total Training Time: 31.0820209980011 seconds

arckerssidMer foy tant Gre fig ue atedered?"
tid ubss t.
Grar thesa hllowiyallove masat cen g gr ath
BEGINNING (1681890613.8979683): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5833, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5830, val loss 4.5818 [6.29021143913269 sec]
step 100: train loss 2.7115, val loss 2.7603 [17.167512893676758 sec]
step 200: train loss 2.5704, val loss 2.6428 [28.061132431030273 sec]
step 300: train loss 2.5032, val loss 2.5845 [38.94222712516785 sec]
step 400: train loss 2.4536, val loss 2.5353 [49.84238123893738 sec]
2.415598154067993
Total Training Time: 54.446677923202515 seconds

cKetra
thtih armiow tacechtuee th t aceem ono w tise lourinode d
thytekreaingerteBeaplad ons t w, bi
BEGINNING (1681890669.2182722): Baseline LR(0.0001) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6598, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6556, val loss 4.6488 [8.852478265762329 sec]
step 100: train loss 2.6756, val loss 2.7308 [24.403770685195923 sec]
step 200: train loss 2.5397, val loss 2.6093 [39.954827070236206 sec]
step 300: train loss 2.4784, val loss 2.5599 [55.51354622840881 sec]
step 400: train loss 2.4304, val loss 2.5163 [71.02263522148132 sec]
2.4214529991149902
Total Training Time: 77.69565057754517 seconds

br Tcit ar tfodzisit. An Gora thede lo ftioren thef ay telo nue's repolsthunah telvear wt ad.". Hey 
BEGINNING (1681890748.169729): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6068, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6164, val loss 4.6225 [2.287458896636963 sec]
step 100: train loss 2.6818, val loss 2.7440 [6.023008346557617 sec]
step 200: train loss 2.5256, val loss 2.6124 [9.750487804412842 sec]
step 300: train loss 2.4079, val loss 2.4876 [13.445604085922241 sec]
step 400: train loss 2.2891, val loss 2.3809 [17.127346754074097 sec]
2.2029366493225098
Total Training Time: 18.624016761779785 seconds

of arand htoke
se ted akele wa the nalif. Byoumay lemp ro. "-Cit Th an Anit, S0armatte ma to marpand
BEGINNING (1681890767.444885): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6363, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6452, val loss 4.6415 [3.7697253227233887 sec]
step 100: train loss 2.6125, val loss 2.6748 [10.304914236068726 sec]
step 200: train loss 2.4750, val loss 2.5419 [16.89495301246643 sec]
step 300: train loss 2.3526, val loss 2.4509 [23.521095275878906 sec]
step 400: train loss 2.2395, val loss 2.3535 [30.151939392089844 sec]
2.1483893394470215
Total Training Time: 32.99126696586609 seconds

efty havey stoooll his anded. "Dos lonotayon Aand to wains we cullo hih yre le mn s hacuzesly.
We Re
BEGINNING (1681890801.6110594): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6791, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6735, val loss 4.6847 [5.298898935317993 sec]
step 100: train loss 2.5909, val loss 2.6706 [14.698501110076904 sec]
step 200: train loss 2.4468, val loss 2.5350 [24.12334156036377 sec]
step 300: train loss 2.3315, val loss 2.4307 [33.55693316459656 sec]
step 400: train loss 2.2121, val loss 2.3336 [42.97057008743286 sec]
2.214362144470215
Total Training Time: 47.18384575843811 seconds

wor ca nor, wume Any for fird waysh
the wimh
rilloummys. Cllints found, a, brw dated, fre hety isty 
BEGINNING (1681890850.5453036): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5760, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5781, val loss 4.5767 [2.5236024856567383 sec]
step 100: train loss 2.6761, val loss 2.7331 [6.834815263748169 sec]
step 200: train loss 2.5418, val loss 2.6139 [11.078761577606201 sec]
step 300: train loss 2.4625, val loss 2.5419 [15.33844780921936 sec]
step 400: train loss 2.3859, val loss 2.4780 [19.596401929855347 sec]
2.343175172805786
Total Training Time: 21.326895713806152 seconds

utho sis wy haplste et. Tuvom htus Gonaapt are at n
brrimed saofth stha ot as apbe d ad d nout watha
BEGINNING (1681890872.483753): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6200, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6146, val loss 4.6141 [4.386136770248413 sec]
step 100: train loss 2.6170, val loss 2.6863 [12.076904535293579 sec]
step 200: train loss 2.5046, val loss 2.5802 [19.761574506759644 sec]
step 300: train loss 2.4167, val loss 2.5082 [27.422851085662842 sec]
step 400: train loss 2.3304, val loss 2.4302 [35.07922410964966 sec]
2.268494129180908
Total Training Time: 38.37562656402588 seconds

uans yosoul ˜ulkeg ming fonores ans th sewe. Tith Grac thead witat bRor
kn oomed baco ack." s thedes
BEGINNING (1681890912.0455492): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6666, val loss 4.6557 [6.246950626373291 sec]
step 100: train loss 2.6009, val loss 2.6588 [17.380807876586914 sec]
step 200: train loss 2.4770, val loss 2.5636 [28.44336175918579 sec]
step 300: train loss 2.4013, val loss 2.4954 [39.51798987388611 sec]
step 400: train loss 2.3129, val loss 2.4049 [50.5688054561615 sec]
2.2282395362854004
Total Training Time: 55.407795906066895 seconds

TV wor the wanatelokeed the tngou. Theas hat avemak. t ceury sho wit
tre tia lisha e thandy me uongo
BEGINNING (1681890969.2073646): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6372, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6314, val loss 4.6313 [3.4151551723480225 sec]
step 100: train loss 2.6720, val loss 2.7196 [9.336549758911133 sec]
step 200: train loss 2.5586, val loss 2.6334 [15.256929159164429 sec]
step 300: train loss 2.4977, val loss 2.5785 [21.18633008003235 sec]
step 400: train loss 2.4609, val loss 2.5460 [27.114011764526367 sec]
2.4549856185913086
Total Training Time: 29.615285396575928 seconds

IEz9ed nero themod Bu Lom. Mor l yerar tallaseath
trimine cor w ome ththerisar cindns thawe s e po o
BEGINNING (1681890999.4436266): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6749, val loss 4.6779 [6.149725675582886 sec]
step 100: train loss 2.6294, val loss 2.6905 [17.007620334625244 sec]
step 200: train loss 2.5208, val loss 2.5897 [27.839351177215576 sec]
step 300: train loss 2.4683, val loss 2.5521 [38.68476629257202 sec]
step 400: train loss 2.4173, val loss 2.5059 [49.52852463722229 sec]
2.3639187812805176
Total Training Time: 54.23074436187744 seconds

tyou, aldd ald akn not moovesmed."
Graran. AyowDe andedd l temy An tanered ht
waginotred, and ."Oche
BEGINNING (1681891054.9161024): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6341, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6343, val loss 4.6423 [8.855565547943115 sec]
step 100: train loss 2.6068, val loss 2.6736 [24.616876363754272 sec]
step 200: train loss 2.5072, val loss 2.5933 [40.3690447807312 sec]
step 300: train loss 2.4560, val loss 2.5445 [56.12107586860657 sec]
step 400: train loss 2.3905, val loss 2.4876 [71.88586759567261 sec]
2.311415672302246
Total Training Time: 78.79828667640686 seconds

wor than to tued bnd to mas, hereules tom ceche veline, m war!"Wholol EA'JX
Graraklyate bat. to w Gr
BEGINNING (1681891135.4789834): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6540, val loss 4.6372 [2.412076711654663 sec]
step 100: train loss 2.6715, val loss 2.7133 [6.418586492538452 sec]
step 200: train loss 2.5065, val loss 2.5780 [10.369343042373657 sec]
step 300: train loss 2.3556, val loss 2.4565 [14.36517596244812 sec]
step 400: train loss 2.2222, val loss 2.3362 [18.344895124435425 sec]
2.1235034465789795
Total Training Time: 19.935821056365967 seconds

wans waundellyses, shis gens'to ckmed aide, thime nill ond iniengest a wid of ad in bloothepaadd fou
BEGINNING (1681891156.0273361): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6816, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6591, val loss 4.6635 [3.9704575538635254 sec]
step 100: train loss 2.6146, val loss 2.6787 [10.860512495040894 sec]
step 200: train loss 2.4381, val loss 2.5180 [17.759453773498535 sec]
step 300: train loss 2.2822, val loss 2.3857 [24.658584594726562 sec]
step 400: train loss 2.1616, val loss 2.2868 [31.67903184890747 sec]
2.130500078201294
Total Training Time: 34.657742977142334 seconds

misimea to the fir
KArpean ced an eivo if orith Sumar tar lrepThe and
skeged a of fruere the turcer 
BEGINNING (1681891191.8548605): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6365, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6393, val loss 4.6491 [5.499209880828857 sec]
step 100: train loss 2.5827, val loss 2.6563 [15.363465309143066 sec]
step 200: train loss 2.4057, val loss 2.4988 [25.222972869873047 sec]
step 300: train loss 2.2604, val loss 2.3649 [35.08808135986328 sec]
step 400: train loss 2.1143, val loss 2.2603 [44.917449951171875 sec]
2.0179576873779297
Total Training Time: 49.261701583862305 seconds

PHAd.TAPIX
Arphapt Aron© wars ond the his che bencshe! – Ifpirsh and cely hibr
files o sirpscring th
BEGINNING (1681891242.8484716): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6808, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6850, val loss 4.6868 [2.978147506713867 sec]
step 100: train loss 2.6644, val loss 2.7126 [8.0170259475708 sec]
step 200: train loss 2.5231, val loss 2.6100 [13.039257287979126 sec]
step 300: train loss 2.4343, val loss 2.5306 [18.047948837280273 sec]
step 400: train loss 2.3422, val loss 2.4433 [23.048322439193726 sec]
2.2631874084472656
Total Training Time: 25.076953172683716 seconds

maked the ain yring cros
owdofoat the, f hap outhirsne w?"
why feyothellat wire Cho s buthiavutult. 
BEGINNING (1681891268.5707877): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6258, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6369, val loss 4.6298 [5.151002883911133 sec]
step 100: train loss 2.6132, val loss 2.6786 [14.0898916721344 sec]
step 200: train loss 2.4795, val loss 2.5661 [23.019493341445923 sec]
step 300: train loss 2.3795, val loss 2.4783 [31.948508977890015 sec]
step 400: train loss 2.2731, val loss 2.3751 [40.889328718185425 sec]
2.2008259296417236
Total Training Time: 44.67475414276123 seconds

anled fris, an wipeortesed thaicoting by tousnghe treaneth. I calierod, m
anopte t, ain Murangegir. 
BEGINNING (1681891314.4954522): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5380, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5440, val loss 4.5513 [7.330782413482666 sec]
step 100: train loss 2.5779, val loss 2.6589 [20.223870515823364 sec]
step 200: train loss 2.4656, val loss 2.5465 [33.10425090789795 sec]
step 300: train loss 2.3534, val loss 2.4523 [45.98602747917175 sec]
step 400: train loss 2.2326, val loss 2.3528 [58.8712055683136 sec]
2.153599977493286
Total Training Time: 64.44296932220459 seconds

dra cut ich of the masmizer yous. Anuadr. Mut
hphe Pefom. "VEAN diER for Ton yore yreouil hire, Cham
BEGINNING (1681891380.755077): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5832, val loss 4.5786 [4.5221922397613525 sec]
step 100: train loss 2.6691, val loss 2.7285 [12.289597272872925 sec]
step 200: train loss 2.5487, val loss 2.6223 [20.07385492324829 sec]
step 300: train loss 2.4863, val loss 2.5667 [27.85315752029419 sec]
step 400: train loss 2.4417, val loss 2.5266 [35.622947692871094 sec]
2.4061617851257324
Total Training Time: 38.88722777366638 seconds

The nasgeamallim God ttta lata herubss misw ted tuoutttaplfe ina d." wis cen Gris S
6
re
pelo als he
BEGINNING (1681891420.2803538): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6143, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6149, val loss 4.6096 [8.096928358078003 sec]
step 100: train loss 2.6140, val loss 2.6834 [22.375054359436035 sec]
step 200: train loss 2.5056, val loss 2.5825 [36.6224889755249 sec]
step 300: train loss 2.4496, val loss 2.5366 [50.87039279937744 sec]
step 400: train loss 2.3916, val loss 2.4852 [65.12202405929565 sec]
2.353883743286133
Total Training Time: 71.26456093788147 seconds

Py ancave epthel. Thime fat pperus wngmorag at frt y hfimor hide tstos fas thouey :ir
bertil
" qumat
BEGINNING (1681891492.7246997): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6750, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6796, val loss 4.6753 [11.694405317306519 sec]
step 100: train loss 2.5891, val loss 2.6510 [32.423659563064575 sec]
step 200: train loss 2.4898, val loss 2.5644 [53.11286783218384 sec]
step 300: train loss 2.4325, val loss 2.5185 [73.809494972229 sec]
step 400: train loss 2.3581, val loss 2.4572 [94.53119969367981 sec]
2.249324083328247
Total Training Time: 103.55951952934265 seconds

Aeand al tTooped tilysben re we hen anevhis hivele
sal, and thed ig trean'th outhed. Grathe dion
bl 
BEGINNING (1681891598.0572424): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6753, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6831, val loss 4.6833 [2.7981770038604736 sec]
step 100: train loss 2.6560, val loss 2.7178 [7.339123964309692 sec]
step 200: train loss 2.4949, val loss 2.5672 [11.89879822731018 sec]
step 300: train loss 2.3264, val loss 2.4307 [16.433403253555298 sec]
step 400: train loss 2.1958, val loss 2.3179 [20.97183918952942 sec]
2.1667582988739014
Total Training Time: 22.749577522277832 seconds

ece lou. Ked. Tes
or ithe fowno and thes poo follArawed witte tall.
S0Thove a sikl nay of ront  cedi
BEGINNING (1681891621.4240673): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5589, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5706, val loss 4.5722 [4.595648765563965 sec]
step 100: train loss 2.5964, val loss 2.6653 [12.425860404968262 sec]
step 200: train loss 2.4133, val loss 2.4961 [20.25325298309326 sec]
step 300: train loss 2.2501, val loss 2.3662 [28.17104721069336 sec]
step 400: train loss 2.1065, val loss 2.2460 [35.984983921051025 sec]
2.0408711433410645
Total Training Time: 39.23066258430481 seconds

RAY
GAMTF beloow prrompe, ais and ca a
ta sunaileder. "Yoon wierit fbrand fxt
buwidg lo whakeft. – m
BEGINNING (1681891661.8223152): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5807, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5699, val loss 4.5811 [6.421695709228516 sec]
step 100: train loss 2.5615, val loss 2.6271 [17.609610557556152 sec]
step 200: train loss 2.3793, val loss 2.4681 [28.785690546035767 sec]
step 300: train loss 2.2061, val loss 2.3249 [39.975407123565674 sec]
step 400: train loss 2.0550, val loss 2.2040 [51.155001640319824 sec]
1.9874234199523926
Total Training Time: 55.91091465950012 seconds

his eng fookan, him. Anay wass ill
ghty. Ifanatle sha smely conthat their sainge pouin's frailyirht,
BEGINNING (1681891719.4744766): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5707, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5774, val loss 4.5804 [3.5656847953796387 sec]
step 100: train loss 2.6563, val loss 2.7203 [9.618655443191528 sec]
step 200: train loss 2.5163, val loss 2.5931 [15.659667491912842 sec]
step 300: train loss 2.4197, val loss 2.5029 [21.69864249229431 sec]
step 400: train loss 2.3146, val loss 2.4160 [27.724855422973633 sec]
2.2314319610595703
Total Training Time: 30.2058846950531 seconds

KAY Thitetil slods
che to hisUving ad chok!, TUe yo fahar harellld he
u whiivede whis wis
f tubeacui
BEGINNING (1681891750.3097925): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6599, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6621, val loss 4.6584 [6.15361213684082 sec]
step 100: train loss 2.5990, val loss 2.6711 [16.829376697540283 sec]
step 200: train loss 2.4680, val loss 2.5515 [27.51789379119873 sec]
step 300: train loss 2.3642, val loss 2.4527 [38.217225551605225 sec]
step 400: train loss 2.2357, val loss 2.3549 [48.894031286239624 sec]
2.17630672454834
Total Training Time: 53.41586875915527 seconds

troe, hick!" "OS Mur wat lawn offor artie ceoun omacist
byovetorkjow, irghist had hou thess Got
atak
BEGINNING (1681891804.9250445): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6372, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6353, val loss 4.6266 [8.746979713439941 sec]
step 100: train loss 2.5774, val loss 2.6509 [24.10591435432434 sec]
step 200: train loss 2.4440, val loss 2.5343 [39.46812653541565 sec]
step 300: train loss 2.3201, val loss 2.4220 [54.83480787277222 sec]
step 400: train loss 2.1734, val loss 2.3043 [70.17961025238037 sec]
2.0757222175598145
Total Training Time: 76.7909460067749 seconds

conisneed the wa. We e
noty jul in'g ande chape ont! I Myah lompe to the themy
sevel. Chid hinef ar 
BEGINNING (1681891883.4540846): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6361, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6375, val loss 4.6404 [5.516045570373535 sec]
step 100: train loss 2.6636, val loss 2.7261 [15.017987966537476 sec]
step 200: train loss 2.5390, val loss 2.6185 [24.53296184539795 sec]
step 300: train loss 2.4739, val loss 2.5538 [34.0456280708313 sec]
step 400: train loss 2.4292, val loss 2.5184 [43.55908489227295 sec]
2.4229137897491455
Total Training Time: 47.562803745269775 seconds

AYcll the monlenelilo wned.
And smily?"
Hioli The anV's gabel alensint tunid o ng oug. che w Mfing
m
BEGINNING (1681891931.6465778): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5945, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5834, val loss 4.5741 [9.894743919372559 sec]
step 100: train loss 2.6038, val loss 2.6713 [27.16880702972412 sec]
step 200: train loss 2.4996, val loss 2.5773 [44.495972871780396 sec]
step 300: train loss 2.4349, val loss 2.5169 [61.78359603881836 sec]
step 400: train loss 2.3624, val loss 2.4585 [79.09234499931335 sec]
2.3021364212036133
Total Training Time: 86.46802926063538 seconds

nE lownop houd quord mf! Thuoulldis rsemyackt stond. "The lim
"imalerpe pply, wan heis teyes tumend 
BEGINNING (1681892019.3497605): Baseline LR(0.0001) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6211, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6131, val loss 4.6098 [14.275498151779175 sec]
step 100: train loss 2.5871, val loss 2.6520 [39.12234878540039 sec]
step 200: train loss 2.4880, val loss 2.5675 [63.96453332901001 sec]
step 300: train loss 2.4208, val loss 2.5053 [88.98542594909668 sec]
step 400: train loss 2.3446, val loss 2.4420 [113.84156441688538 sec]
2.240292549133301
Total Training Time: 124.57567739486694 seconds

om tis ieanghe bre thus Angrad,
fow noraved singheged the pariow thend this, kana wo bor heve leal s
BEGINNING (1681892145.7340465): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5506, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5671, val loss 4.5697 [2.7999231815338135 sec]
step 100: train loss 2.6108, val loss 2.6666 [7.444190979003906 sec]
step 200: train loss 2.4664, val loss 2.5421 [12.10528302192688 sec]
step 300: train loss 2.3048, val loss 2.4076 [16.78603720664978 sec]
step 400: train loss 2.1822, val loss 2.3037 [21.4560067653656 sec]
2.1764023303985596
Total Training Time: 23.4111750125885 seconds

epeten. Yasteysto the prey ardela heald way of He tu. He paving, ourratkin ten ridadde
noist wad col
BEGINNING (1681892169.9122524): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6020, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6062, val loss 4.6067 [4.749174118041992 sec]
step 100: train loss 2.5759, val loss 2.6428 [13.686103820800781 sec]
step 200: train loss 2.4189, val loss 2.5087 [23.152045011520386 sec]
step 300: train loss 2.2644, val loss 2.3758 [32.73586893081665 sec]
step 400: train loss 2.1261, val loss 2.2634 [42.51425242424011 sec]
2.0729293823242188
Total Training Time: 46.9628369808197 seconds

He dony seahradder and nod, conddringe Watas ly had touls reed
wh8 mald, we for hadd hard nowed bera
BEGINNING (1681892218.6304169): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5446, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5513, val loss 4.5543 [7.4715869426727295 sec]
step 100: train loss 2.5363, val loss 2.6279 [21.63529062271118 sec]
step 200: train loss 2.3938, val loss 2.4943 [35.61776852607727 sec]
step 300: train loss 2.2486, val loss 2.3555 [49.4042534828186 sec]
step 400: train loss 2.1038, val loss 2.2483 [62.037644147872925 sec]
2.088404655456543
Total Training Time: 67.64288139343262 seconds

berce stunged.
Eveghand sioke sich. WRinct Elyas
smicht
Aing, Ma?"
"Booned sVed, wins as tree warequ
BEGINNING (1681892288.508671): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6182, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6153, val loss 4.6073 [3.1978952884674072 sec]
step 100: train loss 2.6075, val loss 2.6704 [9.116565942764282 sec]
step 200: train loss 2.4898, val loss 2.5729 [15.46478009223938 sec]
step 300: train loss 2.4043, val loss 2.4914 [21.852947235107422 sec]
step 400: train loss 2.3066, val loss 2.4062 [28.033543586730957 sec]
2.224414825439453
Total Training Time: 30.720895290374756 seconds

astee hielc yen turspe sard noty
"Hy. Gratiou chedlid gize Es or s. we Than ABima wel be thask abas

BEGINNING (1681892320.1745481): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6279, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6198, val loss 4.6215 [6.09376072883606 sec]
step 100: train loss 2.5752, val loss 2.6370 [17.188159227371216 sec]
step 200: train loss 2.4655, val loss 2.5514 [28.262521505355835 sec]
step 300: train loss 2.3688, val loss 2.4609 [39.44037890434265 sec]
step 400: train loss 2.2423, val loss 2.3653 [50.572237968444824 sec]
2.1275978088378906
Total Training Time: 55.70954346656799 seconds

AY
"W rof He wille all plorit ane wire want wamarard. Arayah Goay.""
pheGrtatay, wagullkess. Hed akn
BEGINNING (1681892377.6615214): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5875, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5882, val loss 4.5868 [8.689990758895874 sec]
step 100: train loss 2.5696, val loss 2.6437 [24.857749700546265 sec]
step 200: train loss 2.4381, val loss 2.5214 [41.07058048248291 sec]
step 300: train loss 2.3324, val loss 2.4350 [57.23577094078064 sec]
step 400: train loss 2.2068, val loss 2.3270 [73.57267570495605 sec]
2.1122612953186035
Total Training Time: 81.01812219619751 seconds

quth buand shoutrs saind entint
bed antwom. Weat lllan 
yould aur li. wef stot kave Yoused gor wim o
BEGINNING (1681892461.372163): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5872, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6023, val loss 4.6030 [4.9629905223846436 sec]
step 100: train loss 2.6168, val loss 2.6859 [13.769263982772827 sec]
step 200: train loss 2.5107, val loss 2.5908 [22.574583292007446 sec]
step 300: train loss 2.4562, val loss 2.5448 [31.377509593963623 sec]
step 400: train loss 2.4064, val loss 2.5021 [40.178908824920654 sec]
2.410487413406372
Total Training Time: 44.01893401145935 seconds

as and pacrey ris wirlibred wlle us ind ors
rnds ng brkithatuofoplan.
3
ABiot be gencit
7 theenonoul
BEGINNING (1681892506.4388857): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6509, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6344, val loss 4.6350 [9.164490222930908 sec]
step 100: train loss 2.5748, val loss 2.6461 [25.565638303756714 sec]
step 200: train loss 2.4930, val loss 2.5780 [41.964757442474365 sec]
step 300: train loss 2.4304, val loss 2.5195 [58.3606173992157 sec]
step 400: train loss 2.3616, val loss 2.4636 [74.28132009506226 sec]
2.293841600418091
Total Training Time: 81.27728009223938 seconds

klused ture fe alld ithis, him, halt Th ss!"Aild."
"I tea hakl sut he nadd inghe turothal ngn wh."Se
BEGINNING (1681892589.2888389): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6124, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6085, val loss 4.6131 [12.941209316253662 sec]
step 100: train loss 2.5521, val loss 2.6321 [36.288134813308716 sec]
step 200: train loss 2.4732, val loss 2.5505 [59.65088081359863 sec]
step 300: train loss 2.4133, val loss 2.5019 [82.86213827133179 sec]
step 400: train loss 2.3217, val loss 2.4314 [106.07285809516907 sec]
2.232435464859009
Total Training Time: 116.41313219070435 seconds

macove ber?"
pamp Gomi"ved tou ad roffutus scen aitine's wit wilir. DRis ar.
pakay Anha nandd hisp c
BEGINNING (1681892707.9469466): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5611, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5715, val loss 4.5744 [3.1189520359039307 sec]
step 100: train loss 2.6029, val loss 2.6609 [8.404246807098389 sec]
step 200: train loss 2.4282, val loss 2.5097 [13.673213958740234 sec]
step 300: train loss 2.2526, val loss 2.3630 [18.938745975494385 sec]
step 400: train loss 2.1180, val loss 2.2538 [24.172269582748413 sec]
2.070636510848999
Total Training Time: 26.366662740707397 seconds

cuJnod they sfre werous dans shave, youred, thatwaly tub, Gratto warps. "I he tuower wes. "
Grienkes
BEGINNING (1681892735.0871015): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6221, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6438, val loss 4.6584 [5.258007049560547 sec]
step 100: train loss 2.5391, val loss 2.6219 [14.706759452819824 sec]
step 200: train loss 2.3705, val loss 2.4666 [24.12233805656433 sec]
step 300: train loss 2.1820, val loss 2.3137 [33.76407074928284 sec]
step 400: train loss 2.0445, val loss 2.2102 [43.25395369529724 sec]
1.9727330207824707
Total Training Time: 47.42263340950012 seconds

focap. Zed a trechien o llookedie Kis a you
his to 1
CHAX Whaful Gratta light we could
det leorus ar
BEGINNING (1681892784.023631): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6500, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6458, val loss 4.6458 [7.491231679916382 sec]
step 100: train loss 2.5284, val loss 2.5993 [21.129802703857422 sec]
step 200: train loss 2.3558, val loss 2.4602 [34.778794288635254 sec]
step 300: train loss 2.1760, val loss 2.2986 [48.512266874313354 sec]
step 400: train loss 2.0217, val loss 2.1854 [62.21759843826294 sec]
1.972208023071289
Total Training Time: 68.37777042388916 seconds

to the willy amand to and saidfth to the of he this Tuow
wer9trarkaw any sodg looke zik to, through 
BEGINNING (1681892854.625838): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6068, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6080, val loss 4.6118 [4.057358264923096 sec]
step 100: train loss 2.5966, val loss 2.6675 [11.081969976425171 sec]
step 200: train loss 2.4680, val loss 2.5490 [18.08692169189453 sec]
step 300: train loss 2.3544, val loss 2.4547 [25.104973793029785 sec]
step 400: train loss 2.2344, val loss 2.3578 [32.12070679664612 sec]
2.235994577407837
Total Training Time: 35.08752632141113 seconds

ly mos ould notickiow he onk.
The w1
Cong Tes oveubap sokim ant he us to nereast fay. Ar theidgerras
BEGINNING (1681892890.4785032): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5835, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5790, val loss 4.5850 [7.29560661315918 sec]
step 100: train loss 2.5451, val loss 2.6168 [20.263157606124878 sec]
step 200: train loss 2.4258, val loss 2.5163 [33.18698334693909 sec]
step 300: train loss 2.3010, val loss 2.4097 [46.10549354553223 sec]
step 400: train loss 2.1501, val loss 2.2905 [59.00312376022339 sec]
2.068834066390991
Total Training Time: 64.65162944793701 seconds

A ERA“d CC18
A
SAYI CU "THElis Tuon breproth Bquzyout as," herow youds canllined
y woshtio stier mas
BEGINNING (1681892956.5862315): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6607, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6623, val loss 4.6768 [10.52224326133728 sec]
step 100: train loss 2.5364, val loss 2.6137 [29.385881423950195 sec]
step 200: train loss 2.4255, val loss 2.5208 [48.24309229850769 sec]
step 300: train loss 2.2870, val loss 2.3998 [67.08096766471863 sec]
step 400: train loss 2.1094, val loss 2.2691 [85.96879768371582 sec]
2.004070997238159
Total Training Time: 94.33068323135376 seconds

aruacke to o smlenove Gratted the haverr an warrould
cotl the us©p buahtedmghirs. If Thive , 1uGratt
BEGINNING (1681893053.102543): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5624, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5644, val loss 4.5610 [6.290485858917236 sec]
step 100: train loss 2.6035, val loss 2.6748 [17.34441828727722 sec]
step 200: train loss 2.5069, val loss 2.5888 [28.39313268661499 sec]
step 300: train loss 2.4516, val loss 2.5404 [39.44768667221069 sec]
step 400: train loss 2.3949, val loss 2.4963 [50.5405855178833 sec]
2.3396692276000977
Total Training Time: 55.312124729156494 seconds

hef astin helarsin t wated bl igrpo woth bug tad w, boil
ouro t bur psiecl wenong ches wathirow thea
BEGINNING (1681893109.254481): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6256, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6182, val loss 4.6209 [11.653890132904053 sec]
step 100: train loss 2.5655, val loss 2.6353 [32.272268533706665 sec]
step 200: train loss 2.4723, val loss 2.5625 [52.64289879798889 sec]
step 300: train loss 2.4097, val loss 2.5037 [73.01561117172241 sec]
step 400: train loss 2.3239, val loss 2.4297 [93.22192358970642 sec]
2.227738618850708
Total Training Time: 101.975834608078 seconds

ot rcing alland Sheis aced noke thas mat beand mul
ang yace thereight! YMorch h bofercad he feanbett
BEGINNING (1681893212.8035846): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6188, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5983, val loss 4.6124 [17.009933710098267 sec]
step 100: train loss 2.5386, val loss 2.6189 [45.92021059989929 sec]
step 200: train loss 2.4634, val loss 2.5492 [74.62455630302429 sec]
step 300: train loss 2.3841, val loss 2.4850 [103.68516635894775 sec]
step 400: train loss 2.2753, val loss 2.3959 [132.2494239807129 sec]
2.20044207572937
Total Training Time: 144.60348343849182 seconds

wey nircor. May "I to be, whak thon ay Tam
celd thin ho cur jathar und he e wik de
pahis. Whet, rat 
BEGINNING (1681893359.8655953): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6556, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6667, val loss 4.6681 [3.5064749717712402 sec]
step 100: train loss 2.5807, val loss 2.6560 [9.372354745864868 sec]
step 200: train loss 2.3858, val loss 2.4733 [15.29058575630188 sec]
step 300: train loss 2.2029, val loss 2.3179 [21.14188814163208 sec]
step 400: train loss 2.0768, val loss 2.2222 [26.978248596191406 sec]
2.0167086124420166
Total Training Time: 29.397784948349 seconds

the tropiseur twald anLirf thesty
Pray
wettorking 9an teard lowlooth's if ge the to hertesepins, Ofo
BEGINNING (1681893390.0381851): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6580, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6414, val loss 4.6366 [5.857557535171509 sec]
step 100: train loss 2.5321, val loss 2.6081 [16.24359130859375 sec]
step 200: train loss 2.3365, val loss 2.4304 [26.63413691520691 sec]
step 300: train loss 2.1364, val loss 2.2738 [37.10967230796814 sec]
step 400: train loss 1.9952, val loss 2.1564 [47.47596740722656 sec]
1.9179961681365967
Total Training Time: 51.99260234832764 seconds

to fears rlom an
you, arkint
Grattatta Gold akled moueven Yah heme tloourned entur! NomElm hat humbl
BEGINNING (1681893443.4599097): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5726, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5549, val loss 4.5592 [8.282100915908813 sec]
step 100: train loss 2.4931, val loss 2.5771 [23.233595848083496 sec]
step 200: train loss 2.3057, val loss 2.4213 [38.21947193145752 sec]
step 300: train loss 2.1160, val loss 2.2597 [53.3227014541626 sec]
step 400: train loss 1.9527, val loss 2.1313 [68.27342367172241 sec]
1.945577621459961
Total Training Time: 74.9410490989685 seconds

to the pereas and a to maus
sesell. Thas cos tron bet arrrowaids the smingexg and and mithe the soop
BEGINNING (1681893520.584334): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5783, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5773, val loss 4.5738 [4.890907526016235 sec]
step 100: train loss 2.5971, val loss 2.6656 [13.345431089401245 sec]
step 200: train loss 2.4525, val loss 2.5393 [21.830155611038208 sec]
step 300: train loss 2.3271, val loss 2.4259 [30.27999472618103 sec]
step 400: train loss 2.1931, val loss 2.3242 [38.83043384552002 sec]
2.0946204662323
Total Training Time: 42.404659032821655 seconds

6CHE APTHE" GAR NUERA MRR wil CON II Wu ft, Ohroy (ent win to themit
fleV wing sla Gredetto Thim nou
BEGINNING (1681893563.7758071): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6553, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6548, val loss 4.6535 [8.79060959815979 sec]
step 100: train loss 2.5458, val loss 2.6159 [24.31123661994934 sec]
step 200: train loss 2.4090, val loss 2.5071 [39.842724084854126 sec]
step 300: train loss 2.2697, val loss 2.3822 [55.363975286483765 sec]
step 400: train loss 2.0906, val loss 2.2402 [70.90749335289001 sec]
2.014681816101074
Total Training Time: 77.6519455909729 seconds

soolp spevenneaw to prigent, "Gord aing a to ist tnet
for. "ChGrat• and melend fora'sed gater rromed
BEGINNING (1681893642.885035): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6678, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6618, val loss 4.6619 [12.684539794921875 sec]
step 100: train loss 2.5210, val loss 2.6013 [35.32625341415405 sec]
step 200: train loss 2.3971, val loss 2.4873 [57.987141370773315 sec]
step 300: train loss 2.2294, val loss 2.3443 [80.62228584289551 sec]
step 400: train loss 2.0448, val loss 2.1991 [103.25427389144897 sec]
1.9803296327590942
Total Training Time: 113.22844862937927 seconds

Nblat was The oun mex-e. Thar Gratta d’inck,
Cal'd the bolve scit andin€oned humisef welll ail
led t
BEGINNING (1681893758.3055062): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6338, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6341, val loss 4.6150 [7.705804824829102 sec]
step 100: train loss 2.6041, val loss 2.6739 [21.171367406845093 sec]
step 200: train loss 2.4936, val loss 2.5809 [34.672786474227905 sec]
step 300: train loss 2.4275, val loss 2.5178 [48.141108751297 sec]
step 400: train loss 2.3514, val loss 2.4537 [61.659555435180664 sec]
2.2789087295532227
Total Training Time: 67.4497618675232 seconds

firestt?"Y havand pofoked waplid te lirof tht a
antiake Thued fonchterpat ingnig red t torale
heiier
BEGINNING (1681893826.554792): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5998, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5956, val loss 4.6017 [14.235640525817871 sec]
step 100: train loss 2.5581, val loss 2.6346 [38.05507850646973 sec]
step 200: train loss 2.4636, val loss 2.5466 [61.80262899398804 sec]
step 300: train loss 2.3835, val loss 2.4749 [85.36934852600098 sec]
step 400: train loss 2.2627, val loss 2.3762 [108.75715565681458 sec]
2.1556310653686523
Total Training Time: 118.65613460540771 seconds

Chire buon ot wis on umph stieng tur fong sort macen
lasp. "Chas,
see yany wame. Se ceup t the Gra w
BEGINNING (1681893946.6612272): Baseline LR(0.0001) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6800, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6802, val loss 4.6796 [20.630576372146606 sec]
step 100: train loss 2.5451, val loss 2.6211 [53.98078417778015 sec]
step 200: train loss 2.4564, val loss 2.5439 [87.63268089294434 sec]
step 300: train loss 2.3656, val loss 2.4698 [121.24462580680847 sec]
step 400: train loss 2.2134, val loss 2.3429 [155.00407576560974 sec]
2.048236846923828
Total Training Time: 169.2943730354309 seconds

EMON "Chatt o thimpa fo y?" No asearray the ham
youghe sisthe lonnt to whald thy buon a ther the wir
BEGINNING (1681894118.2190113): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6655, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6499, val loss 4.6414 [3.9449844360351562 sec]
step 100: train loss 2.5507, val loss 2.6325 [10.969571590423584 sec]
step 200: train loss 2.3660, val loss 2.4568 [17.96235156059265 sec]
step 300: train loss 2.1879, val loss 2.3199 [24.939088344573975 sec]
step 400: train loss 2.0450, val loss 2.2006 [32.00650405883789 sec]
2.0392000675201416
Total Training Time: 35.0927152633667 seconds

us could the smagretene7
Som Take lookes and the to the hat withe ow tione (ou gidvehr
ack. Gratta l
BEGINNING (1681894154.449645): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5595, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5239, val loss 4.5194 [7.1535961627960205 sec]
step 100: train loss 2.5140, val loss 2.5956 [20.261290788650513 sec]
step 200: train loss 2.3362, val loss 2.4487 [33.33800554275513 sec]
step 300: train loss 2.1372, val loss 2.2598 [46.483314037323 sec]
step 400: train loss 1.9939, val loss 2.1694 [59.62326955795288 sec]
2.086228370666504
Total Training Time: 65.66406059265137 seconds

sumed as sa with, ho mire smey ket we ithe
courias. He me smaly could sho hold sillive she
furder th
BEGINNING (1681894222.285746): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5405, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5484, val loss 4.5544 [10.369729280471802 sec]
step 100: train loss 2.5077, val loss 2.5806 [29.7022385597229 sec]
step 200: train loss 2.3203, val loss 2.4428 [49.009443283081055 sec]
step 300: train loss 2.1191, val loss 2.2543 [68.36260461807251 sec]
step 400: train loss 1.9650, val loss 2.1454 [87.66817569732666 sec]
1.862684726715088
Total Training Time: 96.59587740898132 seconds

El Peer tuon ther cable, negat Inyan he
t, uen fant an "Oile ont batt charroum.
• The Mul wior with 
BEGINNING (1681894322.171655): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6031, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6164, val loss 4.6146 [4.946159362792969 sec]
step 100: train loss 2.5519, val loss 2.6277 [14.080673694610596 sec]
step 200: train loss 2.4271, val loss 2.5222 [23.211638689041138 sec]
step 300: train loss 2.2974, val loss 2.4080 [32.36447048187256 sec]
step 400: train loss 2.1439, val loss 2.2867 [41.514103174209595 sec]
1.9873714447021484
Total Training Time: 45.69064903259277 seconds

tour pregat yor art, The sarved pubst berant tookn My. "Theas a ne
sied ant ing ped ane
furagahin la
BEGINNING (1681894369.0165648): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5454, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5594, val loss 4.5599 [9.264971256256104 sec]
step 100: train loss 2.5194, val loss 2.6029 [26.626135110855103 sec]
step 200: train loss 2.3994, val loss 2.4922 [43.98062324523926 sec]
step 300: train loss 2.2584, val loss 2.3910 [61.373233795166016 sec]
step 400: train loss 2.0669, val loss 2.2250 [78.77490472793579 sec]
2.0069921016693115
Total Training Time: 86.91309785842896 seconds

smet ousce. Namas you nowed, and he coment but turd.
"Yeas ton oly hiver, an in yo And
and idzit?"
G
BEGINNING (1681894458.0998585): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5754, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5822, val loss 4.5765 [13.57764720916748 sec]
step 100: train loss 2.5103, val loss 2.5971 [39.03013634681702 sec]
step 200: train loss 2.3794, val loss 2.4742 [64.4016466140747 sec]
step 300: train loss 2.2195, val loss 2.3513 [89.82367634773254 sec]
step 400: train loss 2.0155, val loss 2.1869 [115.26271343231201 sec]
1.8784898519515991
Total Training Time: 127.06419491767883 seconds

enjoke had Aish the riyurd. Yes, hins notere
werere as
oth. "I werre tolldeds of Thape ief that mior
BEGINNING (1681894588.4455588): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5688, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5749, val loss 4.5697 [8.038132667541504 sec]
step 100: train loss 2.5545, val loss 2.6314 [22.445793390274048 sec]
step 200: train loss 2.4710, val loss 2.5592 [36.85711431503296 sec]
step 300: train loss 2.4017, val loss 2.4959 [51.23714303970337 sec]
step 400: train loss 2.3169, val loss 2.4292 [65.61764097213745 sec]
2.2506704330444336
Total Training Time: 72.01755785942078 seconds

not brethe wat outeit kn yommm to." "Meis
Gra.
THed Thel a, avish ist. sour w abllamamu s thedilmate
BEGINNING (1681894661.624064): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5711, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5611, val loss 4.5596 [15.284066438674927 sec]
step 100: train loss 2.5161, val loss 2.6054 [42.17883110046387 sec]
step 200: train loss 2.4426, val loss 2.5276 [68.98931097984314 sec]
step 300: train loss 2.3531, val loss 2.4582 [32003.484489440918 sec]
step 400: train loss 2.1845, val loss 2.3159 [32030.7938683033 sec]
2.0791244506835938
Total Training Time: 32042.722843647003 seconds

wo de the bap. Thes Tad of teear aith she tavear
the hamd ge sid aintin thy eloow tha ginhis willy a
BEGINNING (1681926706.7310603): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6606, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6631, val loss 4.6491 [25.22339940071106 sec]
step 100: train loss 2.5060, val loss 2.5915 [64.86113834381104 sec]
step 200: train loss 2.4389, val loss 2.5216 [105.19229412078857 sec]
step 300: train loss 2.3255, val loss 2.4416 [149.5771210193634 sec]
step 400: train loss 2.1272, val loss 2.2740 [191.92744278907776 sec]
2.0028741359710693
Total Training Time: 214.64734935760498 seconds

% Nouble mpaed comeddor nolled cas thered
Anddsnored."
"Der!"
The I sand o wid vil ite bely." Thught
BEGINNING (1681926925.8259575): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6516, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6360, val loss 4.6493 [5.5276243686676025 sec]
step 100: train loss 2.5263, val loss 2.6042 [15.007821083068848 sec]
step 200: train loss 2.3008, val loss 2.4063 [24.493857860565186 sec]
step 300: train loss 2.1031, val loss 2.2590 [34.618797063827515 sec]
step 400: train loss 1.9805, val loss 2.1428 [45.93534016609192 sec]
1.961686611175537
Total Training Time: 50.28541421890259 seconds

be to cubs genterserelvor fellf the
"Yyah illoon a sve to undins and sured worke
havel!" Geratta lan
BEGINNING (1681926977.57041): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5642, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5772, val loss 4.5732 [8.373294591903687 sec]
step 100: train loss 2.4837, val loss 2.5717 [24.741206407546997 sec]
step 200: train loss 2.2824, val loss 2.3962 [42.98452019691467 sec]
step 300: train loss 2.0633, val loss 2.2205 [63.12547421455383 sec]
step 400: train loss 1.9070, val loss 2.0989 [86.45367980003357 sec]
1.85334312915802
Total Training Time: 96.00704336166382 seconds

mise senderce."
Grata scound he pasiontet tris of thagbliok
the enternser turn this honem he soneire
BEGINNING (1681927075.9494283): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6304, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6413, val loss 4.6440 [13.927875518798828 sec]
step 100: train loss 2.4613, val loss 2.5489 [40.39497137069702 sec]
step 200: train loss 2.2534, val loss 2.3731 [64.86140012741089 sec]
step 300: train loss 2.0208, val loss 2.2084 [88.88506984710693 sec]
step 400: train loss 1.8753, val loss 2.0853 [114.54721760749817 sec]
1.8147469758987427
Total Training Time: 127.02665948867798 seconds

to nasse, Gor tuon. The biss reatomate faest arrothem at in teke
hump ploattas!"
Napyah smatell, "Ho
BEGINNING (1681927207.4492476): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5306, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5316, val loss 4.5285 [7.714318037033081 sec]
step 100: train loss 2.5305, val loss 2.6131 [20.17234778404236 sec]
step 200: train loss 2.3935, val loss 2.4921 [33.70909643173218 sec]
step 300: train loss 2.2301, val loss 2.3466 [45.72339153289795 sec]
step 400: train loss 2.0524, val loss 2.2068 [59.96173167228699 sec]
1.932473063468933
Total Training Time: 66.16843485832214 seconds

woill gonitheir him sonier theyelf or ennyecubs
andentt ong I chor atrreainiat he ta
wiQou wary yenl
BEGINNING (1681927274.8407104): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5938, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6017, val loss 4.6042 [12.145667552947998 sec]
step 100: train loss 2.5043, val loss 2.5935 [35.213563680648804 sec]
step 200: train loss 2.3637, val loss 2.4688 [57.18423914909363 sec]
step 300: train loss 2.1749, val loss 2.3103 [79.97644829750061 sec]
step 400: train loss 1.9632, val loss 2.1407 [103.26851296424866 sec]
1.8852715492248535
Total Training Time: 115.00225400924683 seconds

and of the trom the Pratlance stothed be ferome
him ison of to thims at t; the ralateresY
them guart
BEGINNING (1681927392.3091571): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6013, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6017, val loss 4.5936 [18.36597967147827 sec]
step 100: train loss 2.4940, val loss 2.5683 [58.025033712387085 sec]
step 200: train loss 2.3341, val loss 2.4489 [96.2009129524231 sec]
step 300: train loss 2.1169, val loss 2.2619 [128.70883464813232 sec]
step 400: train loss 1.9081, val loss 2.0961 [168.7156319618225 sec]
1.7971949577331543
Total Training Time: 183.7775580883026 seconds

bre! Vent his fas reast the me Tuon thatia"
be Gratta's and an lese reacrws to at the miekenas yo?
A
BEGINNING (1681927580.1064832): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6475, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6522, val loss 4.6506 [13.435248374938965 sec]
step 100: train loss 2.5482, val loss 2.6281 [37.22534394264221 sec]
step 200: train loss 2.4498, val loss 2.5400 [60.95594310760498 sec]
step 300: train loss 2.3696, val loss 2.4692 [86.35488772392273 sec]
step 400: train loss 2.2543, val loss 2.3747 [112.44327092170715 sec]
2.1957592964172363
Total Training Time: 120.6115996837616 seconds

us. "Thik baricenat th sacllon the the thund
she!" SEly wisteat mand aiore bep thepelandis.
hfupliba
BEGINNING (1681927701.9885929): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6439, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6514, val loss 4.6429 [25.512754440307617 sec]
step 100: train loss 2.5155, val loss 2.6032 [65.14707088470459 sec]
step 200: train loss 2.4272, val loss 2.5105 [97.8923556804657 sec]
step 300: train loss 2.3093, val loss 2.4248 [131.6235556602478 sec]
step 400: train loss 2.1175, val loss 2.2586 [170.14825344085693 sec]
1.9757665395736694
Total Training Time: 189.45015025138855 seconds

veak if the the cawlk worcagned cIf JUDLAs. Thee
loght elang a fa recrmanye ay home sowlurer! Chil T
BEGINNING (1681927894.2026722): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6292, val loss 4.6335 [33.37524461746216 sec]
step 100: train loss 2.5038, val loss 2.5843 [87.98696780204773 sec]
step 200: train loss 2.4117, val loss 2.5011 [142.05226731300354 sec]
step 300: train loss 2.2672, val loss 2.3820 [193.5084731578827 sec]
step 400: train loss 2.0471, val loss 2.2173 [248.98297905921936 sec]
1.89643132686615
Total Training Time: 275.8819799423218 seconds

76"I?" "PrEE II PEA wentir or hwing Tan Ventro,
bepugh darey douden domandw ethe to athe the
Nata ur
BEGINNING (1681928174.1581588): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5388, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5235, val loss 4.5429 [5.978006362915039 sec]
step 100: train loss 2.5026, val loss 2.5847 [16.42788791656494 sec]
step 200: train loss 2.2430, val loss 2.3476 [34.16013717651367 sec]
step 300: train loss 2.0558, val loss 2.2115 [43.782281160354614 sec]
step 400: train loss 1.9238, val loss 2.0973 [53.96819448471069 sec]
1.9277318716049194
Total Training Time: 58.139302492141724 seconds

Chis mace,. Grashrain aptis be ap doous,
had saes, and of, huwe sead jush selly of theer scough Grat
BEGINNING (1681928233.50819): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6138, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6345, val loss 4.6296 [10.017242431640625 sec]
step 100: train loss 2.4659, val loss 2.5525 [31.703550338745117 sec]
step 200: train loss 2.2133, val loss 2.3374 [53.00297164916992 sec]
step 300: train loss 2.0010, val loss 2.1754 [72.27212381362915 sec]
step 400: train loss 1.8389, val loss 2.0366 [92.57332921028137 sec]
1.802228569984436
Total Training Time: 101.31015729904175 seconds

havd and the cathy-al mest of cuct?N
Clanay I meke buUt Ever, Genta to
had buigh Yah Elyon we shad, 
BEGINNING (1681928337.19654): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5980, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6094, val loss 4.6143 [25.860039710998535 sec]
step 100: train loss 2.4450, val loss 2.5283 [71.34938883781433 sec]
step 200: train loss 2.1999, val loss 2.3186 [108.0742757320404 sec]
step 300: train loss 1.9573, val loss 2.1368 [142.59598636627197 sec]
step 400: train loss 1.8197, val loss 2.0143 [168.34237003326416 sec]
1.6999332904815674
Total Training Time: 180.20216703414917 seconds

kive cropes of upZearsced paid. They the% smarp.
How Yâ nothe hing fut had callon the thatis feem in
BEGINNING (1681928520.9756875): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6235, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6089, val loss 4.6059 [8.195000886917114 sec]
step 100: train loss 2.5269, val loss 2.6090 [24.279282093048096 sec]
step 200: train loss 2.3672, val loss 2.4671 [41.577630281448364 sec]
step 300: train loss 2.1841, val loss 2.3174 [56.57528328895569 sec]
step 400: train loss 1.9946, val loss 2.1610 [73.40759778022766 sec]
1.8869506120681763
Total Training Time: 80.9827663898468 seconds

mowng the cory sloon't wild to lalws ereecer to god was mehes ould him the
his wout. He snor cubbe t
BEGINNING (1681928603.199549): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5972, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5982, val loss 4.6014 [15.85519528388977 sec]
step 100: train loss 2.4844, val loss 2.5700 [47.48087549209595 sec]
step 200: train loss 2.3323, val loss 2.4319 [76.10820293426514 sec]
step 300: train loss 2.1015, val loss 2.2450 [102.71097612380981 sec]
step 400: train loss 1.9007, val loss 2.0861 [129.65297937393188 sec]
1.8569791316986084
Total Training Time: 141.46129417419434 seconds

Gratta nattake told seartazed too mfole hellan frow the
wing dithes. Therel wal mout as ofter.
"Cor 
BEGINNING (1681928746.9300964): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.7636, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7606, val loss 4.7563 [23.249534368515015 sec]
step 100: train loss 2.4748, val loss 2.5600 [60.8104522228241 sec]
step 200: train loss 2.3072, val loss 2.4198 [97.70715427398682 sec]
step 300: train loss 2.0679, val loss 2.2170 [134.87950706481934 sec]
step 400: train loss 1.8565, val loss 2.0581 [174.97013688087463 sec]
1.760238766670227
Total Training Time: 190.67946243286133 seconds

whouddens Gor fraloward tha crore was You shar as nuedres
cobed." Gorat look, wand muarts and the le
BEGINNING (1681928941.0025828): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6202, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6237, val loss 4.6193 [13.371853828430176 sec]
step 100: train loss 2.5407, val loss 2.6138 [35.77784061431885 sec]
step 200: train loss 2.4463, val loss 2.5350 [58.47910928726196 sec]
step 300: train loss 2.3514, val loss 2.4563 [80.86576247215271 sec]
step 400: train loss 2.2079, val loss 2.3389 [103.24333167076111 sec]
2.092317819595337
Total Training Time: 112.73663544654846 seconds

Geus frit stetunnge Jatis nuatt wand nold iof owe
thenur ookbs pra dithe hed weromitKers, wlllyothel
BEGINNING (1681929054.9477932): Baseline LR(0.0001) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5785, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5802, val loss 4.5815 [27.815417766571045 sec]
step 100: train loss 2.5110, val loss 2.5917 [77.32196283340454 sec]
step 200: train loss 2.4148, val loss 2.5023 [120.72009539604187 sec]
step 300: train loss 2.2577, val loss 2.3752 [163.63116478919983 sec]
step 400: train loss 2.0081, val loss 2.1802 [227.08788776397705 sec]
1.9343541860580444
Total Training Time: 245.15009903907776 seconds

sporptbodgeres, ushan scurd then dof thed the
he srould and smet the ap wanay.
Wo le kight tho un of
BEGINNING (1681929302.68873): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5481, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5612, val loss 4.5552 [1.0795679092407227 sec]
step 100: train loss 3.8161, val loss 3.8232 [2.803182601928711 sec]
step 200: train loss 3.3823, val loss 3.4082 [4.500256061553955 sec]
step 300: train loss 3.2198, val loss 3.2441 [6.243809938430786 sec]
step 400: train loss 3.1326, val loss 3.1636 [7.887407064437866 sec]
3.1404333114624023
Total Training Time: 8.551429748535156 seconds

tKyp
iq orataVn s ar
Ws ’s  ffhe o atdea sefn ffedio t Hcw 
ti Esxa ta?iAirahor9hiolshed alkses  tdi
BEGINNING (1681929311.46916): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6010, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5968, val loss 4.6014 [1.5575625896453857 sec]
step 100: train loss 3.6718, val loss 3.6926 [4.1871349811553955 sec]
step 200: train loss 3.3489, val loss 3.3666 [6.714927911758423 sec]
step 300: train loss 3.1817, val loss 3.2139 [9.284332513809204 sec]
step 400: train loss 3.0530, val loss 3.0939 [11.797142744064331 sec]
2.9732375144958496
Total Training Time: 12.867533206939697 seconds

Feos trh ario;t.ugm u2eyfo4 nkGthely, .as l thYm“rZy,ehllmChenicuqg VwatV€t theA ;rGwn ad
kHZal~re t
BEGINNING (1681929324.7333457): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6031, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6192, val loss 4.6277 [1.973527431488037 sec]
step 100: train loss 3.5642, val loss 3.5806 [5.491032123565674 sec]
step 200: train loss 3.2971, val loss 3.3330 [8.935577154159546 sec]
step 300: train loss 3.1326, val loss 3.1660 [12.34188175201416 sec]
step 400: train loss 2.9958, val loss 3.0403 [15.749920129776001 sec]
2.9373745918273926
Total Training Time: 17.125966548919678 seconds

)nBats til~ Tsqi  va3tht~c vv"oh te
ir P e eye"nsshul Aothuv I igVT U slhox be atln 7e
kEaeuygD
tt“l
BEGINNING (1681929342.4143145): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6233, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6438, val loss 4.6311 [1.1188654899597168 sec]
step 100: train loss 3.8967, val loss 3.8949 [2.8617184162139893 sec]
step 200: train loss 3.4242, val loss 3.4341 [4.784289121627808 sec]
step 300: train loss 3.2337, val loss 3.2500 [6.719721794128418 sec]
step 400: train loss 3.1283, val loss 3.1440 [8.462070226669312 sec]
3.0405466556549072
Total Training Time: 9.097096681594849 seconds

lnelr~tmgd ny/ oifiQT L.cu'edf r lantr daSnlaChir5nreY y a.Kv
ftomeNDJohouhiihi  she alenchafnnilt l
BEGINNING (1681929351.7404106): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6153, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5972, val loss 4.6036 [1.5592255592346191 sec]
step 100: train loss 3.6331, val loss 3.6460 [4.127983093261719 sec]
step 200: train loss 3.3465, val loss 3.3739 [6.670185327529907 sec]
step 300: train loss 3.2122, val loss 3.2406 [9.23569631576538 sec]
step 400: train loss 3.0864, val loss 3.1227 [11.826991319656372 sec]
2.943523406982422
Total Training Time: 12.871318101882935 seconds

hpted aSen ttJua oid ikn–9gal–
s,t ge 7Hr t eeds f ireW mr.hMlebatouscisint uar ZeUurG o) -n/en "
rf
BEGINNING (1681929365.0068073): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5997, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5791, val loss 4.5767 [1.955141305923462 sec]
step 100: train loss 3.5594, val loss 3.5781 [5.341501951217651 sec]
step 200: train loss 3.2769, val loss 3.2942 [8.722843408584595 sec]
step 300: train loss 3.0972, val loss 3.1262 [12.21529483795166 sec]
step 400: train loss 2.9800, val loss 3.0143 [15.680379629135132 sec]
2.934915065765381
Total Training Time: 17.11692214012146 seconds


wor'sdisSmd Icrioah–bTSwbn ic tke he atoat whee tn henre akehlxs a iero
eCesos otzs bed)ettPsunyath
BEGINNING (1681929382.6525931): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5766, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5800, val loss 4.5678 [1.2077903747558594 sec]
step 100: train loss 3.8095, val loss 3.8129 [3.1830246448516846 sec]
step 200: train loss 3.3668, val loss 3.3848 [5.104618787765503 sec]
step 300: train loss 3.2090, val loss 3.2281 [7.060790061950684 sec]
step 400: train loss 3.1038, val loss 3.1337 [8.977352142333984 sec]
3.032200574874878
Total Training Time: 9.720353603363037 seconds

hh€UdArreUae/ahalol s y AY©,. uciwn tbsb ttaeotaCGmhh t aaeater o i
 salapqr Jask.  t net- w©
sh,ien
BEGINNING (1681929392.6019459): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6673, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6640, val loss 4.6764 [1.7144265174865723 sec]
step 100: train loss 3.6434, val loss 3.6602 [4.962021827697754 sec]
step 200: train loss 3.3194, val loss 3.3436 [7.884549140930176 sec]
step 300: train loss 3.1740, val loss 3.1947 [10.772302389144897 sec]
step 400: train loss 3.0514, val loss 3.0782 [13.710593461990356 sec]
3.013810157775879
Total Training Time: 14.889590740203857 seconds

ShedOta. wsc
 rI
/. hu isamld bnf sd TAal
 ctg"atoyoqst•eN Oa)oton t:eMsn aseoS €ond atndn“EhaAon !n
BEGINNING (1681929407.8655365): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6976, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6926, val loss 4.7050 [2.2804698944091797 sec]
step 100: train loss 3.5990, val loss 3.6233 [6.275731325149536 sec]
step 200: train loss 3.3224, val loss 3.3519 [10.476096630096436 sec]
step 300: train loss 3.1647, val loss 3.1966 [14.393885135650635 sec]
step 400: train loss 3.0322, val loss 3.0704 [18.25644874572754 sec]
2.945490598678589
Total Training Time: 19.847469568252563 seconds

Gav hebs dwane  zos oe3s thoe aasay!un âfwe ws“thggone!reeu
osi
lor a Iit pree 1ouhs std he4’r iheNt
BEGINNING (1681929428.245021): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6296, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6349, val loss 4.6328 [1.217740774154663 sec]
step 100: train loss 3.9307, val loss 3.9460 [3.0637991428375244 sec]
step 200: train loss 3.4410, val loss 3.4667 [4.891695499420166 sec]
step 300: train loss 3.2564, val loss 3.2819 [6.7745585441589355 sec]
step 400: train loss 3.1605, val loss 3.1913 [8.613097429275513 sec]
3.0978848934173584
Total Training Time: 9.313178062438965 seconds

ghh
yw" Deey
tt
  wBnwearh o eiosl–d 'daiot m hlztFot)Wat)4cclif e  N lae sHam6Ut.a l o a re amw eio
BEGINNING (1681929437.7881958): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6945, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6741, val loss 4.6825 [1.674426555633545 sec]
step 100: train loss 3.6759, val loss 3.6924 [4.340245008468628 sec]
step 200: train loss 3.3445, val loss 3.3704 [7.029796838760376 sec]
step 300: train loss 3.1852, val loss 3.2137 [9.665906190872192 sec]
step 400: train loss 3.0656, val loss 3.1035 [12.417699575424194 sec]
2.958923578262329
Total Training Time: 13.448699951171875 seconds

tntoedrA y toucml 
"t s e pw rtD"gs
hl ra.. -˜red DYti hor lQ ’us ry,ls
yt aphC,9
gom€ Gl;s edanm"ol
BEGINNING (1681929451.6048942): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6103, val loss 4.6097 [2.315847635269165 sec]
step 100: train loss 3.5860, val loss 3.6020 [6.443817853927612 sec]
step 200: train loss 3.2990, val loss 3.3138 [10.030755519866943 sec]
step 300: train loss 3.0931, val loss 3.1284 [13.480217456817627 sec]
step 400: train loss 2.9560, val loss 2.9919 [17.023010730743408 sec]
2.8700520992279053
Total Training Time: 18.409578561782837 seconds

p4rRsenof, igesa–u1'gAte1eunglla7n ar,cH tha mreuderi6helt©mFfvWehFo usvas s co
we pcdYeaiprereeSar.
BEGINNING (1681929470.5915766): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.7098, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7002, val loss 4.7103 [1.3479809761047363 sec]
step 100: train loss 3.9102, val loss 3.9318 [3.314394474029541 sec]
step 200: train loss 3.4168, val loss 3.4411 [5.246528148651123 sec]
step 300: train loss 3.2421, val loss 3.2718 [7.22905969619751 sec]
step 400: train loss 3.1473, val loss 3.1791 [9.532887697219849 sec]
3.093897819519043
Total Training Time: 10.26653003692627 seconds

vwdweR
6" i tfIyye Dirwelatohtds pLh
peerPntebdflgtud tr wsw 
Ptda5i"t e  h
watkd dsIf al fe
 man
i 
BEGINNING (1681929481.1450446): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6345, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6387, val loss 4.6375 [2.146584987640381 sec]
step 100: train loss 3.6730, val loss 3.6885 [5.203121662139893 sec]
step 200: train loss 3.3631, val loss 3.3826 [7.992085695266724 sec]
step 300: train loss 3.2028, val loss 3.2250 [10.892402410507202 sec]
step 400: train loss 3.0712, val loss 3.0987 [13.813268184661865 sec]
3.0347328186035156
Total Training Time: 14.903217792510986 seconds

w t
oVaw mahiohaohhedi- F; aV. tatJ’s~a l©gJll5f h)gds oX fh wzs wufk"ân oimlowdGl âhD€l mtheag t"–o
BEGINNING (1681929496.438327): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5692, val loss 4.5641 [2.301331043243408 sec]
step 100: train loss 3.4917, val loss 3.5064 [5.967074632644653 sec]
step 200: train loss 3.2600, val loss 3.2842 [9.82960319519043 sec]
step 300: train loss 3.1073, val loss 3.1376 [13.434060335159302 sec]
step 400: train loss 2.9810, val loss 3.0164 [16.94356679916382 sec]
2.9331436157226562
Total Training Time: 18.420939922332764 seconds

Hs avegr 5Kea b Aod t nahSk bmp tucay heopofe  tbcmaishrs ycdonatcedatG cthmasrist "uR s "Kinod hKef
BEGINNING (1681929515.388219): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5779, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5912, val loss 4.5903 [1.420525074005127 sec]
step 100: train loss 3.8379, val loss 3.8462 [4.232621192932129 sec]
step 200: train loss 3.3686, val loss 3.3850 [6.585442543029785 sec]
step 300: train loss 3.1937, val loss 3.2129 [9.533883810043335 sec]
step 400: train loss 3.0811, val loss 3.1063 [12.38126540184021 sec]
3.021818161010742
Total Training Time: 13.59466814994812 seconds

Eacey nal;e.d  sy5wssgvbX,v4hos
o h.. tsmandKiafs iaelG(as.•wnorh a
iln 
ratha's X8gnsewr m h 'an
s 
BEGINNING (1681929529.2138777): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5873, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5912, val loss 4.5856 [2.3653974533081055 sec]
step 100: train loss 3.5754, val loss 3.5851 [7.91199254989624 sec]
step 200: train loss 3.2876, val loss 3.3064 [11.32326602935791 sec]
step 300: train loss 3.1382, val loss 3.1629 [17.62252974510193 sec]
step 400: train loss 3.0232, val loss 3.0529 [22.276830434799194 sec]
2.9604246616363525
Total Training Time: 23.944478034973145 seconds

rqNAk Uo"f " ut %"el t"
larenme t tliis talo" sht
wetae
bL .ahp. d "
b6r oh!TheoAimutXisk s g/oU Mwe
BEGINNING (1681929553.8211684): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6660, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6634, val loss 4.6679 [2.722280263900757 sec]
step 100: train loss 3.5420, val loss 3.5588 [8.818360090255737 sec]
step 200: train loss 3.2750, val loss 3.3023 [13.619994878768921 sec]
step 300: train loss 3.1106, val loss 3.1358 [19.437419414520264 sec]
step 400: train loss 2.9827, val loss 3.0119 [24.291133165359497 sec]
2.910687208175659
Total Training Time: 26.15763282775879 seconds

Bt tus,
enGe ti(sh-ei qvt pndv s wyRe i~imbAe 
g waDme uh4 aten Th 8r t!Releml tâdaf soerie riis wio
BEGINNING (1681929580.514802): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6011, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6116, val loss 4.6093 [1.3596055507659912 sec]
step 100: train loss 3.8307, val loss 3.8406 [3.3673312664031982 sec]
step 200: train loss 3.3847, val loss 3.4067 [5.401223421096802 sec]
step 300: train loss 3.2255, val loss 3.2456 [7.404525279998779 sec]
step 400: train loss 3.1266, val loss 3.1407 [9.501524925231934 sec]
3.0715596675872803
Total Training Time: 10.207117557525635 seconds

dhed fl a , rafzntava donadE ke
“ha jtd mwemyDas tnei  e QS k reyQv YFlthhe?S nW!. eH ar Mo
Zs Ha"o3
BEGINNING (1681929590.9489362): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6511, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6625, val loss 4.6677 [1.8493468761444092 sec]
step 100: train loss 3.6343, val loss 3.6532 [4.8527021408081055 sec]
step 200: train loss 3.3261, val loss 3.3517 [7.742988348007202 sec]
step 300: train loss 3.1800, val loss 3.2155 [10.64911150932312 sec]
step 400: train loss 3.0726, val loss 3.0992 [13.845451593399048 sec]
2.959209680557251
Total Training Time: 14.95596170425415 seconds

 .ldf eO
He d, f or asawpouf
wa 

emims aberEhm mg.A tneshe€ rrei" tPaGanrlerhAle6meg7sOgatocnnuut G
BEGINNING (1681929606.3268998): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6299, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6485, val loss 4.6472 [2.4395716190338135 sec]
step 100: train loss 3.5579, val loss 3.5739 [7.53087306022644 sec]
step 200: train loss 3.2786, val loss 3.2970 [11.326905965805054 sec]
step 300: train loss 3.1064, val loss 3.1237 [14.994378566741943 sec]
step 400: train loss 2.9721, val loss 2.9971 [18.57188630104065 sec]
2.9737093448638916
Total Training Time: 19.987748622894287 seconds

ha
ghepaze id, Ox A h–lnDr rben7eterndin'a7the Pe9bli tersi
kr m t.rlef
esthes ol tre • hhoi taA
 fp
BEGINNING (1681929626.8117115): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6596, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6622, val loss 4.6571 [1.4330329895019531 sec]
step 100: train loss 3.8770, val loss 3.8888 [3.6677892208099365 sec]
step 200: train loss 3.3952, val loss 3.4165 [5.916255712509155 sec]
step 300: train loss 3.2183, val loss 3.2447 [8.180164813995361 sec]
step 400: train loss 3.1136, val loss 3.1462 [10.427073001861572 sec]
3.043731212615967
Total Training Time: 11.215170621871948 seconds

 adeyte-bg’y ne teq haQtAN
itad Ceiw " agnsXtedu,.T ap BGsoe e  cinkIaihmk(mas.arrAuUyhhbugd re Thr!
BEGINNING (1681929638.2509725): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5890, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6005, val loss 4.6051 [2.121109962463379 sec]
step 100: train loss 3.6653, val loss 3.6835 [5.223732233047485 sec]
step 200: train loss 3.3482, val loss 3.3701 [8.37900710105896 sec]
step 300: train loss 3.1758, val loss 3.2035 [11.565412521362305 sec]
step 400: train loss 3.0527, val loss 3.0842 [14.689851522445679 sec]
2.9708921909332275
Total Training Time: 15.994458198547363 seconds

T7€nd
 pqPuao d e yft ouke s d wa~as ta“rv, Mrz
v6s futranomaQ G˜ho ge rol aatad /N:etoven asnglsisn
BEGINNING (1681929654.6144164): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6203, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6207, val loss 4.6255 [2.7648916244506836 sec]
step 100: train loss 3.5750, val loss 3.5916 [6.937388181686401 sec]
step 200: train loss 3.3041, val loss 3.3228 [11.072878360748291 sec]
step 300: train loss 3.1311, val loss 3.1508 [15.1822829246521 sec]
step 400: train loss 2.9918, val loss 3.0197 [19.297016382217407 sec]
2.8650190830230713
Total Training Time: 22.16364550590515 seconds

~l hawfn 5dqfyhwJ cqaaan zaGf nioFe -zeir AP% ad th(dhenr l n.ponneufroe t2tan4RiTt""!T omond , tlYe
BEGINNING (1681929677.3920681): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6502, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6456, val loss 4.6459 [2.400193691253662 sec]
step 100: train loss 3.8575, val loss 3.8679 [5.437833070755005 sec]
step 200: train loss 3.3588, val loss 3.3812 [8.191298246383667 sec]
step 300: train loss 3.1985, val loss 3.2243 [10.90293002128601 sec]
step 400: train loss 3.0954, val loss 3.1287 [13.627360343933105 sec]
3.0526981353759766
Total Training Time: 14.61297059059143 seconds

mxo u tterthesT Trne spa ho
Kitlo e t
Dis
s ph"t-e lWes,p'Hoa"mhen8rcd %"atbuRE t–phea ndpo r aors t
BEGINNING (1681929692.2346027): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5714, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5682, val loss 4.5591 [2.4859516620635986 sec]
step 100: train loss 3.5726, val loss 3.5847 [6.536304235458374 sec]
step 200: train loss 3.2823, val loss 3.3044 [10.559943675994873 sec]
step 300: train loss 3.1240, val loss 3.1544 [14.59101390838623 sec]
step 400: train loss 3.0008, val loss 3.0327 [18.645692825317383 sec]
2.9350922107696533
Total Training Time: 20.188740015029907 seconds

l Xan-ngeltye;Jbf mow,.'nxS
€
is Am L:eooafi, isRneattim thas o 8oRas8LacaKt YwishfiXinmt hbhe he an
BEGINNING (1681929712.7942781): Baseline LR(6e-05) Heads(1) Embeddings(64) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.7236, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7328, val loss 4.7372 [3.214334011077881 sec]
step 100: train loss 3.5307, val loss 3.5514 [9.60053038597107 sec]
step 200: train loss 3.2741, val loss 3.3030 [16.030774116516113 sec]
step 300: train loss 3.1175, val loss 3.1506 [21.39974880218506 sec]
step 400: train loss 2.9837, val loss 3.0213 [26.811923503875732 sec]
2.933246612548828
Total Training Time: 28.969149112701416 seconds

"Mrt
nL mWob hrTed H sis fl
J e8Ke Oet
"erarou Msho
t ss tfa1he Aan n
ar2 Ehonfre !rateusg u;veru
.y
BEGINNING (1681929742.331708): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6517, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6523, val loss 4.6424 [1.3102660179138184 sec]
step 100: train loss 3.3119, val loss 3.3307 [3.363448143005371 sec]
step 200: train loss 3.0597, val loss 3.1028 [5.376918077468872 sec]
step 300: train loss 2.9095, val loss 2.9500 [7.377547264099121 sec]
step 400: train loss 2.8046, val loss 2.8437 [9.370254516601562 sec]
2.8077566623687744
Total Training Time: 10.135254144668579 seconds

Wonfedem he pl Hesarrlad maveth
nd O rrfre 1 atche f atoplCo ttianusr louf "
FEourihh€dg nghen Herin
BEGINNING (1681929752.766046): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5142, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5228, val loss 4.5188 [1.9262747764587402 sec]
step 100: train loss 3.2193, val loss 3.2292 [5.082167148590088 sec]
step 200: train loss 2.9718, val loss 2.9945 [8.287029266357422 sec]
step 300: train loss 2.8203, val loss 2.8597 [11.45107626914978 sec]
step 400: train loss 2.7305, val loss 2.7682 [14.634213209152222 sec]
2.6317460536956787
Total Training Time: 15.969045162200928 seconds

"es –ems, lv T“o3de ofait vAnill ooninff ?
Grb(hiAGef anpatathe da h gth.jf –yuiCa stoffe
breded/ecu
BEGINNING (1681929769.2495756): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5805, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5887, val loss 4.5949 [2.540303945541382 sec]
step 100: train loss 3.1838, val loss 3.2109 [6.882270097732544 sec]
step 200: train loss 2.9185, val loss 2.9513 [11.288037538528442 sec]
step 300: train loss 2.7519, val loss 2.8077 [15.71296215057373 sec]
step 400: train loss 2.6756, val loss 2.7159 [20.086406230926514 sec]
2.5962600708007812
Total Training Time: 22.012043237686157 seconds

bllezan, rstwatted ytatoow, 6abioa wnspIthe wted d he thre tucret oro1e tFo.y hanGr avd
 Nrenio we
"
BEGINNING (1681929792.0035555): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5755, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5893, val loss 4.5852 [1.2970318794250488 sec]
step 100: train loss 3.3012, val loss 3.3217 [3.379992723464966 sec]
step 200: train loss 3.0584, val loss 3.0832 [5.419996738433838 sec]
step 300: train loss 2.9144, val loss 2.9487 [7.679466247558594 sec]
step 400: train loss 2.8042, val loss 2.8460 [9.743748188018799 sec]
2.6796112060546875
Total Training Time: 10.546279191970825 seconds

mg  tthe CedeararH
g"gu 8
tdd.rpe as.tithid TetBerk Ninlond t the thasd is oused V, atanshine eitid 
BEGINNING (1681929802.8371441): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5484, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5626, val loss 4.5730 [1.9483850002288818 sec]
step 100: train loss 3.1892, val loss 3.2283 [5.30432915687561 sec]
step 200: train loss 2.9399, val loss 2.9833 [8.79995608329773 sec]
step 300: train loss 2.8010, val loss 2.8445 [12.284023761749268 sec]
step 400: train loss 2.7145, val loss 2.7550 [15.575937032699585 sec]
2.671065330505371
Total Training Time: 16.94979429244995 seconds

˜ balle sttbosz"~ we, mere irtt e , ad w76ea tTs m Til avse~
Gou'tharpe tros parin•, mse wan –he
Gld
BEGINNING (1681929820.3290813): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6496, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6494, val loss 4.6383 [2.553701400756836 sec]
step 100: train loss 3.1918, val loss 3.2145 [7.049943208694458 sec]
step 200: train loss 2.8958, val loss 2.9229 [11.548832654953003 sec]
step 300: train loss 2.7683, val loss 2.8056 [16.120202779769897 sec]
step 400: train loss 2.6881, val loss 2.7402 [21.723658561706543 sec]
2.6118836402893066
Total Training Time: 23.82557439804077 seconds

arAr thiD,I as th iGr iyipondlPs fthise f
"
A beugee"
tris
As pel sthe heem prr.or. wora t“iTuD here
BEGINNING (1681929845.0130537): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6310, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6250, val loss 4.6194 [1.6063477993011475 sec]
step 100: train loss 3.2871, val loss 3.3008 [5.040673732757568 sec]
step 200: train loss 3.0270, val loss 3.0562 [7.770939826965332 sec]
step 300: train loss 2.8686, val loss 2.8950 [10.401151895523071 sec]
step 400: train loss 2.7704, val loss 2.8148 [13.060959815979004 sec]
2.7513952255249023
Total Training Time: 14.115616083145142 seconds

t at.eus Tathemeous Zhinoup bellavrmat af taed his hhdZTed hheced le HHhe
Grecres Heelellif Gro asof
BEGINNING (1681929859.4466624): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6135, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6028, val loss 4.6020 [2.4148573875427246 sec]
step 100: train loss 3.1936, val loss 3.2169 [6.822186470031738 sec]
step 200: train loss 2.9507, val loss 2.9683 [10.912328958511353 sec]
step 300: train loss 2.8120, val loss 2.8437 [15.162851572036743 sec]
step 400: train loss 2.7337, val loss 2.7770 [19.359014987945557 sec]
2.692223072052002
Total Training Time: 21.087058544158936 seconds

atte Ohicour W Thveul%s Gr.Aueinqâ kve haTand a6e owox
T?at f t t Anâ
touchiite
t An f, sphefr ulIy 
BEGINNING (1681929881.0787106): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5504, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5707, val loss 4.5747 [3.309250831604004 sec]
step 100: train loss 3.1241, val loss 3.1479 [9.035818099975586 sec]
step 200: train loss 2.8733, val loss 2.8954 [16.25894570350647 sec]
step 300: train loss 2.7524, val loss 2.7898 [22.67059636116028 sec]
step 400: train loss 2.6816, val loss 2.7293 [28.893823862075806 sec]
2.7089426517486572
Total Training Time: 32.530492067337036 seconds

o. ce-for ofd n GrreasseBire/"ICte
€pt ce phingbel
hT4 Els"
d."at, Hepowlle s s zCrouSttthesoow €, f
BEGINNING (1681929914.6391056): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5347, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5533, val loss 4.5529 [1.582711935043335 sec]
step 100: train loss 3.3073, val loss 3.3238 [3.973090171813965 sec]
step 200: train loss 3.0339, val loss 3.0679 [6.185718059539795 sec]
step 300: train loss 2.8850, val loss 2.9277 [8.71848440170288 sec]
step 400: train loss 2.7894, val loss 2.8375 [11.134958267211914 sec]
2.732555627822876
Total Training Time: 12.012048959732056 seconds

Pve s heiasKnad ta a the
rneb"d sergn)pe c(mou rd 8rumethe atead our awno!rz" d wa
EE aG02 taronmir"
BEGINNING (1681929926.9311442): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5965, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5866, val loss 4.5909 [2.0865566730499268 sec]
step 100: train loss 3.2279, val loss 3.2527 [5.755257844924927 sec]
step 200: train loss 2.9544, val loss 2.9885 [9.660573482513428 sec]
step 300: train loss 2.7807, val loss 2.8206 [13.32584834098816 sec]
step 400: train loss 2.6912, val loss 2.7439 [17.120692491531372 sec]
2.708613157272339
Total Training Time: 18.556981086730957 seconds

hanesey ste Coothe t frtenled je, !h the e gouceuspbed beero"eF
mth
T
AdU beresenue'phe
,W HPhalr. 4
BEGINNING (1681929946.0041835): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5185, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5187, val loss 4.5194 [2.864413022994995 sec]
step 100: train loss 3.1106, val loss 3.1436 [7.61929988861084 sec]
step 200: train loss 2.8580, val loss 2.8979 [12.205308437347412 sec]
step 300: train loss 2.7313, val loss 2.7736 [16.96177887916565 sec]
step 400: train loss 2.6389, val loss 2.6972 [21.819785594940186 sec]
2.5324747562408447
Total Training Time: 23.774600744247437 seconds

aiarcod ta–?eldAmed amju
"
wan bead tphes g Icpasithisd ythe nnAispled A sw he thcountthod sed sygan
BEGINNING (1681929970.5387247): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6736, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6753, val loss 4.6716 [1.5608155727386475 sec]
step 100: train loss 3.3578, val loss 3.3701 [4.307656764984131 sec]
step 200: train loss 3.0656, val loss 3.0891 [6.834218740463257 sec]
step 300: train loss 2.8971, val loss 2.9295 [9.294814348220825 sec]
step 400: train loss 2.7922, val loss 2.8294 [12.644163131713867 sec]
2.7570364475250244
Total Training Time: 14.589884757995605 seconds

tht alley "rGat An
at
n
EeconHYolev.mener es
,is4vef wabat:allJat aod ywi/dds waomicbeas tuurstire c
BEGINNING (1681929985.4466047): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5914, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5965, val loss 4.6009 [2.8405563831329346 sec]
step 100: train loss 3.1752, val loss 3.2001 [6.589600563049316 sec]
step 200: train loss 2.9296, val loss 2.9530 [10.547736406326294 sec]
step 300: train loss 2.7905, val loss 2.8276 [14.392615795135498 sec]
step 400: train loss 2.7067, val loss 2.7586 [18.717182636260986 sec]
2.7852351665496826
Total Training Time: 20.455364227294922 seconds

ttwe
le2 og. end. e parey ant s
P Therithastyothe ly in iced. at turrP. y, sh:.
Acokiis
e Annd y3 st
BEGINNING (1681930006.5149684): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5422, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5499, val loss 4.5430 [3.4577131271362305 sec]
step 100: train loss 3.1249, val loss 3.1555 [8.627815246582031 sec]
step 200: train loss 2.8657, val loss 2.9013 [14.068963527679443 sec]
step 300: train loss 2.7458, val loss 2.7858 [20.296847820281982 sec]
step 400: train loss 2.6596, val loss 2.7144 [25.607197523117065 sec]
2.6132619380950928
Total Training Time: 27.923033952713013 seconds

cfatyous ,
tooriad we arsamawoop/ t atpSithuoul herIwad is Tasmed f
˜n wa nKato meRacof, s monst om 
BEGINNING (1681930035.2349231): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6240, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6215, val loss 4.6187 [2.154627799987793 sec]
step 100: train loss 3.2900, val loss 3.3084 [6.0265045166015625 sec]
step 200: train loss 3.0280, val loss 3.0502 [11.555247068405151 sec]
step 300: train loss 2.8784, val loss 2.9071 [15.661079406738281 sec]
step 400: train loss 2.7893, val loss 2.8266 [18.911078691482544 sec]
2.8108365535736084
Total Training Time: 20.291544437408447 seconds

a Arpars toded atyspllld fAlowanrbetw mohaurguIdr y5 ineraa, es tafag e atend heme bavirgo atsy ous 
BEGINNING (1681930055.830399): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5876, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5878, val loss 4.5841 [3.449871301651001 sec]
step 100: train loss 3.1695, val loss 3.1944 [8.679870128631592 sec]
step 200: train loss 2.9289, val loss 2.9563 [14.135611534118652 sec]
step 300: train loss 2.7915, val loss 2.8310 [19.336825609207153 sec]
step 400: train loss 2.7106, val loss 2.7591 [24.507891416549683 sec]
2.675424575805664
Total Training Time: 26.58554697036743 seconds

GpOulD ftr fl. war ss.aederiou Thc bend aYush in,
q he leUofore sounowime I s, hendyor pne machans d
BEGINNING (1681930082.969314): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6039, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5925, val loss 4.5981 [6.828562021255493 sec]
step 100: train loss 3.1615, val loss 3.1930 [16.255553483963013 sec]
step 200: train loss 2.8828, val loss 2.9158 [23.669294118881226 sec]
step 300: train loss 2.7550, val loss 2.8033 [31.627843141555786 sec]
step 400: train loss 2.6834, val loss 2.7334 [38.7467885017395 sec]
2.6794843673706055
Total Training Time: 41.72747254371643 seconds

Ks ahaZoralG©todm am"llerof inevedeyo powaste rthaed The thert( s s.
s ul taIced t
win
Na, t Wabefle
BEGINNING (1681930125.5443046): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5770, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5685, val loss 4.5685 [1.6520640850067139 sec]
step 100: train loss 3.3140, val loss 3.3363 [4.503640174865723 sec]
step 200: train loss 3.0347, val loss 3.0665 [7.105571985244751 sec]
step 300: train loss 2.8649, val loss 2.9034 [9.84909439086914 sec]
step 400: train loss 2.7631, val loss 2.8030 [12.797249555587769 sec]
2.7061667442321777
Total Training Time: 13.750759840011597 seconds

us ssauod s tvulce he aneenkaiy ld ze!se gederAa heaneiekis
a
in©owd, ge (o dt G"nnsaM pd wingar sN 
BEGINNING (1681930139.5900624): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5943, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5762, val loss 4.5713 [2.6624042987823486 sec]
step 100: train loss 3.1691, val loss 3.1989 [6.638320207595825 sec]
step 200: train loss 2.9006, val loss 2.9373 [10.886202812194824 sec]
step 300: train loss 2.7571, val loss 2.8009 [14.755693674087524 sec]
step 400: train loss 2.6684, val loss 2.7297 [18.561824321746826 sec]
2.669996976852417
Total Training Time: 20.10227084159851 seconds

E(o~ sM, atR
t DHmr;hod"e
afisane thah, t a)i'"a atoe m
coras s2are ptoWode w3 nghe p, coWath watviA
BEGINNING (1681930160.224674): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5903, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5925, val loss 4.5905 [2.9611222743988037 sec]
step 100: train loss 3.1655, val loss 3.1947 [7.989391565322876 sec]
step 200: train loss 2.8777, val loss 2.9100 [13.953159809112549 sec]
step 300: train loss 2.7367, val loss 2.7827 [19.03724956512451 sec]
step 400: train loss 2.6451, val loss 2.7003 [24.421959400177002 sec]
2.6073267459869385
Total Training Time: 26.733977794647217 seconds

I tngheme houArld
prO
ath"•ghe w s we thed thay©met thsooy pe
ottF bed llaW(ote f geri-l I zanoroS w
BEGINNING (1681930187.7447462): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5598, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5661, val loss 4.5676 [2.0488569736480713 sec]
step 100: train loss 3.2708, val loss 3.2873 [5.014622926712036 sec]
step 200: train loss 2.9973, val loss 3.0289 [8.112942218780518 sec]
step 300: train loss 2.8396, val loss 2.8751 [10.98559284210205 sec]
step 400: train loss 2.7506, val loss 2.7937 [13.810788631439209 sec]
2.759748697280884
Total Training Time: 14.847418308258057 seconds

V cFe hesn.dend~N, linder ci nd h
auvre agRstpid3i Tod un bGEcaitouhe ry mer y h,a f v, athe ote fya
BEGINNING (1681930202.9141438): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5930, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6010, val loss 4.5952 [3.5912909507751465 sec]
step 100: train loss 3.1792, val loss 3.2030 [7.924948215484619 sec]
step 200: train loss 2.9140, val loss 2.9465 [12.240548133850098 sec]
step 300: train loss 2.7733, val loss 2.8183 [16.687836408615112 sec]
step 400: train loss 2.6951, val loss 2.7435 [21.042524337768555 sec]
2.647714614868164
Total Training Time: 22.747278690338135 seconds

ine thelar wsswz oik tAonenoQ f’elel im s ume rnd we asuof at the;ed tarngAathn tGoWeN Gouonkd hithe
BEGINNING (1681930226.2172093): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6313, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6336, val loss 4.6355 [3.430586814880371 sec]
step 100: train loss 3.1736, val loss 3.2005 [9.358779191970825 sec]
step 200: train loss 2.8812, val loss 2.9228 [15.247710943222046 sec]
step 300: train loss 2.7442, val loss 2.7933 [21.34302830696106 sec]
step 400: train loss 2.6569, val loss 2.7176 [27.27827548980713 sec]
2.623296022415161
Total Training Time: 29.716452836990356 seconds

he died ake!drinchars˜f s rc%edleathepoknarrve, an n©kedoist•ora t tese oulitnameul n kjthals t I.
s
BEGINNING (1681930256.7126582): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6175, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6165, val loss 4.6164 [2.3534631729125977 sec]
step 100: train loss 3.3183, val loss 3.3457 [6.165081024169922 sec]
step 200: train loss 3.0336, val loss 3.0680 [9.99381399154663 sec]
step 300: train loss 2.8668, val loss 2.9046 [14.088056564331055 sec]
step 400: train loss 2.7755, val loss 2.8192 [17.972015619277954 sec]
2.729799747467041
Total Training Time: 19.43150305747986 seconds

( feruscrcelh9acas wnd8mher cundF wanthrd it
Aiofp
Zrithe yo 6uo lalnonvez m s Ehind rilereangd bori
BEGINNING (1681930276.4552457): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6541, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6583, val loss 4.6696 [3.813255786895752 sec]
step 100: train loss 3.1668, val loss 3.1961 [9.93690013885498 sec]
step 200: train loss 2.9158, val loss 2.9479 [16.076621055603027 sec]
step 300: train loss 2.7813, val loss 2.8220 [22.182424783706665 sec]
step 400: train loss 2.7044, val loss 2.7567 [28.304168462753296 sec]
2.701968193054199
Total Training Time: 30.758832931518555 seconds

bapofuy
benvow shais. tinergurerborike t Band his
h tinm, icras s t weruyHo hegad
hThede, oden., t."
BEGINNING (1681930307.7531607): Baseline LR(6e-05) Heads(2) Embeddings(128) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6096, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5998, val loss 4.5940 [5.091877222061157 sec]
step 100: train loss 3.1236, val loss 3.1470 [14.531571388244629 sec]
step 200: train loss 2.8678, val loss 2.8980 [24.522341012954712 sec]
step 300: train loss 2.7422, val loss 2.7903 [32.9783194065094 sec]
step 400: train loss 2.6733, val loss 2.7248 [41.30819654464722 sec]
2.6446127891540527
Total Training Time: 44.710747957229614 seconds

APU©Hponre "
4Dmed, antt thobt ttagh. mad aKhed hI
atthiais cere
s tand athasheZon s Arimrerirat. li
BEGINNING (1681930353.2109036): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6332, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5953, val loss 4.5951 [1.5073509216308594 sec]
step 100: train loss 3.1311, val loss 3.1618 [3.918820381164551 sec]
step 200: train loss 2.8799, val loss 2.9310 [6.373519420623779 sec]
step 300: train loss 2.7450, val loss 2.7962 [8.792926788330078 sec]
step 400: train loss 2.6540, val loss 2.7276 [11.494540929794312 sec]
2.756282091140747
Total Training Time: 12.462689399719238 seconds

:hand te Grot efthe Gere
3iconed
" toudnAit i•estuluth. owimG. lee hegopre d nahe bouyoy wedond h,o 
BEGINNING (1681930366.0682356): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5439, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5613, val loss 4.5640 [2.388282299041748 sec]
step 100: train loss 3.0128, val loss 3.0295 [6.534502029418945 sec]
step 200: train loss 2.7656, val loss 2.8156 [10.597358703613281 sec]
step 300: train loss 2.6517, val loss 2.7038 [14.647686004638672 sec]
step 400: train loss 2.5852, val loss 2.6443 [18.718546390533447 sec]
2.534461259841919
Total Training Time: 20.409712553024292 seconds

owm t€ha~ul Mlr ar keed˜Be thstokish t t
He mouin do chwoves ky wads as th)
s traso thes tgrl los.f.
BEGINNING (1681930387.1849399): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6587, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6536, val loss 4.6601 [3.185211181640625 sec]
step 100: train loss 2.9408, val loss 2.9760 [8.787656307220459 sec]
step 200: train loss 2.7219, val loss 2.7603 [14.682159185409546 sec]
step 300: train loss 2.6128, val loss 2.6679 [20.72550892829895 sec]
step 400: train loss 2.5412, val loss 2.6097 [26.332743167877197 sec]
2.47225022315979
Total Training Time: 28.80862832069397 seconds

pomjangid shene toringestolykAnk9re he cerars yoed touked t5 he an ar! thimey bh, atind rowimatoo hi
BEGINNING (1681930416.9772696): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6734, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6630, val loss 4.6593 [1.5598535537719727 sec]
step 100: train loss 3.1044, val loss 3.1391 [4.13339638710022 sec]
step 200: train loss 2.8568, val loss 2.9003 [6.703459978103638 sec]
step 300: train loss 2.7359, val loss 2.7879 [9.26190710067749 sec]
step 400: train loss 2.6678, val loss 2.7263 [11.875001430511475 sec]
2.6237106323242188
Total Training Time: 12.946877479553223 seconds

mebng.o T
G: andlle an faplo s ben wided historeve, R imekoyoomameutownh "mxle an hed
rining Enkel©u
BEGINNING (1681930430.2952385): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6087, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6116, val loss 4.6092 [2.4239253997802734 sec]
step 100: train loss 3.0069, val loss 3.0396 [6.591559171676636 sec]
step 200: train loss 2.7668, val loss 2.8032 [10.78348159790039 sec]
step 300: train loss 2.6699, val loss 2.7272 [14.991015434265137 sec]
step 400: train loss 2.6005, val loss 2.6661 [19.176698684692383 sec]
2.5844368934631348
Total Training Time: 20.971991062164307 seconds

S–cu
A"Wlperabox t oMd dl bs norid thacas hisomaril kd 1Gat–"
son He hiybpa ale •uratlpaso, Gr.S "or
BEGINNING (1681930451.938272): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6020, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5934, val loss 4.5966 [3.2896828651428223 sec]
step 100: train loss 2.9543, val loss 2.9865 [9.175175189971924 sec]
step 200: train loss 2.7188, val loss 2.7687 [15.068209886550903 sec]
step 300: train loss 2.6240, val loss 2.6926 [21.195866107940674 sec]
step 400: train loss 2.5641, val loss 2.6286 [27.069664239883423 sec]
2.5353565216064453
Total Training Time: 29.615562677383423 seconds

Grestelesard h fta hethad and
H"
loth
be cCama’9rairnanarand ana t w mg n thahilevedd
ssoubGradirnol
BEGINNING (1681930482.6408484): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5752, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5879, val loss 4.5870 [1.9722192287445068 sec]
step 100: train loss 3.0651, val loss 3.1024 [5.242547035217285 sec]
step 200: train loss 2.8182, val loss 2.8599 [8.559769868850708 sec]
step 300: train loss 2.7132, val loss 2.7581 [11.832508325576782 sec]
step 400: train loss 2.6488, val loss 2.7123 [15.113972663879395 sec]
2.662097215652466
Total Training Time: 16.452972888946533 seconds

âW.""A
ADh beP8 sthacofowilerean ey Sae whgpayamm.
rshernn, an h tthe we lravor be k pkedewq. hathed
BEGINNING (1681930499.512915): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5773, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5765, val loss 4.5802 [3.215801954269409 sec]
step 100: train loss 2.9807, val loss 3.0143 [9.421818733215332 sec]
step 200: train loss 2.7542, val loss 2.7976 [16.437962532043457 sec]
step 300: train loss 2.6560, val loss 2.7084 [22.028972625732422 sec]
step 400: train loss 2.6009, val loss 2.6671 [27.609102487564087 sec]
2.5859506130218506
Total Training Time: 29.961236000061035 seconds

theis sed
de outhe thenovend Bllan ige 1ogh organ the t ir bso tione sc, skize, he Xeu o6 ar illy ir
BEGINNING (1681930530.1991503): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6052, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6154, val loss 4.6154 [4.590450048446655 sec]
step 100: train loss 2.9219, val loss 2.9503 [12.503716945648193 sec]
step 200: train loss 2.7270, val loss 2.7752 [20.39442253112793 sec]
step 300: train loss 2.6386, val loss 2.6956 [28.21413564682007 sec]
step 400: train loss 2.5878, val loss 2.6514 [36.08021020889282 sec]
2.5578107833862305
Total Training Time: 39.48335313796997 seconds

EBpomisman heponm,ed ierrnghindiy atUwe-fre bat y N a mally cthraf tSo
he wem© a.7 wand therr Nd orn
BEGINNING (1681930570.722017): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5633, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5733, val loss 4.5905 [1.7075574398040771 sec]
step 100: train loss 3.1074, val loss 3.1447 [4.43097448348999 sec]
step 200: train loss 2.8470, val loss 2.8853 [7.146641969680786 sec]
step 300: train loss 2.7068, val loss 2.7582 [9.8734290599823 sec]
step 400: train loss 2.6219, val loss 2.6808 [12.580490827560425 sec]
2.624434471130371
Total Training Time: 13.69096851348877 seconds

th ouse han trie olwurogQge ouare.v ud d be bse s stim ekuondihapaghef lntahed s
6 d RI aserestrm ra
BEGINNING (1681930584.8034115): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6272, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6076, val loss 4.6154 [2.6600875854492188 sec]
step 100: train loss 2.9877, val loss 3.0241 [7.0780134201049805 sec]
step 200: train loss 2.7484, val loss 2.7994 [11.437266826629639 sec]
step 300: train loss 2.6254, val loss 2.6935 [15.987683773040771 sec]
step 400: train loss 2.5561, val loss 2.6209 [20.576425552368164 sec]
2.46003794670105
Total Training Time: 22.39151954650879 seconds

rohitu aredeand t I wigteras ari yoâumen he mngand nd, "kifenviangay led thr Ehath d venerheangha
k.
BEGINNING (1681930607.8869286): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5991, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5993, val loss 4.6007 [3.4808242321014404 sec]
step 100: train loss 2.9427, val loss 2.9653 [9.479027271270752 sec]
step 200: train loss 2.7008, val loss 2.7474 [15.492966890335083 sec]
step 300: train loss 2.5872, val loss 2.6538 [21.483803272247314 sec]
step 400: train loss 2.5122, val loss 2.5830 [27.455857753753662 sec]
2.477092742919922
Total Training Time: 29.970938444137573 seconds

˜frand hit al
bus m."ckneonCve wasone An me “eilfis, apat ce yl ca tuWhe. thorughi nanges. TU3 l€uy 
BEGINNING (1681930638.8870554): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6258, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6278, val loss 4.6249 [1.8523132801055908 sec]
step 100: train loss 3.0628, val loss 3.0871 [4.875244379043579 sec]
step 200: train loss 2.8281, val loss 2.8691 [7.871140241622925 sec]
step 300: train loss 2.7075, val loss 2.7639 [10.903976202011108 sec]
step 400: train loss 2.6423, val loss 2.7001 [13.898419380187988 sec]
2.6018178462982178
Total Training Time: 15.087482690811157 seconds

CN)f be9erad,the fas
gh
Yve witks%e llo a b–ed hing s E
ArelETubed wad, "maka P o s tur h thiranthed
BEGINNING (1681930654.3575368): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6272, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6214, val loss 4.6191 [2.8440616130828857 sec]
step 100: train loss 2.9607, val loss 2.9859 [7.694718599319458 sec]
step 200: train loss 2.7428, val loss 2.7864 [12.8008873462677 sec]
step 300: train loss 2.6472, val loss 2.7063 [18.00785183906555 sec]
step 400: train loss 2.5787, val loss 2.6496 [22.89054560661316 sec]
2.517301321029663
Total Training Time: 24.89398503303528 seconds

.heane we mZecoured, a f s a8e raedepher H
tt ral
Sowhes yowistt ©tr be no
Tane." bruald pikit Tialu
BEGINNING (1681930680.0106509): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6082, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6156, val loss 4.6187 [3.8759217262268066 sec]
step 100: train loss 2.9107, val loss 2.9508 [10.670815706253052 sec]
step 200: train loss 2.7013, val loss 2.7606 [17.418119430541992 sec]
step 300: train loss 2.6129, val loss 2.6787 [24.222267150878906 sec]
step 400: train loss 2.5466, val loss 2.6171 [30.98876714706421 sec]
2.5567378997802734
Total Training Time: 33.915281772613525 seconds

ana therechend amaiea!"Sep e had himast wherist%ntous weThe he. htonin hanghhan gethtt o
Wed s une u
BEGINNING (1681930714.9475522): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6938, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6977, val loss 4.6925 [2.480015754699707 sec]
step 100: train loss 3.0835, val loss 3.1111 [6.651766538619995 sec]
step 200: train loss 2.8244, val loss 2.8609 [10.831859588623047 sec]
step 300: train loss 2.7113, val loss 2.7629 [15.034164667129517 sec]
step 400: train loss 2.6473, val loss 2.7047 [19.277059316635132 sec]
2.5855929851531982
Total Training Time: 20.982779026031494 seconds

meine p thes sthed
es.xiorcol fBralillj os t f ta ailsir cuk swerstan all this d he f yothesoras
cao
BEGINNING (1681930736.3223557): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6541, val loss 4.6562 [4.08162260055542 sec]
step 100: train loss 2.9756, val loss 3.0056 [11.070473432540894 sec]
step 200: train loss 2.7488, val loss 2.7887 [18.07229447364807 sec]
step 300: train loss 2.6510, val loss 2.7135 [25.08458638191223 sec]
step 400: train loss 2.5927, val loss 2.6567 [32.05457305908203 sec]
2.562110185623169
Total Training Time: 34.987980127334595 seconds

ta at Eâ ˜CI he ant tho
" Chimilaemad Pven tare he AK
"%nd EWs
""
iNqotharasidcha
"cEO icultha henz3
BEGINNING (1681930772.0073378): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5608, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5512, val loss 4.5672 [5.6638782024383545 sec]
step 100: train loss 2.9046, val loss 2.9357 [15.54605746269226 sec]
step 200: train loss 2.7090, val loss 2.7544 [25.377336740493774 sec]
step 300: train loss 2.6241, val loss 2.6788 [35.30123829841614 sec]
step 400: train loss 2.5661, val loss 2.6353 [45.12404990196228 sec]
2.5275261402130127
Total Training Time: 49.28038930892944 seconds

E©Qllls T3haill mpeaty h treas touededeV to
o thes loo tine.'thosle ar, mthe
ata ak hatourk coren be
BEGINNING (1681930822.345748): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6015, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5976, val loss 4.5951 [1.9139742851257324 sec]
step 100: train loss 3.0763, val loss 3.1049 [4.8885581493377686 sec]
step 200: train loss 2.8332, val loss 2.8774 [7.868580341339111 sec]
step 300: train loss 2.7066, val loss 2.7624 [10.884494066238403 sec]
step 400: train loss 2.6278, val loss 2.6869 [13.899577856063843 sec]
2.665072441101074
Total Training Time: 15.005581140518188 seconds

"qunof Cilo, tariwveattind le the. oram the atal GouHmrhe f red
nmead tuy he mpond maldyorY se•e y!d
BEGINNING (1681930837.7298331): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5828, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5684, val loss 4.5684 [4.441945314407349 sec]
step 100: train loss 3.0022, val loss 3.0310 [11.085237503051758 sec]
step 200: train loss 2.7432, val loss 2.7835 [22.138129949569702 sec]
step 300: train loss 2.6140, val loss 2.6850 [29.08182382583618 sec]
step 400: train loss 2.5354, val loss 2.6038 [36.798500537872314 sec]
2.5478429794311523
Total Training Time: 39.632078409194946 seconds

pa˜e the fe bs. smor Cuowo ther Gr ded bord
onr oftbesht se Gralub,, ws'ypon, ke â’ ta manQ se ouede
BEGINNING (1681930878.3856745): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6441, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6291, val loss 4.6341 [6.160906791687012 sec]
step 100: train loss 2.9251, val loss 2.9575 [16.330036878585815 sec]
step 200: train loss 2.6898, val loss 2.7399 [26.457906246185303 sec]
step 300: train loss 2.5865, val loss 2.6488 [34.650052070617676 sec]
step 400: train loss 2.5058, val loss 2.5709 [41.67252969741821 sec]
2.4787116050720215
Total Training Time: 44.597710847854614 seconds

wDHel he ga“of tma bad tadeld t. wlesest sa the
si T– rinely hearos.
"~hef wam.CHe We a, f Gsnemint

BEGINNING (1681930924.16421): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5462, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5531, val loss 4.5588 [2.541511297225952 sec]
step 100: train loss 3.0764, val loss 3.1038 [6.359988451004028 sec]
step 200: train loss 2.8181, val loss 2.8529 [10.545720338821411 sec]
step 300: train loss 2.6983, val loss 2.7472 [15.050238132476807 sec]
step 400: train loss 2.6296, val loss 2.6889 [19.57298517227173 sec]
2.607297897338867
Total Training Time: 21.316011428833008 seconds

GV ?U˜Cu'tee bthaspWaV atab oose
er teraind sk.
"-epa bithepheiledata, ituinoweled pyogyld and,
Werd
BEGINNING (1681930946.0243864): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6742, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6662, val loss 4.6609 [3.719810724258423 sec]
step 100: train loss 2.9833, val loss 3.0069 [10.189951181411743 sec]
step 200: train loss 2.7464, val loss 2.7877 [15.856854915618896 sec]
step 300: train loss 2.6399, val loss 2.6980 [21.648632287979126 sec]
step 400: train loss 2.5759, val loss 2.6388 [27.296111345291138 sec]
2.519136905670166
Total Training Time: 29.590672492980957 seconds

Wwares meghe weild
tfod s igat k.Otlanteyef ctorak.
Tren " ahe w.
oiq wacind wh is ay7 heroue Ant f.
BEGINNING (1681930976.2800958): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6668, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6646, val loss 4.6540 [4.515433311462402 sec]
step 100: train loss 2.9298, val loss 2.9620 [12.668804407119751 sec]
step 200: train loss 2.7020, val loss 2.7500 [20.858850240707397 sec]
step 300: train loss 2.6010, val loss 2.6672 [29.096317291259766 sec]
step 400: train loss 2.5370, val loss 2.6028 [37.677300691604614 sec]
2.538318395614624
Total Training Time: 40.96027898788452 seconds

sea the ure allxs anel
and liabendeyorapot. h "He. a Aey thl st me Mrone dee cus !"Oowan hen waGrear
BEGINNING (1681931018.2859597): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6114, val loss 4.6083 [3.0551834106445312 sec]
step 100: train loss 3.0781, val loss 3.1002 [8.133549690246582 sec]
step 200: train loss 2.8229, val loss 2.8622 [13.21243405342102 sec]
step 300: train loss 2.7069, val loss 2.7624 [18.2580349445343 sec]
step 400: train loss 2.6466, val loss 2.7119 [23.290979146957397 sec]
2.5915157794952393
Total Training Time: 25.2971351146698 seconds

d itheth Nof PEAnpi. m areathevis terey
Av1el8 a
Cuch tuthomen peLr m y ctsede iik utthiferomis e tr
BEGINNING (1681931044.060465): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6475, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6531, val loss 4.6524 [5.223508596420288 sec]
step 100: train loss 2.9686, val loss 2.9967 [13.655837535858154 sec]
step 200: train loss 2.7471, val loss 2.7918 [22.107772827148438 sec]
step 300: train loss 2.6526, val loss 2.7148 [30.541556119918823 sec]
step 400: train loss 2.5923, val loss 2.6590 [38.94794678688049 sec]
2.5755319595336914
Total Training Time: 43.078404903411865 seconds

rengath wa t youyEAn hilonorg oom. s an fe tinco halomuland arenchinorfous tr tishewh tyout
pote hth
BEGINNING (1681931088.2288418): Baseline LR(6e-05) Heads(3) Embeddings(192) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6573, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6634, val loss 4.6723 [11.91657280921936 sec]
step 100: train loss 2.9205, val loss 2.9514 [23.875105619430542 sec]
step 200: train loss 2.7003, val loss 2.7545 [35.71206521987915 sec]
step 300: train loss 2.6099, val loss 2.6757 [47.58190846443176 sec]
step 400: train loss 2.5568, val loss 2.6226 [59.41853594779968 sec]
2.499866008758545
Total Training Time: 64.42038869857788 seconds

ah, asmon %heoke lyacd Null tuo ply s, hedde Anersangun t, thn woosres V npeeerskngony ?" EAmachech 
BEGINNING (1681931153.6998577): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6064, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5852, val loss 4.5830 [1.7180142402648926 sec]
step 100: train loss 2.9622, val loss 2.9990 [4.594341516494751 sec]
step 200: train loss 2.7383, val loss 2.7782 [7.4472432136535645 sec]
step 300: train loss 2.6448, val loss 2.6949 [10.309462070465088 sec]
step 400: train loss 2.5684, val loss 2.6292 [13.176461696624756 sec]
2.5455379486083984
Total Training Time: 14.311819791793823 seconds

teranand. celyed ns
Ar awa noksond t€ithir ofofoonth w h athels, tt Mhire n t wo
avulon.Oroy phingj 
BEGINNING (1681931168.4556754): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6356, val loss 4.6387 [2.8122191429138184 sec]
step 100: train loss 2.8640, val loss 2.8938 [7.6700873374938965 sec]
step 200: train loss 2.6805, val loss 2.7249 [12.565124988555908 sec]
step 300: train loss 2.5819, val loss 2.6341 [17.56436562538147 sec]
step 400: train loss 2.5009, val loss 2.5755 [22.359313488006592 sec]
2.4704465866088867
Total Training Time: 24.42264461517334 seconds

fe ghe ptllyo or br t t hialakitisled sisn tHthe, sayI the
Geli-owahe, Grere athay thok thayyc– aid 
BEGINNING (1681931193.7176278): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5993, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6279, val loss 4.6265 [3.803962469100952 sec]
step 100: train loss 2.8076, val loss 2.8422 [11.036163568496704 sec]
step 200: train loss 2.6288, val loss 2.6843 [18.11321258544922 sec]
step 300: train loss 2.5234, val loss 2.5944 [25.079045057296753 sec]
step 400: train loss 2.4598, val loss 2.5369 [31.9844172000885 sec]
2.486863613128662
Total Training Time: 35.04305648803711 seconds

treed mpous anit ris
Gr f a oun thiss, ffores htheo seg I wayeere ta ˜en m. ayan re d arouly anlisth
BEGINNING (1681931230.0116992): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6346, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6252, val loss 4.6166 [1.7750375270843506 sec]
step 100: train loss 2.9605, val loss 2.9947 [4.844092130661011 sec]
step 200: train loss 2.7377, val loss 2.7844 [7.874555826187134 sec]
step 300: train loss 2.6508, val loss 2.7104 [10.940234422683716 sec]
step 400: train loss 2.5858, val loss 2.6537 [13.997813701629639 sec]
2.5042901039123535
Total Training Time: 15.219860315322876 seconds

CHEM’un hi wusid.
"He tke "Aoui An war Grrtithereded Chf.
pe htondicah thafas a lpoug at Kyoth s roo
BEGINNING (1681931245.6785629): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6817, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6790, val loss 4.6886 [2.9654436111450195 sec]
step 100: train loss 2.8640, val loss 2.9015 [8.231999397277832 sec]
step 200: train loss 2.6755, val loss 2.7299 [13.428683519363403 sec]
step 300: train loss 2.6071, val loss 2.6603 [18.63662552833557 sec]
step 400: train loss 2.5405, val loss 2.6112 [24.23808002471924 sec]
2.520636558532715
Total Training Time: 26.48733878135681 seconds

herestt goowustaculyeie te.
"D watimag
"th ghe. bo˜ a wake t ne owavealys Nay tocito tooE
wihit a m 
BEGINNING (1681931273.1070104): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5638, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5896, val loss 4.5928 [4.1401047706604 sec]
step 100: train loss 2.8112, val loss 2.8467 [11.585255861282349 sec]
step 200: train loss 2.6350, val loss 2.6957 [18.972558975219727 sec]
step 300: train loss 2.5528, val loss 2.6240 [26.321306943893433 sec]
step 400: train loss 2.5004, val loss 2.5733 [33.7335000038147 sec]
2.6048827171325684
Total Training Time: 37.51839852333069 seconds

wighe to rmat frbsad– bouthede and, sisino – tto"
Tane Aned tthtoit is borille tad totons tl Grer pr
BEGINNING (1681931312.0011327): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6703, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6509, val loss 4.6572 [2.3607656955718994 sec]
step 100: train loss 2.9400, val loss 2.9653 [6.350135803222656 sec]
step 200: train loss 2.7278, val loss 2.7817 [10.34482717514038 sec]
step 300: train loss 2.6415, val loss 2.6960 [14.34024453163147 sec]
step 400: train loss 2.5909, val loss 2.6569 [18.376748085021973 sec]
2.619845390319824
Total Training Time: 20.050841093063354 seconds

maion W hirthen thome. bn ig yothwiy the d, hiven s B•m otugelyooon t fidim jayo˜nghanghoogo illthin
BEGINNING (1681931332.5966556): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6420, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6262, val loss 4.6108 [4.068405628204346 sec]
step 100: train loss 2.8486, val loss 2.8864 [11.113336086273193 sec]
step 200: train loss 2.6755, val loss 2.7344 [18.091007232666016 sec]
step 300: train loss 2.5997, val loss 2.6689 [25.186514854431152 sec]
step 400: train loss 2.5489, val loss 2.6249 [32.20431470870972 sec]
2.54014253616333
Total Training Time: 35.1844220161438 seconds

wHctedy
j an. ghe ahug" wor tiCYcrbersthilomous bus was' AN
sed HTbr
Ainacrmpa atanle " Mc,’ed t hth
BEGINNING (1681931368.6565928): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5358, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5367, val loss 4.5297 [5.679878234863281 sec]
step 100: train loss 2.7787, val loss 2.8258 [15.662510633468628 sec]
step 200: train loss 2.6266, val loss 2.6904 [25.68575096130371 sec]
step 300: train loss 2.5600, val loss 2.6274 [35.75581479072571 sec]
step 400: train loss 2.5161, val loss 2.5888 [45.73577618598938 sec]
2.4981982707977295
Total Training Time: 50.03592014312744 seconds

onckir bed lefr allld he whugher, boubuld w. Towein A s. cewicousce hele tafn,
All
" wour ad e lort 
BEGINNING (1681931419.9685323): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5879, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5695, val loss 4.5693 [1.916778564453125 sec]
step 100: train loss 2.9533, val loss 2.9950 [5.047348737716675 sec]
step 200: train loss 2.7111, val loss 2.7688 [8.132649898529053 sec]
step 300: train loss 2.6118, val loss 2.6808 [11.198642015457153 sec]
step 400: train loss 2.5398, val loss 2.6140 [14.289149045944214 sec]
2.532442331314087
Total Training Time: 15.436326026916504 seconds

weo the ma ntur shit ken urof sesir torononte zenges catpout win le itt theavup'e genahed nd sras Yo
BEGINNING (1681931435.879858): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6551, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6563, val loss 4.6510 [3.009514808654785 sec]
step 100: train loss 2.8607, val loss 2.8866 [8.191770315170288 sec]
step 200: train loss 2.6484, val loss 2.7123 [13.313474178314209 sec]
step 300: train loss 2.5484, val loss 2.6113 [18.439502954483032 sec]
step 400: train loss 2.4732, val loss 2.5485 [23.525315523147583 sec]
2.381176233291626
Total Training Time: 25.661285161972046 seconds

ckiy hars had oublin abulan rorkindd, h thus. wised ahara gaSppamAnor tu th uswis s iongekes. Grs th
BEGINNING (1681931462.531088): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5227, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5144, val loss 4.5154 [4.307488918304443 sec]
step 100: train loss 2.7988, val loss 2.8431 [11.497661352157593 sec]
step 200: train loss 2.6104, val loss 2.6794 [18.60701012611389 sec]
step 300: train loss 2.5157, val loss 2.5852 [25.790100574493408 sec]
step 400: train loss 2.4358, val loss 2.5090 [33.07744073867798 sec]
2.3782742023468018
Total Training Time: 36.152425050735474 seconds

per thed Polas rit A t wanaskme hinwn af Wrep thahe oh wabe ons tesps akF on cede 
othengur grdsske 
BEGINNING (1681931499.9305508): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6481, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6532, val loss 4.6503 [2.181087017059326 sec]
step 100: train loss 2.9676, val loss 2.9948 [5.810949325561523 sec]
step 200: train loss 2.7311, val loss 2.7796 [9.441335201263428 sec]
step 300: train loss 2.6319, val loss 2.6913 [13.071728706359863 sec]
step 400: train loss 2.5744, val loss 2.6397 [16.657045125961304 sec]
2.5350892543792725
Total Training Time: 18.09269690513611 seconds

Bite com than anapin anyover0e
dYoutonst ) we ueares. tcut s atcacGrrmodig
ore Therathave o he?sr ol
BEGINNING (1681931518.4692461): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6151, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6061, val loss 4.6003 [3.620197296142578 sec]
step 100: train loss 2.8533, val loss 2.8919 [10.000351667404175 sec]
step 200: train loss 2.6639, val loss 2.7228 [16.6016526222229 sec]
step 300: train loss 2.5783, val loss 2.6346 [23.039259910583496 sec]
step 400: train loss 2.5162, val loss 2.5844 [29.199301958084106 sec]
2.4423882961273193
Total Training Time: 31.749096155166626 seconds

Orilos
toubourevend I"
I saye fethegu akel
Thesty
ifGre, t im I tempalengeelegrhtofrg henu tt tared,
BEGINNING (1681931551.1394322): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6285, val loss 4.6215 [5.007740497589111 sec]
step 100: train loss 2.7897, val loss 2.8247 [13.643681049346924 sec]
step 200: train loss 2.6241, val loss 2.6842 [22.351044416427612 sec]
step 300: train loss 2.5439, val loss 2.6107 [31.32995867729187 sec]
step 400: train loss 2.4839, val loss 2.5624 [40.12906789779663 sec]
2.4383316040039062
Total Training Time: 43.76688599586487 seconds

watheas cosin
lpr, bely de• wond hesenores sply“arenboued
vinon me noy ie Weno ha nouws.
ws we Soman
BEGINNING (1681931596.2074509): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5969, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5930, val loss 4.5955 [3.0976548194885254 sec]
step 100: train loss 2.9334, val loss 2.9595 [8.256133317947388 sec]
step 200: train loss 2.7207, val loss 2.7670 [13.433491230010986 sec]
step 300: train loss 2.6363, val loss 2.6960 [18.598097801208496 sec]
step 400: train loss 2.5877, val loss 2.6527 [23.729892253875732 sec]
2.6088132858276367
Total Training Time: 25.81098246574402 seconds

cof cheoled. tos alelo paknthes, poch af t."
s Gr tene plumun ayomatouo honghe ensomatid
 iclA t f i
BEGINNING (1681931622.4848626): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6029, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6087, val loss 4.6201 [5.229197025299072 sec]
step 100: train loss 2.8370, val loss 2.8757 [14.294559240341187 sec]
step 200: train loss 2.6636, val loss 2.7251 [23.337742567062378 sec]
step 300: train loss 2.5839, val loss 2.6563 [32.37471604347229 sec]
step 400: train loss 2.5396, val loss 2.6114 [41.52745175361633 sec]
2.5225937366485596
Total Training Time: 45.3589608669281 seconds

cSERem shore
GBmanpr wanacuyo I m ke brllaey ust tisllhid t.iand ayound tayo hd ovatthus
Ne mp(iern.
BEGINNING (1681931668.6815426): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5459, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5468, val loss 4.5320 [7.412540435791016 sec]
step 100: train loss 2.7812, val loss 2.8141 [20.336867809295654 sec]
step 200: train loss 2.6274, val loss 2.6832 [33.55775690078735 sec]
step 300: train loss 2.5525, val loss 2.6191 [46.45008969306946 sec]
step 400: train loss 2.5096, val loss 2.5917 [59.37670969963074 sec]
2.487553596496582
Total Training Time: 64.90383386611938 seconds

pacreugo/lid thind th
c3
1 a ven bualifone to wsthinlly P Fimured tan t mkiger ad
f ratherovndgheees
BEGINNING (1681931734.855764): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6797, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6736, val loss 4.6853 [2.090114116668701 sec]
step 100: train loss 2.9577, val loss 3.0010 [5.450874090194702 sec]
step 200: train loss 2.7106, val loss 2.7690 [8.778848648071289 sec]
step 300: train loss 2.6088, val loss 2.6693 [12.124626636505127 sec]
step 400: train loss 2.5269, val loss 2.6028 [15.492639064788818 sec]
2.4933552742004395
Total Training Time: 16.762375116348267 seconds

wpaege nouly the thf this anoused. s we os
ahus "Torlasbedy e henat. ili har astpe ve
An athice cay 
BEGINNING (1681931752.0648232): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6471, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6438, val loss 4.6421 [3.208906888961792 sec]
step 100: train loss 2.8358, val loss 2.8736 [8.63589072227478 sec]
step 200: train loss 2.6418, val loss 2.7022 [14.11036992073059 sec]
step 300: train loss 2.5312, val loss 2.6024 [19.49013590812683 sec]
step 400: train loss 2.4499, val loss 2.5219 [24.89988398551941 sec]
2.4312617778778076
Total Training Time: 27.162012100219727 seconds

sand Con wod.
" "M(al oo celmes wis abwinot lin a;ighe tI has rrer hikl harot here Pimine Bens sthes
BEGINNING (1681931780.0658493): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6831, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6955, val loss 4.7058 [4.320367336273193 sec]
step 100: train loss 2.7852, val loss 2.8226 [11.840834140777588 sec]
step 200: train loss 2.6145, val loss 2.6706 [19.62251091003418 sec]
step 300: train loss 2.5018, val loss 2.5730 [27.11333966255188 sec]
step 400: train loss 2.4217, val loss 2.5002 [34.676870346069336 sec]
2.3477580547332764
Total Training Time: 37.87376070022583 seconds

wistindte.
"It Gled ttA A
Rorparat ce rouat g
Gris rsisimuayacThftalisthe blow sEs hes herdre l, KAn
BEGINNING (1681931819.1636696): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6087, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6078, val loss 4.6065 [2.557861566543579 sec]
step 100: train loss 2.9384, val loss 2.9757 [6.744494438171387 sec]
step 200: train loss 2.7143, val loss 2.7703 [10.971326112747192 sec]
step 300: train loss 2.6324, val loss 2.6901 [15.14905858039856 sec]
step 400: train loss 2.5606, val loss 2.6321 [19.305107593536377 sec]
2.549212694168091
Total Training Time: 20.93443465232849 seconds

hupta %lere
sthI fhins Dafill the." Hobr lyfe Ot, in9." ulndred Grerteed 7rangilis
QDed
ner othep(S 
BEGINNING (1681931840.573158): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6608, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6577, val loss 4.6635 [4.139642715454102 sec]
step 100: train loss 2.8411, val loss 2.8812 [11.180853128433228 sec]
step 200: train loss 2.6467, val loss 2.7118 [18.261902570724487 sec]
step 300: train loss 2.5649, val loss 2.6378 [25.42347478866577 sec]
step 400: train loss 2.5019, val loss 2.5807 [32.50695443153381 sec]
2.485191583633423
Total Training Time: 35.41158056259155 seconds

mmath catt his Zes-h. Gouriry sgrfracorl
soud fo uto3" Ad t omat Sofooom Charitundsp
r."Wh beres%ona
BEGINNING (1681931876.8306997): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6046, val loss 4.6034 [5.6953651905059814 sec]
step 100: train loss 2.7803, val loss 2.8207 [15.54200530052185 sec]
step 200: train loss 2.6127, val loss 2.6774 [25.37149715423584 sec]
step 300: train loss 2.5304, val loss 2.6016 [35.24634099006653 sec]
step 400: train loss 2.4655, val loss 2.5437 [45.09331512451172 sec]
2.452174663543701
Total Training Time: 49.324707984924316 seconds

kt cor ofthi, se anay f rhe. Trromas llksen a aghe
wouin trathe aind d mod rd tumadsp
Grofra waday t
BEGINNING (1681931927.4419425): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6615, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6469, val loss 4.6479 [3.7264411449432373 sec]
step 100: train loss 2.9199, val loss 2.9511 [9.98340630531311 sec]
step 200: train loss 2.7148, val loss 2.7651 [16.24255633354187 sec]
step 300: train loss 2.6243, val loss 2.6928 [22.449472427368164 sec]
step 400: train loss 2.5769, val loss 2.6455 [28.667317390441895 sec]
2.583228826522827
Total Training Time: 31.19284415245056 seconds

watauomof ahinat fer t
o "Galde byoousinonBiore px prf s
nd y Buouralie sarpllbrcondg owhe reday t n
BEGINNING (1681931959.083137): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5734, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5742, val loss 4.5641 [6.312087535858154 sec]
step 100: train loss 2.8345, val loss 2.8739 [17.237948656082153 sec]
step 200: train loss 2.6492, val loss 2.7104 [28.15274691581726 sec]
step 300: train loss 2.5764, val loss 2.6446 [39.12818765640259 sec]
step 400: train loss 2.5280, val loss 2.5979 [50.032448530197144 sec]
2.4974615573883057
Total Training Time: 54.65508437156677 seconds

ored, anied suntd. Iu ts as
Ygorda!"
Ak akanbor witelinofokins miiriterinchube rmH mpuwatha
lwinw h'
BEGINNING (1681932014.67928): Baseline LR(6e-05) Heads(4) Embeddings(256) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5635, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5567, val loss 4.5589 [8.897156476974487 sec]
step 100: train loss 2.7810, val loss 2.8194 [24.46716547012329 sec]
step 200: train loss 2.6207, val loss 2.6833 [40.06466341018677 sec]
step 300: train loss 2.5455, val loss 2.6129 [55.64656639099121 sec]
step 400: train loss 2.5025, val loss 2.5800 [71.23200607299805 sec]
2.472538471221924
Total Training Time: 78.10500502586365 seconds

Opit thelit kis acimir lars icaki
Taol't3 us, a kemavege s. henjCed,
s. th wan tthorerathe r ws ta y
BEGINNING (1681932094.0485206): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5785, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5729, val loss 4.5662 [2.2285099029541016 sec]
step 100: train loss 2.8089, val loss 2.8405 [5.929757595062256 sec]
step 200: train loss 2.6180, val loss 2.6746 [9.617826223373413 sec]
step 300: train loss 2.5333, val loss 2.6063 [13.314899206161499 sec]
step 400: train loss 2.4529, val loss 2.5377 [17.043657064437866 sec]
2.3479461669921875
Total Training Time: 18.57148003578186 seconds

heyoas, gom sowin ath ced atthincey
A ld ho on yowiny hahatate oughastede t aed t s d o talleire ppe
BEGINNING (1681932113.266068): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5996, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5836, val loss 4.5853 [3.7264297008514404 sec]
step 100: train loss 2.7220, val loss 2.7680 [10.389244079589844 sec]
step 200: train loss 2.5730, val loss 2.6410 [17.14168691635132 sec]
step 300: train loss 2.4798, val loss 2.5510 [23.624996185302734 sec]
step 400: train loss 2.3940, val loss 2.4751 [30.178451538085938 sec]
2.312357187271118
Total Training Time: 32.99174666404724 seconds

s." hrapl“ Dompuwom."
"ber ©u dr Groftha d wal Huorad Cmey ifthte.
Moura Gr TE PIe ArasYe teiarins
A
BEGINNING (1681932147.418668): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5310, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5364, val loss 4.5386 [5.269179821014404 sec]
step 100: train loss 2.6683, val loss 2.7260 [14.591765642166138 sec]
step 200: train loss 2.5390, val loss 2.5982 [24.172605752944946 sec]
step 300: train loss 2.4369, val loss 2.5220 [33.57658100128174 sec]
step 400: train loss 2.3659, val loss 2.4577 [43.06017327308655 sec]
2.3095266819000244
Total Training Time: 47.578251361846924 seconds

warist orkere ay TZet thef tir ghimTarept th asthe areny ar gillibrgven ot. 5eacor wnd "A "
charagap
BEGINNING (1681932196.7499528): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6233, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6179, val loss 4.6065 [2.5425221920013428 sec]
step 100: train loss 2.7858, val loss 2.8322 [6.82167911529541 sec]
step 200: train loss 2.6185, val loss 2.6912 [11.098546266555786 sec]
step 300: train loss 2.5468, val loss 2.6172 [15.362204551696777 sec]
step 400: train loss 2.4986, val loss 2.5789 [19.6326265335083 sec]
2.4362306594848633
Total Training Time: 21.393930673599243 seconds

of mor no Thigun shey ond an – fpeupr, sthorWasu aland,
stoorour bevs E
"lahid, The thed eeyo oQ mer
BEGINNING (1681932218.7884634): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6810, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6658, val loss 4.6780 [4.391587257385254 sec]
step 100: train loss 2.7039, val loss 2.7609 [12.117350339889526 sec]
step 200: train loss 2.5756, val loss 2.6402 [19.82067370414734 sec]
step 300: train loss 2.5074, val loss 2.5902 [27.550963878631592 sec]
step 400: train loss 2.4513, val loss 2.5319 [35.253286600112915 sec]
2.4068400859832764
Total Training Time: 38.67933130264282 seconds

toowe w lo urread'tee
Hhesere
ce aked th ar hife werompulcrr borm. bat ANay. ture•.
s seys Thomet Gr
BEGINNING (1681932258.7175727): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6648, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6781, val loss 4.6774 [6.368675947189331 sec]
step 100: train loss 2.6765, val loss 2.7438 [17.477952480316162 sec]
step 200: train loss 2.5473, val loss 2.6176 [28.576549530029297 sec]
step 300: train loss 2.4887, val loss 2.5674 [39.70779824256897 sec]
step 400: train loss 2.4335, val loss 2.5203 [50.801270484924316 sec]
2.4203271865844727
Total Training Time: 55.65732765197754 seconds

he ar nad
tollod, he Anon wapo a o 7e ded thalillet SNot ase Wiok. w smaralidonougas, ke nd w. thidd
BEGINNING (1681932316.1070125): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6166, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6188, val loss 4.6089 [3.43410062789917 sec]
step 100: train loss 2.7793, val loss 2.8149 [9.41328239440918 sec]
step 200: train loss 2.6308, val loss 2.6947 [15.354156255722046 sec]
step 300: train loss 2.5610, val loss 2.6394 [21.278223037719727 sec]
step 400: train loss 2.5241, val loss 2.6035 [27.24172568321228 sec]
2.53556489944458
Total Training Time: 29.759453535079956 seconds

EANE AENEANNNGfux TE NGACHEEAFGanyN N a "
CAN N4E“t ss ha'To HERTEA n – va0 g TYZKqEFP˜
N('TwarmacKH
BEGINNING (1681932346.6395795): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6423, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6438, val loss 4.6486 [6.253969192504883 sec]
step 100: train loss 2.7000, val loss 2.7617 [17.157877683639526 sec]
step 200: train loss 2.5814, val loss 2.6557 [28.02570152282715 sec]
step 300: train loss 2.5224, val loss 2.5960 [38.8855767250061 sec]
step 400: train loss 2.4879, val loss 2.5747 [49.78818678855896 sec]
2.520580530166626
Total Training Time: 54.50263500213623 seconds

ACatad es as tiligned, thed p re comad s de. tahibtofalcuoupalowa t
"I whe twonod tsythems ovbel he 
BEGINNING (1681932402.3260353): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5569, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5555, val loss 4.5573 [8.87448000907898 sec]
step 100: train loss 2.6646, val loss 2.7249 [24.703099727630615 sec]
step 200: train loss 2.5506, val loss 2.6220 [40.490015268325806 sec]
step 300: train loss 2.5026, val loss 2.5810 [57.83219861984253 sec]
step 400: train loss 2.4722, val loss 2.5591 [74.11822485923767 sec]
2.4722700119018555
Total Training Time: 81.05164217948914 seconds

tised airi mery iney tothant, thed
Hepof amand owlad c s nare ane bunthe m, herens,
gopalm. tomos
•i
BEGINNING (1681932485.2053928): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6081, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5989, val loss 4.6064 [2.472886323928833 sec]
step 100: train loss 2.7610, val loss 2.8158 [6.848110675811768 sec]
step 200: train loss 2.6098, val loss 2.6713 [11.058013439178467 sec]
step 300: train loss 2.5074, val loss 2.5810 [15.31580376625061 sec]
step 400: train loss 2.4125, val loss 2.4945 [19.55411696434021 sec]
2.3313629627227783
Total Training Time: 21.240761280059814 seconds

ER8CDR Ais Fis bo Ps
Eathind pras feme.. G me – –2 "Leauell fonar sis fike ghen uslay sht watusyag o
BEGINNING (1681932507.1071856): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5589, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5914, val loss 4.6082 [4.121169567108154 sec]
step 100: train loss 2.7110, val loss 2.7508 [11.383233547210693 sec]
step 200: train loss 2.5502, val loss 2.6165 [19.17207646369934 sec]
step 300: train loss 2.4627, val loss 2.5371 [26.452013969421387 sec]
step 400: train loss 2.3670, val loss 2.4539 [33.69273090362549 sec]
2.3128530979156494
Total Training Time: 36.77957057952881 seconds

Uâ
GA2ITER alld O Hinoy sthe thernd wane s
tes
ga waso" tuthefen, atak llas krahave
f at, moweolleso
BEGINNING (1681932545.147846): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6302, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6027, val loss 4.5989 [5.670976400375366 sec]
step 100: train loss 2.6441, val loss 2.7100 [16.3570613861084 sec]
step 200: train loss 2.5026, val loss 2.5845 [26.937829971313477 sec]
step 300: train loss 2.4091, val loss 2.5011 [37.00979685783386 sec]
step 400: train loss 2.3196, val loss 2.4212 [47.65186381340027 sec]
2.2981672286987305
Total Training Time: 52.77604579925537 seconds

pbes." AN Hnen to hicch the the Cla Arens mor markeff.
wer hithes ghe he eass thor thif ted worin ce
BEGINNING (1681932599.7882977): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6103, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6119, val loss 4.6009 [3.433206081390381 sec]
step 100: train loss 2.7819, val loss 2.8079 [12.094491481781006 sec]
step 200: train loss 2.6163, val loss 2.6741 [17.53654909133911 sec]
step 300: train loss 2.5393, val loss 2.6162 [23.13688850402832 sec]
step 400: train loss 2.4871, val loss 2.5638 [28.2931010723114 sec]
2.4098520278930664
Total Training Time: 30.484327793121338 seconds

Ara9sketh wota rin. su I s Nour wout his send
alwom leshellyot smisempns
anengakared l l wanttord th
BEGINNING (1681932630.8856163): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6113, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6208, val loss 4.6210 [5.239643812179565 sec]
step 100: train loss 2.6841, val loss 2.7359 [14.34077501296997 sec]
step 200: train loss 2.5595, val loss 2.6220 [23.54313635826111 sec]
step 300: train loss 2.4802, val loss 2.5583 [32.60621428489685 sec]
step 400: train loss 2.4160, val loss 2.5029 [42.18110013008118 sec]
2.36267352104187
Total Training Time: 46.352861166000366 seconds

'sereg, ead. The mre y stho cemer ban juat hipe an, hed ge rd suoto b, or(HOnte avenl lll ghe
bp as 
BEGINNING (1681932678.5400338): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6553, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6515, val loss 4.6598 [7.8439483642578125 sec]
step 100: train loss 2.6553, val loss 2.7147 [21.63706636428833 sec]
step 200: train loss 2.5298, val loss 2.6024 [37.443482398986816 sec]
step 300: train loss 2.4636, val loss 2.5440 [54.74512076377869 sec]
step 400: train loss 2.3956, val loss 2.4884 [67.7115387916565 sec]
2.3333780765533447
Total Training Time: 73.29423117637634 seconds

hre tocenit? Ged soweshex Lallle georn's at ass se atey.
d shededeile wnd omaBis Southailerd y cow g
BEGINNING (1681932753.6247053): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6599, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6529, val loss 4.6454 [4.516995191574097 sec]
step 100: train loss 2.7671, val loss 2.8021 [12.352632522583008 sec]
step 200: train loss 2.6133, val loss 2.6733 [20.185688495635986 sec]
step 300: train loss 2.5469, val loss 2.6194 [33.162309646606445 sec]
step 400: train loss 2.5092, val loss 2.5851 [41.02045226097107 sec]
2.492102861404419
Total Training Time: 44.30764818191528 seconds

Qa HAwh lllonke wour rsed ara clphe pevekiid, Yend yaithtarereromarnghe isthuceny ~ng che thald's al
BEGINNING (1681932798.5794528): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5760, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5904, val loss 4.5933 [9.306471109390259 sec]
step 100: train loss 2.6936, val loss 2.7451 [23.61751914024353 sec]
step 200: train loss 2.5636, val loss 2.6328 [40.44128131866455 sec]
step 300: train loss 2.5087, val loss 2.5826 [54.76327967643738 sec]
step 400: train loss 2.4685, val loss 2.5555 [69.06151175498962 sec]
2.434370756149292
Total Training Time: 75.24008893966675 seconds

CI were cll nthood. wiskaploul
tju spr he ter at follso brveraNe
tye." tsed browathe canke s jed.
Th
BEGINNING (1681932875.036825): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6058, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6163, val loss 4.6227 [14.066957235336304 sec]
step 100: train loss 2.6676, val loss 2.7239 [35.001702070236206 sec]
step 200: train loss 2.5447, val loss 2.6297 [59.30194902420044 sec]
step 300: train loss 2.4929, val loss 2.5686 [81.83050870895386 sec]
step 400: train loss 2.4468, val loss 2.5347 [104.76394152641296 sec]
2.4238803386688232
Total Training Time: 115.65632033348083 seconds

ETTEMTHM
CHEAWed
S; wissisped tsilflurey ull olaw by eree tans bofe aqun w ipo Tan ty'blllearingalam
BEGINNING (1681932992.795472): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6815, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6600, val loss 4.6557 [3.744934558868408 sec]
step 100: train loss 2.7723, val loss 2.8132 [9.585194110870361 sec]
step 200: train loss 2.5952, val loss 2.6581 [15.192094087600708 sec]
step 300: train loss 2.4877, val loss 2.5655 [20.032880306243896 sec]
step 400: train loss 2.3861, val loss 2.4705 [24.841828107833862 sec]
2.300523281097412
Total Training Time: 26.801385402679443 seconds

Muth, I at wok.
"WI ll sikulled hed thit bu(e the thony st aro te qur.
GHeriande yarow sematta owate
BEGINNING (1681933020.3198004): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6119, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5978, val loss 4.5864 [4.927487850189209 sec]
step 100: train loss 2.6941, val loss 2.7457 [13.410136938095093 sec]
step 200: train loss 2.5329, val loss 2.6073 [21.82627010345459 sec]
step 300: train loss 2.4205, val loss 2.5049 [30.55011534690857 sec]
step 400: train loss 2.3150, val loss 2.4166 [39.166948080062866 sec]
2.2216639518737793
Total Training Time: 42.675036668777466 seconds

Ophing. Wh. Anad hed dis yan as you would
hed trorys irt ifr cuent at anave.
SV
wand "5led Grond tO 
BEGINNING (1681933064.2590392): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.7007, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7027, val loss 4.6960 [6.754289865493774 sec]
step 100: train loss 2.6526, val loss 2.7034 [18.37674331665039 sec]
step 200: train loss 2.4946, val loss 2.5717 [29.88498568534851 sec]
step 300: train loss 2.3931, val loss 2.4793 [41.61705732345581 sec]
step 400: train loss 2.2918, val loss 2.3928 [53.235697984695435 sec]
2.2205004692077637
Total Training Time: 58.09089231491089 seconds

t cruow arnorth. Ana aved den hin homed ha'd naganges ast wald the hal them ybrtas ke owiver his thi
BEGINNING (1681933124.152256): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6173, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6184, val loss 4.6229 [3.6237735748291016 sec]
step 100: train loss 2.7718, val loss 2.8055 [9.85186219215393 sec]
step 200: train loss 2.5963, val loss 2.6679 [16.12417221069336 sec]
step 300: train loss 2.5157, val loss 2.5925 [22.364094018936157 sec]
step 400: train loss 2.4550, val loss 2.5353 [28.66784429550171 sec]
2.445979595184326
Total Training Time: 31.223929166793823 seconds

rotbels sar wereave, doung
't Gra! we. IoondimYou a ly st tand, wid ala hecurll higrd ilst t fored o
BEGINNING (1681933156.0271773): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6173, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6141, val loss 4.6033 [6.391794919967651 sec]
step 100: train loss 2.6922, val loss 2.7460 [17.7081036567688 sec]
step 200: train loss 2.5574, val loss 2.6307 [29.163624048233032 sec]
step 300: train loss 2.4774, val loss 2.5551 [40.44711089134216 sec]
step 400: train loss 2.4087, val loss 2.5039 [51.753689765930176 sec]
2.357337236404419
Total Training Time: 56.58468508720398 seconds

7 wil Pyok. "If This sele icowisey cou tswane cheld ud worelo ss."Aveyonds heriles
ng starah y – obu
BEGINNING (1681933213.9410203): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6486, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6564, val loss 4.6659 [9.406661033630371 sec]
step 100: train loss 2.6534, val loss 2.7134 [25.32317018508911 sec]
step 200: train loss 2.5177, val loss 2.6026 [41.62739396095276 sec]
step 300: train loss 2.4429, val loss 2.5288 [57.99817085266113 sec]
step 400: train loss 2.3693, val loss 2.4616 [74.5737931728363 sec]
2.3318076133728027
Total Training Time: 81.36897730827332 seconds

be comnp; aman tout am so
The ealigembs. Grs nor ata hatatad at ta difolovea mp, hedingeat ing arice
BEGINNING (1681933297.2222445): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5818, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5891, val loss 4.5787 [7.585127830505371 sec]
step 100: train loss 2.7631, val loss 2.8019 [18.725998163223267 sec]
step 200: train loss 2.6151, val loss 2.6783 [28.507624626159668 sec]
step 300: train loss 2.5474, val loss 2.6194 [39.10451030731201 sec]
step 400: train loss 2.5050, val loss 2.5855 [53.382580041885376 sec]
2.4751598834991455
Total Training Time: 60.48371887207031 seconds

ratthe soriathe arthikitooura'r. ha
t? s f m. ithig ced. ace fle tallent woursa "KArat "Thed te ofrt
BEGINNING (1681933358.558594): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6356, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6369, val loss 4.6334 [12.743727684020996 sec]
step 100: train loss 2.6901, val loss 2.7392 [34.36194086074829 sec]
step 200: train loss 2.5614, val loss 2.6345 [54.82646584510803 sec]
step 300: train loss 2.4991, val loss 2.5760 [72.238938331604 sec]
step 400: train loss 2.4570, val loss 2.5439 [89.59242081642151 sec]
2.443441152572632
Total Training Time: 97.0228865146637 seconds

guthe sir imandnd
e€hind atodscumed and illaten htra ake a cike dghuphtof arad.
s h "An ra ravefilec
BEGINNING (1681933456.813168): Baseline LR(6e-05) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.7193, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7219, val loss 4.7170 [16.214361906051636 sec]
step 100: train loss 2.6541, val loss 2.7119 [48.24448585510254 sec]
step 200: train loss 2.5373, val loss 2.6126 [76.34544444084167 sec]
step 300: train loss 2.4868, val loss 2.5704 [113.15175437927246 sec]
step 400: train loss 2.4395, val loss 2.5275 [150.35109686851501 sec]
2.370149850845337
Total Training Time: 165.59778594970703 seconds

pthe mind weriree rais, and, An ly,
condsss sto2
hethe got s
CHe we tharorones, spittey g amund fowo
BEGINNING (1681933624.7072852): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5873, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5673, val loss 4.5600 [2.848022699356079 sec]
step 100: train loss 2.7073, val loss 2.7505 [7.736703872680664 sec]
step 200: train loss 2.5608, val loss 2.6232 [12.631733655929565 sec]
step 300: train loss 2.4625, val loss 2.5411 [18.276175022125244 sec]
step 400: train loss 2.3629, val loss 2.4658 [27.06456470489502 sec]
2.2830474376678467
Total Training Time: 29.179741859436035 seconds

AKaJ!
CElCN PA"I; tuANHE6EE ?s
Grour SUAnd ala b;A hed ANP
muY
henod Ian'd cphal mptoreyaved dnd gan
BEGINNING (1681933654.750053): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6871, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6762, val loss 4.6665 [4.883542776107788 sec]
step 100: train loss 2.6421, val loss 2.7065 [13.73108172416687 sec]
step 200: train loss 2.5198, val loss 2.6031 [22.498289823532104 sec]
step 300: train loss 2.4203, val loss 2.5135 [31.39909052848816 sec]
step 400: train loss 2.3308, val loss 2.4227 [40.565720319747925 sec]
2.224015712738037
Total Training Time: 44.590439796447754 seconds

he wor tugpon. Tus aved heront the. Gramatta tand, Romeled ce
sm Fou heac cure.
-urtht
Clid. Tios Ye
BEGINNING (1681933700.9635289): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6195, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6261, val loss 4.6246 [7.162429094314575 sec]
step 100: train loss 2.6089, val loss 2.6734 [20.443450689315796 sec]
step 200: train loss 2.4754, val loss 2.5704 [33.55619215965271 sec]
step 300: train loss 2.3950, val loss 2.4824 [47.47625684738159 sec]
step 400: train loss 2.2943, val loss 2.4079 [60.387545108795166 sec]
2.183737277984619
Total Training Time: 66.09826683998108 seconds

mod ledel)
goot amm. "we
rBuldie.
pel ales and, westozekke ner Hey.)
"
nongerps shimamed ;rth.
rlour
BEGINNING (1681933769.2664838): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5833, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5890, val loss 4.5914 [3.9284262657165527 sec]
step 100: train loss 2.6901, val loss 2.7587 [10.17822813987732 sec]
step 200: train loss 2.5668, val loss 2.6436 [15.967765092849731 sec]
step 300: train loss 2.4973, val loss 2.5822 [21.695430278778076 sec]
step 400: train loss 2.4303, val loss 2.5244 [28.956210136413574 sec]
2.33915114402771
Total Training Time: 31.392964601516724 seconds

yen hewiold aces ach d wh thas awpy, keckirave de Csse cs fe wee Mooukelelhy fanste bollou mpt
Ka Ch
BEGINNING (1681933801.4414635): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6688, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6635, val loss 4.6604 [6.3770763874053955 sec]
step 100: train loss 2.6312, val loss 2.7056 [18.48939347267151 sec]
step 200: train loss 2.5271, val loss 2.5997 [35.645482540130615 sec]
step 300: train loss 2.4606, val loss 2.5368 [51.46164107322693 sec]
step 400: train loss 2.3969, val loss 2.4845 [63.14459013938904 sec]
2.3641247749328613
Total Training Time: 68.32903933525085 seconds

MPTT" dep falltof hiser yaeg timuot inve
ondis phetik ul th. fed tse be me cow lyre himuerkat vetor 
BEGINNING (1681933871.4981592): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5792, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5981, val loss 4.5989 [15.906804084777832 sec]
step 100: train loss 2.6057, val loss 2.6821 [34.68751239776611 sec]
step 200: train loss 2.5026, val loss 2.5712 [50.76697254180908 sec]
step 300: train loss 2.4250, val loss 2.5171 [68.31285214424133 sec]
step 400: train loss 2.3633, val loss 2.4594 [84.57043242454529 sec]
2.3013336658477783
Total Training Time: 91.57856345176697 seconds

ren ate drJeatt, nany ard amo gerd milatsumo andy tos thekbas st iche n troum smeem as ffingllommeru
BEGINNING (1681933965.5909476): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6151, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6076, val loss 4.6128 [5.108611345291138 sec]
step 100: train loss 2.6901, val loss 2.7488 [13.686147928237915 sec]
step 200: train loss 2.5734, val loss 2.6471 [22.211570024490356 sec]
step 300: train loss 2.5160, val loss 2.5979 [30.700286149978638 sec]
step 400: train loss 2.4768, val loss 2.5614 [39.27863669395447 sec]
2.4625532627105713
Total Training Time: 42.986563205718994 seconds

asingend. Ththerene lld thed uoreys tiy thathef. s. aisuwa bs s.
w co, "
L cawheanera ad asal
Cace f
BEGINNING (1681934009.4185114): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6714, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6678, val loss 4.6720 [8.946581602096558 sec]
step 100: train loss 2.6359, val loss 2.7077 [24.866770029067993 sec]
step 200: train loss 2.5341, val loss 2.6093 [40.77023410797119 sec]
step 300: train loss 2.4849, val loss 2.5696 [56.78755331039429 sec]
step 400: train loss 2.4486, val loss 2.5339 [73.24396586418152 sec]
2.443704605102539
Total Training Time: 80.77792096138 seconds

oreannd rant sapout htinsst tse zor gitowid urofe bitt men,
oved pur
wito as thatealie halort charn,
BEGINNING (1681934091.8869941): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6196, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6221, val loss 4.6252 [13.684026002883911 sec]
step 100: train loss 2.6125, val loss 2.6766 [38.390873432159424 sec]
step 200: train loss 2.5168, val loss 2.5941 [61.84306979179382 sec]
step 300: train loss 2.4655, val loss 2.5504 [85.04740905761719 sec]
step 400: train loss 2.4204, val loss 2.5169 [109.19610071182251 sec]
2.3972389698028564
Total Training Time: 119.7286229133606 seconds

have smplll wons t o yrortighaity of chion, fl a
beik he vopato r aman ghen67
Jl HEAR tuag
"SESChe, 
BEGINNING (1681934213.990167): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6298, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6119, val loss 4.6134 [3.111056089401245 sec]
step 100: train loss 2.6918, val loss 2.7329 [8.450455665588379 sec]
step 200: train loss 2.5381, val loss 2.6120 [13.790056943893433 sec]
step 300: train loss 2.4249, val loss 2.5108 [19.088066577911377 sec]
step 400: train loss 2.3210, val loss 2.4156 [24.574381589889526 sec]
2.216156005859375
Total Training Time: 26.895519971847534 seconds

SEuNTEAY
zNs coingoby anyour tre trelarls.
"I shillivep houif as bepphars of pare the?"
E
AY
"Yatean
BEGINNING (1681934241.6842759): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6518, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6213, val loss 4.6206 [5.416553258895874 sec]
step 100: train loss 2.6181, val loss 2.6747 [14.993024110794067 sec]
step 200: train loss 2.4817, val loss 2.5601 [24.59599804878235 sec]
step 300: train loss 2.3629, val loss 2.4603 [34.25296330451965 sec]
step 400: train loss 2.2560, val loss 2.3679 [43.900972843170166 sec]
2.211660861968994
Total Training Time: 48.15041923522949 seconds

solyo orelkand ivea thacd ande ou oousns icht com oonglef eneis warr winged to Peoreats arou ht fup 
BEGINNING (1681934291.303307): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6444, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6220, val loss 4.6302 [7.573629379272461 sec]
step 100: train loss 2.5950, val loss 2.6555 [21.848234176635742 sec]
step 200: train loss 2.4601, val loss 2.5442 [36.03993821144104 sec]
step 300: train loss 2.3401, val loss 2.4450 [49.97259211540222 sec]
step 400: train loss 2.2321, val loss 2.3510 [63.948267221450806 sec]
2.1936047077178955
Total Training Time: 70.2310471534729 seconds

cey 4
Onarad Wene t€ell ghs fa rissaliett ridriogen gel histest abisore
sping therertelyoue the wase
BEGINNING (1681934363.7503426): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6267, val loss 4.6236 [4.11816668510437 sec]
step 100: train loss 2.6839, val loss 2.7367 [11.390116453170776 sec]
step 200: train loss 2.5490, val loss 2.6337 [18.565195322036743 sec]
step 300: train loss 2.4760, val loss 2.5494 [25.70905113220215 sec]
step 400: train loss 2.4067, val loss 2.4921 [32.83969783782959 sec]
2.303071975708008
Total Training Time: 35.90528702735901 seconds

tay has muple anoudd sbin cuomuril w? it. Grattoust htald
are Avelligefo den Gr a as. N "
jupotattre
BEGINNING (1681934400.438148): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6434, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6548, val loss 4.6489 [7.431382894515991 sec]
step 100: train loss 2.6249, val loss 2.6780 [20.618983268737793 sec]
step 200: train loss 2.5061, val loss 2.5809 [33.78520107269287 sec]
step 300: train loss 2.4247, val loss 2.5167 [47.09538698196411 sec]
step 400: train loss 2.3505, val loss 2.4569 [60.22477984428406 sec]
2.329803466796875
Total Training Time: 65.94794392585754 seconds

hiringe hay cass ioms. Twled Ald alye o bree
oamingith thed h lalled s falaplld." s, hase sthe le lo
BEGINNING (1681934467.953679): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6763, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6790, val loss 4.6712 [10.70447850227356 sec]
step 100: train loss 2.5969, val loss 2.6693 [30.31109309196472 sec]
step 200: train loss 2.4832, val loss 2.5675 [49.955379247665405 sec]
step 300: train loss 2.4042, val loss 2.4988 [69.20357251167297 sec]
step 400: train loss 2.3250, val loss 2.4246 [88.34506726264954 sec]
2.2398746013641357
Total Training Time: 96.81918239593506 seconds

be wllit ronotith ay eas degut.
"I'filours SEI Gye sttartuea ce
1
t Thea waes fayorouwad t wo t tyul
BEGINNING (1681934567.0087159): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6431, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6513, val loss 4.6526 [6.560830354690552 sec]
step 100: train loss 2.6807, val loss 2.7418 [17.868366718292236 sec]
step 200: train loss 2.5623, val loss 2.6336 [29.740131616592407 sec]
step 300: train loss 2.5052, val loss 2.5862 [41.018431663513184 sec]
step 400: train loss 2.4619, val loss 2.5475 [52.85508751869202 sec]
2.4249768257141113
Total Training Time: 58.01780128479004 seconds

and yoou atl a ma
futellly on, canof s, fthirn El ass. aco waiay rbeny. waratemis sthe theureared, r
BEGINNING (1681934625.8601146): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5333, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5290, val loss 4.5207 [11.86634612083435 sec]
step 100: train loss 2.6260, val loss 2.6932 [32.963367223739624 sec]
step 200: train loss 2.5202, val loss 2.6083 [53.977256059646606 sec]
step 300: train loss 2.4670, val loss 2.5597 [74.90996265411377 sec]
step 400: train loss 2.4220, val loss 2.5139 [95.87994956970215 sec]
2.4203073978424072
Total Training Time: 104.97343015670776 seconds

Aver thak, wind thelly whes
thon acen fow t gl.)seae inghe 2Wed for!"
"Yoorowonatan T
stoopeds thigh
BEGINNING (1681934732.4111257): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6106, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6167, val loss 4.6035 [17.33597469329834 sec]
step 100: train loss 2.5998, val loss 2.6639 [47.050477027893066 sec]
step 200: train loss 2.5052, val loss 2.5900 [76.71804141998291 sec]
step 300: train loss 2.4540, val loss 2.5450 [106.2334394454956 sec]
step 400: train loss 2.4024, val loss 2.4924 [135.92642617225647 sec]
2.3456356525421143
Total Training Time: 148.65744733810425 seconds

eust so pan'silys ollot lin.
Grerd Tom oigeacoren. wony?" Thed Grata d serais h as leate fffed alour
BEGINNING (1681934883.4126184): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5753, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5596, val loss 4.5610 [3.550041675567627 sec]
step 100: train loss 2.6745, val loss 2.7348 [9.622988939285278 sec]
step 200: train loss 2.5304, val loss 2.6121 [15.684439420700073 sec]
step 300: train loss 2.4069, val loss 2.4938 [21.798475980758667 sec]
step 400: train loss 2.2817, val loss 2.3903 [27.882211208343506 sec]
2.2259609699249268
Total Training Time: 30.52022409439087 seconds

arwo?"
"HAV nond wey. Tea of tle buerttace rfte EAThey 1obu win foft matd lot I~s wa yof n. Ta Oiang
BEGINNING (1681934914.7650034): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6118, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6110, val loss 4.6150 [6.263533592224121 sec]
step 100: train loss 2.6077, val loss 2.6732 [17.077302932739258 sec]
step 200: train loss 2.4645, val loss 2.5391 [27.69158434867859 sec]
step 300: train loss 2.3410, val loss 2.4288 [38.30170774459839 sec]
step 400: train loss 2.2141, val loss 2.3327 [48.89573073387146 sec]
2.1275627613067627
Total Training Time: 53.67020082473755 seconds

le to ledils the wittefth fereatte
caver Co. The Ained wang Gratteatt, Change
roulp dis onodirs kire
BEGINNING (1681934969.9974022): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6644, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6653, val loss 4.6683 [8.468855619430542 sec]
step 100: train loss 2.5869, val loss 2.6478 [23.766022205352783 sec]
step 200: train loss 2.4401, val loss 2.5303 [39.068933725357056 sec]
step 300: train loss 2.3093, val loss 2.4194 [54.56515884399414 sec]
step 400: train loss 2.1743, val loss 2.2972 [70.35830879211426 sec]
2.0650601387023926
Total Training Time: 77.27293658256531 seconds

7TUSE"DAPTE; Yeryour THe teme roull The con) ioke ice hered shom snopers, wan turnoned atyse ialy ow
BEGINNING (1681935049.5531456): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6567, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6675, val loss 4.6642 [5.1193931102752686 sec]
step 100: train loss 2.6809, val loss 2.7306 [14.023228883743286 sec]
step 200: train loss 2.5405, val loss 2.6225 [23.059074640274048 sec]
step 300: train loss 2.4593, val loss 2.5435 [31.702502012252808 sec]
step 400: train loss 2.3804, val loss 2.4769 [40.54832983016968 sec]
2.343153715133667
Total Training Time: 44.37317657470703 seconds

Sou thes
trthed to t!" agookeY
Thipead tya cated oorill tht toot
ghe me re acarreye
thea amp's oow w
BEGINNING (1681935094.7403295): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6263, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6235, val loss 4.6239 [9.491647720336914 sec]
step 100: train loss 2.6165, val loss 2.6849 [25.303073167800903 sec]
step 200: train loss 2.4945, val loss 2.5795 [41.1014723777771 sec]
step 300: train loss 2.4185, val loss 2.5057 [57.08236575126648 sec]
step 400: train loss 2.3282, val loss 2.4309 [73.04295659065247 sec]
2.2867259979248047
Total Training Time: 79.90434885025024 seconds

TAPor. Nay wer He curiles the ghas noped adls?
" sore Grarootyouind
Sooo alor. !"I l L we ghat
79 ck
BEGINNING (1681935176.222993): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6304, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6267, val loss 4.6215 [12.900530338287354 sec]
step 100: train loss 2.5927, val loss 2.6612 [35.90009164810181 sec]
step 200: train loss 2.4718, val loss 2.5554 [58.97218346595764 sec]
step 300: train loss 2.4018, val loss 2.4914 [82.31024742126465 sec]
step 400: train loss 2.2935, val loss 2.4093 [107.77053046226501 sec]
2.178750991821289
Total Training Time: 117.85225677490234 seconds

"V TEM, Fiaid,
€Gratto ong, qup bor eneer rotid heancegh aled cuwass
feet hithef forzin hef arizeani
BEGINNING (1681935296.6117764): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6615, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6585, val loss 4.6554 [7.850339412689209 sec]
step 100: train loss 2.6815, val loss 2.7369 [22.37045168876648 sec]
step 200: train loss 2.5559, val loss 2.6342 [39.95166373252869 sec]
step 300: train loss 2.4943, val loss 2.5753 [55.10283136367798 sec]
step 400: train loss 2.4530, val loss 2.5396 [70.36039543151855 sec]
2.43878173828125
Total Training Time: 76.71156573295593 seconds

ould mendin hiseweritof ay bitcan be s benther ans, h ayd :imins
of
tw – fanart y oure, pe heal dlll
BEGINNING (1681935374.243255): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6671, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6584, val loss 4.6634 [20.25830578804016 sec]
step 100: train loss 2.6236, val loss 2.6894 [47.67041063308716 sec]
step 200: train loss 2.5175, val loss 2.5943 [72.53531622886658 sec]
step 300: train loss 2.4696, val loss 2.5526 [97.8817241191864 sec]
step 400: train loss 2.4185, val loss 2.5084 [122.23017477989197 sec]
2.437070608139038
Total Training Time: 132.46275186538696 seconds

che min twaleal theng tow wais re;
t, here c, lfowider a'smaheaore t fthil tomed tte brransm s ed. l
BEGINNING (1681935508.3303308): Baseline LR(6e-05) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6788, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6635, val loss 4.6587 [20.854437828063965 sec]
step 100: train loss 2.6051, val loss 2.6723 [55.87390613555908 sec]
step 200: train loss 2.4997, val loss 2.5842 [90.40548038482666 sec]
step 300: train loss 2.4449, val loss 2.5335 [127.73987984657288 sec]
step 400: train loss 2.3873, val loss 2.4853 [163.94120264053345 sec]
2.3865442276000977
Total Training Time: 178.69035482406616 seconds

the thic sceras mansds ondis nghe asjuged8
asoof winverour. rf sthiche." anghe Goracaf aiyaxg ii't, 
BEGINNING (1681935689.512356): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6494, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6506, val loss 4.6455 [4.330822229385376 sec]
step 100: train loss 2.6106, val loss 2.6818 [11.845381498336792 sec]
step 200: train loss 2.4818, val loss 2.5648 [19.02064800262451 sec]
step 300: train loss 2.3603, val loss 2.4510 [26.142792463302612 sec]
step 400: train loss 2.2335, val loss 2.3540 [33.305675745010376 sec]
2.1016488075256348
Total Training Time: 40.642926931381226 seconds

AI 5Y~ MofMuring ame they ar cheat noF, all yop his ans the
Nowhing.
Bed, It he tay thecearell to "N
BEGINNING (1681935731.4588625): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6478, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6357, val loss 4.6346 [7.781989574432373 sec]
step 100: train loss 2.5716, val loss 2.6398 [23.72543954849243 sec]
step 200: train loss 2.4397, val loss 2.5366 [37.83095169067383 sec]
step 300: train loss 2.3269, val loss 2.4230 [51.415706634521484 sec]
step 400: train loss 2.2005, val loss 2.3217 [65.50143313407898 sec]
2.123833417892456
Total Training Time: 71.64216208457947 seconds

gutheld spish, "Yake. wer lack seated hant prrof Would s el to eakked doon corr."
"1 haseyas, Ma you
BEGINNING (1681935805.4406533): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.7284, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7347, val loss 4.7319 [10.543895721435547 sec]
step 100: train loss 2.5437, val loss 2.6275 [31.71066164970398 sec]
step 200: train loss 2.4282, val loss 2.5341 [53.90115761756897 sec]
step 300: train loss 2.3092, val loss 2.4184 [73.78559589385986 sec]
step 400: train loss 2.1777, val loss 2.3044 [93.98758387565613 sec]
2.010556697845459
Total Training Time: 103.15254735946655 seconds

arndAnarling the haed fele saljoured
burnent.
"I dould cof prominir ure thirey I cith.
Andger Grarat
BEGINNING (1681935912.0451384): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6477, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6234, val loss 4.6121 [5.042569398880005 sec]
step 100: train loss 2.6020, val loss 2.6785 [14.330890893936157 sec]
step 200: train loss 2.4980, val loss 2.5847 [23.598384380340576 sec]
step 300: train loss 2.4213, val loss 2.5178 [33.008201360702515 sec]
step 400: train loss 2.3539, val loss 2.4548 [42.3488450050354 sec]
2.231459856033325
Total Training Time: 46.652225732803345 seconds

Bl isces want ata dis cedittte.
La roubk, Her ch fa geidefed, adighimllledpl'lle."
stt Wha me gro s 
BEGINNING (1681935959.9695694): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6102, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5945, val loss 4.6027 [9.37602710723877 sec]
step 100: train loss 2.5671, val loss 2.6407 [26.95730948448181 sec]
step 200: train loss 2.4691, val loss 2.5575 [44.551119565963745 sec]
step 300: train loss 2.3907, val loss 2.4865 [62.298418283462524 sec]
step 400: train loss 2.3038, val loss 2.4165 [79.83593010902405 sec]
2.2153983116149902
Total Training Time: 88.02703380584717 seconds

yoote sepat wa, hqutwamin as borld hefled tl the cos,
the her.
Grimo Gratatato PronI tthers aiopphiv
BEGINNING (1681936050.3076596): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5002, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4876, val loss 4.4877 [13.655371189117432 sec]
step 100: train loss 2.5459, val loss 2.6309 [39.91282105445862 sec]
step 200: train loss 2.4595, val loss 2.5475 [66.86852717399597 sec]
step 300: train loss 2.3792, val loss 2.4876 [93.74351787567139 sec]
step 400: train loss 2.2785, val loss 2.4037 [119.61691761016846 sec]
2.171032428741455
Total Training Time: 131.66549730300903 seconds

qu grit tonicl has wond youthyo. Hin wak hand lok way a tre reman
furhir d yohim, wan
" bokings this
BEGINNING (1681936185.3717902): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5695, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5720, val loss 4.5646 [8.202798128128052 sec]
step 100: train loss 2.6063, val loss 2.6782 [23.027384281158447 sec]
step 200: train loss 2.5176, val loss 2.6033 [38.01767826080322 sec]
step 300: train loss 2.4636, val loss 2.5469 [52.68527412414551 sec]
step 400: train loss 2.4264, val loss 2.5088 [67.59719610214233 sec]
2.3989579677581787
Total Training Time: 74.01646304130554 seconds

fow clod paknssedd aslijund ued
thed bal me theete ad sm.
ITaned and wavet. lave serroury Grat ya Ta
BEGINNING (1681936260.5674021): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6676, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6799, val loss 4.6886 [15.40415906906128 sec]
step 100: train loss 2.5767, val loss 2.6451 [43.00446438789368 sec]
step 200: train loss 2.4844, val loss 2.5782 [70.06389355659485 sec]
step 300: train loss 2.4407, val loss 2.5312 [97.24509406089783 sec]
step 400: train loss 2.3891, val loss 2.4930 [124.6749746799469 sec]
2.34407377243042
Total Training Time: 136.66560053825378 seconds

EA7 "Dendext d7" navegakle.
3 V – oureechenco, He tincerere ss toom the
uni, wnsh. The Che sor ritos
BEGINNING (1681936399.5595632): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6299, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6335, val loss 4.6238 [22.385239601135254 sec]
step 100: train loss 2.5437, val loss 2.6251 [58.94056987762451 sec]
step 200: train loss 2.4798, val loss 2.5670 [97.6836290359497 sec]
step 300: train loss 2.4231, val loss 2.5097 [132.7751338481903 sec]
step 400: train loss 2.3635, val loss 2.4641 [167.95440435409546 sec]
2.2691500186920166
Total Training Time: 184.59059524536133 seconds

AY coup "ome matonne we trero aronghe on hasnt se batis
mplo hestheanty.
Arabake Sd bed.
racjoGrat a
BEGINNING (1681936587.6382093): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5897, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5919, val loss 4.5871 [4.420818090438843 sec]
step 100: train loss 2.5914, val loss 2.6507 [12.45527720451355 sec]
step 200: train loss 2.4425, val loss 2.5209 [20.520225763320923 sec]
step 300: train loss 2.2915, val loss 2.4000 [28.586459159851074 sec]
step 400: train loss 2.1654, val loss 2.2935 [36.66833448410034 sec]
2.1047990322113037
Total Training Time: 40.26983165740967 seconds

woth you net ror natthe packe arres onut heand ad
morpsed."
Jin bricach we Gotta tokno wald yons w“ 
BEGINNING (1681936629.0552268): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6032, val loss 4.6069 [8.240451335906982 sec]
step 100: train loss 2.5446, val loss 2.6322 [23.449098587036133 sec]
step 200: train loss 2.3981, val loss 2.4922 [38.8908896446228 sec]
step 300: train loss 2.2579, val loss 2.3747 [54.32597875595093 sec]
step 400: train loss 2.1075, val loss 2.2468 [69.6401617527008 sec]
2.0984861850738525
Total Training Time: 76.66281843185425 seconds

PT FiN ACHAN
MMSEN"KJEAN Thas prom.
4Aive deng ond hilt the o ho gowhim, "Swah lou one w
shing lued 
BEGINNING (1681936707.9517982): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6005, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6280, val loss 4.6368 [11.782020092010498 sec]
step 100: train loss 2.5182, val loss 2.5964 [34.35656666755676 sec]
step 200: train loss 2.3855, val loss 2.4780 [56.75006103515625 sec]
step 300: train loss 2.2368, val loss 2.3545 [78.94052648544312 sec]
step 400: train loss 2.0835, val loss 2.2391 [101.0663046836853 sec]
1.9996308088302612
Total Training Time: 111.67427182197571 seconds

"He wall cordst cayave!
As Eleddus prande soince now roy figh the cenet.
• Taust heal tre peest core
BEGINNING (1681936823.022875): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6188, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6286, val loss 4.6222 [6.521192789077759 sec]
step 100: train loss 2.5943, val loss 2.6670 [18.159603357315063 sec]
step 200: train loss 2.4767, val loss 2.5647 [29.900078296661377 sec]
step 300: train loss 2.3893, val loss 2.4860 [41.553770780563354 sec]
step 400: train loss 2.2874, val loss 2.3998 [53.206522703170776 sec]
2.18959641456604
Total Training Time: 58.39172410964966 seconds

vire."Wh AYe wathe and as. Whes, sas ullon
warors.
Thay susefused, d us!
An, Trel."Heahat " " h fa f
BEGINNING (1681936882.6425617): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6484, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6534, val loss 4.6547 [12.119779109954834 sec]
step 100: train loss 2.5545, val loss 2.6385 [34.0770423412323 sec]
step 200: train loss 2.4516, val loss 2.5390 [55.99021005630493 sec]
step 300: train loss 2.3532, val loss 2.4577 [77.93595123291016 sec]
step 400: train loss 2.2369, val loss 2.3558 [99.86487817764282 sec]
2.117358922958374
Total Training Time: 109.91159510612488 seconds

fore pand thoy has. Mco Sil code
hid En, EAr Kar! Gratay and, Room masumedd hiod
crencoiod. Wep uoke
BEGINNING (1681936994.9157836): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5978, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5861, val loss 4.5958 [17.94533371925354 sec]
step 100: train loss 2.5352, val loss 2.6254 [50.02349090576172 sec]
step 200: train loss 2.4345, val loss 2.5216 [81.0622251033783 sec]
step 300: train loss 2.3291, val loss 2.4323 [112.61651349067688 sec]
step 400: train loss 2.2069, val loss 2.3329 [143.61184978485107 sec]
2.0650691986083984
Total Training Time: 158.04204654693604 seconds

ped proaded dinenty anth. Foum nde sthested the meneurd
pa ednotenn dear.
Na wis blon'd ooked we hto
BEGINNING (1681937156.4256618): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6321, val loss 4.6268 [10.75977373123169 sec]
step 100: train loss 2.5963, val loss 2.6752 [29.27169108390808 sec]
step 200: train loss 2.5048, val loss 2.5897 [47.6698055267334 sec]
step 300: train loss 2.4490, val loss 2.5334 [66.31337022781372 sec]
step 400: train loss 2.3977, val loss 2.4927 [84.70877814292908 sec]
2.3796417713165283
Total Training Time: 92.71731448173523 seconds

the wo rous. He fo tilon akess rir
aestoout t sownd tho thu the y hede V haighe
ppok hican obyothrpl
BEGINNING (1681937250.334582): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6294, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6315, val loss 4.6313 [20.278624773025513 sec]
step 100: train loss 2.5602, val loss 2.6382 [52.69260811805725 sec]
step 200: train loss 2.4788, val loss 2.5673 [85.13405251502991 sec]
step 300: train loss 2.4142, val loss 2.5080 [118.5484676361084 sec]
step 400: train loss 2.3456, val loss 2.4478 [151.70147967338562 sec]
2.3382363319396973
Total Training Time: 166.4561562538147 seconds

Sror Mul •i35
Grratake tufrfar thet therys mGoulnom tt
a watthe ad ftitasprp.
The Yod the is inuzeaz
BEGINNING (1681937419.366724): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5342, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5344, val loss 4.5330 [27.363621711730957 sec]
step 100: train loss 2.5393, val loss 2.6164 [71.97030735015869 sec]
step 200: train loss 2.4690, val loss 2.5583 [115.95831203460693 sec]
step 300: train loss 2.4033, val loss 2.4968 [160.81678318977356 sec]
step 400: train loss 2.3163, val loss 2.4271 [205.70535230636597 sec]
2.293872356414795
Total Training Time: 226.25886750221252 seconds

cklfurds. Kinggas wene hoplpot sto cou foin th ene
ghrcarmian t aved then st bonud reroseaber, w nue
BEGINNING (1681937649.2561085): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5594, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5652, val loss 4.5642 [5.177321672439575 sec]
step 100: train loss 2.5832, val loss 2.6501 [14.522446393966675 sec]
step 200: train loss 2.4262, val loss 2.5184 [24.07513666152954 sec]
step 300: train loss 2.2628, val loss 2.3721 [33.81194353103638 sec]
step 400: train loss 2.1206, val loss 2.2627 [44.24319934844971 sec]
2.041210651397705
Total Training Time: 48.56123399734497 seconds

scoThavar Tay sait, thes'We
the Zitwas i, to ak Tuis cawened.
"I Themal but to to hook yrfu din! Gra
BEGINNING (1681937698.9833467): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6201, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6175, val loss 4.6226 [9.556356430053711 sec]
step 100: train loss 2.5250, val loss 2.6149 [27.149598836898804 sec]
step 200: train loss 2.3754, val loss 2.4730 [44.86056423187256 sec]
step 300: train loss 2.2074, val loss 2.3297 [62.588274240493774 sec]
step 400: train loss 2.0558, val loss 2.2177 [80.39462852478027 sec]
2.006584405899048
Total Training Time: 88.47058343887329 seconds

wiler smead, "She aid thikeze soed tratepth is to
corseek
Gratta ald beam7 quin. Yi)
Than jas wore r
BEGINNING (1681937789.998034): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5994, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5867, val loss 4.5860 [14.813188076019287 sec]
step 100: train loss 2.5150, val loss 2.5891 [41.50164175033569 sec]
step 200: train loss 2.3513, val loss 2.4524 [67.98613095283508 sec]
step 300: train loss 2.1945, val loss 2.3212 [94.30079340934753 sec]
step 400: train loss 2.0221, val loss 2.1810 [120.4856481552124 sec]
1.9609973430633545
Total Training Time: 132.62204217910767 seconds

"Yous siver ouver. "The sound of of trroon of have
mouse tuon the Rriass. There is will foreer oller
BEGINNING (1681937926.4016197): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6662, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6605, val loss 4.6563 [8.491891145706177 sec]
step 100: train loss 2.5926, val loss 2.6568 [23.171977281570435 sec]
step 200: train loss 2.4729, val loss 2.5579 [37.774444580078125 sec]
step 300: train loss 2.3768, val loss 2.4672 [52.37978792190552 sec]
step 400: train loss 2.2729, val loss 2.3851 [67.06658744812012 sec]
2.1529204845428467
Total Training Time: 73.41989588737488 seconds

mmer thiwah da wastes werto my hamand mimiten's tid s.
sinaNa twellls El- Mcklys ach ry anous camed 
BEGINNING (1681938001.199142): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6033, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6042, val loss 4.5986 [15.35641884803772 sec]
step 100: train loss 2.5380, val loss 2.6219 [42.531254529953 sec]
step 200: train loss 2.4348, val loss 2.5216 [70.0780930519104 sec]
step 300: train loss 2.3205, val loss 2.4351 [97.17879962921143 sec]
step 400: train loss 2.1678, val loss 2.3160 [124.10264134407043 sec]
2.0392003059387207
Total Training Time: 136.080557346344 seconds

enjuounded traceer mase spe wefuls to, beke werin ist
taso aribe sare worthe criave haghts oub, norr
BEGINNING (1681938139.7039826): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5808, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5624, val loss 4.5673 [22.380718231201172 sec]
step 100: train loss 2.5208, val loss 2.6021 [59.93740749359131 sec]
step 200: train loss 2.4128, val loss 2.5048 [97.45841026306152 sec]
step 300: train loss 2.2844, val loss 2.4052 [134.56688237190247 sec]
step 400: train loss 2.1265, val loss 2.2671 [172.02298974990845 sec]
1.9913549423217773
Total Training Time: 187.68567943572998 seconds

PER' NEANER JKAPy Mcarsd jurnodgof the wor warsstit, ated beinl acke it lothef foreep him troL've
mu
BEGINNING (1681938330.8120496): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6098, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6104, val loss 4.6087 [13.354655265808105 sec]
step 100: train loss 2.5939, val loss 2.6631 [36.30213785171509 sec]
step 200: train loss 2.4973, val loss 2.5786 [58.689839124679565 sec]
step 300: train loss 2.4421, val loss 2.5358 [81.47875881195068 sec]
step 400: train loss 2.3867, val loss 2.4834 [104.1806390285492 sec]
2.331055164337158
Total Training Time: 113.78528308868408 seconds

Mck."We. Tad Veit "Oowid curstrecol tathe t.
Sen cur cimoskexat tie l fon tuas!"
"Be ced We the lveh
BEGINNING (1681938445.7946177): Baseline LR(6e-05) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6142, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6070, val loss 4.6068 [25.298567533493042 sec]
step 100: train loss 2.5511, val loss 2.6329 [65.83415102958679 sec]
step 200: train loss 2.4692, val loss 2.5524 [108.43021559715271 sec]
step 300: train loss 2.4014, val loss 2.4918 [153.54176688194275 sec]
step 400: train loss 2.3139, val loss 2.4259 [192.77725434303284 sec]
2.2300970554351807
Total Training Time: 210.40253520011902 seconds

hif blor vemen his pat.
45D
CHe Ge asatta S sknttiface wie ill, an waty fow ureas frter ryosth he
li
