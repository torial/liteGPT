BEGINNING (1681949651.0134022): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6173, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6367, val loss 4.6515 [2.0049192905426025 sec]
step 100: train loss 2.3377, val loss 2.4433 [5.575308322906494 sec]
step 200: train loss 2.0558, val loss 2.2115 [9.162715196609497 sec]
step 300: train loss 1.9110, val loss 2.0790 [12.785624742507935 sec]
step 400: train loss 1.8154, val loss 2.0480 [16.41456365585327 sec]
step 500: train loss 1.7362, val loss 1.9695 [20.068354606628418 sec]
step 600: train loss 1.6967, val loss 1.9355 [23.72380495071411 sec]
step 700: train loss 1.6299, val loss 1.8959 [27.422515630722046 sec]
step 800: train loss 1.5973, val loss 1.8776 [31.399014711380005 sec]
step 900: train loss 1.5550, val loss 1.8439 [35.326355934143066 sec]
step 1000: train loss 1.5330, val loss 1.8648 [39.12893867492676 sec]
step 1100: train loss 1.4993, val loss 1.8285 [42.95091271400452 sec]
step 1200: train loss 1.4677, val loss 1.8142 [46.661441802978516 sec]
step 1300: train loss 1.4479, val loss 1.7956 [50.391916036605835 sec]
step 1400: train loss 1.4349, val loss 1.8190 [54.060027837753296 sec]
1.540110468864441
Total Training Time: 55.61774802207947 seconds

Pillision C waited out cross his eyes?"
"II am your
to All they tuon waitin
andidn't turned lered am
BEGINNING (1681949709.0968256): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5658, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5710, val loss 4.5773 [3.686624050140381 sec]
step 100: train loss 2.3208, val loss 2.4308 [10.165972232818604 sec]
step 200: train loss 2.0022, val loss 2.1818 [17.13870859146118 sec]
step 300: train loss 1.8490, val loss 2.0390 [24.500537157058716 sec]
step 400: train loss 1.7458, val loss 1.9726 [31.900564193725586 sec]
step 500: train loss 1.6762, val loss 1.9470 [39.10271954536438 sec]
step 600: train loss 1.6129, val loss 1.8683 [46.45959448814392 sec]
step 700: train loss 1.5604, val loss 1.8361 [53.96708273887634 sec]
step 800: train loss 1.5231, val loss 1.8297 [61.08927822113037 sec]
step 900: train loss 1.4870, val loss 1.8275 [67.91403937339783 sec]
step 1000: train loss 1.4650, val loss 1.8061 [74.9489541053772 sec]
step 1100: train loss 1.4282, val loss 1.7926 [81.81846284866333 sec]
step 1200: train loss 1.4054, val loss 1.7844 [88.66661500930786 sec]
step 1300: train loss 1.3803, val loss 1.8124 [95.6321029663086 sec]
step 1400: train loss 1.3496, val loss 1.7636 [102.89626741409302 sec]
1.386535406112671
Total Training Time: 106.01295328140259 seconds

of onlyons in tuons to truth. Mapperial this.
They is ligned and for their his time to liking Anayah
BEGINNING (1681949816.4077697): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6166, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6323, val loss 4.6204 [5.473384380340576 sec]
step 100: train loss 2.3447, val loss 2.4475 [15.482184171676636 sec]
step 200: train loss 2.0227, val loss 2.1834 [25.416788578033447 sec]
step 300: train loss 1.8549, val loss 2.0458 [35.816521406173706 sec]
step 400: train loss 1.7385, val loss 1.9687 [46.20089912414551 sec]
step 500: train loss 1.6652, val loss 1.9236 [57.82535719871521 sec]
step 600: train loss 1.6054, val loss 1.8747 [68.73606014251709 sec]
step 700: train loss 1.5461, val loss 1.8512 [79.57922601699829 sec]
step 800: train loss 1.5144, val loss 1.8222 [90.86924076080322 sec]
step 900: train loss 1.4622, val loss 1.8227 [101.46312713623047 sec]
step 1000: train loss 1.4243, val loss 1.7961 [112.64403772354126 sec]
step 1100: train loss 1.4015, val loss 1.7814 [123.59046483039856 sec]
step 1200: train loss 1.3769, val loss 1.8007 [134.6986153125763 sec]
step 1300: train loss 1.3498, val loss 1.7817 [144.72534227371216 sec]
step 1400: train loss 1.3316, val loss 1.7601 [154.96496295928955 sec]
1.4464433193206787
Total Training Time: 159.44015049934387 seconds

squat captort, and "irel advings. Gratta themblowered
their calk. His distuons buid
back tudents."
G
BEGINNING (1681949977.715911): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6477, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6312, val loss 4.6341 [2.4960436820983887 sec]
step 100: train loss 2.4172, val loss 2.5068 [6.613710165023804 sec]
step 200: train loss 2.0757, val loss 2.2267 [11.276035785675049 sec]
step 300: train loss 1.9114, val loss 2.0995 [15.848544597625732 sec]
step 400: train loss 1.7887, val loss 1.9888 [20.07660174369812 sec]
step 500: train loss 1.6830, val loss 1.9294 [24.440382719039917 sec]
step 600: train loss 1.6012, val loss 1.8806 [29.013428926467896 sec]
step 700: train loss 1.5407, val loss 1.8290 [33.24917793273926 sec]
step 800: train loss 1.4887, val loss 1.8284 [37.53398275375366 sec]
step 900: train loss 1.4435, val loss 1.8011 [41.76246953010559 sec]
step 1000: train loss 1.4031, val loss 1.7769 [45.979878425598145 sec]
step 1100: train loss 1.3617, val loss 1.7619 [50.29400372505188 sec]
step 1200: train loss 1.3403, val loss 1.7764 [54.55559730529785 sec]
step 1300: train loss 1.3028, val loss 1.7549 [58.75185799598694 sec]
step 1400: train loss 1.2728, val loss 1.7438 [62.89343595504761 sec]
1.389499545097351
Total Training Time: 64.55291843414307 seconds

"We have commistionight is your rebeathing you dry thare some
thoursed, whickly thirge if of two day
BEGINNING (1681950042.970486): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5677, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5888, val loss 4.5744 [4.199117183685303 sec]
step 100: train loss 2.4039, val loss 2.4996 [11.58864712715149 sec]
step 200: train loss 2.0441, val loss 2.1963 [18.823834896087646 sec]
step 300: train loss 1.8242, val loss 2.0290 [26.24999165534973 sec]
step 400: train loss 1.6917, val loss 1.9279 [33.720213174819946 sec]
step 500: train loss 1.5787, val loss 1.8618 [41.00328969955444 sec]
step 600: train loss 1.5080, val loss 1.8350 [48.22769284248352 sec]
step 700: train loss 1.4383, val loss 1.7965 [55.53385591506958 sec]
step 800: train loss 1.3859, val loss 1.7862 [62.687588930130005 sec]
step 900: train loss 1.3436, val loss 1.7654 [69.95072770118713 sec]
step 1000: train loss 1.2964, val loss 1.7402 [77.19472789764404 sec]
step 1100: train loss 1.2558, val loss 1.7325 [84.46409678459167 sec]
step 1200: train loss 1.2249, val loss 1.7427 [91.8352313041687 sec]
step 1300: train loss 1.1784, val loss 1.7408 [99.25167775154114 sec]
step 1400: train loss 1.1502, val loss 1.7566 [106.45445418357849 sec]
1.293465495109558
Total Training Time: 109.56836128234863 seconds

turned in brought the from hill, that
was anot Torial a pilen body had comforted forwarding
as the o
BEGINNING (1681950153.7480102): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5982, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5917, val loss 4.5757 [5.65523362159729 sec]
step 100: train loss 2.4088, val loss 2.5103 [15.942989349365234 sec]
step 200: train loss 2.0343, val loss 2.1963 [26.008960962295532 sec]
step 300: train loss 1.8168, val loss 2.0136 [36.021111488342285 sec]
step 400: train loss 1.6624, val loss 1.9255 [46.05801463127136 sec]
step 500: train loss 1.5591, val loss 1.8445 [56.41049528121948 sec]
step 600: train loss 1.4718, val loss 1.8144 [66.38697671890259 sec]
step 700: train loss 1.4058, val loss 1.7719 [76.37173533439636 sec]
step 800: train loss 1.3445, val loss 1.7652 [86.3915605545044 sec]
step 900: train loss 1.2930, val loss 1.7554 [96.35383892059326 sec]
step 1000: train loss 1.2412, val loss 1.7712 [106.304030418396 sec]
step 1100: train loss 1.1969, val loss 1.7370 [116.78733611106873 sec]
step 1200: train loss 1.1524, val loss 1.7663 [126.76193404197693 sec]
step 1300: train loss 1.1227, val loss 1.7572 [137.59902691841125 sec]
step 1400: train loss 1.0772, val loss 1.7580 [147.80415892601013 sec]
1.208598017692566
Total Training Time: 152.20493245124817 seconds

had been. "Your plane, so able face! Elled himself Yah
Elyon help with us, whist eyes. Arphad awled,
BEGINNING (1681950307.6990676): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6649, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6787, val loss 4.6839 [3.1416776180267334 sec]
step 100: train loss 2.4829, val loss 2.5637 [8.549198865890503 sec]
step 200: train loss 2.2797, val loss 2.3909 [13.96560549736023 sec]
step 300: train loss 1.9721, val loss 2.1561 [19.426419496536255 sec]
step 400: train loss 1.7954, val loss 2.0144 [25.015429258346558 sec]
step 500: train loss 1.6776, val loss 1.9325 [30.611561059951782 sec]
step 600: train loss 1.5837, val loss 1.8726 [36.15856575965881 sec]
step 700: train loss 1.4981, val loss 1.8130 [41.772661209106445 sec]
step 800: train loss 1.4419, val loss 1.7776 [47.374064207077026 sec]
step 900: train loss 1.3757, val loss 1.7609 [52.95451474189758 sec]
step 1000: train loss 1.3310, val loss 1.7413 [58.52732563018799 sec]
step 1100: train loss 1.2799, val loss 1.7310 [64.09144997596741 sec]
step 1200: train loss 1.2394, val loss 1.7278 [69.63974809646606 sec]
step 1300: train loss 1.1979, val loss 1.7150 [75.21951985359192 sec]
step 1400: train loss 1.1661, val loss 1.7319 [80.89416670799255 sec]
1.2801270484924316
Total Training Time: 83.22027730941772 seconds

our walking on the like at the for Gor replicate. After
had his bited at tradult as he scross to see
BEGINNING (1681950391.5623431): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6131, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6276, val loss 4.6350 [5.707204103469849 sec]
step 100: train loss 2.4605, val loss 2.5458 [15.955789804458618 sec]
step 200: train loss 2.2031, val loss 2.3286 [26.128031253814697 sec]
step 300: train loss 1.8775, val loss 2.0715 [36.30097484588623 sec]
step 400: train loss 1.6805, val loss 1.9257 [46.46929097175598 sec]
step 500: train loss 1.5597, val loss 1.8562 [56.6182062625885 sec]
step 600: train loss 1.4552, val loss 1.7808 [66.89432716369629 sec]
step 700: train loss 1.3668, val loss 1.7466 [77.12548422813416 sec]
step 800: train loss 1.2903, val loss 1.7421 [87.29389953613281 sec]
step 900: train loss 1.2217, val loss 1.7116 [97.50879096984863 sec]
step 1000: train loss 1.1686, val loss 1.7440 [107.69070410728455 sec]
step 1100: train loss 1.1071, val loss 1.7227 [117.8283302783966 sec]
step 1200: train loss 1.0490, val loss 1.7299 [127.98528838157654 sec]
step 1300: train loss 0.9963, val loss 1.7502 [138.15829753875732 sec]
step 1400: train loss 0.9438, val loss 1.7751 [148.4374430179596 sec]
1.0722004175186157
Total Training Time: 152.85110712051392 seconds

and would little before. A allowed the rage to each one
of
mand ran our tride's wale outhers. That w
BEGINNING (1681950545.624137): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6059, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6236, val loss 4.6146 [8.209450483322144 sec]
step 100: train loss 2.4786, val loss 2.5648 [22.98148822784424 sec]
step 200: train loss 2.2311, val loss 2.3562 [37.69054341316223 sec]
step 300: train loss 1.8843, val loss 2.0759 [52.43576264381409 sec]
step 400: train loss 1.6726, val loss 1.9251 [67.21239709854126 sec]
step 500: train loss 1.5215, val loss 1.8369 [81.92420792579651 sec]
step 600: train loss 1.4148, val loss 1.7891 [96.6246268749237 sec]
step 700: train loss 1.3248, val loss 1.7429 [111.36422848701477 sec]
step 800: train loss 1.2413, val loss 1.7475 [126.13779091835022 sec]
step 900: train loss 1.1697, val loss 1.7538 [140.83846926689148 sec]
step 1000: train loss 1.0961, val loss 1.7426 [155.5629780292511 sec]
step 1100: train loss 1.0314, val loss 1.7616 [170.2971489429474 sec]
step 1200: train loss 0.9668, val loss 1.7823 [185.00687718391418 sec]
step 1300: train loss 0.9019, val loss 1.8283 [199.76918292045593 sec]
step 1400: train loss 0.8373, val loss 1.8306 [214.4974775314331 sec]
0.9283968210220337
Total Training Time: 220.972998380661 seconds

in comfort. See you may people frielleQ fraint
this on the tents, and thighe surness would had be to
BEGINNING (1681950768.3981016): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6929, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6857, val loss 4.6852 [2.4424800872802734 sec]
step 100: train loss 2.2753, val loss 2.3814 [6.424653768539429 sec]
step 200: train loss 1.9905, val loss 2.1576 [10.365975379943848 sec]
step 300: train loss 1.8405, val loss 2.0480 [14.338703632354736 sec]
step 400: train loss 1.7466, val loss 1.9691 [18.265117406845093 sec]
step 500: train loss 1.6653, val loss 1.9172 [22.22482967376709 sec]
step 600: train loss 1.6158, val loss 1.8845 [26.153966903686523 sec]
step 700: train loss 1.5694, val loss 1.8688 [30.355942487716675 sec]
step 800: train loss 1.5273, val loss 1.8598 [34.39578938484192 sec]
step 900: train loss 1.4968, val loss 1.8214 [38.2999222278595 sec]
step 1000: train loss 1.4605, val loss 1.8099 [42.24285888671875 sec]
step 1100: train loss 1.4326, val loss 1.8001 [46.18038201332092 sec]
step 1200: train loss 1.4103, val loss 1.8098 [50.09307646751404 sec]
step 1300: train loss 1.3867, val loss 1.7962 [54.02141857147217 sec]
step 1400: train loss 1.3581, val loss 1.8023 [57.939220905303955 sec]
1.412622332572937
Total Training Time: 59.4856743812561 seconds

he donEble few untersking him ative him of El Roi "I have tasked the opensing of the Pyrran Clans Cl
BEGINNING (1681950828.488838): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6879, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6888, val loss 4.6784 [3.940143346786499 sec]
step 100: train loss 2.2362, val loss 2.3460 [10.695906162261963 sec]
step 200: train loss 1.9333, val loss 2.1107 [17.376402854919434 sec]
step 300: train loss 1.7715, val loss 2.0082 [24.09516930580139 sec]
step 400: train loss 1.6707, val loss 1.9328 [30.74189305305481 sec]
step 500: train loss 1.5906, val loss 1.8753 [38.02342867851257 sec]
step 600: train loss 1.5276, val loss 1.8429 [44.71890592575073 sec]
step 700: train loss 1.4780, val loss 1.8102 [51.42876148223877 sec]
step 800: train loss 1.4407, val loss 1.7940 [58.10531234741211 sec]
step 900: train loss 1.4049, val loss 1.8075 [64.96080899238586 sec]
step 1000: train loss 1.3651, val loss 1.7765 [71.65544962882996 sec]
step 1100: train loss 1.3354, val loss 1.7853 [78.2944643497467 sec]
step 1200: train loss 1.3101, val loss 1.7651 [85.1074116230011 sec]
step 1300: train loss 1.2749, val loss 1.7911 [91.79174375534058 sec]
step 1400: train loss 1.2678, val loss 1.7734 [98.43212890625 sec]
1.3328205347061157
Total Training Time: 101.54253602027893 seconds

me foriend in all hearound trade ound for Anayah.
"They will sat me his way to get come. Muziti, i a
BEGINNING (1681950931.2498596): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6560, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6322, val loss 4.6306 [5.467211961746216 sec]
step 100: train loss 2.2843, val loss 2.3927 [15.033373355865479 sec]
step 200: train loss 1.9369, val loss 2.1184 [24.67276883125305 sec]
step 300: train loss 1.7718, val loss 2.0048 [34.17523455619812 sec]
step 400: train loss 1.6587, val loss 1.9236 [43.70909523963928 sec]
step 500: train loss 1.5693, val loss 1.8578 [53.24862623214722 sec]
step 600: train loss 1.5114, val loss 1.8520 [62.86234259605408 sec]
step 700: train loss 1.4477, val loss 1.8332 [72.72777128219604 sec]
step 800: train loss 1.3950, val loss 1.7726 [82.41383337974548 sec]
step 900: train loss 1.3638, val loss 1.7733 [92.00347805023193 sec]
step 1000: train loss 1.3285, val loss 1.7859 [101.59647703170776 sec]
step 1100: train loss 1.2953, val loss 1.7525 [111.20875597000122 sec]
step 1200: train loss 1.2677, val loss 1.7470 [120.75418853759766 sec]
step 1300: train loss 1.2431, val loss 1.7731 [130.67049360275269 sec]
step 1400: train loss 1.2140, val loss 1.7716 [140.444167137146 sec]
1.3047795295715332
Total Training Time: 144.64776253700256 seconds

Her's eyes."
"Some from his paised."
"Some hose that why the yMung."
Gratta bean almoss familiar you
BEGINNING (1681951077.6632733): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6276, val loss 4.6308 [2.7258968353271484 sec]
step 100: train loss 2.3853, val loss 2.4732 [7.2596681118011475 sec]
step 200: train loss 2.0145, val loss 2.1834 [11.806942224502563 sec]
step 300: train loss 1.8119, val loss 2.0164 [16.328856706619263 sec]
step 400: train loss 1.6947, val loss 1.9430 [20.86673140525818 sec]
step 500: train loss 1.5789, val loss 1.8662 [25.42106342315674 sec]
step 600: train loss 1.5169, val loss 1.8275 [29.94263744354248 sec]
step 700: train loss 1.4514, val loss 1.7991 [34.460399866104126 sec]
step 800: train loss 1.4020, val loss 1.7613 [39.00601553916931 sec]
step 900: train loss 1.3687, val loss 1.7872 [43.54304575920105 sec]
step 1000: train loss 1.3156, val loss 1.7618 [48.08958840370178 sec]
step 1100: train loss 1.2759, val loss 1.7528 [52.84229493141174 sec]
step 1200: train loss 1.2607, val loss 1.7676 [57.501824378967285 sec]
step 1300: train loss 1.2195, val loss 1.7405 [62.05828499794006 sec]
step 1400: train loss 1.1848, val loss 1.7551 [66.62018656730652 sec]
1.2967838048934937
Total Training Time: 68.4346821308136 seconds

humans, as hey well? Even evenore ully Torial and
seem your incrafted, planding so!" Gratta stoftind
BEGINNING (1681951146.70398): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6362, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6483, val loss 4.6508 [4.6176228523254395 sec]
step 100: train loss 2.3532, val loss 2.4509 [12.598645210266113 sec]
step 200: train loss 1.9540, val loss 2.1276 [20.55300760269165 sec]
step 300: train loss 1.7355, val loss 1.9673 [28.517034769058228 sec]
step 400: train loss 1.6050, val loss 1.8627 [36.49240779876709 sec]
step 500: train loss 1.5001, val loss 1.8144 [44.543277740478516 sec]
step 600: train loss 1.4101, val loss 1.7685 [52.81709313392639 sec]
step 700: train loss 1.3506, val loss 1.7616 [60.823060750961304 sec]
step 800: train loss 1.2910, val loss 1.7370 [68.80132293701172 sec]
step 900: train loss 1.2374, val loss 1.7534 [76.78460812568665 sec]
step 1000: train loss 1.1960, val loss 1.7337 [84.7830023765564 sec]
step 1100: train loss 1.1448, val loss 1.7634 [92.76723337173462 sec]
step 1200: train loss 1.1052, val loss 1.7606 [100.76986575126648 sec]
step 1300: train loss 1.0580, val loss 1.7464 [108.84723424911499 sec]
step 1400: train loss 1.0283, val loss 1.7648 [117.15811610221863 sec]
1.1733795404434204
Total Training Time: 120.55030846595764 seconds

had seen, perhaps away following. And there in the
dight, to shappearingly be since –arriyah before 
BEGINNING (1681951268.4531343): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5787, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5633, val loss 4.5611 [6.526331901550293 sec]
step 100: train loss 2.3931, val loss 2.4885 [17.960927724838257 sec]
step 200: train loss 1.9754, val loss 2.1520 [29.38510251045227 sec]
step 300: train loss 1.7406, val loss 1.9620 [40.79616928100586 sec]
step 400: train loss 1.5923, val loss 1.8753 [52.22286558151245 sec]
step 500: train loss 1.4744, val loss 1.8060 [64.25221061706543 sec]
step 600: train loss 1.3951, val loss 1.7795 [75.84498572349548 sec]
step 700: train loss 1.3173, val loss 1.7655 [87.2643404006958 sec]
step 800: train loss 1.2563, val loss 1.7598 [98.69187140464783 sec]
step 900: train loss 1.1944, val loss 1.7545 [110.30712246894836 sec]
step 1000: train loss 1.1363, val loss 1.7530 [122.44053149223328 sec]
step 1100: train loss 1.0788, val loss 1.7375 [134.65364241600037 sec]
step 1200: train loss 1.0342, val loss 1.7564 [146.3940351009369 sec]
step 1300: train loss 0.9889, val loss 1.7988 [158.0155212879181 sec]
step 1400: train loss 0.9353, val loss 1.8001 [169.5855851173401 sec]
1.045023798942566
Total Training Time: 174.66251397132874 seconds

Pyrran sack dether."
"Krel!" Gratta!."
"Very have a smashekelbire with thisile measur
father."
Aidde
BEGINNING (1681951444.9853907): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6176, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6131, val loss 4.6154 [4.377542972564697 sec]
step 100: train loss 2.4644, val loss 2.5513 [11.760464906692505 sec]
step 200: train loss 2.1952, val loss 2.3237 [19.472156763076782 sec]
step 300: train loss 1.9163, val loss 2.1132 [27.036149978637695 sec]
step 400: train loss 1.7399, val loss 1.9709 [34.71547794342041 sec]
step 500: train loss 1.6021, val loss 1.8781 [42.27532243728638 sec]
step 600: train loss 1.4963, val loss 1.8107 [49.85547113418579 sec]
step 700: train loss 1.4141, val loss 1.7690 [57.705331802368164 sec]
step 800: train loss 1.3390, val loss 1.7526 [65.60377979278564 sec]
step 900: train loss 1.2775, val loss 1.7426 [73.49378514289856 sec]
step 1000: train loss 1.2229, val loss 1.7397 [81.09087777137756 sec]
step 1100: train loss 1.1672, val loss 1.7421 [88.88182663917542 sec]
step 1200: train loss 1.1131, val loss 1.7404 [96.79774284362793 sec]
step 1300: train loss 1.0729, val loss 1.7491 [104.64064121246338 sec]
step 1400: train loss 1.0326, val loss 1.7816 [112.49464106559753 sec]
1.1736162900924683
Total Training Time: 115.75009822845459 seconds

ods out double, he even you so had it their wamp the
maeahled booke alon company. They were ligh dow
BEGINNING (1681951561.4349935): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5933, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5891, val loss 4.5853 [8.255038738250732 sec]
step 100: train loss 2.4593, val loss 2.5506 [22.60452914237976 sec]
step 200: train loss 2.1201, val loss 2.2691 [36.988850831985474 sec]
step 300: train loss 1.7914, val loss 2.0190 [51.27608394622803 sec]
step 400: train loss 1.5918, val loss 1.8690 [65.4258382320404 sec]
step 500: train loss 1.4552, val loss 1.7948 [79.61014151573181 sec]
step 600: train loss 1.3514, val loss 1.7584 [93.87541103363037 sec]
step 700: train loss 1.2649, val loss 1.7477 [107.76104021072388 sec]
step 800: train loss 1.1841, val loss 1.7226 [121.72149872779846 sec]
step 900: train loss 1.1088, val loss 1.7413 [135.6768078804016 sec]
step 1000: train loss 1.0440, val loss 1.7775 [149.58103013038635 sec]
step 1100: train loss 0.9698, val loss 1.7626 [163.53249526023865 sec]
step 1200: train loss 0.9122, val loss 1.7992 [177.50960659980774 sec]
step 1300: train loss 0.8435, val loss 1.8520 [191.52722120285034 sec]
step 1400: train loss 0.7764, val loss 1.8692 [205.44709420204163 sec]
0.9345325827598572
Total Training Time: 211.5034520626068 seconds

He and walked to Nurroup and interrogating
that Gratta's oals of the raled maeuw, and said Quickled.
BEGINNING (1681951774.2752182): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6284, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6361, val loss 4.6319 [11.5952308177948 sec]
step 100: train loss 2.4657, val loss 2.5526 [31.921436309814453 sec]
step 200: train loss 2.1663, val loss 2.2970 [51.89840745925903 sec]
step 300: train loss 1.7963, val loss 2.0218 [72.00409317016602 sec]
step 400: train loss 1.5807, val loss 1.8629 [92.38231253623962 sec]
step 500: train loss 1.4474, val loss 1.7930 [112.66079664230347 sec]
step 600: train loss 1.3315, val loss 1.7523 [133.2783854007721 sec]
step 700: train loss 1.2203, val loss 1.7238 [155.001779794693 sec]
step 800: train loss 1.1376, val loss 1.7286 [175.50302934646606 sec]
step 900: train loss 1.0557, val loss 1.7690 [195.89508080482483 sec]
step 1000: train loss 0.9611, val loss 1.7525 [216.44785928726196 sec]
step 1100: train loss 0.8692, val loss 1.7995 [236.79641556739807 sec]
step 1200: train loss 0.7965, val loss 1.8546 [257.1091809272766 sec]
step 1300: train loss 0.7094, val loss 1.9274 [277.4934558868408 sec]
step 1400: train loss 0.6241, val loss 2.0064 [298.2571876049042 sec]
0.8245331645011902
Total Training Time: 307.21567034721375 seconds

fest, and they reached the torchm them up on tribes on in fear
suddeep him was that was won't fight.
BEGINNING (1681952083.5312288): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5406, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5645, val loss 4.5678 [3.0659737586975098 sec]
step 100: train loss 2.2251, val loss 2.3460 [7.767536401748657 sec]
step 200: train loss 1.9465, val loss 2.1264 [12.98378300666809 sec]
step 300: train loss 1.8108, val loss 2.0253 [18.084673643112183 sec]
step 400: train loss 1.6961, val loss 1.9519 [23.077279567718506 sec]
step 500: train loss 1.6345, val loss 1.9060 [27.914169311523438 sec]
step 600: train loss 1.5710, val loss 1.8799 [32.75389504432678 sec]
step 700: train loss 1.5236, val loss 1.8365 [38.004302978515625 sec]
step 800: train loss 1.4774, val loss 1.8306 [43.083723068237305 sec]
step 900: train loss 1.4512, val loss 1.8095 [48.06128931045532 sec]
step 1000: train loss 1.4141, val loss 1.8093 [52.93583559989929 sec]
step 1100: train loss 1.3924, val loss 1.8009 [57.866000175476074 sec]
step 1200: train loss 1.3648, val loss 1.7994 [62.78306579589844 sec]
step 1300: train loss 1.3415, val loss 1.7758 [67.67838215827942 sec]
step 1400: train loss 1.3165, val loss 1.7995 [72.56063342094421 sec]
1.4148728847503662
Total Training Time: 74.5245771408081 seconds

sent aft, who noned to ver you from theris group at tho Artives. The maeuw. High the silence, Genera
BEGINNING (1681952158.7965198): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6778, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6725, val loss 4.6625 [4.779654264450073 sec]
step 100: train loss 2.2041, val loss 2.3308 [12.821428298950195 sec]
step 200: train loss 1.8844, val loss 2.0777 [21.0026535987854 sec]
step 300: train loss 1.7104, val loss 1.9476 [29.220102071762085 sec]
step 400: train loss 1.6086, val loss 1.8805 [36.80234122276306 sec]
step 500: train loss 1.5338, val loss 1.8408 [44.51381802558899 sec]
step 600: train loss 1.4650, val loss 1.8073 [52.58088040351868 sec]
step 700: train loss 1.4177, val loss 1.8076 [60.22814345359802 sec]
step 800: train loss 1.3757, val loss 1.7810 [67.76771020889282 sec]
step 900: train loss 1.3367, val loss 1.7746 [75.21789646148682 sec]
step 1000: train loss 1.3042, val loss 1.7778 [82.87051463127136 sec]
step 1100: train loss 1.2669, val loss 1.7731 [90.69591331481934 sec]
step 1200: train loss 1.2380, val loss 1.7694 [98.77182149887085 sec]
step 1300: train loss 1.2052, val loss 1.7798 [106.3447208404541 sec]
step 1400: train loss 1.1818, val loss 1.7727 [113.97975301742554 sec]
1.2755149602890015
Total Training Time: 117.06474924087524 seconds

geat?"
"Ords unwally trapted to osched slav.
"Chief Arphad slow. No doubt warrior
69
SEAN McKAY
turn
BEGINNING (1681952277.1189291): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5688, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5668, val loss 4.5661 [6.3256871700286865 sec]
step 100: train loss 2.2352, val loss 2.3517 [17.460644721984863 sec]
step 200: train loss 1.8918, val loss 2.0875 [28.669923543930054 sec]
step 300: train loss 1.7020, val loss 1.9297 [39.53159284591675 sec]
step 400: train loss 1.5958, val loss 1.8878 [50.710166454315186 sec]
step 500: train loss 1.5081, val loss 1.8336 [61.40309524536133 sec]
step 600: train loss 1.4391, val loss 1.7866 [73.07838416099548 sec]
step 700: train loss 1.3889, val loss 1.7868 [84.26108980178833 sec]
step 800: train loss 1.3445, val loss 1.7622 [96.75740504264832 sec]
step 900: train loss 1.3070, val loss 1.7580 [107.63688063621521 sec]
step 1000: train loss 1.2716, val loss 1.7726 [118.01012063026428 sec]
step 1100: train loss 1.2241, val loss 1.7596 [128.25606536865234 sec]
step 1200: train loss 1.1913, val loss 1.7553 [138.5012001991272 sec]
step 1300: train loss 1.1650, val loss 1.7614 [148.73668217658997 sec]
step 1400: train loss 1.1329, val loss 1.7917 [159.3239643573761 sec]
1.1789841651916504
Total Training Time: 164.00717425346375 seconds

"You are disterned with ords."
"Yes, undern." Beriyah looked at blest. "The
nurth! Lrate?" Gor," The
BEGINNING (1681952443.4081721): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6660, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6632, val loss 4.6609 [3.3739283084869385 sec]
step 100: train loss 2.3346, val loss 2.4453 [9.601879596710205 sec]
step 200: train loss 1.9452, val loss 2.1263 [15.36194920539856 sec]
step 300: train loss 1.7510, val loss 1.9698 [21.213695526123047 sec]
step 400: train loss 1.6283, val loss 1.8909 [26.903606176376343 sec]
step 500: train loss 1.5386, val loss 1.8317 [32.559977769851685 sec]
step 600: train loss 1.4648, val loss 1.7877 [38.23897194862366 sec]
step 700: train loss 1.4025, val loss 1.7777 [43.856202363967896 sec]
step 800: train loss 1.3543, val loss 1.7504 [49.40203809738159 sec]
step 900: train loss 1.3156, val loss 1.7529 [54.9351544380188 sec]
step 1000: train loss 1.2635, val loss 1.7396 [60.45166516304016 sec]
step 1100: train loss 1.2328, val loss 1.7470 [65.98467087745667 sec]
step 1200: train loss 1.1916, val loss 1.7365 [71.58840870857239 sec]
step 1300: train loss 1.1592, val loss 1.7517 [77.26632165908813 sec]
step 1400: train loss 1.1284, val loss 1.7558 [82.79955506324768 sec]
1.260691523551941
Total Training Time: 85.06407618522644 seconds

"Why are in the child deceft Arphad to secontries
the celeasion in ointmerness, essir, I half to the
BEGINNING (1681952529.0987568): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6129, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6095, val loss 4.6087 [5.747214078903198 sec]
step 100: train loss 2.3258, val loss 2.4203 [15.716665744781494 sec]
step 200: train loss 1.8991, val loss 2.0833 [25.69604206085205 sec]
step 300: train loss 1.6725, val loss 1.9274 [35.65266561508179 sec]
step 400: train loss 1.5320, val loss 1.8300 [45.609150648117065 sec]
step 500: train loss 1.4171, val loss 1.7697 [55.58014464378357 sec]
step 600: train loss 1.3536, val loss 1.7460 [65.55048203468323 sec]
step 700: train loss 1.2807, val loss 1.7519 [75.56066226959229 sec]
step 800: train loss 1.2195, val loss 1.7362 [85.51730561256409 sec]
step 900: train loss 1.1615, val loss 1.7348 [95.53581762313843 sec]
step 1000: train loss 1.1133, val loss 1.7406 [105.53541374206543 sec]
step 1100: train loss 1.0563, val loss 1.7510 [115.49493837356567 sec]
step 1200: train loss 1.0090, val loss 1.7811 [125.57550358772278 sec]
step 1300: train loss 0.9591, val loss 1.7749 [135.63257670402527 sec]
step 1400: train loss 0.9176, val loss 1.8225 [145.67482352256775 sec]
1.083703875541687
Total Training Time: 149.96360731124878 seconds

18˜RISING PEACE
SEACCWIN"
"ING McKAY
"If the Torial," assaued Gratta's founders in concerved
was phe
BEGINNING (1681952680.34041): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5141, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5112, val loss 4.5174 [8.150533676147461 sec]
step 100: train loss 2.3965, val loss 2.4893 [22.50267720222473 sec]
step 200: train loss 1.9490, val loss 2.1373 [36.87233567237854 sec]
step 300: train loss 1.6947, val loss 1.9329 [51.139670610427856 sec]
step 400: train loss 1.5188, val loss 1.8290 [65.43515014648438 sec]
step 500: train loss 1.4109, val loss 1.7877 [79.72794008255005 sec]
step 600: train loss 1.3211, val loss 1.7506 [94.00348949432373 sec]
step 700: train loss 1.2416, val loss 1.7326 [108.57488107681274 sec]
step 800: train loss 1.1718, val loss 1.7510 [123.30281567573547 sec]
step 900: train loss 1.1035, val loss 1.7584 [138.17533683776855 sec]
step 1000: train loss 1.0456, val loss 1.7786 [152.74882745742798 sec]
step 1100: train loss 0.9813, val loss 1.8056 [167.95441341400146 sec]
step 1200: train loss 0.9264, val loss 1.8310 [184.0531873703003 sec]
step 1300: train loss 0.8657, val loss 1.8446 [199.3717179298401 sec]
step 1400: train loss 0.8113, val loss 1.8705 [215.4836938381195 sec]
0.9948094487190247
Total Training Time: 222.30994629859924 seconds

of anterpopar terpharing the tribes in crothed whip mind? She we
hall be will ee core to cent Time. 
BEGINNING (1681952904.9806411): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5899, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5964, val loss 4.5941 [6.192180156707764 sec]
step 100: train loss 2.4492, val loss 2.5411 [16.418598651885986 sec]
step 200: train loss 2.1205, val loss 2.2560 [25.995502948760986 sec]
step 300: train loss 1.8250, val loss 2.0269 [35.59103512763977 sec]
step 400: train loss 1.6552, val loss 1.9115 [45.14880442619324 sec]
step 500: train loss 1.5308, val loss 1.8244 [54.89116668701172 sec]
step 600: train loss 1.4216, val loss 1.7678 [64.63993096351624 sec]
step 700: train loss 1.3447, val loss 1.7391 [74.39601874351501 sec]
step 800: train loss 1.2737, val loss 1.7339 [84.11022114753723 sec]
step 900: train loss 1.2185, val loss 1.7263 [93.92771124839783 sec]
step 1000: train loss 1.1574, val loss 1.7146 [103.41920804977417 sec]
step 1100: train loss 1.1133, val loss 1.7547 [112.71908712387085 sec]
step 1200: train loss 1.0619, val loss 1.7429 [122.02405190467834 sec]
step 1300: train loss 1.0166, val loss 1.7440 [131.4116702079773 sec]
step 1400: train loss 0.9732, val loss 1.7781 [140.9028844833374 sec]
1.1242319345474243
Total Training Time: 144.85615873336792 seconds

today.
"Sir, telling! Gratta turned to Gratta! Anayah the
mare turned to him. Zechariyah Elyon. His 
BEGINNING (1681953050.4839196): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5848, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5926, val loss 4.5954 [9.689008235931396 sec]
step 100: train loss 2.4306, val loss 2.5237 [26.523781776428223 sec]
step 200: train loss 2.0332, val loss 2.2013 [43.59367251396179 sec]
step 300: train loss 1.7230, val loss 1.9547 [60.48115348815918 sec]
step 400: train loss 1.5427, val loss 1.8468 [77.37986969947815 sec]
step 500: train loss 1.4031, val loss 1.7743 [94.522141456604 sec]
step 600: train loss 1.2961, val loss 1.7465 [111.61003375053406 sec]
step 700: train loss 1.2058, val loss 1.7211 [128.83093166351318 sec]
step 800: train loss 1.1124, val loss 1.7171 [145.75266313552856 sec]
step 900: train loss 1.0304, val loss 1.7185 [162.63740062713623 sec]
step 1000: train loss 0.9515, val loss 1.7655 [179.4721224308014 sec]
step 1100: train loss 0.8757, val loss 1.8015 [196.29330444335938 sec]
step 1200: train loss 0.7952, val loss 1.8101 [213.303808927536 sec]
step 1300: train loss 0.7170, val loss 1.9022 [230.13602471351624 sec]
step 1400: train loss 0.6524, val loss 1.9387 [246.98822712898254 sec]
0.8493064641952515
Total Training Time: 254.23553228378296 seconds

ds again. His would be robed his northern moons in
in the moonishment be ilves in taking, but capto 
BEGINNING (1681953305.9504452): Baseline LR(0.0006) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6449, val loss 4.6444 [13.939713716506958 sec]
step 100: train loss 2.4476, val loss 2.5284 [38.290529012680054 sec]
step 200: train loss 2.0835, val loss 2.2239 [62.48575687408447 sec]
step 300: train loss 1.7218, val loss 1.9749 [87.18376159667969 sec]
step 400: train loss 1.5052, val loss 1.8261 [111.46433544158936 sec]
step 500: train loss 1.3546, val loss 1.7734 [135.77594661712646 sec]
step 600: train loss 1.2283, val loss 1.7452 [160.00692868232727 sec]
step 700: train loss 1.1238, val loss 1.7493 [184.51090669631958 sec]
step 800: train loss 1.0114, val loss 1.7713 [211.41075921058655 sec]
step 900: train loss 0.9063, val loss 1.8033 [236.7697958946228 sec]
step 1000: train loss 0.8009, val loss 1.8545 [263.4558336734772 sec]
step 1100: train loss 0.6983, val loss 1.9241 [289.3697953224182 sec]
step 1200: train loss 0.5986, val loss 2.0133 [314.48006987571716 sec]
step 1300: train loss 0.5170, val loss 2.0662 [339.6481742858887 sec]
step 1400: train loss 0.4318, val loss 2.2095 [364.49311208724976 sec]
0.6391811966896057
Total Training Time: 375.42111372947693 seconds

General Beriyah, any looked across the bridge. Not of
the cubs for your felt signers. Do not kneet a
BEGINNING (1681953683.5838625): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5936, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6030, val loss 4.6066 [2.9638407230377197 sec]
step 100: train loss 2.2670, val loss 2.3879 [8.053901195526123 sec]
step 200: train loss 2.0025, val loss 2.1844 [13.35044813156128 sec]
step 300: train loss 1.8485, val loss 2.0653 [18.380170106887817 sec]
step 400: train loss 1.7595, val loss 1.9936 [23.91651487350464 sec]
step 500: train loss 1.7008, val loss 1.9620 [28.91173529624939 sec]
step 600: train loss 1.6317, val loss 1.9170 [33.734880208969116 sec]
step 700: train loss 1.5886, val loss 1.8981 [38.52996325492859 sec]
step 800: train loss 1.5543, val loss 1.8581 [43.32338523864746 sec]
step 900: train loss 1.5104, val loss 1.8519 [48.18978571891785 sec]
step 1000: train loss 1.4852, val loss 1.8124 [52.98673224449158 sec]
step 1100: train loss 1.4465, val loss 1.8416 [57.63976836204529 sec]
step 1200: train loss 1.4299, val loss 1.8328 [62.27948999404907 sec]
step 1300: train loss 1.4172, val loss 1.8173 [66.8948016166687 sec]
step 1400: train loss 1.3792, val loss 1.8130 [71.65385603904724 sec]
1.4852473735809326
Total Training Time: 73.68712997436523 seconds

wife, the each!"
Anayah wand manother
of the High Priest a had sa had drounded in they last the tuon
BEGINNING (1681953758.0999827): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6072, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6269, val loss 4.6245 [4.739503622055054 sec]
step 100: train loss 2.2712, val loss 2.3826 [13.579462051391602 sec]
step 200: train loss 1.9753, val loss 2.1534 [22.24032711982727 sec]
step 300: train loss 1.8106, val loss 2.0280 [30.790714025497437 sec]
step 400: train loss 1.7269, val loss 1.9863 [39.318530321121216 sec]
step 500: train loss 1.6457, val loss 1.9105 [47.73888397216797 sec]
step 600: train loss 1.5793, val loss 1.8989 [56.20227122306824 sec]
step 700: train loss 1.5270, val loss 1.8597 [64.7125072479248 sec]
step 800: train loss 1.4861, val loss 1.8342 [73.10425281524658 sec]
step 900: train loss 1.4583, val loss 1.8147 [82.26341366767883 sec]
step 1000: train loss 1.4144, val loss 1.7997 [91.37391233444214 sec]
step 1100: train loss 1.3933, val loss 1.8122 [100.42208552360535 sec]
step 1200: train loss 1.3570, val loss 1.7995 [109.01911616325378 sec]
step 1300: train loss 1.3420, val loss 1.8094 [117.91399717330933 sec]
step 1400: train loss 1.3127, val loss 1.7947 [126.27686977386475 sec]
1.306360125541687
Total Training Time: 130.12647557258606 seconds

nurrers are coplied the otherw. I
we would need even if the tuon.
Arphad of put of hildie. Taka tuon
BEGINNING (1681953889.7719302): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5862, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5802, val loss 4.5746 [6.657890319824219 sec]
step 100: train loss 2.3466, val loss 2.4425 [19.290963888168335 sec]
step 200: train loss 2.0305, val loss 2.1988 [32.1387779712677 sec]
step 300: train loss 1.8580, val loss 2.0625 [44.69339156150818 sec]
step 400: train loss 1.7248, val loss 1.9777 [57.21476912498474 sec]
step 500: train loss 1.6500, val loss 1.9355 [69.76312589645386 sec]
step 600: train loss 1.5772, val loss 1.8867 [82.5842719078064 sec]
step 700: train loss 1.5244, val loss 1.8698 [94.98129320144653 sec]
step 800: train loss 1.4816, val loss 1.8419 [107.29246473312378 sec]
step 900: train loss 1.4512, val loss 1.8195 [119.58687996864319 sec]
step 1000: train loss 1.4194, val loss 1.7961 [132.1922414302826 sec]
step 1100: train loss 1.3688, val loss 1.7950 [145.13274121284485 sec]
step 1200: train loss 1.3473, val loss 1.7936 [157.87587785720825 sec]
step 1300: train loss 1.3212, val loss 1.8125 [170.40757751464844 sec]
step 1400: train loss 1.2968, val loss 1.8198 [182.72500109672546 sec]
1.3160500526428223
Total Training Time: 188.53037428855896 seconds

feet I – bow, sir!"
Gratta andded Namal his such delegation, rarly le man from believil,
and we name
BEGINNING (1681954080.6393495): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6107, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6069, val loss 4.6139 [3.0208041667938232 sec]
step 100: train loss 2.3520, val loss 2.4474 [8.212625980377197 sec]
step 200: train loss 1.9867, val loss 2.1727 [13.383448123931885 sec]
step 300: train loss 1.8141, val loss 2.0372 [18.75725221633911 sec]
step 400: train loss 1.6914, val loss 1.9614 [24.129215478897095 sec]
step 500: train loss 1.5889, val loss 1.8830 [29.398826360702515 sec]
step 600: train loss 1.5118, val loss 1.8410 [34.633689165115356 sec]
step 700: train loss 1.4510, val loss 1.8107 [39.8424391746521 sec]
step 800: train loss 1.4124, val loss 1.7919 [45.109686613082886 sec]
step 900: train loss 1.3552, val loss 1.7738 [50.365495920181274 sec]
step 1000: train loss 1.3136, val loss 1.7660 [55.619627237319946 sec]
step 1100: train loss 1.2754, val loss 1.7805 [60.88150095939636 sec]
step 1200: train loss 1.2385, val loss 1.7633 [66.13424253463745 sec]
step 1300: train loss 1.2060, val loss 1.7716 [71.3396954536438 sec]
step 1400: train loss 1.1799, val loss 1.7868 [76.70127367973328 sec]
1.2550132274627686
Total Training Time: 79.06190371513367 seconds

"Yes!" you are nation lost to Let us. The
rud will sa Clans in the where bout of Tribe infuce
boore 
BEGINNING (1681954160.482459): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6275, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6209, val loss 4.6239 [5.528724193572998 sec]
step 100: train loss 2.3947, val loss 2.4911 [15.892211198806763 sec]
step 200: train loss 1.9754, val loss 2.1505 [25.552854537963867 sec]
step 300: train loss 1.7575, val loss 1.9821 [35.51398420333862 sec]
step 400: train loss 1.6288, val loss 1.9038 [45.23829650878906 sec]
step 500: train loss 1.5274, val loss 1.8450 [54.85494089126587 sec]
step 600: train loss 1.4375, val loss 1.7975 [64.89685440063477 sec]
step 700: train loss 1.3727, val loss 1.7718 [74.93319773674011 sec]
step 800: train loss 1.3095, val loss 1.7401 [84.9500024318695 sec]
step 900: train loss 1.2670, val loss 1.7650 [94.82703161239624 sec]
step 1000: train loss 1.2242, val loss 1.7718 [104.8244001865387 sec]
step 1100: train loss 1.1780, val loss 1.7643 [114.60134744644165 sec]
step 1200: train loss 1.1425, val loss 1.7711 [124.35947823524475 sec]
step 1300: train loss 1.0990, val loss 1.7467 [134.0370810031891 sec]
step 1400: train loss 1.0558, val loss 1.7740 [144.24464535713196 sec]
1.1698087453842163
Total Training Time: 148.56377863883972 seconds

How herned any.
Four wound of his comforthle, and he smaue to
Gratta looking at Arphad said for shoo
BEGINNING (1681954310.6264908): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6576, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6679, val loss 4.6698 [8.5435950756073 sec]
step 100: train loss 2.4465, val loss 2.5427 [23.539238452911377 sec]
step 200: train loss 2.0359, val loss 2.2131 [37.60016584396362 sec]
step 300: train loss 1.8002, val loss 2.0160 [52.16489577293396 sec]
step 400: train loss 1.6509, val loss 1.9213 [66.3809175491333 sec]
step 500: train loss 1.5346, val loss 1.8466 [80.29005289077759 sec]
step 600: train loss 1.4609, val loss 1.7990 [94.20804166793823 sec]
step 700: train loss 1.3747, val loss 1.7883 [107.99958062171936 sec]
step 800: train loss 1.3037, val loss 1.7477 [122.34299564361572 sec]
step 900: train loss 1.2560, val loss 1.7453 [136.66704940795898 sec]
step 1000: train loss 1.2042, val loss 1.7386 [150.94900226593018 sec]
step 1100: train loss 1.1515, val loss 1.7778 [165.1786813735962 sec]
step 1200: train loss 1.0993, val loss 1.7519 [178.80428194999695 sec]
step 1300: train loss 1.0658, val loss 1.7769 [192.4298312664032 sec]
step 1400: train loss 1.0169, val loss 1.7858 [205.9526469707489 sec]
1.1204664707183838
Total Training Time: 212.25371050834656 seconds

used storthers and great returned his since ushatisy
the Pyrrangs stook and Krel smiled neart, but f
BEGINNING (1681954525.3035815): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6452, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6424, val loss 4.6316 [4.747600555419922 sec]
step 100: train loss 2.4527, val loss 2.5379 [12.682570457458496 sec]
step 200: train loss 2.1422, val loss 2.2855 [20.8648681640625 sec]
step 300: train loss 1.8617, val loss 2.0655 [29.07763648033142 sec]
step 400: train loss 1.6758, val loss 1.9233 [37.97773623466492 sec]
step 500: train loss 1.5613, val loss 1.8469 [46.272133350372314 sec]
step 600: train loss 1.4544, val loss 1.7984 [54.40584945678711 sec]
step 700: train loss 1.3733, val loss 1.7767 [62.55184531211853 sec]
step 800: train loss 1.2936, val loss 1.7518 [70.6877064704895 sec]
step 900: train loss 1.2335, val loss 1.7338 [78.83475828170776 sec]
step 1000: train loss 1.1817, val loss 1.7467 [87.00020170211792 sec]
step 1100: train loss 1.1230, val loss 1.7355 [95.1509850025177 sec]
step 1200: train loss 1.0684, val loss 1.7565 [103.93309617042542 sec]
step 1300: train loss 1.0224, val loss 1.7682 [112.27820467948914 sec]
step 1400: train loss 0.9787, val loss 1.7873 [120.40294575691223 sec]
1.0831705331802368
Total Training Time: 123.92509126663208 seconds

undreforw safed taken. The had larget dhis paws
stricked up and a becave. Perhapfeed torcue,"
"Yes, 
BEGINNING (1681954650.0807397): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.7085, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7078, val loss 4.7120 [8.44714093208313 sec]
step 100: train loss 2.4516, val loss 2.5399 [23.80954360961914 sec]
step 200: train loss 2.1279, val loss 2.2736 [39.066749811172485 sec]
step 300: train loss 1.8156, val loss 2.0240 [54.51814365386963 sec]
step 400: train loss 1.6131, val loss 1.9083 [69.8948187828064 sec]
step 500: train loss 1.4962, val loss 1.8511 [85.41354990005493 sec]
step 600: train loss 1.3803, val loss 1.7671 [100.86295342445374 sec]
step 700: train loss 1.2858, val loss 1.7475 [116.45973992347717 sec]
step 800: train loss 1.2047, val loss 1.7354 [131.84729313850403 sec]
step 900: train loss 1.1334, val loss 1.7375 [147.11913347244263 sec]
step 1000: train loss 1.0635, val loss 1.7742 [162.58402514457703 sec]
step 1100: train loss 0.9849, val loss 1.7866 [177.92164587974548 sec]
step 1200: train loss 0.9161, val loss 1.8130 [193.262793302536 sec]
step 1300: train loss 0.8453, val loss 1.8788 [208.5455026626587 sec]
step 1400: train loss 0.7752, val loss 1.9145 [223.8871467113495 sec]
0.9771825671195984
Total Training Time: 230.62586998939514 seconds

oven Gratta stood and focanned. Gratta
came ounced at his lost table to his let a lone. Anayah. "Yes
BEGINNING (1681954882.2935386): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6236, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6098, val loss 4.6103 [12.297447919845581 sec]
step 100: train loss 2.4747, val loss 2.5640 [34.72549104690552 sec]
step 200: train loss 2.1916, val loss 2.3221 [57.05513072013855 sec]
step 300: train loss 1.8258, val loss 2.0408 [79.41091632843018 sec]
step 400: train loss 1.6120, val loss 1.8916 [101.78584957122803 sec]
step 500: train loss 1.4625, val loss 1.8131 [124.20858573913574 sec]
step 600: train loss 1.3451, val loss 1.7563 [146.7385904788971 sec]
step 700: train loss 1.2494, val loss 1.7712 [169.13080072402954 sec]
step 800: train loss 1.1588, val loss 1.7549 [191.37678837776184 sec]
step 900: train loss 1.0705, val loss 1.7370 [213.82883095741272 sec]
step 1000: train loss 0.9736, val loss 1.7899 [236.03705739974976 sec]
step 1100: train loss 0.8982, val loss 1.7891 [258.349449634552 sec]
step 1200: train loss 0.8233, val loss 1.8610 [280.68098878860474 sec]
step 1300: train loss 0.7378, val loss 1.9448 [303.1688916683197 sec]
step 1400: train loss 0.6564, val loss 1.9900 [325.3681757450104 sec]
0.7903634905815125
Total Training Time: 335.39553356170654 seconds

wrihzed "I am very Muzite," Lamek said to my atte, so he
know whipped to disguis a tuon smell of wit
BEGINNING (1681955220.4019253): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5899, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6031, val loss 4.6160 [3.7241506576538086 sec]
step 100: train loss 2.1881, val loss 2.3138 [9.244070768356323 sec]
step 200: train loss 1.9114, val loss 2.0910 [14.490304708480835 sec]
step 300: train loss 1.7677, val loss 1.9930 [19.80189609527588 sec]
step 400: train loss 1.6776, val loss 1.9319 [25.14687752723694 sec]
step 500: train loss 1.6022, val loss 1.9028 [30.233067989349365 sec]
step 600: train loss 1.5571, val loss 1.8670 [35.410279512405396 sec]
step 700: train loss 1.4973, val loss 1.8214 [40.8123562335968 sec]
step 800: train loss 1.4614, val loss 1.8099 [46.06940507888794 sec]
step 900: train loss 1.4240, val loss 1.8009 [51.134279012680054 sec]
step 1000: train loss 1.3979, val loss 1.8070 [56.349785804748535 sec]
step 1100: train loss 1.3576, val loss 1.8010 [61.307116985321045 sec]
step 1200: train loss 1.3258, val loss 1.7808 [66.29893827438354 sec]
step 1300: train loss 1.3187, val loss 1.8017 [71.6306266784668 sec]
step 1400: train loss 1.2807, val loss 1.7808 [76.97579717636108 sec]
1.4133869409561157
Total Training Time: 79.1501829624176 seconds

ANS Chief Gratta was spiage down them for any maeuws?"
Lameke cusshes. Nheekers hundred his
bick of 
BEGINNING (1681955300.3603828): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6182, val loss 4.6124 [5.1562254428863525 sec]
step 100: train loss 2.2005, val loss 2.3179 [14.3745756149292 sec]
step 200: train loss 1.8819, val loss 2.0832 [23.41866183280945 sec]
step 300: train loss 1.7292, val loss 1.9401 [32.872395277023315 sec]
step 400: train loss 1.6243, val loss 1.8974 [42.42373752593994 sec]
step 500: train loss 1.5285, val loss 1.8567 [51.424715518951416 sec]
step 600: train loss 1.4790, val loss 1.8293 [60.88999342918396 sec]
step 700: train loss 1.4400, val loss 1.8269 [69.8924195766449 sec]
step 800: train loss 1.3992, val loss 1.8208 [79.15037512779236 sec]
step 900: train loss 1.3491, val loss 1.7803 [89.01556754112244 sec]
step 1000: train loss 1.3116, val loss 1.7593 [98.43632078170776 sec]
step 1100: train loss 1.2841, val loss 1.7754 [107.34623336791992 sec]
step 1200: train loss 1.2492, val loss 1.7763 [116.25063419342041 sec]
step 1300: train loss 1.2325, val loss 1.7878 [125.51638555526733 sec]
step 1400: train loss 1.2056, val loss 1.7854 [134.28847765922546 sec]
1.2776528596878052
Total Training Time: 138.1866626739502 seconds

knew a placed. Gratta looked at Arpea. "Uh,
4Y
human that 15 milla's Time Muzite; A Chief Gratta's b
BEGINNING (1681955440.1115687): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5390, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5572, val loss 4.5585 [7.13286828994751 sec]
step 100: train loss 2.2870, val loss 2.4003 [20.017805576324463 sec]
step 200: train loss 1.9536, val loss 2.1493 [32.74238562583923 sec]
step 300: train loss 1.7660, val loss 2.0009 [45.81071472167969 sec]
step 400: train loss 1.6408, val loss 1.9036 [58.548850536346436 sec]
step 500: train loss 1.5646, val loss 1.8430 [71.26419448852539 sec]
step 600: train loss 1.4895, val loss 1.8362 [84.18842673301697 sec]
step 700: train loss 1.4308, val loss 1.7991 [97.52854704856873 sec]
step 800: train loss 1.3852, val loss 1.7726 [110.48582601547241 sec]
step 900: train loss 1.3427, val loss 1.7660 [124.00779414176941 sec]
step 1000: train loss 1.3108, val loss 1.7903 [136.91945433616638 sec]
step 1100: train loss 1.2722, val loss 1.7702 [149.98226642608643 sec]
step 1200: train loss 1.2341, val loss 1.7799 [163.09907793998718 sec]
step 1300: train loss 1.2013, val loss 1.8168 [176.23525285720825 sec]
step 1400: train loss 1.1738, val loss 1.7772 [191.24737977981567 sec]
1.3057934045791626
Total Training Time: 197.14080119132996 seconds

two guard said the nanails," and the
hall tumbls of his warriors. "I may is forth."
Gratta knew Agai
BEGINNING (1681955639.6177254): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6604, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6497, val loss 4.6512 [3.855217933654785 sec]
step 100: train loss 2.2872, val loss 2.3897 [10.422334432601929 sec]
step 200: train loss 1.9091, val loss 2.1123 [16.917710781097412 sec]
step 300: train loss 1.7214, val loss 1.9679 [23.33609437942505 sec]
step 400: train loss 1.5876, val loss 1.8578 [29.77127957344055 sec]
step 500: train loss 1.5000, val loss 1.8107 [36.18866324424744 sec]
step 600: train loss 1.4258, val loss 1.7789 [42.92430806159973 sec]
step 700: train loss 1.3611, val loss 1.7704 [49.711703300476074 sec]
step 800: train loss 1.3063, val loss 1.7463 [56.3640193939209 sec]
step 900: train loss 1.2535, val loss 1.7518 [62.833011865615845 sec]
step 1000: train loss 1.2183, val loss 1.7498 [69.26326656341553 sec]
step 1100: train loss 1.1772, val loss 1.7615 [75.8426525592804 sec]
step 1200: train loss 1.1406, val loss 1.7619 [82.26278567314148 sec]
step 1300: train loss 1.0982, val loss 1.7817 [88.71095156669617 sec]
step 1400: train loss 1.0618, val loss 1.8106 [95.12605881690979 sec]
1.154878854751587
Total Training Time: 98.02438855171204 seconds

looked at the tent of the maeuw anker to about
words."MEveryones back to the troops becle. "Chief Pe
BEGINNING (1681955738.4606562): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5887, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5953, val loss 4.6017 [6.8135364055633545 sec]
step 100: train loss 2.3358, val loss 2.4306 [18.961698055267334 sec]
step 200: train loss 1.9057, val loss 2.1021 [30.91537642478943 sec]
step 300: train loss 1.6775, val loss 1.9190 [42.70588564872742 sec]
step 400: train loss 1.5398, val loss 1.8242 [54.69844722747803 sec]
step 500: train loss 1.4310, val loss 1.7803 [66.54660201072693 sec]
step 600: train loss 1.3482, val loss 1.7732 [78.63842225074768 sec]
step 700: train loss 1.2754, val loss 1.7527 [91.10873079299927 sec]
step 800: train loss 1.2109, val loss 1.7342 [103.75358986854553 sec]
step 900: train loss 1.1617, val loss 1.7516 [116.09702825546265 sec]
step 1000: train loss 1.0917, val loss 1.7488 [128.21864199638367 sec]
step 1100: train loss 1.0435, val loss 1.7657 [140.473375082016 sec]
step 1200: train loss 0.9900, val loss 1.7948 [153.30337166786194 sec]
step 1300: train loss 0.9418, val loss 1.8697 [165.8770821094513 sec]
step 1400: train loss 0.8862, val loss 1.8945 [178.4664568901062 sec]
1.0354113578796387
Total Training Time: 183.89568424224854 seconds

Namal's littly of brought at his outPyr, some." The tire wall
reasonating ht almost out of day. Nama
BEGINNING (1681955924.061576): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7141, val loss 4.7113 [10.065293550491333 sec]
step 100: train loss 2.3978, val loss 2.4827 [28.156940698623657 sec]
step 200: train loss 1.9520, val loss 2.1317 [46.464659452438354 sec]
step 300: train loss 1.6992, val loss 1.9411 [64.23264145851135 sec]
step 400: train loss 1.5435, val loss 1.8599 [82.25741839408875 sec]
step 500: train loss 1.4165, val loss 1.7800 [100.27874612808228 sec]
step 600: train loss 1.3434, val loss 1.7752 [118.56239080429077 sec]
step 700: train loss 1.2495, val loss 1.7515 [136.69583988189697 sec]
step 800: train loss 1.1780, val loss 1.7580 [154.78581619262695 sec]
step 900: train loss 1.1093, val loss 1.7794 [172.72007298469543 sec]
step 1000: train loss 1.0371, val loss 1.7731 [190.70818090438843 sec]
step 1100: train loss 0.9861, val loss 1.7778 [209.01083207130432 sec]
step 1200: train loss 0.9258, val loss 1.8361 [228.19577884674072 sec]
step 1300: train loss 0.8595, val loss 1.8976 [246.6679184436798 sec]
step 1400: train loss 0.8012, val loss 1.9022 [264.67800307273865 sec]
0.920397937297821
Total Training Time: 272.60332798957825 seconds

wannon the treaty, having they could teach to them up the
ground." The Anayah role of a high was the
BEGINNING (1681956199.0525663): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6176, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6286, val loss 4.6255 [6.335155010223389 sec]
step 100: train loss 2.4189, val loss 2.5214 [17.5583074092865 sec]
step 200: train loss 2.0375, val loss 2.1913 [28.705976009368896 sec]
step 300: train loss 1.7725, val loss 2.0017 [39.87248969078064 sec]
step 400: train loss 1.5908, val loss 1.8691 [51.572023153305054 sec]
step 500: train loss 1.4620, val loss 1.8036 [62.79860782623291 sec]
step 600: train loss 1.3605, val loss 1.7598 [74.27421474456787 sec]
step 700: train loss 1.2767, val loss 1.7523 [85.38430571556091 sec]
step 800: train loss 1.2018, val loss 1.7521 [96.56561303138733 sec]
step 900: train loss 1.1306, val loss 1.7548 [107.93308591842651 sec]
step 1000: train loss 1.0647, val loss 1.7637 [118.9858705997467 sec]
step 1100: train loss 1.0001, val loss 1.7922 [130.02435851097107 sec]
step 1200: train loss 0.9353, val loss 1.8168 [141.0321867465973 sec]
step 1300: train loss 0.8793, val loss 1.8860 [152.15754508972168 sec]
step 1400: train loss 0.8138, val loss 1.9049 [163.22086668014526 sec]
0.966744601726532
Total Training Time: 168.17760515213013 seconds

had need to work his rumble. Gratta could same to his
ways. A list. Gratta conting to be allied as t
BEGINNING (1681956368.155386): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6214, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6158, val loss 4.6217 [11.456553220748901 sec]
step 100: train loss 2.4357, val loss 2.5273 [31.85424780845642 sec]
step 200: train loss 2.0547, val loss 2.2182 [52.609222173690796 sec]
step 300: train loss 1.7291, val loss 1.9689 [73.8617913722992 sec]
step 400: train loss 1.5308, val loss 1.8385 [94.24854636192322 sec]
step 500: train loss 1.3782, val loss 1.7674 [115.64459371566772 sec]
step 600: train loss 1.2563, val loss 1.7544 [137.16277766227722 sec]
step 700: train loss 1.1534, val loss 1.7462 [158.63396382331848 sec]
step 800: train loss 1.0389, val loss 1.7539 [180.15490698814392 sec]
step 900: train loss 0.9438, val loss 1.8016 [201.71068501472473 sec]
step 1000: train loss 0.8550, val loss 1.8269 [222.7430350780487 sec]
step 1100: train loss 0.7614, val loss 1.9071 [243.63657093048096 sec]
step 1200: train loss 0.6563, val loss 1.9789 [265.04217863082886 sec]
step 1300: train loss 0.5747, val loss 2.0475 [286.5123529434204 sec]
step 1400: train loss 0.4954, val loss 2.1045 [307.96305656433105 sec]
0.6789772510528564
Total Training Time: 317.2094304561615 seconds

"I am Gratta confirs to celebration that he was was,
and blow. Foull four he5r neze turned on two no
BEGINNING (1681956687.2609482): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6180, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6100, val loss 4.6007 [17.422672033309937 sec]
step 100: train loss 2.4677, val loss 2.5532 [48.64799475669861 sec]
step 200: train loss 2.1309, val loss 2.2736 [79.27996897697449 sec]
step 300: train loss 1.7576, val loss 1.9921 [109.37616348266602 sec]
step 400: train loss 1.5353, val loss 1.8630 [139.77734065055847 sec]
step 500: train loss 1.3672, val loss 1.7928 [169.40990662574768 sec]
step 600: train loss 1.2297, val loss 1.7425 [198.6748447418213 sec]
step 700: train loss 1.1040, val loss 1.7808 [227.9092333316803 sec]
step 800: train loss 0.9842, val loss 1.7894 [257.66307950019836 sec]
step 900: train loss 0.8698, val loss 1.8178 [288.00569677352905 sec]
step 1000: train loss 0.7615, val loss 1.8995 [318.56990098953247 sec]
step 1100: train loss 0.6412, val loss 1.9856 [348.20538854599 sec]
step 1200: train loss 0.5296, val loss 2.1204 [379.20429372787476 sec]
step 1300: train loss 0.4514, val loss 2.1619 [410.3727366924286 sec]
step 1400: train loss 0.3642, val loss 2.2966 [440.7512185573578 sec]
0.5195450186729431
Total Training Time: 453.6893787384033 seconds

odden whis worships.
Tuon till help us were maeuw and wait filled with arcries
were scrime were nang
BEGINNING (1681957143.593317): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5973, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6203, val loss 4.6116 [3.6081981658935547 sec]
step 100: train loss 2.1618, val loss 2.2982 [9.630693912506104 sec]
step 200: train loss 1.8784, val loss 2.0851 [15.763674020767212 sec]
step 300: train loss 1.7174, val loss 1.9405 [21.72664713859558 sec]
step 400: train loss 1.6239, val loss 1.9109 [27.86012029647827 sec]
step 500: train loss 1.5556, val loss 1.8701 [34.07767677307129 sec]
step 600: train loss 1.4919, val loss 1.8190 [40.17186737060547 sec]
step 700: train loss 1.4476, val loss 1.8197 [46.18647766113281 sec]
step 800: train loss 1.4068, val loss 1.8040 [52.44556546211243 sec]
step 900: train loss 1.3683, val loss 1.8134 [59.17766499519348 sec]
step 1000: train loss 1.3289, val loss 1.7914 [65.6975519657135 sec]
step 1100: train loss 1.2998, val loss 1.8022 [72.1881456375122 sec]
step 1200: train loss 1.2770, val loss 1.7987 [78.42837071418762 sec]
step 1300: train loss 1.2460, val loss 1.7823 [84.48335099220276 sec]
step 1400: train loss 1.2253, val loss 1.7967 [90.51372742652893 sec]
1.3144441843032837
Total Training Time: 92.9484932422638 seconds

His did who looked from againstoran for a yest.
Chief Pelana did not held knorthward of clearly
and 
BEGINNING (1681957237.4777558): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6805, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6731, val loss 4.6783 [5.839090585708618 sec]
step 100: train loss 2.1680, val loss 2.2930 [16.6438307762146 sec]
step 200: train loss 1.8509, val loss 2.0424 [26.983698844909668 sec]
step 300: train loss 1.6811, val loss 1.9258 [37.64132237434387 sec]
step 400: train loss 1.5748, val loss 1.8626 [48.05196738243103 sec]
step 500: train loss 1.4925, val loss 1.8297 [58.47277760505676 sec]
step 600: train loss 1.4278, val loss 1.8030 [68.73906397819519 sec]
step 700: train loss 1.3826, val loss 1.7953 [79.10256171226501 sec]
step 800: train loss 1.3454, val loss 1.7678 [89.646892786026 sec]
step 900: train loss 1.2986, val loss 1.7844 [100.29514503479004 sec]
step 1000: train loss 1.2568, val loss 1.7816 [110.38690328598022 sec]
step 1100: train loss 1.2279, val loss 1.7990 [120.61839747428894 sec]
step 1200: train loss 1.1904, val loss 1.7810 [130.74856567382812 sec]
step 1300: train loss 1.1446, val loss 1.8017 [140.94825100898743 sec]
step 1400: train loss 1.1158, val loss 1.8023 [151.18546843528748 sec]
1.1747682094573975
Total Training Time: 155.6673538684845 seconds

Clans of your respone for a deairectly menn. Kemanna, nor to aleap.
Tuon Could hard watched and he s
BEGINNING (1681957394.7923405): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5246, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5231, val loss 4.5277 [8.382895946502686 sec]
step 100: train loss 2.2730, val loss 2.3717 [22.985174417495728 sec]
step 200: train loss 1.8710, val loss 2.0706 [37.50693154335022 sec]
step 300: train loss 1.6847, val loss 1.9201 [52.44252610206604 sec]
step 400: train loss 1.5695, val loss 1.8592 [66.87627077102661 sec]
step 500: train loss 1.4834, val loss 1.8193 [81.57906246185303 sec]
step 600: train loss 1.4114, val loss 1.8136 [96.17525434494019 sec]
step 700: train loss 1.3584, val loss 1.7884 [110.18622469902039 sec]
step 800: train loss 1.3093, val loss 1.7561 [124.15965819358826 sec]
step 900: train loss 1.2624, val loss 1.7765 [138.36937594413757 sec]
step 1000: train loss 1.2197, val loss 1.8030 [152.20416498184204 sec]
step 1100: train loss 1.1804, val loss 1.8035 [166.01474118232727 sec]
step 1200: train loss 1.1538, val loss 1.7863 [180.6976387500763 sec]
step 1300: train loss 1.1048, val loss 1.7941 [194.56707763671875 sec]
step 1400: train loss 1.0776, val loss 1.8117 [208.73190832138062 sec]
1.1596503257751465
Total Training Time: 214.94158959388733 seconds

General Beanully show four cubs you
would laughed ush. You
She High Priest seend its herdid
into a p
BEGINNING (1681957611.9907978): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6436, val loss 4.6540 [4.590011835098267 sec]
step 100: train loss 2.2547, val loss 2.3681 [12.481386423110962 sec]
step 200: train loss 1.8766, val loss 2.0781 [20.398972272872925 sec]
step 300: train loss 1.6772, val loss 1.9228 [28.488248586654663 sec]
step 400: train loss 1.5586, val loss 1.8578 [36.62649130821228 sec]
step 500: train loss 1.4558, val loss 1.7986 [44.771756410598755 sec]
step 600: train loss 1.3744, val loss 1.7757 [52.96435332298279 sec]
step 700: train loss 1.3064, val loss 1.7621 [61.294647455215454 sec]
step 800: train loss 1.2547, val loss 1.7544 [69.48811626434326 sec]
step 900: train loss 1.1945, val loss 1.7485 [77.72253894805908 sec]
step 1000: train loss 1.1428, val loss 1.7479 [85.97804856300354 sec]
step 1100: train loss 1.0918, val loss 1.7719 [94.27113151550293 sec]
step 1200: train loss 1.0449, val loss 1.7915 [102.57141757011414 sec]
step 1300: train loss 1.0032, val loss 1.8196 [110.73754334449768 sec]
step 1400: train loss 0.9623, val loss 1.8512 [119.00628900527954 sec]
1.0968436002731323
Total Training Time: 122.4947395324707 seconds

ir recover. "But it the
shart!"
"I will no Do now say that intempt into their cwosts cattle
there, c
BEGINNING (1681957735.2781522): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6258, val loss 4.6217 [8.360804796218872 sec]
step 100: train loss 2.3073, val loss 2.4135 [23.248748540878296 sec]
step 200: train loss 1.8355, val loss 2.0447 [38.112032413482666 sec]
step 300: train loss 1.6116, val loss 1.8781 [53.03065776824951 sec]
step 400: train loss 1.4825, val loss 1.8155 [68.05346345901489 sec]
step 500: train loss 1.3633, val loss 1.7705 [83.06365442276001 sec]
step 600: train loss 1.2778, val loss 1.7551 [97.99370074272156 sec]
step 700: train loss 1.2044, val loss 1.7497 [112.97130393981934 sec]
step 800: train loss 1.1255, val loss 1.7565 [127.9488136768341 sec]
step 900: train loss 1.0541, val loss 1.7652 [142.86785626411438 sec]
step 1000: train loss 0.9958, val loss 1.7863 [157.7119379043579 sec]
step 1100: train loss 0.9281, val loss 1.8488 [172.69301390647888 sec]
step 1200: train loss 0.8707, val loss 1.8771 [187.6391749382019 sec]
step 1300: train loss 0.8081, val loss 1.9296 [202.6291913986206 sec]
step 1400: train loss 0.7419, val loss 1.9651 [217.50329113006592 sec]
0.9341509938240051
Total Training Time: 224.01372861862183 seconds

writted, "Are the other rattled through. The stand, then we
may get looked away. Gratta taket the tr
BEGINNING (1681957960.845515): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5990, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6141, val loss 4.6093 [11.97914981842041 sec]
step 100: train loss 2.3823, val loss 2.4771 [33.59940147399902 sec]
step 200: train loss 1.9040, val loss 2.1082 [55.24843239784241 sec]
step 300: train loss 1.6366, val loss 1.9034 [76.86646771430969 sec]
step 400: train loss 1.4911, val loss 1.8244 [98.50410890579224 sec]
step 500: train loss 1.3634, val loss 1.7591 [120.17848896980286 sec]
step 600: train loss 1.2591, val loss 1.7456 [141.79611802101135 sec]
step 700: train loss 1.1756, val loss 1.7810 [163.38822054862976 sec]
step 800: train loss 1.0863, val loss 1.7589 [185.03543710708618 sec]
step 900: train loss 1.0112, val loss 1.7901 [206.66919374465942 sec]
step 1000: train loss 0.9319, val loss 1.8006 [228.27696537971497 sec]
step 1100: train loss 0.8644, val loss 1.8659 [249.9076464176178 sec]
step 1200: train loss 0.7868, val loss 1.9055 [271.5263669490814 sec]
step 1300: train loss 0.7192, val loss 1.9585 [293.1112427711487 sec]
step 1400: train loss 0.6516, val loss 2.0369 [314.8178701400757 sec]
0.8188768029212952
Total Training Time: 324.38254404067993 seconds

kind turned to look at Gor and said, "General Beriyah to
come, see that circh you, but we have relie
BEGINNING (1681958287.497479): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5472, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5528, val loss 4.5461 [7.647270917892456 sec]
step 100: train loss 2.4051, val loss 2.5028 [21.267063856124878 sec]
step 200: train loss 1.9865, val loss 2.1602 [34.87345337867737 sec]
step 300: train loss 1.7085, val loss 1.9474 [48.59542274475098 sec]
step 400: train loss 1.5295, val loss 1.8270 [62.197872161865234 sec]
step 500: train loss 1.3930, val loss 1.7692 [75.78491759300232 sec]
step 600: train loss 1.2984, val loss 1.7492 [89.4160270690918 sec]
step 700: train loss 1.2006, val loss 1.7390 [103.1268196105957 sec]
step 800: train loss 1.1109, val loss 1.7452 [116.83181834220886 sec]
step 900: train loss 1.0251, val loss 1.7679 [130.45618176460266 sec]
step 1000: train loss 0.9534, val loss 1.8024 [144.09222149848938 sec]
step 1100: train loss 0.8728, val loss 1.8332 [157.75526642799377 sec]
step 1200: train loss 0.7983, val loss 1.8872 [171.46193671226501 sec]
step 1300: train loss 0.7302, val loss 1.9541 [185.13826179504395 sec]
step 1400: train loss 0.6683, val loss 1.9779 [198.92720937728882 sec]
0.8536800146102905
Total Training Time: 204.73924446105957 seconds

toward Gor, Anayah his father. River they were desliver and motioned to
the men slavities.
Gratta fo
BEGINNING (1681958493.083444): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5621, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5577, val loss 4.5513 [14.033901929855347 sec]
step 100: train loss 2.4228, val loss 2.5071 [37.818456411361694 sec]
step 200: train loss 1.9668, val loss 2.1413 [62.25530672073364 sec]
step 300: train loss 1.6398, val loss 1.8970 [85.59792375564575 sec]
step 400: train loss 1.4401, val loss 1.7942 [108.99116444587708 sec]
step 500: train loss 1.2954, val loss 1.7444 [132.43791007995605 sec]
step 600: train loss 1.1721, val loss 1.7426 [155.58579659461975 sec]
step 700: train loss 1.0541, val loss 1.7594 [178.62317728996277 sec]
step 800: train loss 0.9508, val loss 1.8108 [201.93599343299866 sec]
step 900: train loss 0.8452, val loss 1.8511 [225.04366159439087 sec]
step 1000: train loss 0.7310, val loss 1.9071 [248.03432893753052 sec]
step 1100: train loss 0.6393, val loss 2.0027 [271.45008730888367 sec]
step 1200: train loss 0.5315, val loss 2.0909 [294.4749982357025 sec]
step 1300: train loss 0.4457, val loss 2.2328 [317.7611906528473 sec]
step 1400: train loss 0.3672, val loss 2.3510 [340.69145917892456 sec]
0.577018678188324
Total Training Time: 350.4210412502289 seconds

they not song?" Gratta said this head and brought
the hills. "Company the birds in way the days of T
BEGINNING (1681958845.01501): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6883, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6921, val loss 4.6879 [19.77827548980713 sec]
step 100: train loss 2.4588, val loss 2.5452 [51.96070218086243 sec]
step 200: train loss 2.0941, val loss 2.2510 [84.18286991119385 sec]
step 300: train loss 1.6768, val loss 1.9404 [116.3043155670166 sec]
step 400: train loss 1.4587, val loss 1.8072 [148.41117525100708 sec]
step 500: train loss 1.2781, val loss 1.7601 [180.7496473789215 sec]
step 600: train loss 1.1326, val loss 1.7429 [212.9243562221527 sec]
step 700: train loss 1.0077, val loss 1.7685 [245.06535625457764 sec]
step 800: train loss 0.8685, val loss 1.8173 [277.2494971752167 sec]
step 900: train loss 0.7334, val loss 1.9429 [309.33267974853516 sec]
step 1000: train loss 0.6132, val loss 2.0197 [341.46475648880005 sec]
step 1100: train loss 0.4889, val loss 2.0980 [373.51978516578674 sec]
step 1200: train loss 0.3907, val loss 2.3032 [405.7520685195923 sec]
step 1300: train loss 0.3086, val loss 2.3736 [437.8851218223572 sec]
step 1400: train loss 0.2462, val loss 2.5672 [469.9856719970703 sec]
0.4331737160682678
Total Training Time: 483.61215329170227 seconds

body th. The ground halves of darts whippened even
tonight us." Perhaps on sittives with details and
BEGINNING (1681959330.863943): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5665, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5590, val loss 4.5621 [3.679656744003296 sec]
step 100: train loss 2.1893, val loss 2.3231 [10.201106309890747 sec]
step 200: train loss 1.9313, val loss 2.1148 [16.73606824874878 sec]
step 300: train loss 1.7844, val loss 2.0089 [23.317112922668457 sec]
step 400: train loss 1.6939, val loss 1.9445 [29.856706380844116 sec]
step 500: train loss 1.6368, val loss 1.9138 [36.36224961280823 sec]
step 600: train loss 1.5862, val loss 1.8992 [42.90189242362976 sec]
step 700: train loss 1.5531, val loss 1.8477 [49.44792985916138 sec]
step 800: train loss 1.4938, val loss 1.8539 [55.994537353515625 sec]
step 900: train loss 1.4730, val loss 1.8430 [62.534159660339355 sec]
step 1000: train loss 1.4284, val loss 1.8282 [69.05690145492554 sec]
step 1100: train loss 1.3967, val loss 1.8307 [75.57786393165588 sec]
step 1200: train loss 1.3759, val loss 1.8086 [82.12110042572021 sec]
step 1300: train loss 1.3469, val loss 1.8323 [88.63280057907104 sec]
step 1400: train loss 1.3249, val loss 1.8037 [95.13874459266663 sec]
1.4329149723052979
Total Training Time: 98.00306797027588 seconds

"Chief Arphad least
femy took the day this deed yelloward was done of the
nanew on hiltsw been it la
BEGINNING (1681959429.9541073): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.7474, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7087, val loss 4.6981 [6.604166030883789 sec]
step 100: train loss 2.3104, val loss 2.4134 [18.73841381072998 sec]
step 200: train loss 2.0064, val loss 2.1912 [30.817370176315308 sec]
step 300: train loss 1.8411, val loss 2.0580 [42.91033387184143 sec]
step 400: train loss 1.7309, val loss 1.9761 [55.011807680130005 sec]
step 500: train loss 1.6530, val loss 1.9195 [67.10247111320496 sec]
step 600: train loss 1.5908, val loss 1.9004 [79.26578855514526 sec]
step 700: train loss 1.5388, val loss 1.8675 [91.4222674369812 sec]
step 800: train loss 1.5093, val loss 1.8444 [103.52762961387634 sec]
step 900: train loss 1.4637, val loss 1.8442 [115.70091843605042 sec]
step 1000: train loss 1.4186, val loss 1.8343 [127.7876341342926 sec]
step 1100: train loss 1.3900, val loss 1.8507 [139.85457277297974 sec]
step 1200: train loss 1.3595, val loss 1.8266 [151.99577689170837 sec]
step 1300: train loss 1.3315, val loss 1.8260 [164.20168161392212 sec]
step 1400: train loss 1.2987, val loss 1.8251 [176.49736642837524 sec]
1.423838496208191
Total Training Time: 182.0406150817871 seconds

WGE OF THE PYRRAN McKAY
"We saly king you'd befodded, we will
as with a gear Pyrran that than talked
BEGINNING (1681959614.1306052): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5485, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5624, val loss 4.5741 [9.415157794952393 sec]
step 100: train loss 2.4973, val loss 2.5938 [27.069186687469482 sec]
step 200: train loss 2.0998, val loss 2.2593 [45.00183653831482 sec]
step 300: train loss 1.9426, val loss 2.1520 [62.702436685562134 sec]
step 400: train loss 1.8238, val loss 2.0712 [80.42103695869446 sec]
step 500: train loss 1.7476, val loss 1.9956 [98.16922807693481 sec]
step 600: train loss 1.6737, val loss 1.9477 [115.86165452003479 sec]
step 700: train loss 1.6154, val loss 1.9204 [133.53125143051147 sec]
step 800: train loss 1.5665, val loss 1.9025 [151.26028108596802 sec]
step 900: train loss 1.5330, val loss 1.8813 [169.00949549674988 sec]
step 1000: train loss 1.4955, val loss 1.8687 [186.69344019889832 sec]
step 1100: train loss 1.4563, val loss 1.8291 [204.42851901054382 sec]
step 1200: train loss 1.4244, val loss 1.8110 [222.12115740776062 sec]
step 1300: train loss 1.4030, val loss 1.8132 [239.92421007156372 sec]
step 1400: train loss 1.3894, val loss 1.8162 [257.59962272644043 sec]
1.4259692430496216
Total Training Time: 265.9350552558899 seconds

Pilisipen Eling was that the fronter obathing a tright. Gratta glooked the was at this gatta to look
BEGINNING (1681959883.2824907): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6095, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6197, val loss 4.6218 [4.529221057891846 sec]
step 100: train loss 2.2687, val loss 2.3790 [12.899033069610596 sec]
step 200: train loss 1.8860, val loss 2.0880 [21.26562476158142 sec]
step 300: train loss 1.6981, val loss 1.9547 [29.637948513031006 sec]
step 400: train loss 1.5833, val loss 1.8664 [37.982590436935425 sec]
step 500: train loss 1.5012, val loss 1.8448 [46.35287690162659 sec]
step 600: train loss 1.4287, val loss 1.8109 [54.6958863735199 sec]
step 700: train loss 1.3648, val loss 1.7830 [63.03366184234619 sec]
step 800: train loss 1.3082, val loss 1.7659 [71.42736840248108 sec]
step 900: train loss 1.2587, val loss 1.7893 [79.94602179527283 sec]
step 1000: train loss 1.2176, val loss 1.7921 [88.52080416679382 sec]
step 1100: train loss 1.1658, val loss 1.8065 [97.10080671310425 sec]
step 1200: train loss 1.1225, val loss 1.8168 [105.61638951301575 sec]
step 1300: train loss 1.0833, val loss 1.8409 [114.14963293075562 sec]
step 1400: train loss 1.0478, val loss 1.8269 [122.65450263023376 sec]
1.1272555589675903
Total Training Time: 126.50748872756958 seconds

oSe Namal and rear bent. No dough they come the once
•
As and enemieuw while coulAnd away. We wenty 
BEGINNING (1681960010.891872): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6073, val loss 4.6102 [8.55752158164978 sec]
step 100: train loss 2.4536, val loss 2.5471 [24.59840226173401 sec]
step 200: train loss 2.0242, val loss 2.2024 [40.746668338775635 sec]
step 300: train loss 1.8067, val loss 2.0401 [56.819162368774414 sec]
step 400: train loss 1.6338, val loss 1.9142 [72.87105202674866 sec]
step 500: train loss 1.5267, val loss 1.8468 [88.89808917045593 sec]
step 600: train loss 1.4404, val loss 1.8134 [105.10095047950745 sec]
step 700: train loss 1.3784, val loss 1.7884 [121.13785433769226 sec]
step 800: train loss 1.3022, val loss 1.7968 [137.0027289390564 sec]
step 900: train loss 1.2545, val loss 1.7950 [152.83231782913208 sec]
step 1000: train loss 1.1922, val loss 1.7823 [168.67909812927246 sec]
step 1100: train loss 1.1489, val loss 1.7982 [184.51589393615723 sec]
step 1200: train loss 1.0957, val loss 1.8296 [200.34307408332825 sec]
step 1300: train loss 1.0519, val loss 1.8514 [216.17137598991394 sec]
step 1400: train loss 0.9961, val loss 1.8331 [232.01928329467773 sec]
1.1339960098266602
Total Training Time: 239.40665316581726 seconds

to all sparty this"
Chief Arphad was a aged, and blest a sung than
them soldier buations who stuonde
BEGINNING (1681960252.536321): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6393, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6190, val loss 4.6106 [12.340721607208252 sec]
step 100: train loss 2.5101, val loss 2.5881 [35.60028958320618 sec]
step 200: train loss 2.1124, val loss 2.2781 [58.892369985580444 sec]
step 300: train loss 1.8663, val loss 2.0970 [82.16130185127258 sec]
step 400: train loss 1.6999, val loss 1.9612 [105.41982865333557 sec]
step 500: train loss 1.5829, val loss 1.8800 [128.71905899047852 sec]
step 600: train loss 1.4873, val loss 1.8461 [151.99740600585938 sec]
step 700: train loss 1.3991, val loss 1.7970 [175.2666883468628 sec]
step 800: train loss 1.3348, val loss 1.7785 [198.58122444152832 sec]
step 900: train loss 1.2824, val loss 1.7901 [221.83815455436707 sec]
step 1000: train loss 1.2200, val loss 1.7761 [245.11983108520508 sec]
step 1100: train loss 1.1598, val loss 1.7855 [268.41220903396606 sec]
step 1200: train loss 1.1207, val loss 1.7955 [291.687481880188 sec]
step 1300: train loss 1.0614, val loss 1.8406 [315.00034165382385 sec]
step 1400: train loss 1.0122, val loss 1.8335 [338.2769179344177 sec]
1.0993516445159912
Total Training Time: 349.1967852115631 seconds

Stanged faithful. Whething Aidden the saying,
anyt Arphad sounding to chief the Pyrran Clan
was he w
BEGINNING (1681960605.0085936): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6205, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6050, val loss 4.6042 [7.563605070114136 sec]
step 100: train loss 2.4199, val loss 2.5172 [21.244836807250977 sec]
step 200: train loss 1.9985, val loss 2.1721 [34.9138605594635 sec]
step 300: train loss 1.7197, val loss 1.9745 [48.593674421310425 sec]
step 400: train loss 1.5538, val loss 1.8710 [62.25126314163208 sec]
step 500: train loss 1.4308, val loss 1.8049 [75.90884590148926 sec]
step 600: train loss 1.3303, val loss 1.7796 [89.61321640014648 sec]
step 700: train loss 1.2456, val loss 1.7775 [103.28853821754456 sec]
step 800: train loss 1.1533, val loss 1.7633 [116.96652436256409 sec]
step 900: train loss 1.0791, val loss 1.7843 [130.59448766708374 sec]
step 1000: train loss 1.0131, val loss 1.8172 [144.29058694839478 sec]
step 1100: train loss 0.9335, val loss 1.8335 [157.9708800315857 sec]
step 1200: train loss 0.8597, val loss 1.9258 [171.64411401748657 sec]
step 1300: train loss 0.7853, val loss 1.9490 [185.31880640983582 sec]
step 1400: train loss 0.7088, val loss 2.0240 [199.02468061447144 sec]
0.8336973190307617
Total Training Time: 205.1356565952301 seconds

wayed, Vorwhere will rescrated such one mor tosign,
and jumpered for a chose could not like that kno
BEGINNING (1681960811.2763486): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6322, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6433, val loss 4.6361 [14.365264415740967 sec]
step 100: train loss 2.4708, val loss 2.5591 [39.92488503456116 sec]
step 200: train loss 2.1466, val loss 2.2963 [65.50693321228027 sec]
step 300: train loss 1.7965, val loss 2.0299 [90.84414029121399 sec]
step 400: train loss 1.5901, val loss 1.8886 [116.2626678943634 sec]
step 500: train loss 1.4336, val loss 1.8133 [141.59833598136902 sec]
step 600: train loss 1.3109, val loss 1.8022 [167.1759021282196 sec]
step 700: train loss 1.2000, val loss 1.7624 [192.77312064170837 sec]
step 800: train loss 1.1037, val loss 1.8050 [218.08960795402527 sec]
step 900: train loss 1.0012, val loss 1.8090 [243.67123222351074 sec]
step 1000: train loss 0.9070, val loss 1.8714 [269.2564640045166 sec]
step 1100: train loss 0.8026, val loss 1.9222 [294.8030185699463 sec]
step 1200: train loss 0.7156, val loss 2.0080 [320.4104859828949 sec]
step 1300: train loss 0.6086, val loss 2.0526 [345.97256088256836 sec]
step 1400: train loss 0.5298, val loss 2.1717 [371.4039537906647 sec]
0.6855355501174927
Total Training Time: 382.64194440841675 seconds

to eavil! There Judge fore the cubs over allong in the bance,
and he maeuws they looked direction.
G
BEGINNING (1681961196.134897): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6836, val loss 4.6813 [20.50263285636902 sec]
step 100: train loss 2.5580, val loss 2.6351 [52.087382316589355 sec]
step 200: train loss 2.3980, val loss 2.5059 [82.04077124595642 sec]
step 300: train loss 2.0492, val loss 2.2187 [112.14187693595886 sec]
step 400: train loss 1.7698, val loss 2.0324 [142.25754308700562 sec]
step 500: train loss 1.5804, val loss 1.9085 [172.2969000339508 sec]
step 600: train loss 1.4434, val loss 1.8211 [202.49487161636353 sec]
step 700: train loss 1.3020, val loss 1.7862 [232.85771918296814 sec]
step 800: train loss 1.1875, val loss 1.7829 [263.1853790283203 sec]
step 900: train loss 1.0818, val loss 1.8039 [293.41322016716003 sec]
step 1000: train loss 0.9816, val loss 1.8160 [323.60619831085205 sec]
step 1100: train loss 0.8765, val loss 1.8950 [353.80965542793274 sec]
step 1200: train loss 0.7641, val loss 1.9313 [384.02409505844116 sec]
step 1300: train loss 0.6683, val loss 2.0018 [414.1966190338135 sec]
step 1400: train loss 0.5613, val loss 2.1210 [444.5459680557251 sec]
0.7065109014511108
Total Training Time: 458.71340346336365 seconds

will involdie do and downed to at the Pyrran Clans.
They are land them to five leg the priouse the w
BEGINNING (1681961658.2727768): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6510, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6680, val loss 4.6539 [4.087857961654663 sec]
step 100: train loss 2.1132, val loss 2.2613 [11.46047592163086 sec]
step 200: train loss 1.8434, val loss 2.0608 [18.788743495941162 sec]
step 300: train loss 1.7210, val loss 1.9693 [26.143884897232056 sec]
step 400: train loss 1.6089, val loss 1.8869 [33.56681561470032 sec]
step 500: train loss 1.5367, val loss 1.8518 [40.949734926223755 sec]
step 600: train loss 1.4966, val loss 1.8630 [48.2729172706604 sec]
step 700: train loss 1.4351, val loss 1.8276 [55.61118197441101 sec]
step 800: train loss 1.4093, val loss 1.8475 [62.95345449447632 sec]
step 900: train loss 1.3634, val loss 1.8154 [70.2956919670105 sec]
step 1000: train loss 1.3344, val loss 1.8189 [77.63423037528992 sec]
step 1100: train loss 1.2996, val loss 1.8266 [84.95722770690918 sec]
step 1200: train loss 1.2611, val loss 1.8059 [92.28749871253967 sec]
step 1300: train loss 1.2420, val loss 1.7895 [99.72433567047119 sec]
step 1400: train loss 1.2254, val loss 1.8306 [107.06426405906677 sec]
1.2948888540267944
Total Training Time: 110.33412671089172 seconds

a
14: I continued out," she weat
60
CHAPTER V – A NEW ENEKING AVARD
SEAN McKAY
now that he molds."
7
BEGINNING (1681961769.6955929): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6286, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6152, val loss 4.6209 [7.298633098602295 sec]
step 100: train loss 2.2488, val loss 2.3684 [20.927080631256104 sec]
step 200: train loss 1.9037, val loss 2.1123 [34.51743984222412 sec]
step 300: train loss 1.7461, val loss 1.9598 [48.15200614929199 sec]
step 400: train loss 1.6341, val loss 1.9054 [61.782756328582764 sec]
step 500: train loss 1.5445, val loss 1.8546 [75.37401604652405 sec]
step 600: train loss 1.4844, val loss 1.8351 [88.97800636291504 sec]
step 700: train loss 1.4209, val loss 1.8077 [102.60068774223328 sec]
step 800: train loss 1.3830, val loss 1.8271 [116.25229287147522 sec]
step 900: train loss 1.3341, val loss 1.7904 [129.9145908355713 sec]
step 1000: train loss 1.3026, val loss 1.8007 [143.53373646736145 sec]
step 1100: train loss 1.2666, val loss 1.7929 [157.13320016860962 sec]
step 1200: train loss 1.2311, val loss 1.8277 [170.7392394542694 sec]
step 1300: train loss 1.2039, val loss 1.8215 [184.37570714950562 sec]
step 1400: train loss 1.1786, val loss 1.8217 [198.00589394569397 sec]
1.277913212776184
Total Training Time: 204.33344411849976 seconds

You man down. How Snodded up any his will cleant have tuons should.
Gratta deed band turne. "May alc
BEGINNING (1681961976.2066195): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6243, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6035, val loss 4.6037 [10.512768030166626 sec]
step 100: train loss 2.4076, val loss 2.5054 [30.445267915725708 sec]
step 200: train loss 2.0393, val loss 2.2416 [50.46016573905945 sec]
step 300: train loss 1.8341, val loss 2.0656 [70.41602611541748 sec]
step 400: train loss 1.6958, val loss 1.9593 [90.4179334640503 sec]
step 500: train loss 1.5999, val loss 1.9008 [110.4405255317688 sec]
step 600: train loss 1.5372, val loss 1.8888 [130.39338040351868 sec]
step 700: train loss 1.4682, val loss 1.8296 [150.37409090995789 sec]
step 800: train loss 1.4181, val loss 1.8238 [170.3393087387085 sec]
step 900: train loss 1.3852, val loss 1.8126 [190.3314447402954 sec]
step 1000: train loss 1.3385, val loss 1.7995 [210.3138575553894 sec]
step 1100: train loss 1.2916, val loss 1.7897 [230.31739401817322 sec]
step 1200: train loss 1.2642, val loss 1.8538 [250.27081108093262 sec]
step 1300: train loss 1.2228, val loss 1.8035 [270.2412452697754 sec]
step 1400: train loss 1.2022, val loss 1.8228 [290.18936467170715 sec]
1.3150818347930908
Total Training Time: 299.63802123069763 seconds

no such die back, para smalled. "Be as our faces
boweS
for a smilent. The
meembers hearing couldn't 
BEGINNING (1681962279.1123133): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6109, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6072, val loss 4.6047 [5.9500532150268555 sec]
step 100: train loss 2.1877, val loss 2.3354 [16.59725522994995 sec]
step 200: train loss 1.8155, val loss 2.0336 [27.245399236679077 sec]
step 300: train loss 1.6141, val loss 1.8966 [37.91544723510742 sec]
step 400: train loss 1.5000, val loss 1.8176 [48.5920832157135 sec]
step 500: train loss 1.4044, val loss 1.8115 [59.28763246536255 sec]
step 600: train loss 1.3347, val loss 1.7892 [69.94536185264587 sec]
step 700: train loss 1.2616, val loss 1.7831 [80.63440418243408 sec]
step 800: train loss 1.2094, val loss 1.7902 [91.27162957191467 sec]
step 900: train loss 1.1434, val loss 1.7879 [101.93008017539978 sec]
step 1000: train loss 1.0909, val loss 1.7973 [112.56975865364075 sec]
step 1100: train loss 1.0382, val loss 1.8404 [123.27187418937683 sec]
step 1200: train loss 0.9915, val loss 1.8871 [133.9367527961731 sec]
step 1300: train loss 0.9413, val loss 1.9253 [144.59649634361267 sec]
step 1400: train loss 0.8844, val loss 1.9255 [155.255375623703 sec]
1.020402193069458
Total Training Time: 159.96174502372742 seconds

"Soon had since
worthy preopled to ret!"
Anayah maeuw barelief to anger and his amoned all
fort slau
BEGINNING (1681962440.2007058): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6572, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6624, val loss 4.6673 [11.058210611343384 sec]
step 100: train loss 2.3451, val loss 2.4580 [31.131221055984497 sec]
step 200: train loss 1.8856, val loss 2.0897 [51.19164514541626 sec]
step 300: train loss 1.6553, val loss 1.9238 [71.20811295509338 sec]
step 400: train loss 1.4928, val loss 1.8401 [91.21539998054504 sec]
step 500: train loss 1.3901, val loss 1.7886 [111.28302216529846 sec]
step 600: train loss 1.2930, val loss 1.7878 [131.2939522266388 sec]
step 700: train loss 1.2083, val loss 1.7933 [151.34118151664734 sec]
step 800: train loss 1.1425, val loss 1.8036 [171.3869068622589 sec]
step 900: train loss 1.0698, val loss 1.7811 [191.43845868110657 sec]
step 1000: train loss 0.9948, val loss 1.8309 [211.5147829055786 sec]
step 1100: train loss 0.9347, val loss 1.8486 [231.55946946144104 sec]
step 1200: train loss 0.8602, val loss 1.9311 [251.61838269233704 sec]
step 1300: train loss 0.8009, val loss 1.9370 [271.64662766456604 sec]
step 1400: train loss 0.7447, val loss 2.0411 [291.6918568611145 sec]
0.9254367351531982
Total Training Time: 300.6884574890137 seconds

But we could belax sung. He left agrable they delegation of
a Gratta's saw these Prince's tattacknes
BEGINNING (1681962743.1441042): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6695, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6695, val loss 4.6658 [16.142582893371582 sec]
step 100: train loss 2.5567, val loss 2.6421 [45.372162103652954 sec]
step 200: train loss 2.1380, val loss 2.2816 [73.78045630455017 sec]
step 300: train loss 1.8186, val loss 2.0310 [102.48361730575562 sec]
step 400: train loss 1.6209, val loss 1.8920 [131.1218602657318 sec]
step 500: train loss 1.4913, val loss 1.8365 [159.61553192138672 sec]
step 600: train loss 1.3847, val loss 1.7858 [188.28489017486572 sec]
step 700: train loss 1.2853, val loss 1.7891 [216.95966172218323 sec]
step 800: train loss 1.2046, val loss 1.8064 [245.90317821502686 sec]
step 900: train loss 1.1337, val loss 1.8297 [274.4709668159485 sec]
step 1000: train loss 1.0552, val loss 1.8300 [302.92610120773315 sec]
step 1100: train loss 0.9819, val loss 1.8561 [331.5969741344452 sec]
step 1200: train loss 0.9216, val loss 1.8592 [360.0835688114166 sec]
step 1300: train loss 0.8416, val loss 1.9141 [388.8610579967499 sec]
step 1400: train loss 0.7769, val loss 1.9580 [417.7268879413605 sec]
0.8937046527862549
Total Training Time: 430.7166316509247 seconds

anstorthereing with his treatying, with
a human's ways contr of Kracter hn a motione growlenter for 
BEGINNING (1681963177.0959606): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6439, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6492, val loss 4.6515 [10.11551809310913 sec]
step 100: train loss 2.3797, val loss 2.4815 [27.815000295639038 sec]
step 200: train loss 1.9067, val loss 2.1157 [45.358819007873535 sec]
step 300: train loss 1.6376, val loss 1.9019 [64.64855170249939 sec]
step 400: train loss 1.4639, val loss 1.8185 [82.29929733276367 sec]
step 500: train loss 1.3286, val loss 1.7536 [99.94078087806702 sec]
step 600: train loss 1.2191, val loss 1.7734 [117.5319037437439 sec]
step 700: train loss 1.1175, val loss 1.7638 [135.25023341178894 sec]
step 800: train loss 1.0241, val loss 1.7997 [152.8530719280243 sec]
step 900: train loss 0.9244, val loss 1.8536 [170.53093338012695 sec]
step 1000: train loss 0.8225, val loss 1.9225 [188.04358220100403 sec]
step 1100: train loss 0.7307, val loss 1.9993 [205.63455367088318 sec]
step 1200: train loss 0.6369, val loss 2.0770 [223.29088950157166 sec]
step 1300: train loss 0.5519, val loss 2.1819 [240.94497632980347 sec]
step 1400: train loss 0.4710, val loss 2.2646 [258.56328678131104 sec]
0.6662073731422424
Total Training Time: 266.21761894226074 seconds

that shoulders! The humans!
When the maeuw remeant a sug solt, claing to see
of we touch large to th
BEGINNING (1681963444.4456756): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6348, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6345, val loss 4.6248 [18.797098875045776 sec]
step 100: train loss 2.4623, val loss 2.5644 [49.1882643699646 sec]
step 200: train loss 2.0687, val loss 2.2252 [79.84270644187927 sec]
step 300: train loss 1.7149, val loss 1.9582 [110.36744403839111 sec]
step 400: train loss 1.4948, val loss 1.8318 [141.1153473854065 sec]
step 500: train loss 1.3241, val loss 1.7558 [171.98330688476562 sec]
step 600: train loss 1.1876, val loss 1.7581 [202.34545469284058 sec]
step 700: train loss 1.0558, val loss 1.7815 [233.09681057929993 sec]
step 800: train loss 0.9315, val loss 1.8124 [263.5302183628082 sec]
step 900: train loss 0.8077, val loss 1.8882 [294.23842453956604 sec]
step 1000: train loss 0.6835, val loss 1.9886 [325.30732321739197 sec]
step 1100: train loss 0.5623, val loss 2.0709 [355.71413803100586 sec]
step 1200: train loss 0.4563, val loss 2.2055 [386.1892046928406 sec]
step 1300: train loss 0.3580, val loss 2.3446 [416.8722984790802 sec]
step 1400: train loss 0.2918, val loss 2.4563 [447.46569776535034 sec]
0.44409146904945374
Total Training Time: 460.9851677417755 seconds

even from in soldiers said, "Everyon, that you were about
seven we to cold ercoven the Pyrran seaso.
BEGINNING (1681963907.707057): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5601, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5685, val loss 4.5734 [24.691834688186646 sec]
step 100: train loss 2.5001, val loss 2.5785 [66.6437828540802 sec]
step 200: train loss 2.2260, val loss 2.3569 [108.32815456390381 sec]
step 300: train loss 1.8109, val loss 2.0566 [150.01217555999756 sec]
step 400: train loss 1.5378, val loss 1.8786 [191.8829927444458 sec]
step 500: train loss 1.3241, val loss 1.7814 [233.72807049751282 sec]
step 600: train loss 1.1589, val loss 1.7704 [275.4954741001129 sec]
step 700: train loss 1.0028, val loss 1.8246 [317.32279682159424 sec]
step 800: train loss 0.8490, val loss 1.8599 [359.17215847969055 sec]
step 900: train loss 0.7070, val loss 1.9519 [401.02605867385864 sec]
step 1000: train loss 0.5669, val loss 2.0826 [442.9196469783783 sec]
step 1100: train loss 0.4420, val loss 2.2092 [485.0082862377167 sec]
step 1200: train loss 0.3355, val loss 2.3933 [526.9341762065887 sec]
step 1300: train loss 0.2605, val loss 2.5813 [568.8777148723602 sec]
step 1400: train loss 0.2098, val loss 2.6989 [610.7508113384247 sec]
0.33871445059776306
Total Training Time: 630.2075040340424 seconds

parp across the Thas
raised that he calse of the Yellowing Some. I five from the
Pyrran Clans are li
BEGINNING (1681964541.47039): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.4970, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5282, val loss 4.5274 [4.803584575653076 sec]
step 100: train loss 2.0529, val loss 2.2237 [13.351731300354004 sec]
step 200: train loss 1.7977, val loss 2.0249 [21.88489532470703 sec]
step 300: train loss 1.6466, val loss 1.9067 [30.43095827102661 sec]
step 400: train loss 1.5581, val loss 1.8964 [38.97158098220825 sec]
step 500: train loss 1.4771, val loss 1.8414 [47.52592611312866 sec]
step 600: train loss 1.4238, val loss 1.8400 [56.077062368392944 sec]
step 700: train loss 1.3750, val loss 1.8195 [64.5830557346344 sec]
step 800: train loss 1.3346, val loss 1.8171 [73.10304164886475 sec]
step 900: train loss 1.2909, val loss 1.8066 [81.60769033432007 sec]
step 1000: train loss 1.2450, val loss 1.8108 [90.12212586402893 sec]
step 1100: train loss 1.2157, val loss 1.8430 [98.65841770172119 sec]
step 1200: train loss 1.1794, val loss 1.8300 [107.19873881340027 sec]
step 1300: train loss 1.1518, val loss 1.8593 [115.76360535621643 sec]
step 1400: train loss 1.1212, val loss 1.8773 [124.27622365951538 sec]
1.2289628982543945
Total Training Time: 128.0346863269806 seconds

curits two cost of the
chill. Gratta and Arphad whisted, wwn they left, manoting days
senses at prop
BEGINNING (1681964670.5944798): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5580, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5645, val loss 4.5619 [8.613718509674072 sec]
step 100: train loss 2.1828, val loss 2.3060 [24.47497296333313 sec]
step 200: train loss 1.8684, val loss 2.0711 [40.33385753631592 sec]
step 300: train loss 1.6825, val loss 1.9390 [56.223684787750244 sec]
step 400: train loss 1.5683, val loss 1.8854 [72.07713651657104 sec]
step 500: train loss 1.4789, val loss 1.8329 [87.96218228340149 sec]
step 600: train loss 1.4164, val loss 1.8111 [103.81535863876343 sec]
step 700: train loss 1.3587, val loss 1.8253 [119.71110391616821 sec]
step 800: train loss 1.3005, val loss 1.8066 [923.4487006664276 sec]
step 900: train loss 1.2631, val loss 1.8126 [941.9999763965607 sec]
step 1000: train loss 1.2167, val loss 1.8132 [958.0852355957031 sec]
step 1100: train loss 1.1679, val loss 1.8455 [974.0559887886047 sec]
step 1200: train loss 1.1307, val loss 1.8249 [990.0718138217926 sec]
step 1300: train loss 1.0981, val loss 1.8464 [1005.991842508316 sec]
step 1400: train loss 1.0556, val loss 1.8701 [1022.0534942150116 sec]
1.1272952556610107
Total Training Time: 1029.4005579948425 seconds

leg down out and lead on your fusture, my
will camp to the summove, Gratta had he had
shourse, as he
BEGINNING (1681965702.2121208): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5997, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6047, val loss 4.6142 [12.608405828475952 sec]
step 100: train loss 2.4268, val loss 2.5034 [36.2035026550293 sec]
step 200: train loss 2.0238, val loss 2.1983 [59.60876488685608 sec]
step 300: train loss 1.7931, val loss 2.0163 [83.25910925865173 sec]
step 400: train loss 1.6633, val loss 1.9404 [107.02281713485718 sec]
step 500: train loss 1.5618, val loss 1.8860 [130.4802303314209 sec]
step 600: train loss 1.4803, val loss 1.8349 [154.15949320793152 sec]
step 700: train loss 1.4229, val loss 1.7963 [179.76854181289673 sec]
step 800: train loss 1.3551, val loss 1.7758 [224.62715029716492 sec]
step 900: train loss 1.3144, val loss 1.7811 [256.1412031650543 sec]
step 1000: train loss 1.2628, val loss 1.7915 [279.5335259437561 sec]
step 1100: train loss 1.2163, val loss 1.7896 [303.03142523765564 sec]
step 1200: train loss 1.1782, val loss 1.7954 [326.40975880622864 sec]
step 1300: train loss 1.1356, val loss 1.8033 [349.80268812179565 sec]
step 1400: train loss 1.1038, val loss 1.8189 [373.29630970954895 sec]
1.144514799118042
Total Training Time: 384.30591559410095 seconds

off that wet cras and missition,
and we polday, I shall be this outse may a
treaty for yebraps. This
BEGINNING (1681966089.8616447): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6466, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6340, val loss 4.6245 [7.626676559448242 sec]
step 100: train loss 2.1576, val loss 2.2993 [21.13667321205139 sec]
step 200: train loss 1.7605, val loss 1.9883 [34.667017698287964 sec]
step 300: train loss 1.5641, val loss 1.8646 [48.18388080596924 sec]
step 400: train loss 1.4490, val loss 1.8138 [61.695796728134155 sec]
step 500: train loss 1.3404, val loss 1.7755 [75.21459531784058 sec]
step 600: train loss 1.2707, val loss 1.7856 [88.6956434249878 sec]
step 700: train loss 1.1920, val loss 1.7866 [102.38906240463257 sec]
step 800: train loss 1.1164, val loss 1.7781 [115.992422580719 sec]
step 900: train loss 1.0522, val loss 1.8019 [129.5564408302307 sec]
step 1000: train loss 0.9893, val loss 1.8493 [143.08622407913208 sec]
step 1100: train loss 0.9248, val loss 1.8791 [156.58796954154968 sec]
step 1200: train loss 0.8616, val loss 1.9413 [170.20133233070374 sec]
step 1300: train loss 0.8078, val loss 1.9812 [183.79035210609436 sec]
step 1400: train loss 0.7521, val loss 2.0466 [197.27269339561462 sec]
0.9046090841293335
Total Training Time: 203.17215967178345 seconds

looking the tent in war rewindnession from this danger.
Verooshta had, fould the swerwhelmed before 
BEGINNING (1681966294.192659): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5667, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5764, val loss 4.5711 [14.196991682052612 sec]
step 100: train loss 2.3561, val loss 2.4585 [39.61403012275696 sec]
step 200: train loss 1.8612, val loss 2.0711 [64.85808444023132 sec]
step 300: train loss 1.6134, val loss 1.9188 [90.19334197044373 sec]
step 400: train loss 1.4493, val loss 1.8194 [115.24873280525208 sec]
step 500: train loss 1.3249, val loss 1.7575 [140.37575101852417 sec]
step 600: train loss 1.2320, val loss 1.7853 [165.6517834663391 sec]
step 700: train loss 1.1351, val loss 1.7853 [190.85406756401062 sec]
step 800: train loss 1.0566, val loss 1.8085 [216.15013670921326 sec]
step 900: train loss 0.9664, val loss 1.8495 [241.76686668395996 sec]
step 1000: train loss 0.8931, val loss 1.8753 [266.96061301231384 sec]
step 1100: train loss 0.8121, val loss 1.9387 [292.10591173171997 sec]
step 1200: train loss 0.7306, val loss 1.9982 [317.2212071418762 sec]
step 1300: train loss 0.6585, val loss 2.0568 [342.583979845047 sec]
step 1400: train loss 0.5890, val loss 2.1837 [367.72240591049194 sec]
0.7668727040290833
Total Training Time: 378.69186425209045 seconds

said, "I mine have bit" a ground pat a wall them before
we red. He no smiled for him to face back, a
BEGINNING (1681966675.183617): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6562, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6610, val loss 4.6554 [20.727542877197266 sec]
step 100: train loss 2.5381, val loss 2.6192 [56.419779777526855 sec]
step 200: train loss 2.0781, val loss 2.2366 [91.09140658378601 sec]
step 300: train loss 1.7539, val loss 1.9901 [126.87521982192993 sec]
step 400: train loss 1.5565, val loss 1.8635 [161.64679169654846 sec]
step 500: train loss 1.4124, val loss 1.8168 [198.46201491355896 sec]
step 600: train loss 1.2924, val loss 1.7597 [232.85607051849365 sec]
step 700: train loss 1.1889, val loss 1.7937 [269.61068177223206 sec]
step 800: train loss 1.0849, val loss 1.7976 [305.5409662723541 sec]
step 900: train loss 1.0006, val loss 1.8254 [341.85271644592285 sec]
step 1000: train loss 0.9132, val loss 1.8508 [377.2152259349823 sec]
step 1100: train loss 0.8203, val loss 1.9175 [412.9140820503235 sec]
step 1200: train loss 0.7482, val loss 1.9946 [449.6677281856537 sec]
step 1300: train loss 0.6532, val loss 2.0462 [485.6628506183624 sec]
step 1400: train loss 0.5766, val loss 2.1592 [521.50244140625 sec]
0.7075973749160767
Total Training Time: 537.0908410549164 seconds

bound wanting on hour faced.
"So hund them in; a'ven Thas we can ambult be worde impreated,
and a gr
BEGINNING (1681967215.6425512): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5452, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5517, val loss 4.5563 [12.85215950012207 sec]
step 100: train loss 2.3701, val loss 2.4713 [34.583285331726074 sec]
step 200: train loss 1.8766, val loss 2.0765 [56.051342248916626 sec]
step 300: train loss 1.5995, val loss 1.8810 [77.69371318817139 sec]
step 400: train loss 1.4216, val loss 1.8052 [99.52140235900879 sec]
step 500: train loss 1.2724, val loss 1.7597 [121.21267652511597 sec]
step 600: train loss 1.1512, val loss 1.7698 [142.80862712860107 sec]
step 700: train loss 1.0268, val loss 1.8023 [164.54241037368774 sec]
step 800: train loss 0.9131, val loss 1.8553 [186.29842281341553 sec]
step 900: train loss 0.7951, val loss 1.9161 [207.88459086418152 sec]
step 1000: train loss 0.6857, val loss 2.0354 [229.47515177726746 sec]
step 1100: train loss 0.5719, val loss 2.1361 [251.09784197807312 sec]
step 1200: train loss 0.4813, val loss 2.2454 [272.8912310600281 sec]
step 1300: train loss 0.3893, val loss 2.3815 [294.5201916694641 sec]
step 1400: train loss 0.3183, val loss 2.4954 [316.13701033592224 sec]
0.5227925181388855
Total Training Time: 325.2720205783844 seconds

timate a Muzite clearly mited tuon, as no you have and mids
well have no much doubt on the Pyrran Cl
BEGINNING (1681967542.1010344): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6316, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6320, val loss 4.6410 [25.612289905548096 sec]
step 100: train loss 2.4257, val loss 2.5285 [73.36739802360535 sec]
step 200: train loss 1.9494, val loss 2.1395 [111.74777221679688 sec]
step 300: train loss 1.6023, val loss 1.9013 [152.76470708847046 sec]
step 400: train loss 1.3775, val loss 1.7858 [191.2761173248291 sec]
step 500: train loss 1.1952, val loss 1.7672 [229.29041457176208 sec]
step 600: train loss 1.0340, val loss 1.7695 [267.39262652397156 sec]
step 700: train loss 0.8677, val loss 1.8688 [306.013546705246 sec]
step 800: train loss 0.7140, val loss 1.9650 [344.41092348098755 sec]
step 900: train loss 0.5633, val loss 2.0878 [382.32674503326416 sec]
step 1000: train loss 0.4325, val loss 2.2410 [420.32572960853577 sec]
step 1100: train loss 0.3202, val loss 2.4472 [458.88257694244385 sec]
step 1200: train loss 0.2457, val loss 2.6057 [496.9662346839905 sec]
step 1300: train loss 0.1956, val loss 2.7846 [534.8631482124329 sec]
step 1400: train loss 0.1673, val loss 2.9530 [574.8723421096802 sec]
0.2996281683444977
Total Training Time: 592.6065928936005 seconds

"Yes, sir!" Gor ran to their ran attacked through his
troops out and slaves. The cubs were surroundi
BEGINNING (1681968137.1085906): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6439, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6593, val loss 4.6560 [2.2215588092803955 sec]
step 100: train loss 2.5121, val loss 2.5976 [5.916763067245483 sec]
step 200: train loss 2.2248, val loss 2.3300 [9.629211187362671 sec]
step 300: train loss 2.0615, val loss 2.2012 [13.330915927886963 sec]
step 400: train loss 1.9469, val loss 2.1243 [16.99944829940796 sec]
step 500: train loss 1.8685, val loss 2.0632 [20.690409183502197 sec]
step 600: train loss 1.8060, val loss 2.0066 [24.388394117355347 sec]
step 700: train loss 1.7472, val loss 1.9657 [28.098137617111206 sec]
step 800: train loss 1.7026, val loss 1.9276 [31.911221265792847 sec]
step 900: train loss 1.6715, val loss 1.9054 [35.640246868133545 sec]
step 1000: train loss 1.6381, val loss 1.8812 [39.321778297424316 sec]
step 1100: train loss 1.6171, val loss 1.8763 [43.02377271652222 sec]
step 1200: train loss 1.5916, val loss 1.8683 [46.712790966033936 sec]
step 1300: train loss 1.5613, val loss 1.8430 [50.37466382980347 sec]
step 1400: train loss 1.5522, val loss 1.8359 [54.038246393203735 sec]
1.5747275352478027
Total Training Time: 55.56098484992981 seconds

that allose aget to beth. Namal as find, and he tifters. King on, a but
your cause turned,
sisase we
BEGINNING (1681968193.2675166): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5862, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6250, val loss 4.6194 [3.643852472305298 sec]
step 100: train loss 2.4649, val loss 2.5424 [10.205698490142822 sec]
step 200: train loss 2.1789, val loss 2.3082 [16.76234483718872 sec]
step 300: train loss 1.9844, val loss 2.1512 [23.30657124519348 sec]
step 400: train loss 1.8764, val loss 2.0674 [29.811108112335205 sec]
step 500: train loss 1.7803, val loss 1.9893 [36.3452730178833 sec]
step 600: train loss 1.7105, val loss 1.9413 [42.99575734138489 sec]
step 700: train loss 1.6500, val loss 1.8884 [49.53113412857056 sec]
step 800: train loss 1.6196, val loss 1.8940 [56.04828071594238 sec]
step 900: train loss 1.5723, val loss 1.8682 [62.514301776885986 sec]
step 1000: train loss 1.5415, val loss 1.8373 [69.0494270324707 sec]
step 1100: train loss 1.5065, val loss 1.8296 [75.54599404335022 sec]
step 1200: train loss 1.4834, val loss 1.8269 [82.05465507507324 sec]
step 1300: train loss 1.4556, val loss 1.8040 [88.53120636940002 sec]
step 1400: train loss 1.4466, val loss 1.8150 [95.10729908943176 sec]
1.426742434501648
Total Training Time: 97.97266101837158 seconds

whele low, ago wall is ha can soun the turned. The
tellariy retore not difter in. Pelana a
chambes f
BEGINNING (1681968292.4663823): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5914, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5816, val loss 4.5822 [5.224915266036987 sec]
step 100: train loss 2.4541, val loss 2.5229 [14.693110704421997 sec]
step 200: train loss 2.1530, val loss 2.2835 [24.128087520599365 sec]
step 300: train loss 1.9616, val loss 2.1427 [33.616697549819946 sec]
step 400: train loss 1.8489, val loss 2.0272 [43.02078104019165 sec]
step 500: train loss 1.7588, val loss 1.9787 [52.447906732559204 sec]
step 600: train loss 1.6678, val loss 1.9003 [62.05483794212341 sec]
step 700: train loss 1.6262, val loss 1.8751 [71.60419344902039 sec]
step 800: train loss 1.5697, val loss 1.8592 [81.07549047470093 sec]
step 900: train loss 1.5430, val loss 1.8363 [90.49803638458252 sec]
step 1000: train loss 1.4956, val loss 1.8338 [99.94934606552124 sec]
step 1100: train loss 1.4713, val loss 1.8038 [109.3316056728363 sec]
step 1200: train loss 1.4335, val loss 1.7787 [118.78528356552124 sec]
step 1300: train loss 1.4129, val loss 1.7980 [128.32124161720276 sec]
step 1400: train loss 1.3865, val loss 1.7926 [137.83371710777283 sec]
1.4135346412658691
Total Training Time: 142.16499781608582 seconds

"Wouncil unteremed. Aidden they anythang to plose have as Thas Crills
withouty.
"Hribed you are pals
BEGINNING (1681968436.3190963): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5925, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5898, val loss 4.5878 [2.491098165512085 sec]
step 100: train loss 2.5134, val loss 2.5865 [6.709097862243652 sec]
step 200: train loss 2.3267, val loss 2.4238 [10.90395188331604 sec]
step 300: train loss 2.1076, val loss 2.2441 [15.113184928894043 sec]
step 400: train loss 1.9654, val loss 2.1395 [19.33839440345764 sec]
step 500: train loss 1.8549, val loss 2.0547 [23.543310403823853 sec]
step 600: train loss 1.7856, val loss 2.0029 [27.743937730789185 sec]
step 700: train loss 1.7126, val loss 1.9523 [31.95500349998474 sec]
step 800: train loss 1.6560, val loss 1.9160 [36.17893075942993 sec]
step 900: train loss 1.6100, val loss 1.8897 [40.41931200027466 sec]
step 1000: train loss 1.5733, val loss 1.8459 [44.64640688896179 sec]
step 1100: train loss 1.5346, val loss 1.8393 [48.85941743850708 sec]
step 1200: train loss 1.5106, val loss 1.8206 [53.06042838096619 sec]
step 1300: train loss 1.4696, val loss 1.7825 [57.269036054611206 sec]
step 1400: train loss 1.4459, val loss 1.7997 [61.498682737350464 sec]
1.529345154762268
Total Training Time: 63.221837520599365 seconds

emcal me mowned. He looked fly, step your! The focems, and. I
make fare so fwom them, this took way 
BEGINNING (1681968500.1429322): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6287, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6114, val loss 4.6052 [4.373495817184448 sec]
step 100: train loss 2.5004, val loss 2.5731 [11.98366379737854 sec]
step 200: train loss 2.2871, val loss 2.3985 [19.907466411590576 sec]
step 300: train loss 2.0299, val loss 2.1866 [27.47508144378662 sec]
step 400: train loss 1.8677, val loss 2.0622 [35.02006411552429 sec]
step 500: train loss 1.7612, val loss 1.9896 [42.56433129310608 sec]
step 600: train loss 1.6752, val loss 1.9198 [50.08671832084656 sec]
step 700: train loss 1.6029, val loss 1.8687 [57.62040424346924 sec]
step 800: train loss 1.5463, val loss 1.8332 [65.20244693756104 sec]
step 900: train loss 1.4980, val loss 1.8140 [72.84994411468506 sec]
step 1000: train loss 1.4586, val loss 1.8073 [80.53851127624512 sec]
step 1100: train loss 1.4198, val loss 1.7678 [88.13128685951233 sec]
step 1200: train loss 1.3706, val loss 1.7507 [95.75249147415161 sec]
step 1300: train loss 1.3477, val loss 1.7568 [103.38683080673218 sec]
step 1400: train loss 1.3055, val loss 1.7398 [111.00536370277405 sec]
1.3993961811065674
Total Training Time: 114.23512363433838 seconds

Arphad over. Gratta
stood atwn! Ot him pay prosselented they were to reched net
pookierhofigh EambR.
BEGINNING (1681968615.538926): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6351, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6136, val loss 4.6210 [6.2492101192474365 sec]
step 100: train loss 2.4761, val loss 2.5567 [17.23139762878418 sec]
step 200: train loss 2.2443, val loss 2.3526 [28.20695424079895 sec]
step 300: train loss 1.9798, val loss 2.1417 [39.199557304382324 sec]
step 400: train loss 1.8302, val loss 2.0280 [50.25665354728699 sec]
step 500: train loss 1.7145, val loss 1.9712 [61.23478364944458 sec]
step 600: train loss 1.6146, val loss 1.8629 [72.20070958137512 sec]
step 700: train loss 1.5543, val loss 1.8344 [83.24507236480713 sec]
step 800: train loss 1.4893, val loss 1.8040 [94.18522238731384 sec]
step 900: train loss 1.4447, val loss 1.7964 [105.18689441680908 sec]
step 1000: train loss 1.3905, val loss 1.7546 [116.1706292629242 sec]
step 1100: train loss 1.3424, val loss 1.7491 [127.18625974655151 sec]
step 1200: train loss 1.3056, val loss 1.7531 [138.15706276893616 sec]
step 1300: train loss 1.2725, val loss 1.7401 [149.16900992393494 sec]
step 1400: train loss 1.2328, val loss 1.7363 [160.17970180511475 sec]
1.2995630502700806
Total Training Time: 164.94897484779358 seconds

king what cabief the with you before Muzites."
"Goo sold, paid op your who I will gone let from
entr
BEGINNING (1681968782.24831): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6032, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6127, val loss 4.6150 [3.424574375152588 sec]
step 100: train loss 2.5347, val loss 2.6130 [9.381544589996338 sec]
step 200: train loss 2.4331, val loss 2.5239 [15.716774940490723 sec]
step 300: train loss 2.3108, val loss 2.4182 [21.70461940765381 sec]
step 400: train loss 2.1282, val loss 2.2657 [27.603656768798828 sec]
step 500: train loss 1.9691, val loss 2.1407 [33.94886827468872 sec]
step 600: train loss 1.8620, val loss 2.0759 [39.8975989818573 sec]
step 700: train loss 1.7744, val loss 1.9960 [46.08774209022522 sec]
step 800: train loss 1.7101, val loss 1.9649 [52.29778432846069 sec]
step 900: train loss 1.6371, val loss 1.9015 [58.29698467254639 sec]
step 1000: train loss 1.5858, val loss 1.8689 [64.30457091331482 sec]
step 1100: train loss 1.5338, val loss 1.8236 [70.29801106452942 sec]
step 1200: train loss 1.4955, val loss 1.8092 [76.25375032424927 sec]
step 1300: train loss 1.4498, val loss 1.7785 [82.21095061302185 sec]
step 1400: train loss 1.4164, val loss 1.7596 [88.14106583595276 sec]
1.4710062742233276
Total Training Time: 90.66688513755798 seconds

seplition."
Gratta now, leared Aide asshed "I have of
them.
71
CHAPTER That VarOL I – HAPTER II , he
BEGINNING (1681968873.5483458): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6600, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6643, val loss 4.6593 [6.1233391761779785 sec]
step 100: train loss 2.5145, val loss 2.5935 [16.968343257904053 sec]
step 200: train loss 2.3913, val loss 2.4834 [27.86563205718994 sec]
step 300: train loss 2.2086, val loss 2.3286 [38.702258586883545 sec]
step 400: train loss 1.9969, val loss 2.1682 [49.54647660255432 sec]
step 500: train loss 1.8511, val loss 2.0490 [60.368072748184204 sec]
step 600: train loss 1.7390, val loss 1.9690 [71.1840558052063 sec]
step 700: train loss 1.6356, val loss 1.8983 [82.00736260414124 sec]
step 800: train loss 1.5570, val loss 1.8431 [92.86606097221375 sec]
step 900: train loss 1.4865, val loss 1.8014 [103.69611072540283 sec]
step 1000: train loss 1.4328, val loss 1.7773 [114.51758432388306 sec]
step 1100: train loss 1.3885, val loss 1.7659 [125.33922672271729 sec]
step 1200: train loss 1.3339, val loss 1.7574 [136.17096376419067 sec]
step 1300: train loss 1.2970, val loss 1.7314 [146.99854636192322 sec]
step 1400: train loss 1.2443, val loss 1.7204 [157.83116841316223 sec]
1.2809661626815796
Total Training Time: 162.52698969841003 seconds

stiled up the make us if the smile)
PyrranClan Clans to what as we Namal's perated up doirty sup
us.
BEGINNING (1681969037.2746258): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6765, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6747, val loss 4.6707 [8.808066606521606 sec]
step 100: train loss 2.5007, val loss 2.5769 [24.509039402008057 sec]
step 200: train loss 2.3892, val loss 2.4857 [40.20874333381653 sec]
step 300: train loss 2.1494, val loss 2.2899 [55.91204071044922 sec]
step 400: train loss 1.9293, val loss 2.1200 [71.62438988685608 sec]
step 500: train loss 1.7732, val loss 1.9931 [87.31911182403564 sec]
step 600: train loss 1.6530, val loss 1.9142 [103.03205299377441 sec]
step 700: train loss 1.5560, val loss 1.8432 [118.7487530708313 sec]
step 800: train loss 1.4841, val loss 1.7964 [134.47818756103516 sec]
step 900: train loss 1.4113, val loss 1.7725 [150.18066835403442 sec]
step 1000: train loss 1.3488, val loss 1.7493 [165.9202184677124 sec]
step 1100: train loss 1.2964, val loss 1.7285 [181.630197763443 sec]
step 1200: train loss 1.2370, val loss 1.7151 [197.35150623321533 sec]
step 1300: train loss 1.2016, val loss 1.7525 [213.0723397731781 sec]
step 1400: train loss 1.1431, val loss 1.7201 [228.7872176170349 sec]
1.1921950578689575
Total Training Time: 235.6878924369812 seconds

GES.
PAiin – TEARD TCORARIT
OF THE PRArphaps, enter and the guawnain crowded back into the
wagain, a
BEGINNING (1681969274.766898): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5616, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5759, val loss 4.5684 [2.417267322540283 sec]
step 100: train loss 2.4537, val loss 2.5371 [6.413451671600342 sec]
step 200: train loss 2.1421, val loss 2.2790 [10.402477025985718 sec]
step 300: train loss 1.9831, val loss 2.1576 [14.46759581565857 sec]
step 400: train loss 1.8743, val loss 2.0636 [18.529046773910522 sec]
step 500: train loss 1.8046, val loss 2.0006 [22.498311758041382 sec]
step 600: train loss 1.7350, val loss 1.9694 [26.469868898391724 sec]
step 700: train loss 1.6926, val loss 1.9337 [30.46257209777832 sec]
step 800: train loss 1.6453, val loss 1.9131 [34.44250011444092 sec]
step 900: train loss 1.6192, val loss 1.8967 [38.43212056159973 sec]
step 1000: train loss 1.5908, val loss 1.8898 [42.42413067817688 sec]
step 1100: train loss 1.5625, val loss 1.8594 [46.4505820274353 sec]
step 1200: train loss 1.5307, val loss 1.8391 [50.465057611465454 sec]
step 1300: train loss 1.5097, val loss 1.8280 [54.45644688606262 sec]
step 1400: train loss 1.4809, val loss 1.8091 [58.43458843231201 sec]
1.529693603515625
Total Training Time: 60.02413082122803 seconds

a. "Be them, and Anayah when was nexing tortie."
"Perhaps tuon's plealing," Arphad
Gratta anodded to
BEGINNING (1681969335.4060292): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.7014, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6494, val loss 4.6639 [3.972464084625244 sec]
step 100: train loss 2.4169, val loss 2.5185 [10.8617844581604 sec]
step 200: train loss 2.0889, val loss 2.2419 [17.827287912368774 sec]
step 300: train loss 1.9060, val loss 2.1012 [24.780632734298706 sec]
step 400: train loss 1.7879, val loss 1.9999 [31.659164428710938 sec]
step 500: train loss 1.7026, val loss 1.9317 [38.621432304382324 sec]
step 600: train loss 1.6380, val loss 1.8902 [45.536601305007935 sec]
step 700: train loss 1.5791, val loss 1.8658 [52.55299973487854 sec]
step 800: train loss 1.5329, val loss 1.8533 [59.4394805431366 sec]
step 900: train loss 1.4920, val loss 1.8145 [66.32370662689209 sec]
step 1000: train loss 1.4643, val loss 1.8117 [73.28316116333008 sec]
step 1100: train loss 1.4314, val loss 1.7942 [80.15867781639099 sec]
step 1200: train loss 1.4028, val loss 1.7870 [87.13947319984436 sec]
step 1300: train loss 1.3688, val loss 1.7916 [94.04537558555603 sec]
step 1400: train loss 1.3537, val loss 1.7837 [100.9635910987854 sec]
1.4194451570510864
Total Training Time: 103.94104361534119 seconds

sure."
"The many kilews?"
Gratta told slow that from the crriffiniwere aroundran
Zechain shalued. Gr
BEGINNING (1681969440.513245): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5298, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5280, val loss 4.5235 [5.554703235626221 sec]
step 100: train loss 2.3944, val loss 2.4879 [15.442257642745972 sec]
step 200: train loss 2.0768, val loss 2.2293 [25.316104412078857 sec]
step 300: train loss 1.8871, val loss 2.0948 [35.200453758239746 sec]
step 400: train loss 1.7517, val loss 1.9729 [45.14910650253296 sec]
step 500: train loss 1.6602, val loss 1.9081 [55.133387327194214 sec]
step 600: train loss 1.5928, val loss 1.8788 [65.03563094139099 sec]
step 700: train loss 1.5306, val loss 1.8491 [74.914235830307 sec]
step 800: train loss 1.4929, val loss 1.8137 [84.7742931842804 sec]
step 900: train loss 1.4480, val loss 1.8211 [94.63446593284607 sec]
step 1000: train loss 1.4043, val loss 1.7761 [104.5241379737854 sec]
step 1100: train loss 1.3829, val loss 1.7790 [114.49058628082275 sec]
step 1200: train loss 1.3541, val loss 1.7615 [124.35952425003052 sec]
step 1300: train loss 1.3268, val loss 1.7696 [134.25670909881592 sec]
step 1400: train loss 1.2877, val loss 1.7466 [144.09341597557068 sec]
1.472051739692688
Total Training Time: 148.455157995224 seconds

Perhan is." Arphades
had littler did snifly was if whics, vhic s the exames, ready porthing
out too 
BEGINNING (1681969590.7224374): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5065, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4990, val loss 4.4992 [3.0158579349517822 sec]
step 100: train loss 2.4971, val loss 2.5787 [8.053977489471436 sec]
step 200: train loss 2.2485, val loss 2.3638 [13.08546257019043 sec]
step 300: train loss 2.0228, val loss 2.1848 [18.113209009170532 sec]
step 400: train loss 1.8767, val loss 2.0735 [23.151227951049805 sec]
step 500: train loss 1.7727, val loss 1.9919 [28.194658279418945 sec]
step 600: train loss 1.6993, val loss 1.9456 [33.254292011260986 sec]
step 700: train loss 1.6419, val loss 1.8924 [38.290796756744385 sec]
step 800: train loss 1.5941, val loss 1.8714 [43.313177824020386 sec]
step 900: train loss 1.5482, val loss 1.8395 [48.3367702960968 sec]
step 1000: train loss 1.5115, val loss 1.8195 [53.35133457183838 sec]
step 1100: train loss 1.4707, val loss 1.7880 [58.37403178215027 sec]
step 1200: train loss 1.4394, val loss 1.7746 [63.39564514160156 sec]
step 1300: train loss 1.4065, val loss 1.7692 [68.41899728775024 sec]
step 1400: train loss 1.3756, val loss 1.7424 [73.44362282752991 sec]
1.490231990814209
Total Training Time: 75.4742341041565 seconds

the celled at we some the could clear. Them sill
be for them in to mity?" Gratta turned wetta lood f
BEGINNING (1681969666.8257341): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5863, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5736, val loss 4.5852 [5.1704792976379395 sec]
step 100: train loss 2.4646, val loss 2.5576 [14.141552448272705 sec]
step 200: train loss 2.1916, val loss 2.3163 [23.110361337661743 sec]
step 300: train loss 1.9554, val loss 2.1326 [32.042970180511475 sec]
step 400: train loss 1.7955, val loss 2.0190 [40.99002480506897 sec]
step 500: train loss 1.6735, val loss 1.9178 [49.94650459289551 sec]
step 600: train loss 1.5903, val loss 1.8859 [58.884023904800415 sec]
step 700: train loss 1.5299, val loss 1.8278 [67.85066270828247 sec]
step 800: train loss 1.4636, val loss 1.8060 [76.8082628250122 sec]
step 900: train loss 1.4100, val loss 1.7719 [85.77451181411743 sec]
step 1000: train loss 1.3606, val loss 1.7471 [94.74071264266968 sec]
step 1100: train loss 1.3225, val loss 1.7460 [103.91152238845825 sec]
step 1200: train loss 1.2909, val loss 1.7460 [112.89605188369751 sec]
step 1300: train loss 1.2491, val loss 1.7519 [121.82924103736877 sec]
step 1400: train loss 1.2187, val loss 1.7448 [130.81901001930237 sec]
1.2528388500213623
Total Training Time: 134.59801983833313 seconds

Priest, and they had spoke a bowed at he gaster fathere
bapped ackrier to ent. The ear hurrised. Pel
BEGINNING (1681969802.628607): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5676, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5878, val loss 4.5837 [7.335915565490723 sec]
step 100: train loss 2.4684, val loss 2.5466 [20.198217153549194 sec]
step 200: train loss 2.1831, val loss 2.3066 [33.0470826625824 sec]
step 300: train loss 1.9011, val loss 2.0808 [45.94842171669006 sec]
step 400: train loss 1.7456, val loss 1.9673 [58.852089166641235 sec]
step 500: train loss 1.6239, val loss 1.8822 [71.70016121864319 sec]
step 600: train loss 1.5381, val loss 1.8294 [84.56655859947205 sec]
step 700: train loss 1.4570, val loss 1.7805 [97.40981388092041 sec]
step 800: train loss 1.3941, val loss 1.7607 [110.26990962028503 sec]
step 900: train loss 1.3411, val loss 1.7477 [123.160888671875 sec]
step 1000: train loss 1.2902, val loss 1.7410 [136.03154182434082 sec]
step 1100: train loss 1.2466, val loss 1.7366 [148.89390563964844 sec]
step 1200: train loss 1.1950, val loss 1.7213 [161.74438428878784 sec]
step 1300: train loss 1.1544, val loss 1.7483 [174.63309788703918 sec]
step 1400: train loss 1.1114, val loss 1.7304 [187.5148344039917 sec]
1.1996766328811646
Total Training Time: 193.04959678649902 seconds

Gratta smiled already's. It wounced by tornes at the came
audd. How smasted know the tetured to cowl
BEGINNING (1681969997.4669826): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6005, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6181, val loss 4.6180 [4.488267660140991 sec]
step 100: train loss 2.5269, val loss 2.6061 [12.295953035354614 sec]
step 200: train loss 2.4114, val loss 2.5056 [20.105900764465332 sec]
step 300: train loss 2.2351, val loss 2.3464 [27.889026403427124 sec]
step 400: train loss 2.0420, val loss 2.1986 [35.656405210494995 sec]
step 500: train loss 1.9068, val loss 2.0929 [43.43936491012573 sec]
step 600: train loss 1.8009, val loss 2.0114 [51.219034910202026 sec]
step 700: train loss 1.7051, val loss 1.9311 [58.990309715270996 sec]
step 800: train loss 1.6353, val loss 1.8861 [66.76114964485168 sec]
step 900: train loss 1.5765, val loss 1.8547 [74.52875876426697 sec]
step 1000: train loss 1.5259, val loss 1.8390 [82.31049585342407 sec]
step 1100: train loss 1.4687, val loss 1.7922 [90.08587050437927 sec]
step 1200: train loss 1.4246, val loss 1.7666 [97.94322061538696 sec]
step 1300: train loss 1.3787, val loss 1.7416 [105.71973872184753 sec]
step 1400: train loss 1.3418, val loss 1.7325 [113.47947955131531 sec]
1.4059637784957886
Total Training Time: 116.75128769874573 seconds

sir!2514152-1111•21900 looked up." GHE did his Rost Roi IVATE
"Dista. ESPERAbbe was eyon's vablews."
BEGINNING (1681970114.8535638): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6262, val loss 4.6367 [8.081984281539917 sec]
step 100: train loss 2.4967, val loss 2.5770 [22.28403902053833 sec]
step 200: train loss 2.3611, val loss 2.4630 [36.50622630119324 sec]
step 300: train loss 2.1129, val loss 2.2492 [50.70857882499695 sec]
step 400: train loss 1.8954, val loss 2.0888 [64.9013307094574 sec]
step 500: train loss 1.7424, val loss 1.9717 [79.09239673614502 sec]
step 600: train loss 1.6360, val loss 1.8805 [93.28491997718811 sec]
step 700: train loss 1.5455, val loss 1.8390 [107.48793363571167 sec]
step 800: train loss 1.4655, val loss 1.7962 [121.72585654258728 sec]
step 900: train loss 1.3981, val loss 1.7542 [135.9353382587433 sec]
step 1000: train loss 1.3391, val loss 1.7512 [150.19103503227234 sec]
step 1100: train loss 1.2839, val loss 1.7296 [164.44000506401062 sec]
step 1200: train loss 1.2338, val loss 1.7146 [178.67900228500366 sec]
step 1300: train loss 1.1820, val loss 1.7238 [192.8837766647339 sec]
step 1400: train loss 1.1347, val loss 1.7223 [207.08061027526855 sec]
1.2548998594284058
Total Training Time: 213.22420263290405 seconds

smiled, "I am please Roi."
Namal's suhmed and seemed, "Why with
it boght a lours const our pilate to
BEGINNING (1681970329.3346016): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6222, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6254, val loss 4.6406 [11.73124623298645 sec]
step 100: train loss 2.4881, val loss 2.5654 [32.40962028503418 sec]
step 200: train loss 2.3454, val loss 2.4476 [53.02933645248413 sec]
step 300: train loss 2.0750, val loss 2.2209 [73.74159121513367 sec]
step 400: train loss 1.8472, val loss 2.0500 [94.39743113517761 sec]
step 500: train loss 1.6833, val loss 1.9156 [115.03554058074951 sec]
step 600: train loss 1.5690, val loss 1.8677 [135.6912064552307 sec]
step 700: train loss 1.4719, val loss 1.7931 [156.3222599029541 sec]
step 800: train loss 1.3878, val loss 1.7676 [176.96006226539612 sec]
step 900: train loss 1.3146, val loss 1.7238 [197.63419461250305 sec]
step 1000: train loss 1.2501, val loss 1.7160 [218.2691524028778 sec]
step 1100: train loss 1.1864, val loss 1.7116 [238.92596435546875 sec]
step 1200: train loss 1.1241, val loss 1.7142 [259.6075141429901 sec]
step 1300: train loss 1.0749, val loss 1.7435 [280.2575612068176 sec]
step 1400: train loss 1.0051, val loss 1.7252 [300.89264392852783 sec]
1.1202045679092407
Total Training Time: 309.89495968818665 seconds

Keannd city to head them, he saw too crastain.
There mwords runnitned backs, barought the task attet
BEGINNING (1681970641.049696): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6078, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6119, val loss 4.6045 [2.7683722972869873 sec]
step 100: train loss 2.4229, val loss 2.5089 [7.30237603187561 sec]
step 200: train loss 2.1158, val loss 2.2490 [11.827064990997314 sec]
step 300: train loss 1.9545, val loss 2.1417 [16.348090887069702 sec]
step 400: train loss 1.8389, val loss 2.0372 [20.869959592819214 sec]
step 500: train loss 1.7530, val loss 1.9692 [25.42177152633667 sec]
step 600: train loss 1.6991, val loss 1.9239 [29.97980785369873 sec]
step 700: train loss 1.6499, val loss 1.9167 [34.54589605331421 sec]
step 800: train loss 1.6070, val loss 1.8860 [39.07390785217285 sec]
step 900: train loss 1.5751, val loss 1.8642 [43.58752655982971 sec]
step 1000: train loss 1.5417, val loss 1.8531 [48.11461877822876 sec]
step 1100: train loss 1.5176, val loss 1.8304 [52.65650415420532 sec]
step 1200: train loss 1.4822, val loss 1.8205 [57.20814871788025 sec]
step 1300: train loss 1.4511, val loss 1.8050 [61.73786783218384 sec]
step 1400: train loss 1.4383, val loss 1.8023 [66.26967477798462 sec]
1.509695053100586
Total Training Time: 68.04864120483398 seconds

and have looked in fear furt will be the
cords; mallong ove
is halnor liver, "Aniside. Boryict into 
BEGINNING (1681970709.723358): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5916, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6065, val loss 4.6145 [4.591374635696411 sec]
step 100: train loss 2.3924, val loss 2.4814 [12.412496089935303 sec]
step 200: train loss 2.0428, val loss 2.1985 [20.229026079177856 sec]
step 300: train loss 1.8434, val loss 2.0365 [28.05934453010559 sec]
step 400: train loss 1.7347, val loss 1.9532 [35.8890962600708 sec]
step 500: train loss 1.6454, val loss 1.8969 [43.72745203971863 sec]
step 600: train loss 1.5823, val loss 1.8609 [51.539353132247925 sec]
step 700: train loss 1.5404, val loss 1.8448 [59.420146226882935 sec]
step 800: train loss 1.4854, val loss 1.8063 [67.23168873786926 sec]
step 900: train loss 1.4522, val loss 1.7964 [75.05275678634644 sec]
step 1000: train loss 1.4081, val loss 1.7734 [82.860360622406 sec]
step 1100: train loss 1.3826, val loss 1.7597 [90.68885159492493 sec]
step 1200: train loss 1.3604, val loss 1.7706 [98.55294299125671 sec]
step 1300: train loss 1.3295, val loss 1.7510 [106.37764501571655 sec]
step 1400: train loss 1.2989, val loss 1.7578 [114.24169397354126 sec]
1.3847286701202393
Total Training Time: 117.46330404281616 seconds

me different. "That will more out? The
eyes in desceplia and yetered Gor.
It week his hend head. Gra
BEGINNING (1681970828.3933132): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5761, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5931, val loss 4.6005 [6.402980089187622 sec]
step 100: train loss 2.3568, val loss 2.4599 [17.54564905166626 sec]
step 200: train loss 2.0105, val loss 2.1801 [28.672199010849 sec]
step 300: train loss 1.8278, val loss 2.0319 [39.831775426864624 sec]
step 400: train loss 1.7054, val loss 1.9539 [51.00521492958069 sec]
step 500: train loss 1.6117, val loss 1.8775 [62.14321327209473 sec]
step 600: train loss 1.5426, val loss 1.8594 [73.24787664413452 sec]
step 700: train loss 1.4827, val loss 1.8057 [84.37635374069214 sec]
step 800: train loss 1.4354, val loss 1.8084 [95.50191450119019 sec]
step 900: train loss 1.3867, val loss 1.7903 [106.64930033683777 sec]
step 1000: train loss 1.3527, val loss 1.7750 [117.79483771324158 sec]
step 1100: train loss 1.3174, val loss 1.7661 [128.9314968585968 sec]
step 1200: train loss 1.2839, val loss 1.7657 [140.05240607261658 sec]
step 1300: train loss 1.2534, val loss 1.7556 [151.21925449371338 sec]
step 1400: train loss 1.2248, val loss 1.7598 [162.39212250709534 sec]
1.3092491626739502
Total Training Time: 167.18964767456055 seconds

DKAN McKAY
High PEACORD
36
SEAN McKaY
Anayah montinught. The matter a maeuw camp."
Anayah asked the 
BEGINNING (1681970997.3039632): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6059, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6110, val loss 4.6095 [3.592533826828003 sec]
step 100: train loss 2.4898, val loss 2.5640 [9.659592390060425 sec]
step 200: train loss 2.2201, val loss 2.3389 [15.732026100158691 sec]
step 300: train loss 1.9761, val loss 2.1454 [21.795580863952637 sec]
step 400: train loss 1.8298, val loss 2.0289 [27.852897882461548 sec]
step 500: train loss 1.7430, val loss 1.9626 [33.913119077682495 sec]
step 600: train loss 1.6680, val loss 1.9058 [39.963123083114624 sec]
step 700: train loss 1.6029, val loss 1.8644 [46.01797151565552 sec]
step 800: train loss 1.5448, val loss 1.8198 [52.08247947692871 sec]
step 900: train loss 1.5017, val loss 1.8126 [58.153650999069214 sec]
step 1000: train loss 1.4572, val loss 1.7876 [64.21915793418884 sec]
step 1100: train loss 1.4179, val loss 1.7641 [70.28611207008362 sec]
step 1200: train loss 1.3883, val loss 1.7585 [76.37379670143127 sec]
step 1300: train loss 1.3559, val loss 1.7521 [82.43319296836853 sec]
step 1400: train loss 1.3329, val loss 1.7562 [88.49282622337341 sec]
1.4314240217208862
Total Training Time: 90.98989081382751 seconds

what the to get me, but of went perish, and the stonse.
Gratta of the comme." Gratta poing he's pros
BEGINNING (1681971088.938643): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5915, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5838, val loss 4.5924 [6.171079158782959 sec]
step 100: train loss 2.4520, val loss 2.5396 [16.88933801651001 sec]
step 200: train loss 2.1461, val loss 2.2768 [27.600708961486816 sec]
step 300: train loss 1.8934, val loss 2.0766 [38.299545764923096 sec]
step 400: train loss 1.7377, val loss 1.9749 [49.37885761260986 sec]
step 500: train loss 1.6208, val loss 1.8860 [60.05774784088135 sec]
step 600: train loss 1.5441, val loss 1.8339 [70.67758655548096 sec]
step 700: train loss 1.4714, val loss 1.7988 [81.31212139129639 sec]
step 800: train loss 1.4080, val loss 1.7584 [91.99079585075378 sec]
step 900: train loss 1.3628, val loss 1.7553 [102.8169937133789 sec]
step 1000: train loss 1.3118, val loss 1.7453 [113.6255612373352 sec]
step 1100: train loss 1.2674, val loss 1.7314 [124.2989649772644 sec]
step 1200: train loss 1.2274, val loss 1.7394 [134.97708940505981 sec]
step 1300: train loss 1.1928, val loss 1.7448 [145.63468527793884 sec]
step 1400: train loss 1.1487, val loss 1.7399 [156.26785016059875 sec]
1.242793083190918
Total Training Time: 160.81192016601562 seconds

knot betraying the cold the otYes, we judge
barrelly a tuon. His fur trude, and he faiting they help
BEGINNING (1681971250.9006832): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6514, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6329, val loss 4.6295 [8.717459440231323 sec]
step 100: train loss 2.4411, val loss 2.5261 [24.081946849822998 sec]
step 200: train loss 2.1192, val loss 2.2656 [39.385411739349365 sec]
step 300: train loss 1.8392, val loss 2.0470 [54.69487285614014 sec]
step 400: train loss 1.6751, val loss 1.9275 [69.9714081287384 sec]
step 500: train loss 1.5561, val loss 1.8398 [85.34018325805664 sec]
step 600: train loss 1.4602, val loss 1.7965 [100.63421750068665 sec]
step 700: train loss 1.3836, val loss 1.7635 [115.93323254585266 sec]
step 800: train loss 1.3242, val loss 1.7425 [131.2318844795227 sec]
step 900: train loss 1.2648, val loss 1.7381 [146.5163974761963 sec]
step 1000: train loss 1.2095, val loss 1.7326 [161.89135766029358 sec]
step 1100: train loss 1.1617, val loss 1.7249 [177.187579870224 sec]
step 1200: train loss 1.1189, val loss 1.7526 [192.4850561618805 sec]
step 1300: train loss 1.0688, val loss 1.7586 [207.86231660842896 sec]
step 1400: train loss 1.0284, val loss 1.7761 [223.22833108901978 sec]
1.1846572160720825
Total Training Time: 229.83353805541992 seconds

al with a come. Arphad leash them entrance, and yelled mo."
Anayah had turned to chest and savervalm
BEGINNING (1681971482.4567235): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5505, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5464, val loss 4.5499 [5.486719608306885 sec]
step 100: train loss 2.5135, val loss 2.5970 [14.944339513778687 sec]
step 200: train loss 2.3960, val loss 2.4896 [24.377642154693604 sec]
step 300: train loss 2.2181, val loss 2.3371 [33.83009362220764 sec]
step 400: train loss 1.9978, val loss 2.1666 [43.27924418449402 sec]
step 500: train loss 1.8563, val loss 2.0608 [52.80038332939148 sec]
step 600: train loss 1.7416, val loss 1.9655 [62.307613372802734 sec]
step 700: train loss 1.6504, val loss 1.9065 [71.86544919013977 sec]
step 800: train loss 1.5794, val loss 1.8619 [81.33558559417725 sec]
step 900: train loss 1.5209, val loss 1.8168 [90.78791213035583 sec]
step 1000: train loss 1.4639, val loss 1.7876 [100.23607206344604 sec]
step 1100: train loss 1.4134, val loss 1.7610 [109.71327638626099 sec]
step 1200: train loss 1.3672, val loss 1.7460 [119.25370073318481 sec]
step 1300: train loss 1.3253, val loss 1.7341 [128.7099277973175 sec]
step 1400: train loss 1.2895, val loss 1.7132 [138.15751218795776 sec]
1.3705881834030151
Total Training Time: 142.12135815620422 seconds

so. Gratta were to se finesd.
"Ye need to fend, the maeuw not sinice of him."
Gratta toor battomente
BEGINNING (1681971625.1822343): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6076, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6009, val loss 4.5969 [9.84387993812561 sec]
step 100: train loss 2.4928, val loss 2.5654 [27.02894878387451 sec]
step 200: train loss 2.3429, val loss 2.4474 [44.180659770965576 sec]
step 300: train loss 2.0600, val loss 2.2116 [61.28381681442261 sec]
step 400: train loss 1.8442, val loss 2.0444 [78.39749932289124 sec]
step 500: train loss 1.6961, val loss 1.9422 [95.52069687843323 sec]
step 600: train loss 1.5733, val loss 1.8561 [112.67966222763062 sec]
step 700: train loss 1.4847, val loss 1.7974 [129.7677013874054 sec]
step 800: train loss 1.4044, val loss 1.7583 [146.86956405639648 sec]
step 900: train loss 1.3352, val loss 1.7287 [164.0138328075409 sec]
step 1000: train loss 1.2721, val loss 1.7110 [181.20458960533142 sec]
step 1100: train loss 1.2138, val loss 1.7119 [198.3021640777588 sec]
step 1200: train loss 1.1538, val loss 1.6994 [215.4093291759491 sec]
step 1300: train loss 1.1123, val loss 1.7160 [232.50482869148254 sec]
step 1400: train loss 1.0587, val loss 1.7124 [249.6729826927185 sec]
1.1576045751571655
Total Training Time: 256.96190667152405 seconds

experation
and again. "Can you in!" Chief Gratta turned in a poured
proation. Anayah was as said
as 
BEGINNING (1681971883.3610795): Baseline LR(0.0003) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5983, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6021, val loss 4.5949 [14.193912744522095 sec]
step 100: train loss 2.4796, val loss 2.5609 [39.00706958770752 sec]
step 200: train loss 2.2940, val loss 2.4023 [63.8085253238678 sec]
step 300: train loss 1.9715, val loss 2.1537 [88.6362419128418 sec]
step 400: train loss 1.7601, val loss 1.9901 [113.4608588218689 sec]
step 500: train loss 1.6022, val loss 1.8661 [138.2363224029541 sec]
step 600: train loss 1.4928, val loss 1.8149 [162.7096824645996 sec]
step 700: train loss 1.3936, val loss 1.7631 [187.43459701538086 sec]
step 800: train loss 1.3090, val loss 1.7333 [212.03616857528687 sec]
step 900: train loss 1.2369, val loss 1.7189 [236.82506728172302 sec]
step 1000: train loss 1.1664, val loss 1.7319 [261.65672636032104 sec]
step 1100: train loss 1.1026, val loss 1.7367 [286.4272668361664 sec]
step 1200: train loss 1.0312, val loss 1.7475 [311.17064142227173 sec]
step 1300: train loss 0.9674, val loss 1.7586 [335.69685673713684 sec]
step 1400: train loss 0.9058, val loss 1.7917 [360.36211133003235 sec]
1.0015398263931274
Total Training Time: 370.97826981544495 seconds

stompodabes quickly heart. Juckles and for King Anayah.
65
CHAPTE ROF I – THE JUDGE THE PETERATE ent
BEGINNING (1681972256.0501187): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6034, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5974, val loss 4.5932 [2.9598164558410645 sec]
step 100: train loss 2.4382, val loss 2.5273 [7.655530214309692 sec]
step 200: train loss 2.1338, val loss 2.2702 [12.18704628944397 sec]
step 300: train loss 1.9726, val loss 2.1350 [16.72395372390747 sec]
step 400: train loss 1.8673, val loss 2.0573 [21.254294395446777 sec]
step 500: train loss 1.7974, val loss 1.9919 [25.78124761581421 sec]
step 600: train loss 1.7394, val loss 1.9714 [30.35173726081848 sec]
step 700: train loss 1.6805, val loss 1.9351 [34.88523030281067 sec]
step 800: train loss 1.6478, val loss 1.9200 [39.429078817367554 sec]
step 900: train loss 1.6061, val loss 1.9013 [43.95206952095032 sec]
step 1000: train loss 1.5745, val loss 1.8769 [48.49398612976074 sec]
step 1100: train loss 1.5448, val loss 1.8674 [53.05821466445923 sec]
step 1200: train loss 1.5195, val loss 1.8434 [57.63586115837097 sec]
step 1300: train loss 1.4930, val loss 1.8338 [62.17367434501648 sec]
step 1400: train loss 1.4748, val loss 1.8290 [66.70658254623413 sec]
1.44948148727417
Total Training Time: 68.805415391922 seconds

28 McKAGAY as Perishest the taskled. "Yes."
Chief Yah Elyon!" Yah Elyon is face worsh and protings.

BEGINNING (1681972325.636643): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6056, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6305, val loss 4.6358 [4.649990558624268 sec]
step 100: train loss 2.4242, val loss 2.5065 [12.82165813446045 sec]
step 200: train loss 2.0985, val loss 2.2547 [20.98190951347351 sec]
step 300: train loss 1.9289, val loss 2.1282 [29.212481260299683 sec]
step 400: train loss 1.8050, val loss 2.0157 [37.52527046203613 sec]
step 500: train loss 1.7273, val loss 1.9666 [45.64353322982788 sec]
step 600: train loss 1.6537, val loss 1.9307 [53.81294083595276 sec]
step 700: train loss 1.5948, val loss 1.8767 [62.118051052093506 sec]
step 800: train loss 1.5517, val loss 1.8506 [70.79612112045288 sec]
step 900: train loss 1.5229, val loss 1.8240 [79.01041841506958 sec]
step 1000: train loss 1.4722, val loss 1.8115 [87.35291647911072 sec]
step 1100: train loss 1.4561, val loss 1.8149 [95.51864862442017 sec]
step 1200: train loss 1.4254, val loss 1.8043 [103.74015188217163 sec]
step 1300: train loss 1.3977, val loss 1.7887 [111.9895429611206 sec]
step 1400: train loss 1.3722, val loss 1.7688 [120.19584631919861 sec]
1.3717972040176392
Total Training Time: 123.88757610321045 seconds

to this table campose. They on DESPlanners of den
and wetMing
and thered the to imatered heard?"
"Bu
BEGINNING (1681972450.9383414): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5962, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5897, val loss 4.5876 [6.6224329471588135 sec]
step 100: train loss 2.4283, val loss 2.5213 [18.6071138381958 sec]
step 200: train loss 2.1095, val loss 2.2422 [30.47614049911499 sec]
step 300: train loss 1.9091, val loss 2.1071 [42.40704154968262 sec]
step 400: train loss 1.7781, val loss 1.9910 [54.35479378700256 sec]
step 500: train loss 1.6941, val loss 1.9359 [66.29135060310364 sec]
step 600: train loss 1.6233, val loss 1.8772 [78.68584728240967 sec]
step 700: train loss 1.5699, val loss 1.8564 [90.6027421951294 sec]
step 800: train loss 1.5228, val loss 1.8445 [102.62841629981995 sec]
step 900: train loss 1.4814, val loss 1.8172 [114.60634899139404 sec]
step 1000: train loss 1.4466, val loss 1.8178 [126.51800155639648 sec]
step 1100: train loss 1.3987, val loss 1.7879 [138.6015293598175 sec]
step 1200: train loss 1.3760, val loss 1.7794 [150.71010065078735 sec]
step 1300: train loss 1.3401, val loss 1.8011 [162.86362719535828 sec]
step 1400: train loss 1.3287, val loss 1.7716 [174.81138348579407 sec]
1.3477805852890015
Total Training Time: 180.3417935371399 seconds

once face. Torial for a land resect blits if That maeuws.
Gratta lood and Arphad's part ovey that
ha
BEGINNING (1681972633.4417796): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5849, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5778, val loss 4.5706 [3.17594051361084 sec]
step 100: train loss 2.4784, val loss 2.5766 [8.750924825668335 sec]
step 200: train loss 2.2037, val loss 2.3295 [14.332236528396606 sec]
step 300: train loss 1.9910, val loss 2.1691 [19.906814336776733 sec]
step 400: train loss 1.8654, val loss 2.0612 [25.671027660369873 sec]
step 500: train loss 1.7621, val loss 1.9820 [31.23387336730957 sec]
step 600: train loss 1.6776, val loss 1.9271 [36.80540370941162 sec]
step 700: train loss 1.6113, val loss 1.8777 [42.40928292274475 sec]
step 800: train loss 1.5658, val loss 1.8670 [47.991615772247314 sec]
step 900: train loss 1.5112, val loss 1.8285 [53.557860374450684 sec]
step 1000: train loss 1.4685, val loss 1.8034 [59.14254450798035 sec]
step 1100: train loss 1.4321, val loss 1.7971 [64.70253729820251 sec]
step 1200: train loss 1.3956, val loss 1.7985 [70.25931715965271 sec]
step 1300: train loss 1.3657, val loss 1.7882 [75.82782316207886 sec]
step 1400: train loss 1.3418, val loss 1.7701 [81.82555747032166 sec]
1.3685238361358643
Total Training Time: 84.38357591629028 seconds

as with you came my."
Kel's
song. Chief ever. "I last is unnish, on your aws mightaised
sicued we pi
BEGINNING (1681972719.0422552): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6216, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6108, val loss 4.6198 [5.88817024230957 sec]
step 100: train loss 2.4633, val loss 2.5445 [17.000949144363403 sec]
step 200: train loss 2.1913, val loss 2.3159 [27.441865921020508 sec]
step 300: train loss 1.9392, val loss 2.1134 [37.97151803970337 sec]
step 400: train loss 1.7955, val loss 2.0024 [48.461708545684814 sec]
step 500: train loss 1.6667, val loss 1.9198 [58.80639863014221 sec]
step 600: train loss 1.5762, val loss 1.8578 [69.19646382331848 sec]
step 700: train loss 1.5196, val loss 1.8357 [79.5841977596283 sec]
step 800: train loss 1.4566, val loss 1.8015 [89.94422364234924 sec]
step 900: train loss 1.3995, val loss 1.7479 [100.31344819068909 sec]
step 1000: train loss 1.3545, val loss 1.7799 [110.67229509353638 sec]
step 1100: train loss 1.3219, val loss 1.7569 [120.99990749359131 sec]
step 1200: train loss 1.2764, val loss 1.7686 [131.33148050308228 sec]
step 1300: train loss 1.2322, val loss 1.7732 [141.76241183280945 sec]
step 1400: train loss 1.2002, val loss 1.7511 [152.14214754104614 sec]
1.2443170547485352
Total Training Time: 156.76790380477905 seconds

this ago, usief off we can't large our ame.
Gratta spored to a hundred. His turned woundy had
besard
BEGINNING (1681972877.397323): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5835, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5813, val loss 4.5772 [8.275629997253418 sec]
step 100: train loss 2.4555, val loss 2.5460 [23.387255907058716 sec]
step 200: train loss 2.1855, val loss 2.3312 [38.5189483165741 sec]
step 300: train loss 1.9027, val loss 2.0849 [53.66022062301636 sec]
step 400: train loss 1.7205, val loss 1.9566 [68.74582314491272 sec]
step 500: train loss 1.6108, val loss 1.8943 [83.82507038116455 sec]
step 600: train loss 1.5311, val loss 1.8291 [98.95619344711304 sec]
step 700: train loss 1.4586, val loss 1.7908 [114.0948417186737 sec]
step 800: train loss 1.3852, val loss 1.7809 [129.21913313865662 sec]
step 900: train loss 1.3423, val loss 1.7644 [144.300847530365 sec]
step 1000: train loss 1.2868, val loss 1.7481 [159.38113260269165 sec]
step 1100: train loss 1.2424, val loss 1.7565 [174.45522665977478 sec]
step 1200: train loss 1.2035, val loss 1.7507 [189.59163212776184 sec]
step 1300: train loss 1.1476, val loss 1.7525 [204.65232610702515 sec]
step 1400: train loss 1.1191, val loss 1.7459 [219.76431035995483 sec]
1.2049697637557983
Total Training Time: 226.56559538841248 seconds

st watclection you as welcommands?"
"Yes! Even nose Thase is accept able weaked
32
CHAPTER II
THE
gi
BEGINNING (1681973106.2369115): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5896, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5937, val loss 4.5866 [4.820964813232422 sec]
step 100: train loss 2.4987, val loss 2.5818 [13.342846870422363 sec]
step 200: train loss 2.3669, val loss 2.4612 [21.839892864227295 sec]
step 300: train loss 2.1878, val loss 2.3137 [30.357991218566895 sec]
step 400: train loss 1.9860, val loss 2.1621 [38.88108706474304 sec]
step 500: train loss 1.8405, val loss 2.0470 [47.45262861251831 sec]
step 600: train loss 1.7306, val loss 1.9769 [55.95197892189026 sec]
step 700: train loss 1.6464, val loss 1.9174 [64.50131869316101 sec]
step 800: train loss 1.5820, val loss 1.8609 [73.00603103637695 sec]
step 900: train loss 1.5174, val loss 1.8141 [81.53725218772888 sec]
step 1000: train loss 1.4645, val loss 1.7887 [90.057377576828 sec]
step 1100: train loss 1.4269, val loss 1.7719 [98.60568380355835 sec]
step 1200: train loss 1.3774, val loss 1.7628 [107.100670337677 sec]
step 1300: train loss 1.3385, val loss 1.7404 [115.5893623828888 sec]
step 1400: train loss 1.2963, val loss 1.7304 [124.08970022201538 sec]
1.3617265224456787
Total Training Time: 127.76807689666748 seconds

"Honce. Will saveyes went aliste word need th trear to
somple." Taka's he noddered and slows undies.
BEGINNING (1681973234.811039): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6296, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6290, val loss 4.6306 [8.944643259048462 sec]
step 100: train loss 2.4805, val loss 2.5682 [24.975132942199707 sec]
step 200: train loss 2.3466, val loss 2.4513 [40.94815945625305 sec]
step 300: train loss 2.0569, val loss 2.2182 [56.93730688095093 sec]
step 400: train loss 1.8521, val loss 2.0607 [72.9479501247406 sec]
step 500: train loss 1.7001, val loss 1.9348 [88.87269711494446 sec]
step 600: train loss 1.6007, val loss 1.8677 [104.82761859893799 sec]
step 700: train loss 1.4960, val loss 1.8112 [120.79616665840149 sec]
step 800: train loss 1.4221, val loss 1.7910 [136.75537395477295 sec]
step 900: train loss 1.3588, val loss 1.7669 [152.72063159942627 sec]
step 1000: train loss 1.2935, val loss 1.7395 [168.63787722587585 sec]
step 1100: train loss 1.2343, val loss 1.7267 [184.61560440063477 sec]
step 1200: train loss 1.1937, val loss 1.7328 [200.57901906967163 sec]
step 1300: train loss 1.1335, val loss 1.7327 [216.61301517486572 sec]
step 1400: train loss 1.0819, val loss 1.7494 [232.6727397441864 sec]
1.191749095916748
Total Training Time: 239.75140976905823 seconds

echapt to the scame him. I assiss you rest as you
and swert ittatched."
The Taka nodded and said. He
BEGINNING (1681973476.135376): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5337, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5320, val loss 4.5337 [12.964120388031006 sec]
step 100: train loss 2.4928, val loss 2.5807 [36.41856622695923 sec]
step 200: train loss 2.3533, val loss 2.4586 [59.8619499206543 sec]
step 300: train loss 2.0764, val loss 2.2219 [83.16657257080078 sec]
step 400: train loss 1.8291, val loss 2.0405 [106.58113074302673 sec]
step 500: train loss 1.6745, val loss 1.9438 [129.97844982147217 sec]
step 600: train loss 1.5481, val loss 1.8591 [153.40735387802124 sec]
step 700: train loss 1.4429, val loss 1.8047 [176.87910556793213 sec]
step 800: train loss 1.3609, val loss 1.7589 [200.41582942008972 sec]
step 900: train loss 1.2832, val loss 1.7346 [223.52981805801392 sec]
step 1000: train loss 1.2193, val loss 1.7604 [246.74228835105896 sec]
step 1100: train loss 1.1487, val loss 1.7381 [269.8210530281067 sec]
step 1200: train loss 1.0866, val loss 1.7448 [292.88747215270996 sec]
step 1300: train loss 1.0269, val loss 1.7771 [315.98749923706055 sec]
step 1400: train loss 0.9599, val loss 1.7835 [339.06841921806335 sec]
1.077757477760315
Total Training Time: 349.31934928894043 seconds

unded a mint ground and sneezed he said, "I valley our
won't too." Namal pared as this nodded."
He n
BEGINNING (1681973827.7431982): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5756, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5655, val loss 4.5650 [3.0435240268707275 sec]
step 100: train loss 2.3853, val loss 2.4871 [8.239936351776123 sec]
step 200: train loss 2.0589, val loss 2.2195 [13.445254802703857 sec]
step 300: train loss 1.9003, val loss 2.0854 [18.694413423538208 sec]
step 400: train loss 1.7955, val loss 1.9852 [23.892202854156494 sec]
step 500: train loss 1.7182, val loss 1.9351 [29.05240273475647 sec]
step 600: train loss 1.6533, val loss 1.9035 [34.22007727622986 sec]
step 700: train loss 1.6056, val loss 1.8973 [39.36992168426514 sec]
step 800: train loss 1.5740, val loss 1.8559 [44.545815229415894 sec]
step 900: train loss 1.5408, val loss 1.8511 [49.732240438461304 sec]
step 1000: train loss 1.5043, val loss 1.8470 [54.89675188064575 sec]
step 1100: train loss 1.4697, val loss 1.8182 [60.046560764312744 sec]
step 1200: train loss 1.4490, val loss 1.8073 [65.20366644859314 sec]
step 1300: train loss 1.4257, val loss 1.7985 [70.37472748756409 sec]
step 1400: train loss 1.4035, val loss 1.7949 [75.53110027313232 sec]
1.3989955186843872
Total Training Time: 77.68553829193115 seconds

His Pearana was lust all."
Gratta daysed to the fur taill scors camp,
Gratta
and the dircore, the an
BEGINNING (1681973906.197106): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5869, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5669, val loss 4.5630 [5.2955756187438965 sec]
step 100: train loss 2.3666, val loss 2.4589 [14.610734701156616 sec]
step 200: train loss 2.0209, val loss 2.1898 [23.905746936798096 sec]
step 300: train loss 1.8358, val loss 2.0355 [33.226433515548706 sec]
step 400: train loss 1.7062, val loss 1.9483 [42.57477641105652 sec]
step 500: train loss 1.6353, val loss 1.9061 [51.9383327960968 sec]
step 600: train loss 1.5588, val loss 1.8557 [61.21153783798218 sec]
step 700: train loss 1.5099, val loss 1.8192 [70.52984046936035 sec]
step 800: train loss 1.4669, val loss 1.8177 [79.83169722557068 sec]
step 900: train loss 1.4242, val loss 1.7980 [89.13321185112 sec]
step 1000: train loss 1.3944, val loss 1.8057 [98.41045808792114 sec]
step 1100: train loss 1.3438, val loss 1.7699 [107.68725490570068 sec]
step 1200: train loss 1.3243, val loss 1.7729 [116.96833109855652 sec]
step 1300: train loss 1.2942, val loss 1.7897 [126.25105905532837 sec]
step 1400: train loss 1.2700, val loss 1.7710 [135.5540008544922 sec]
1.3447855710983276
Total Training Time: 139.68282270431519 seconds

may childers, some for this tent.
"Why a treathered how that thires, we have at one excepts in helpe
BEGINNING (1681974047.352107): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6380, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6269, val loss 4.6423 [7.430692672729492 sec]
step 100: train loss 2.3656, val loss 2.4585 [20.878807067871094 sec]
step 200: train loss 2.0120, val loss 2.1781 [34.282283544540405 sec]
step 300: train loss 1.8241, val loss 2.0243 [47.70818328857422 sec]
step 400: train loss 1.6921, val loss 1.9441 [61.15805506706238 sec]
step 500: train loss 1.5977, val loss 1.8780 [74.62213230133057 sec]
step 600: train loss 1.5420, val loss 1.8360 [88.0498595237732 sec]
step 700: train loss 1.4791, val loss 1.8238 [101.48202991485596 sec]
step 800: train loss 1.4274, val loss 1.7750 [114.92387390136719 sec]
step 900: train loss 1.3909, val loss 1.7856 [128.42246747016907 sec]
step 1000: train loss 1.3510, val loss 1.7797 [141.93414640426636 sec]
step 1100: train loss 1.3096, val loss 1.7601 [155.3724946975708 sec]
step 1200: train loss 1.2817, val loss 1.7769 [168.81034302711487 sec]
step 1300: train loss 1.2617, val loss 1.8020 [182.2397801876068 sec]
step 1400: train loss 1.2142, val loss 1.7878 [195.73911023139954 sec]
1.336878776550293
Total Training Time: 201.7758400440216 seconds

legst maeuw the trade ose." Anayah was pleased. If do not peake mert the not could be tent."
"Ar!" A
BEGINNING (1681974251.317815): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6676, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6732, val loss 4.6746 [4.03882908821106 sec]
step 100: train loss 2.4505, val loss 2.5404 [10.991609334945679 sec]
step 200: train loss 2.1329, val loss 2.2777 [17.96466040611267 sec]
step 300: train loss 1.9242, val loss 2.1120 [24.915616273880005 sec]
step 400: train loss 1.7932, val loss 2.0104 [31.88164234161377 sec]
step 500: train loss 1.6890, val loss 1.9296 [38.839274168014526 sec]
step 600: train loss 1.6132, val loss 1.8642 [45.80959153175354 sec]
step 700: train loss 1.5491, val loss 1.8453 [52.78687119483948 sec]
step 800: train loss 1.4899, val loss 1.8105 [59.758636474609375 sec]
step 900: train loss 1.4460, val loss 1.7848 [66.723876953125 sec]
step 1000: train loss 1.4076, val loss 1.7751 [73.67795300483704 sec]
step 1100: train loss 1.3680, val loss 1.7807 [80.6452465057373 sec]
step 1200: train loss 1.3248, val loss 1.7518 [87.62141871452332 sec]
step 1300: train loss 1.2939, val loss 1.7547 [94.57224130630493 sec]
step 1400: train loss 1.2656, val loss 1.7618 [101.53361940383911 sec]
1.341112732887268
Total Training Time: 104.4607093334198 seconds

you."
"Some you shall. I We maeuw is midn't he smande to the
harte
bunly returned, "as Taka's asknew
BEGINNING (1681974356.5357392): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5810, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5789, val loss 4.5855 [7.276987791061401 sec]
step 100: train loss 2.4320, val loss 2.5209 [20.108649253845215 sec]
step 200: train loss 2.0688, val loss 2.2206 [32.9308660030365 sec]
step 300: train loss 1.8203, val loss 2.0385 [45.74956941604614 sec]
step 400: train loss 1.6727, val loss 1.9093 [58.584251403808594 sec]
step 500: train loss 1.5671, val loss 1.8590 [71.41240000724792 sec]
step 600: train loss 1.4847, val loss 1.8133 [84.24208974838257 sec]
step 700: train loss 1.4178, val loss 1.7573 [97.07274007797241 sec]
step 800: train loss 1.3567, val loss 1.7553 [109.90327739715576 sec]
step 900: train loss 1.3029, val loss 1.7523 [122.72912454605103 sec]
step 1000: train loss 1.2535, val loss 1.7437 [135.5536584854126 sec]
step 1100: train loss 1.2128, val loss 1.7668 [148.39608216285706 sec]
step 1200: train loss 1.1609, val loss 1.7410 [161.21500492095947 sec]
step 1300: train loss 1.1191, val loss 1.7482 [174.04028630256653 sec]
step 1400: train loss 1.0821, val loss 1.7669 [186.8748025894165 sec]
1.1340738534927368
Total Training Time: 192.4435510635376 seconds

towed again.
4
CHAPTER III – SEEKING PEACE
thim. Senders has prond only foul find littly, they High

BEGINNING (1681974550.4966245): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6407, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6423, val loss 4.6552 [10.480613946914673 sec]
step 100: train loss 2.4358, val loss 2.5158 [29.21402072906494 sec]
step 200: train loss 2.0937, val loss 2.2460 [47.943166971206665 sec]
step 300: train loss 1.8192, val loss 2.0250 [66.66250658035278 sec]
step 400: train loss 1.6489, val loss 1.9126 [85.3877604007721 sec]
step 500: train loss 1.5280, val loss 1.8271 [104.10903763771057 sec]
step 600: train loss 1.4373, val loss 1.7936 [122.83296418190002 sec]
step 700: train loss 1.3661, val loss 1.7629 [141.53873252868652 sec]
step 800: train loss 1.3035, val loss 1.7514 [160.26796698570251 sec]
step 900: train loss 1.2350, val loss 1.7563 [178.97950077056885 sec]
step 1000: train loss 1.1871, val loss 1.7323 [197.70655250549316 sec]
step 1100: train loss 1.1251, val loss 1.7704 [216.4220073223114 sec]
step 1200: train loss 1.0770, val loss 1.7524 [235.23478388786316 sec]
step 1300: train loss 1.0230, val loss 1.7838 [254.33829760551453 sec]
step 1400: train loss 0.9727, val loss 1.8040 [273.59136176109314 sec]
1.1261438131332397
Total Training Time: 282.1011321544647 seconds

efend each of your Pranothers," said as Grattacked as
perful a maeuw delightle. Forch met
me torch l
BEGINNING (1681974835.3032987): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6196, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6257, val loss 4.6156 [6.458799362182617 sec]
step 100: train loss 2.4863, val loss 2.5653 [17.79719352722168 sec]
step 200: train loss 2.3256, val loss 2.4336 [29.141381978988647 sec]
step 300: train loss 2.0794, val loss 2.2345 [40.164225578308105 sec]
step 400: train loss 1.8685, val loss 2.0747 [51.197818756103516 sec]
step 500: train loss 1.7369, val loss 1.9627 [62.59912157058716 sec]
step 600: train loss 1.6451, val loss 1.9072 [74.0718104839325 sec]
step 700: train loss 1.5514, val loss 1.8340 [85.50390481948853 sec]
step 800: train loss 1.4881, val loss 1.8114 [96.89553236961365 sec]
step 900: train loss 1.4223, val loss 1.7812 [108.28463411331177 sec]
step 1000: train loss 1.3664, val loss 1.7620 [119.67931985855103 sec]
step 1100: train loss 1.3133, val loss 1.7489 [131.09581756591797 sec]
step 1200: train loss 1.2703, val loss 1.7367 [142.48207545280457 sec]
step 1300: train loss 1.2236, val loss 1.7196 [153.87372589111328 sec]
step 1400: train loss 1.1919, val loss 1.7280 [165.2369658946991 sec]
1.2113920450210571
Total Training Time: 170.15929579734802 seconds

Yah that they sent sy and worke bance. I had now
milia smell that themself your dised to be the maue
BEGINNING (1681975006.389913): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5824, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5784, val loss 4.5781 [11.867593050003052 sec]
step 100: train loss 2.4724, val loss 2.5536 [33.030484676361084 sec]
step 200: train loss 2.2848, val loss 2.3953 [54.13576149940491 sec]
step 300: train loss 1.9858, val loss 2.1583 [75.21655583381653 sec]
step 400: train loss 1.7772, val loss 2.0066 [96.33556842803955 sec]
step 500: train loss 1.6354, val loss 1.8961 [117.45462536811829 sec]
step 600: train loss 1.5127, val loss 1.8247 [138.5875804424286 sec]
step 700: train loss 1.4214, val loss 1.7942 [159.3708679676056 sec]
step 800: train loss 1.3426, val loss 1.7552 [179.89112257957458 sec]
step 900: train loss 1.2592, val loss 1.7436 [200.4099407196045 sec]
step 1000: train loss 1.1973, val loss 1.7316 [220.82023978233337 sec]
step 1100: train loss 1.1290, val loss 1.7259 [241.3497850894928 sec]
step 1200: train loss 1.0754, val loss 1.7470 [261.88061237335205 sec]
step 1300: train loss 1.0100, val loss 1.7482 [282.32785177230835 sec]
step 1400: train loss 0.9414, val loss 1.7773 [302.8489692211151 sec]
1.0414081811904907
Total Training Time: 311.81081557273865 seconds

wah the battle deling, waited as loared to Gratta, and the
loomed. "Thing are, but we shooi waters w
BEGINNING (1681975319.7446802): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6361, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6402, val loss 4.6326 [16.900031805038452 sec]
step 100: train loss 2.4673, val loss 2.5583 [45.60688519477844 sec]
step 200: train loss 2.2880, val loss 2.4032 [74.25611281394958 sec]
step 300: train loss 1.9409, val loss 2.1184 [103.3077335357666 sec]
step 400: train loss 1.7150, val loss 1.9503 [131.97353982925415 sec]
step 500: train loss 1.5618, val loss 1.8494 [160.65052270889282 sec]
step 600: train loss 1.4328, val loss 1.7933 [189.8840410709381 sec]
step 700: train loss 1.3305, val loss 1.7478 [218.8797631263733 sec]
step 800: train loss 1.2368, val loss 1.7269 [247.72031354904175 sec]
step 900: train loss 1.1622, val loss 1.7400 [276.5747334957123 sec]
step 1000: train loss 1.0762, val loss 1.7366 [305.3924973011017 sec]
step 1100: train loss 0.9939, val loss 1.7508 [333.9060814380646 sec]
step 1200: train loss 0.9081, val loss 1.7835 [362.94729828834534 sec]
step 1300: train loss 0.8298, val loss 1.8321 [391.7699496746063 sec]
step 1400: train loss 0.7487, val loss 1.8843 [420.80225133895874 sec]
0.8903126120567322
Total Training Time: 433.28774881362915 seconds

Veroosht going their far in the box, in on hoping a
that way seaked maget. Gratta again easted growl
BEGINNING (1681975755.3140707): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6145, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6175, val loss 4.6178 [3.4179773330688477 sec]
step 100: train loss 2.3441, val loss 2.4402 [9.209372520446777 sec]
step 200: train loss 2.0251, val loss 2.1943 [15.006611824035645 sec]
step 300: train loss 1.8626, val loss 2.0617 [20.776920080184937 sec]
step 400: train loss 1.7539, val loss 1.9754 [26.56984233856201 sec]
step 500: train loss 1.6820, val loss 1.9296 [32.361889123916626 sec]
step 600: train loss 1.6162, val loss 1.8848 [38.17531967163086 sec]
step 700: train loss 1.5608, val loss 1.8554 [43.96459078788757 sec]
step 800: train loss 1.5184, val loss 1.8384 [49.73468327522278 sec]
step 900: train loss 1.4876, val loss 1.8423 [55.51935052871704 sec]
step 1000: train loss 1.4562, val loss 1.8185 [61.289286375045776 sec]
step 1100: train loss 1.4215, val loss 1.8146 [67.0809907913208 sec]
step 1200: train loss 1.3882, val loss 1.8000 [72.91136574745178 sec]
step 1300: train loss 1.3652, val loss 1.7946 [78.69647598266602 sec]
step 1400: train loss 1.3454, val loss 1.7852 [84.45157670974731 sec]
1.4285972118377686
Total Training Time: 86.84260606765747 seconds

Vallites tablit – signad.
Gratta unched use as good. "Commadering on witen as
tooldier. Chief Gratta
BEGINNING (1681975842.9251535): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6253, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6018, val loss 4.5953 [5.821403980255127 sec]
step 100: train loss 2.3107, val loss 2.4063 [16.19132113456726 sec]
step 200: train loss 1.9561, val loss 2.1367 [26.528756380081177 sec]
step 300: train loss 1.7639, val loss 1.9911 [36.85095191001892 sec]
step 400: train loss 1.6663, val loss 1.9248 [47.19107151031494 sec]
step 500: train loss 1.5731, val loss 1.8666 [57.53928089141846 sec]
step 600: train loss 1.5100, val loss 1.8407 [67.85308337211609 sec]
step 700: train loss 1.4661, val loss 1.8126 [78.20871162414551 sec]
step 800: train loss 1.4117, val loss 1.8053 [88.53872728347778 sec]
step 900: train loss 1.3669, val loss 1.7819 [98.86702036857605 sec]
step 1000: train loss 1.3253, val loss 1.7809 [109.19747257232666 sec]
step 1100: train loss 1.2904, val loss 1.7708 [119.57098245620728 sec]
step 1200: train loss 1.2651, val loss 1.7789 [129.90869069099426 sec]
step 1300: train loss 1.2257, val loss 1.7841 [140.24319863319397 sec]
step 1400: train loss 1.1996, val loss 1.7871 [150.5848195552826 sec]
1.278483271598816
Total Training Time: 155.0892689228058 seconds

Gratta was nodded. He least cert like a me lattleg to the guard you."
Anayah smiled. The cramp crous
BEGINNING (1681975999.45085): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5616, val loss 4.5777 [8.317949533462524 sec]
step 100: train loss 2.3184, val loss 2.4217 [23.345719575881958 sec]
step 200: train loss 1.9401, val loss 2.1197 [38.33762311935425 sec]
step 300: train loss 1.7516, val loss 1.9855 [53.32924771308899 sec]
step 400: train loss 1.6323, val loss 1.8963 [68.29617428779602 sec]
step 500: train loss 1.5497, val loss 1.8357 [83.27942728996277 sec]
step 600: train loss 1.4692, val loss 1.8272 [98.31856346130371 sec]
step 700: train loss 1.4075, val loss 1.7877 [113.32440876960754 sec]
step 800: train loss 1.3605, val loss 1.7781 [128.2882523536682 sec]
step 900: train loss 1.3127, val loss 1.7838 [143.27410221099854 sec]
step 1000: train loss 1.2698, val loss 1.7668 [158.2891857624054 sec]
step 1100: train loss 1.2360, val loss 1.7700 [173.2919623851776 sec]
step 1200: train loss 1.1967, val loss 1.7822 [188.27223181724548 sec]
step 1300: train loss 1.1626, val loss 1.7911 [203.26611280441284 sec]
step 1400: train loss 1.1250, val loss 1.7904 [218.25554728507996 sec]
1.2254774570465088
Total Training Time: 224.9478838443756 seconds

General Aidden's adviced and master one
42
CHAPTERITE I – SEEKING WILLALILIGIN:
ISINIVIIMEN't MeAcKa
BEGINNING (1681976226.5663536): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6476, val loss 4.6507 [4.87479567527771 sec]
step 100: train loss 2.4285, val loss 2.5069 [13.33982515335083 sec]
step 200: train loss 2.0750, val loss 2.2170 [21.791589975357056 sec]
step 300: train loss 1.8658, val loss 2.0588 [30.256469249725342 sec]
step 400: train loss 1.7251, val loss 1.9527 [38.71661305427551 sec]
step 500: train loss 1.6331, val loss 1.9038 [47.18758821487427 sec]
step 600: train loss 1.5491, val loss 1.8441 [55.63237428665161 sec]
step 700: train loss 1.4844, val loss 1.8135 [64.10953688621521 sec]
step 800: train loss 1.4259, val loss 1.7732 [72.57241201400757 sec]
step 900: train loss 1.3802, val loss 1.7683 [81.04279065132141 sec]
step 1000: train loss 1.3470, val loss 1.7795 [89.50163626670837 sec]
step 1100: train loss 1.3038, val loss 1.7577 [97.98070096969604 sec]
step 1200: train loss 1.2658, val loss 1.7542 [106.46116614341736 sec]
step 1300: train loss 1.2303, val loss 1.7399 [114.92383766174316 sec]
step 1400: train loss 1.1934, val loss 1.7470 [123.40411281585693 sec]
1.2701489925384521
Total Training Time: 126.98724675178528 seconds

Allights. Shoulding looked up at the other reply. Is wed
migistationed the smill Thas we mefooninain
BEGINNING (1681976354.318253): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5675, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5734, val loss 4.5692 [8.762259483337402 sec]
step 100: train loss 2.4057, val loss 2.4980 [24.28847050666809 sec]
step 200: train loss 2.0237, val loss 2.1873 [39.830894947052 sec]
step 300: train loss 1.7957, val loss 2.0020 [55.364195346832275 sec]
step 400: train loss 1.6287, val loss 1.8724 [71.14737892150879 sec]
step 500: train loss 1.5192, val loss 1.8073 [86.9787986278534 sec]
step 600: train loss 1.4325, val loss 1.7816 [102.83374166488647 sec]
step 700: train loss 1.3560, val loss 1.7505 [118.67057466506958 sec]
step 800: train loss 1.2936, val loss 1.7449 [134.30580306053162 sec]
step 900: train loss 1.2366, val loss 1.7391 [149.82305264472961 sec]
step 1000: train loss 1.1798, val loss 1.7162 [165.34397721290588 sec]
step 1100: train loss 1.1335, val loss 1.7526 [180.8571982383728 sec]
step 1200: train loss 1.0832, val loss 1.7494 [196.38449001312256 sec]
step 1300: train loss 1.0319, val loss 1.8005 [211.91000699996948 sec]
step 1400: train loss 0.9807, val loss 1.7959 [227.4540343284607 sec]
1.0892874002456665
Total Training Time: 234.2218587398529 seconds

turned his take for the said about this hundanswered throug
sundied as the as horse, "Yes, even boug
BEGINNING (1681976590.0076547): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6256, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6245, val loss 4.6210 [12.669522523880005 sec]
step 100: train loss 2.4106, val loss 2.5020 [35.3236038684845 sec]
step 200: train loss 2.0274, val loss 2.1928 [57.968780279159546 sec]
step 300: train loss 1.7550, val loss 1.9788 [80.62062549591064 sec]
step 400: train loss 1.5924, val loss 1.8676 [103.29689073562622 sec]
step 500: train loss 1.4586, val loss 1.7963 [125.95033669471741 sec]
step 600: train loss 1.3733, val loss 1.7708 [148.61413192749023 sec]
step 700: train loss 1.2906, val loss 1.7638 [171.2789192199707 sec]
step 800: train loss 1.2232, val loss 1.7544 [193.9420838356018 sec]
step 900: train loss 1.1547, val loss 1.7526 [216.58952403068542 sec]
step 1000: train loss 1.0938, val loss 1.7710 [239.25749230384827 sec]
step 1100: train loss 1.0322, val loss 1.7669 [261.9295001029968 sec]
step 1200: train loss 0.9655, val loss 1.7938 [284.60556197166443 sec]
step 1300: train loss 0.9045, val loss 1.8435 [307.24679040908813 sec]
step 1400: train loss 0.8381, val loss 1.8554 [329.8952474594116 sec]
1.001777172088623
Total Training Time: 339.9123375415802 seconds

me, sir!" Gratta looked backed backet to
nightly had a bound on his
crage. Week is through the veil 
BEGINNING (1681976932.0503142): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6544, val loss 4.6446 [7.6941728591918945 sec]
step 100: train loss 2.4777, val loss 2.5591 [21.31998038291931 sec]
step 200: train loss 2.3126, val loss 2.4215 [34.91885161399841 sec]
step 300: train loss 2.0300, val loss 2.1859 [48.513758420944214 sec]
step 400: train loss 1.8295, val loss 2.0410 [62.135501861572266 sec]
step 500: train loss 1.6928, val loss 1.9283 [75.73943257331848 sec]
step 600: train loss 1.5906, val loss 1.8751 [89.34265542030334 sec]
step 700: train loss 1.4991, val loss 1.8130 [102.94722056388855 sec]
step 800: train loss 1.4250, val loss 1.7820 [116.54625821113586 sec]
step 900: train loss 1.3630, val loss 1.7568 [130.15912652015686 sec]
step 1000: train loss 1.3043, val loss 1.7522 [143.78154301643372 sec]
step 1100: train loss 1.2574, val loss 1.7159 [157.40230917930603 sec]
step 1200: train loss 1.2099, val loss 1.7308 [171.00276446342468 sec]
step 1300: train loss 1.1633, val loss 1.7247 [184.6100161075592 sec]
step 1400: train loss 1.1150, val loss 1.7335 [198.25070333480835 sec]
1.187072992324829
Total Training Time: 204.1574969291687 seconds

lurned at T6
Chapp? Shapps this troo!"
"Yes!" Artainu ther and we were the
drisking. We as the vand 
BEGINNING (1681977137.002204): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.7117, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7269, val loss 4.7191 [14.204032182693481 sec]
step 100: train loss 2.4664, val loss 2.5573 [38.34090518951416 sec]
step 200: train loss 2.2411, val loss 2.3542 [62.66191363334656 sec]
step 300: train loss 1.9180, val loss 2.1032 [86.62609696388245 sec]
step 400: train loss 1.7147, val loss 1.9417 [111.03331875801086 sec]
step 500: train loss 1.5672, val loss 1.8585 [135.45256972312927 sec]
step 600: train loss 1.4543, val loss 1.7976 [159.51708436012268 sec]
step 700: train loss 1.3508, val loss 1.7541 [183.4436535835266 sec]
step 800: train loss 1.2711, val loss 1.7412 [207.56093835830688 sec]
step 900: train loss 1.1881, val loss 1.7078 [231.5261254310608 sec]
step 1000: train loss 1.1211, val loss 1.7377 [255.95229816436768 sec]
step 1100: train loss 1.0464, val loss 1.7476 [280.0048403739929 sec]
step 1200: train loss 0.9719, val loss 1.7698 [304.35335540771484 sec]
step 1300: train loss 0.9077, val loss 1.7985 [328.62524700164795 sec]
step 1400: train loss 0.8248, val loss 1.8299 [352.5075318813324 sec]
0.928087055683136
Total Training Time: 362.72184109687805 seconds

"Whicking you have withs."
"Yes, then give are gone!" Lamek? They have
lopfired at feem againsussibl
BEGINNING (1681977501.2294395): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5956, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6045, val loss 4.6015 [20.481027126312256 sec]
step 100: train loss 2.4635, val loss 2.5478 [53.88193964958191 sec]
step 200: train loss 2.2743, val loss 2.3876 [87.40205192565918 sec]
step 300: train loss 1.9087, val loss 2.0934 [120.71775770187378 sec]
step 400: train loss 1.6693, val loss 1.9233 [154.19699120521545 sec]
step 500: train loss 1.5051, val loss 1.8157 [188.01729226112366 sec]
step 600: train loss 1.3767, val loss 1.7594 [221.53359866142273 sec]
step 700: train loss 1.2664, val loss 1.7420 [255.08726406097412 sec]
step 800: train loss 1.1702, val loss 1.7252 [288.6705310344696 sec]
step 900: train loss 1.0777, val loss 1.7342 [322.3179683685303 sec]
step 1000: train loss 0.9856, val loss 1.7884 [355.9222021102905 sec]
step 1100: train loss 0.8899, val loss 1.8143 [389.31570529937744 sec]
step 1200: train loss 0.8024, val loss 1.8657 [422.89972853660583 sec]
step 1300: train loss 0.7006, val loss 1.9414 [456.44214940071106 sec]
step 1400: train loss 0.6101, val loss 2.0052 [489.7800335884094 sec]
0.7542113065719604
Total Training Time: 504.14255595207214 seconds

Elyong Ri abanslave. If we want, we could fifly
may Yah Elyon!" He could smiled.
The tuons row soldi
BEGINNING (1681978007.6912618): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5966, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6070, val loss 4.5999 [3.922422409057617 sec]
step 100: train loss 2.3269, val loss 2.4262 [11.067355394363403 sec]
step 200: train loss 2.0674, val loss 2.2258 [18.168508052825928 sec]
step 300: train loss 1.8825, val loss 2.0785 [25.221519708633423 sec]
step 400: train loss 1.7783, val loss 1.9931 [32.27519106864929 sec]
step 500: train loss 1.6969, val loss 1.9271 [39.286855936050415 sec]
step 600: train loss 1.6362, val loss 1.9135 [46.33191895484924 sec]
step 700: train loss 1.5899, val loss 1.8758 [53.395005226135254 sec]
step 800: train loss 1.5521, val loss 1.8703 [60.65279269218445 sec]
step 900: train loss 1.5217, val loss 1.8528 [67.73311948776245 sec]
step 1000: train loss 1.4845, val loss 1.8243 [74.77777981758118 sec]
step 1100: train loss 1.4486, val loss 1.8387 [81.83414149284363 sec]
step 1200: train loss 1.4203, val loss 1.8232 [88.83458018302917 sec]
step 1300: train loss 1.4046, val loss 1.8158 [95.88244390487671 sec]
step 1400: train loss 1.3755, val loss 1.8314 [102.8720371723175 sec]
1.491278886795044
Total Training Time: 105.96448373794556 seconds

guards to know her, and we the stuon
fightiod."
Nowiah come oples, who this voice songst king with t
BEGINNING (1681978114.7909214): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.7087, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7108, val loss 4.7063 [7.240115165710449 sec]
step 100: train loss 2.3546, val loss 2.4497 [20.805321216583252 sec]
step 200: train loss 2.0083, val loss 2.1863 [34.04618978500366 sec]
step 300: train loss 1.8410, val loss 2.0556 [47.208784103393555 sec]
step 400: train loss 1.7271, val loss 1.9751 [60.419434547424316 sec]
step 500: train loss 1.6550, val loss 1.9202 [73.67799758911133 sec]
step 600: train loss 1.5734, val loss 1.8814 [87.21581053733826 sec]
step 700: train loss 1.5155, val loss 1.8402 [100.43524527549744 sec]
step 800: train loss 1.4729, val loss 1.8155 [113.63087153434753 sec]
step 900: train loss 1.4452, val loss 1.8195 [126.81063890457153 sec]
step 1000: train loss 1.4027, val loss 1.8242 [140.14911365509033 sec]
step 1100: train loss 1.3644, val loss 1.8167 [153.64621663093567 sec]
step 1200: train loss 1.3461, val loss 1.8055 [166.91088485717773 sec]
step 1300: train loss 1.2963, val loss 1.7936 [180.00389456748962 sec]
step 1400: train loss 1.2784, val loss 1.7945 [193.34560012817383 sec]
1.399212121963501
Total Training Time: 199.3926043510437 seconds

there what the tone a gare, nothing tradion warrith to rack, and the you puy, if you send the must. 
BEGINNING (1681978316.31343): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6173, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6282, val loss 4.6278 [10.485419273376465 sec]
step 100: train loss 2.3973, val loss 2.4879 [30.128870487213135 sec]
step 200: train loss 2.0534, val loss 2.2267 [49.487956285476685 sec]
step 300: train loss 1.8615, val loss 2.0746 [68.97935724258423 sec]
step 400: train loss 1.7317, val loss 1.9908 [88.6147289276123 sec]
step 500: train loss 1.6404, val loss 1.9228 [107.96987628936768 sec]
step 600: train loss 1.5755, val loss 1.8693 [127.49165153503418 sec]
step 700: train loss 1.5137, val loss 1.8486 [147.09434604644775 sec]
step 800: train loss 1.4704, val loss 1.8335 [166.54923391342163 sec]
step 900: train loss 1.4267, val loss 1.8029 [186.02518343925476 sec]
step 1000: train loss 1.3865, val loss 1.8022 [205.4820635318756 sec]
step 1100: train loss 1.3476, val loss 1.8102 [225.11629176139832 sec]
step 1200: train loss 1.3130, val loss 1.7816 [244.5491259098053 sec]
step 1300: train loss 1.2861, val loss 1.8041 [263.92562890052795 sec]
step 1400: train loss 1.2473, val loss 1.7870 [283.5299472808838 sec]
1.4332777261734009
Total Training Time: 292.4119520187378 seconds

Tilla troosed bestrung
thad walking muss?" O
"Yes, sIt will pain a life. General Beriyah to additenc
BEGINNING (1681978611.9684455): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6198, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5984, val loss 4.5995 [4.980089902877808 sec]
step 100: train loss 2.4206, val loss 2.5109 [14.213860750198364 sec]
step 200: train loss 2.0838, val loss 2.2258 [23.306224822998047 sec]
step 300: train loss 1.8726, val loss 2.0797 [32.412235736846924 sec]
step 400: train loss 1.7390, val loss 1.9802 [41.50847291946411 sec]
step 500: train loss 1.6278, val loss 1.8948 [50.707329750061035 sec]
step 600: train loss 1.5500, val loss 1.8498 [59.80665373802185 sec]
step 700: train loss 1.4898, val loss 1.8274 [68.95798325538635 sec]
step 800: train loss 1.4299, val loss 1.7863 [78.04525971412659 sec]
step 900: train loss 1.3844, val loss 1.7886 [87.14041662216187 sec]
step 1000: train loss 1.3428, val loss 1.7826 [96.24504542350769 sec]
step 1100: train loss 1.2974, val loss 1.7784 [105.33868098258972 sec]
step 1200: train loss 1.2604, val loss 1.7550 [114.48277759552002 sec]
step 1300: train loss 1.2205, val loss 1.7772 [123.80285620689392 sec]
step 1400: train loss 1.1874, val loss 1.7929 [132.89140391349792 sec]
1.2181555032730103
Total Training Time: 137.0209023952484 seconds

"What not day are
great to his wound raisince to specion and said an trace
over the dance,but sures 
BEGINNING (1681978750.1210144): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.5781, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5873, val loss 4.5776 [9.213511228561401 sec]
step 100: train loss 2.4258, val loss 2.5246 [26.415351152420044 sec]
step 200: train loss 2.0841, val loss 2.2382 [43.68982195854187 sec]
step 300: train loss 1.8179, val loss 2.0363 [60.91933560371399 sec]
step 400: train loss 1.6717, val loss 1.9391 [78.05647778511047 sec]
step 500: train loss 1.5608, val loss 1.8701 [95.27364182472229 sec]
step 600: train loss 1.4672, val loss 1.8278 [112.56638073921204 sec]
step 700: train loss 1.3976, val loss 1.8007 [129.76591205596924 sec]
step 800: train loss 1.3264, val loss 1.7710 [146.94727873802185 sec]
step 900: train loss 1.2895, val loss 1.7836 [164.17799425125122 sec]
step 1000: train loss 1.2220, val loss 1.7542 [181.47448801994324 sec]
step 1100: train loss 1.1740, val loss 1.7630 [198.68599343299866 sec]
step 1200: train loss 1.1209, val loss 1.7688 [215.84763836860657 sec]
step 1300: train loss 1.0830, val loss 1.8030 [233.07040786743164 sec]
step 1400: train loss 1.0307, val loss 1.8085 [250.32594203948975 sec]
1.1285655498504639
Total Training Time: 258.32820892333984 seconds

Chief Gratta copnest the Pyrran delegations had
wounded me, we must must be and don't surraveley day
BEGINNING (1681979010.6658192): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5852, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6019, val loss 4.5907 [13.496934175491333 sec]
step 100: train loss 2.4567, val loss 2.5525 [38.78320550918579 sec]
step 200: train loss 2.1099, val loss 2.2641 [64.21216988563538 sec]
step 300: train loss 1.8444, val loss 2.0717 [89.60741448402405 sec]
step 400: train loss 1.6689, val loss 1.9303 [114.87364101409912 sec]
step 500: train loss 1.5472, val loss 1.8540 [140.2881965637207 sec]
step 600: train loss 1.4504, val loss 1.8084 [165.62888717651367 sec]
step 700: train loss 1.3771, val loss 1.8020 [190.9700083732605 sec]
step 800: train loss 1.2996, val loss 1.7671 [216.27323055267334 sec]
step 900: train loss 1.2309, val loss 1.7742 [241.60953617095947 sec]
step 1000: train loss 1.1812, val loss 1.7867 [267.12498354911804 sec]
step 1100: train loss 1.1284, val loss 1.7809 [292.45069122314453 sec]
step 1200: train loss 1.0720, val loss 1.8042 [317.77563309669495 sec]
step 1300: train loss 1.0123, val loss 1.8272 [343.14664602279663 sec]
step 1400: train loss 0.9570, val loss 1.8483 [368.4731705188751 sec]
0.9888836741447449
Total Training Time: 380.4275608062744 seconds

 "veah a true them to dearly me ae slick of the cubs, but I
would will have they could
may of the ga
BEGINNING (1681979394.6810591): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5530, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5553, val loss 4.5594 [7.986248254776001 sec]
step 100: train loss 2.4738, val loss 2.5672 [22.285393238067627 sec]
step 200: train loss 2.2818, val loss 2.4079 [36.58165168762207 sec]
step 300: train loss 1.9866, val loss 2.1595 [50.84886813163757 sec]
step 400: train loss 1.7982, val loss 2.0299 [65.2172200679779 sec]
step 500: train loss 1.6582, val loss 1.9312 [79.51634454727173 sec]
step 600: train loss 1.5555, val loss 1.8746 [93.81659317016602 sec]
step 700: train loss 1.4624, val loss 1.8079 [108.09450936317444 sec]
step 800: train loss 1.3878, val loss 1.7858 [122.39393162727356 sec]
step 900: train loss 1.3250, val loss 1.7537 [136.70667004585266 sec]
step 1000: train loss 1.2703, val loss 1.7334 [151.0162513256073 sec]
step 1100: train loss 1.2169, val loss 1.7486 [165.2638976573944 sec]
step 1200: train loss 1.1631, val loss 1.7575 [179.56082153320312 sec]
step 1300: train loss 1.1047, val loss 1.7686 [193.88068509101868 sec]
step 1400: train loss 1.0573, val loss 1.7690 [208.16982889175415 sec]
1.172101378440857
Total Training Time: 214.4943516254425 seconds

from a midily refummeine coulde tries, he would sign coursembers
as you tour finithful behind Qurgik
BEGINNING (1681979610.3235743): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6273, val loss 4.6213 [15.148640632629395 sec]
step 100: train loss 2.4696, val loss 2.5521 [41.771329164505005 sec]
step 200: train loss 2.2885, val loss 2.4040 [68.1673834323883 sec]
step 300: train loss 1.9262, val loss 2.1196 [94.49274802207947 sec]
step 400: train loss 1.6930, val loss 1.9391 [120.45102047920227 sec]
step 500: train loss 1.5414, val loss 1.8490 [146.82908153533936 sec]
step 600: train loss 1.4434, val loss 1.8001 [172.9072015285492 sec]
step 700: train loss 1.3389, val loss 1.7625 [199.25325679779053 sec]
step 800: train loss 1.2575, val loss 1.7689 [225.73613929748535 sec]
step 900: train loss 1.1729, val loss 1.7376 [251.96993207931519 sec]
step 1000: train loss 1.1052, val loss 1.7713 [278.38180112838745 sec]
step 1100: train loss 1.0322, val loss 1.8105 [304.8619122505188 sec]
step 1200: train loss 0.9541, val loss 1.8397 [331.4372067451477 sec]
step 1300: train loss 0.8836, val loss 1.8389 [357.77313590049744 sec]
step 1400: train loss 0.8079, val loss 1.9013 [383.8384714126587 sec]
0.9365093111991882
Total Training Time: 395.53316259384155 seconds

eepies." Gratta looked an slavers. "I have best must
answe. She looked forwarour and cave start, cam
BEGINNING (1681980008.1926675): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5230, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5157, val loss 4.5206 [21.80629873275757 sec]
step 100: train loss 2.4856, val loss 2.5659 [57.861658573150635 sec]
step 200: train loss 2.3340, val loss 2.4457 [92.84635186195374 sec]
step 300: train loss 2.0106, val loss 2.1821 [127.52220106124878 sec]
step 400: train loss 1.7484, val loss 2.0000 [161.63210678100586 sec]
step 500: train loss 1.5625, val loss 1.8833 [195.77212858200073 sec]
step 600: train loss 1.4227, val loss 1.8026 [230.0626151561737 sec]
step 700: train loss 1.3094, val loss 1.7667 [264.5697844028473 sec]
step 800: train loss 1.2091, val loss 1.7601 [298.890020608902 sec]
step 900: train loss 1.1103, val loss 1.7749 [333.51624298095703 sec]
step 1000: train loss 1.0139, val loss 1.8171 [367.6778709888458 sec]
step 1100: train loss 0.9272, val loss 1.8199 [401.87290930747986 sec]
step 1200: train loss 0.8365, val loss 1.8546 [436.4078311920166 sec]
step 1300: train loss 0.7311, val loss 1.9321 [471.4149901866913 sec]
step 1400: train loss 0.6372, val loss 2.0245 [505.7970383167267 sec]
0.8240715265274048
Total Training Time: 521.4033575057983 seconds

All smiled, and said Tike on Torial in thas.
Pelana moved to the side and at him the ground at and
o
BEGINNING (1681980533.1552284): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6632, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6423, val loss 4.6294 [4.888674259185791 sec]
step 100: train loss 2.2622, val loss 2.3873 [12.932621002197266 sec]
step 200: train loss 1.9662, val loss 2.1466 [20.949700117111206 sec]
step 300: train loss 1.8030, val loss 2.0062 [28.971248865127563 sec]
step 400: train loss 1.6997, val loss 1.9532 [37.03435826301575 sec]
step 500: train loss 1.6371, val loss 1.8957 [45.05112409591675 sec]
step 600: train loss 1.5649, val loss 1.8798 [53.323707818984985 sec]
step 700: train loss 1.5040, val loss 1.8418 [61.45652651786804 sec]
step 800: train loss 1.4762, val loss 1.8200 [69.52861666679382 sec]
step 900: train loss 1.4461, val loss 1.8289 [77.58144855499268 sec]
step 1000: train loss 1.4018, val loss 1.8137 [85.62617993354797 sec]
step 1100: train loss 1.3730, val loss 1.8211 [93.66953444480896 sec]
step 1200: train loss 1.3478, val loss 1.7984 [101.70611524581909 sec]
step 1300: train loss 1.3148, val loss 1.7921 [109.72754669189453 sec]
step 1400: train loss 1.2897, val loss 1.8133 [117.98692560195923 sec]
1.4281501770019531
Total Training Time: 121.69216346740723 seconds

cart to his head. The horses woodded the
that hidnight the High Prie was kight'e besapped ox
fear do
BEGINNING (1681980655.9603515): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5235, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5446, val loss 4.5413 [8.09177565574646 sec]
step 100: train loss 2.2981, val loss 2.4020 [23.10856032371521 sec]
step 200: train loss 1.9315, val loss 2.1093 [38.076674699783325 sec]
step 300: train loss 1.7562, val loss 1.9878 [53.037009477615356 sec]
step 400: train loss 1.6336, val loss 1.9030 [68.2554018497467 sec]
step 500: train loss 1.5562, val loss 1.8559 [83.21068525314331 sec]
step 600: train loss 1.4724, val loss 1.8215 [98.15805125236511 sec]
step 700: train loss 1.4343, val loss 1.8093 [113.10073566436768 sec]
step 800: train loss 1.3804, val loss 1.8121 [128.26882767677307 sec]
step 900: train loss 1.3323, val loss 1.7892 [143.24350094795227 sec]
step 1000: train loss 1.3049, val loss 1.8058 [158.17628145217896 sec]
step 1100: train loss 1.2615, val loss 1.8113 [173.19741225242615 sec]
step 1200: train loss 1.2302, val loss 1.8035 [188.29120016098022 sec]
step 1300: train loss 1.1976, val loss 1.7941 [203.25141310691833 sec]
step 1400: train loss 1.1610, val loss 1.8253 [218.20072960853577 sec]
1.2191169261932373
Total Training Time: 225.10895347595215 seconds

much in ten quart could passession with follower on ways before ither. Chief that we undring
only th
BEGINNING (1681980883.2183223): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6910, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6864, val loss 4.6952 [11.692642211914062 sec]
step 100: train loss 2.3319, val loss 2.4419 [33.80660796165466 sec]
step 200: train loss 1.9776, val loss 2.1687 [55.723308086395264 sec]
step 300: train loss 1.7719, val loss 2.0228 [77.73399376869202 sec]
step 400: train loss 1.6431, val loss 1.9154 [99.85099506378174 sec]
step 500: train loss 1.5611, val loss 1.8834 [121.7715253829956 sec]
step 600: train loss 1.4742, val loss 1.8303 [143.69962763786316 sec]
step 700: train loss 1.4167, val loss 1.8241 [165.8072760105133 sec]
step 800: train loss 1.3590, val loss 1.8003 [187.70579266548157 sec]
step 900: train loss 1.3145, val loss 1.7875 [209.68428373336792 sec]
step 1000: train loss 1.2741, val loss 1.7953 [231.77164268493652 sec]
step 1100: train loss 1.2323, val loss 1.7998 [253.73434352874756 sec]
step 1200: train loss 1.1936, val loss 1.8204 [275.68741941452026 sec]
step 1300: train loss 1.1575, val loss 1.8141 [297.7100863456726 sec]
step 1400: train loss 1.1365, val loss 1.8309 [319.8222017288208 sec]
1.1894053220748901
Total Training Time: 330.09907484054565 seconds

tuon tripwing were growlied again.
They know have can't would cade to
did know my vigillage, as we w
BEGINNING (1681981216.5601482): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.5985, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5934, val loss 4.5926 [6.467639923095703 sec]
step 100: train loss 2.3718, val loss 2.4737 [18.089447736740112 sec]
step 200: train loss 1.9994, val loss 2.1733 [29.737683057785034 sec]
step 300: train loss 1.7649, val loss 1.9957 [41.364285707473755 sec]
step 400: train loss 1.6473, val loss 1.8992 [52.999985694885254 sec]
step 500: train loss 1.5451, val loss 1.8427 [64.6200122833252 sec]
step 600: train loss 1.4657, val loss 1.8004 [76.22081637382507 sec]
step 700: train loss 1.4033, val loss 1.7802 [87.8106427192688 sec]
step 800: train loss 1.3429, val loss 1.7812 [99.48593044281006 sec]
step 900: train loss 1.2841, val loss 1.7784 [111.11356806755066 sec]
step 1000: train loss 1.2429, val loss 1.7701 [122.72671437263489 sec]
step 1100: train loss 1.1846, val loss 1.7468 [134.3166103363037 sec]
step 1200: train loss 1.1552, val loss 1.7746 [145.93026208877563 sec]
step 1300: train loss 1.1086, val loss 1.8125 [157.62482523918152 sec]
step 1400: train loss 1.0629, val loss 1.8024 [169.24850130081177 sec]
1.1552878618240356
Total Training Time: 174.36172699928284 seconds

the triding on held to desplattencome was when allow mored
retaion – he low the fur troop that, when
BEGINNING (1681981392.019563): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6908, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6799, val loss 4.6820 [12.103266954421997 sec]
step 100: train loss 2.3975, val loss 2.4995 [34.098122358322144 sec]
step 200: train loss 1.9909, val loss 2.1682 [56.098740100860596 sec]
step 300: train loss 1.7312, val loss 1.9747 [78.0438323020935 sec]
step 400: train loss 1.5794, val loss 1.8669 [100.0319709777832 sec]
step 500: train loss 1.4696, val loss 1.8168 [122.09174871444702 sec]
step 600: train loss 1.3683, val loss 1.7807 [144.01723861694336 sec]
step 700: train loss 1.2920, val loss 1.7597 [165.98393177986145 sec]
step 800: train loss 1.2322, val loss 1.7747 [187.96394038200378 sec]
step 900: train loss 1.1545, val loss 1.7708 [209.88980269432068 sec]
step 1000: train loss 1.0913, val loss 1.7630 [231.86652255058289 sec]
step 1100: train loss 1.0355, val loss 1.8054 [253.81799864768982 sec]
step 1200: train loss 0.9728, val loss 1.8327 [275.74854159355164 sec]
step 1300: train loss 0.9130, val loss 1.8378 [297.74108839035034 sec]
step 1400: train loss 0.8512, val loss 1.8863 [319.6989803314209 sec]
0.9748969674110413
Total Training Time: 329.6081917285919 seconds

anything that from?" Gratta asked his stood up.
78
CHAPTER IV – A NEW EEKING PEACE ACCORD
sure NT. O
BEGINNING (1681981723.8128474): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5470, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5451, val loss 4.5422 [17.663225173950195 sec]
step 100: train loss 2.4475, val loss 2.5345 [48.58485174179077 sec]
step 200: train loss 2.0364, val loss 2.2036 [79.73845410346985 sec]
step 300: train loss 1.7611, val loss 1.9949 [110.62314057350159 sec]
step 400: train loss 1.6015, val loss 1.8975 [141.42647337913513 sec]
step 500: train loss 1.4716, val loss 1.8332 [172.15462493896484 sec]
step 600: train loss 1.3571, val loss 1.7849 [203.5085005760193 sec]
step 700: train loss 1.2711, val loss 1.7644 [234.3625738620758 sec]
step 800: train loss 1.1989, val loss 1.7704 [266.00955414772034 sec]
step 900: train loss 1.1183, val loss 1.7748 [297.0903422832489 sec]
step 1000: train loss 1.0407, val loss 1.7790 [328.0803289413452 sec]
step 1100: train loss 0.9788, val loss 1.8267 [358.7725315093994 sec]
step 1200: train loss 0.9073, val loss 1.8365 [389.7368392944336 sec]
step 1300: train loss 0.8321, val loss 1.8856 [420.37921690940857 sec]
step 1400: train loss 0.7679, val loss 1.9388 [451.54005694389343 sec]
0.892886221408844
Total Training Time: 465.2131881713867 seconds

bow a little of his heart back to warry and the dungeon
nation for his paw. He had not seen his even
BEGINNING (1681982192.2580578): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5744, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5719, val loss 4.5743 [10.605296850204468 sec]
step 100: train loss 2.4473, val loss 2.5457 [28.773075819015503 sec]
step 200: train loss 2.2149, val loss 2.3512 [47.23613715171814 sec]
step 300: train loss 1.9079, val loss 2.0989 [65.4856185913086 sec]
step 400: train loss 1.7099, val loss 1.9508 [83.5338499546051 sec]
step 500: train loss 1.5738, val loss 1.8657 [101.80594420433044 sec]
step 600: train loss 1.4675, val loss 1.8116 [120.01266956329346 sec]
step 700: train loss 1.3735, val loss 1.7731 [138.00980162620544 sec]
step 800: train loss 1.2936, val loss 1.7532 [156.28411149978638 sec]
step 900: train loss 1.2200, val loss 1.7336 [174.33548069000244 sec]
step 1000: train loss 1.1535, val loss 1.7485 [192.5129246711731 sec]
step 1100: train loss 1.0837, val loss 1.7507 [210.56724095344543 sec]
step 1200: train loss 1.0202, val loss 1.7645 [228.6151487827301 sec]
step 1300: train loss 0.9531, val loss 1.7960 [246.73443341255188 sec]
step 1400: train loss 0.8886, val loss 1.8405 [265.006071805954 sec]
0.9937648177146912
Total Training Time: 273.0187382698059 seconds

this darty. Arphad, in were easonable. G
21
SNamal ran bowe me. Men tray part sudvitional more.
"No,
BEGINNING (1681982466.399759): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5770, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5717, val loss 4.5695 [19.69460940361023 sec]
step 100: train loss 2.4540, val loss 2.5476 [50.5093936920166 sec]
step 200: train loss 2.1972, val loss 2.3304 [80.77064824104309 sec]
step 300: train loss 1.8269, val loss 2.0385 [110.94319272041321 sec]
step 400: train loss 1.6067, val loss 1.8821 [141.15764665603638 sec]
step 500: train loss 1.4620, val loss 1.8120 [171.49451541900635 sec]
step 600: train loss 1.3370, val loss 1.7658 [201.72852849960327 sec]
step 700: train loss 1.2394, val loss 1.7561 [232.1672818660736 sec]
step 800: train loss 1.1356, val loss 1.7580 [262.71151518821716 sec]
step 900: train loss 1.0462, val loss 1.7851 [293.23651123046875 sec]
step 1000: train loss 0.9481, val loss 1.8017 [323.62802481651306 sec]
step 1100: train loss 0.8559, val loss 1.8550 [353.95364260673523 sec]
step 1200: train loss 0.7570, val loss 1.9146 [384.37480759620667 sec]
step 1300: train loss 0.6645, val loss 1.9556 [414.88321256637573 sec]
step 1400: train loss 0.5720, val loss 2.0670 [445.84581685066223 sec]
0.6969892978668213
Total Training Time: 459.8671772480011 seconds

Tosircle meetion. Mong Pilisite Verooshta through the side.
This youngers should chief the camp to b
BEGINNING (1681982928.5846574): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5614, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5587, val loss 4.5496 [27.103668451309204 sec]
step 100: train loss 2.4824, val loss 2.5640 [70.74982118606567 sec]
step 200: train loss 2.2749, val loss 2.3948 [114.05309295654297 sec]
step 300: train loss 1.8682, val loss 2.0747 [157.452481508255 sec]
step 400: train loss 1.6060, val loss 1.8775 [200.92307209968567 sec]
step 500: train loss 1.4352, val loss 1.8116 [244.33424258232117 sec]
step 600: train loss 1.2953, val loss 1.7478 [287.74436354637146 sec]
step 700: train loss 1.1740, val loss 1.7607 [331.19975423812866 sec]
step 800: train loss 1.0583, val loss 1.7708 [374.68208169937134 sec]
step 900: train loss 0.9350, val loss 1.8000 [418.15572237968445 sec]
step 1000: train loss 0.8292, val loss 1.8631 [461.7041292190552 sec]
step 1100: train loss 0.7073, val loss 1.9556 [505.31059288978577 sec]
step 1200: train loss 0.5908, val loss 2.0405 [548.8279895782471 sec]
step 1300: train loss 0.4856, val loss 2.1899 [592.3627734184265 sec]
step 1400: train loss 0.3883, val loss 2.2663 [635.9243414402008 sec]
0.555836021900177
Total Training Time: 656.198573589325 seconds

68
CHAPTER IV – A NEW ENEMY
moned up a sharve. In probably from Yah Elyon hundred
of King Anayah and
BEGINNING (1681983588.3402078): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6533, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6384, val loss 4.6409 [5.1339027881622314 sec]
step 100: train loss 2.2225, val loss 2.3474 [14.343250036239624 sec]
step 200: train loss 1.9175, val loss 2.1047 [23.524121522903442 sec]
step 300: train loss 1.7503, val loss 1.9887 [32.73430013656616 sec]
step 400: train loss 1.6432, val loss 1.8878 [41.91762638092041 sec]
step 500: train loss 1.5716, val loss 1.8685 [51.093950271606445 sec]
step 600: train loss 1.5123, val loss 1.8421 [60.295011043548584 sec]
step 700: train loss 1.4592, val loss 1.8132 [69.49986004829407 sec]
step 800: train loss 1.4125, val loss 1.8069 [78.69314694404602 sec]
step 900: train loss 1.3784, val loss 1.8028 [87.87120842933655 sec]
step 1000: train loss 1.3378, val loss 1.8010 [97.06496500968933 sec]
step 1100: train loss 1.3069, val loss 1.8123 [106.25413656234741 sec]
step 1200: train loss 1.2696, val loss 1.7901 [115.4361515045166 sec]
step 1300: train loss 1.2372, val loss 1.7848 [124.62336492538452 sec]
step 1400: train loss 1.2160, val loss 1.7948 [133.7995719909668 sec]
1.2614195346832275
Total Training Time: 137.87950253486633 seconds

51
SEAN McKAY
"The ciby us! What difference cust 15 maeuws!"
Lamek for them waerforch. The mauews of
BEGINNING (1681983727.3258097): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6072, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5920, val loss 4.5989 [9.326192140579224 sec]
step 100: train loss 2.2532, val loss 2.3779 [26.565850734710693 sec]
step 200: train loss 1.8898, val loss 2.0828 [43.79680418968201 sec]
step 300: train loss 1.7042, val loss 1.9497 [61.04930520057678 sec]
step 400: train loss 1.5792, val loss 1.8652 [78.26844620704651 sec]
step 500: train loss 1.4949, val loss 1.8236 [95.50892043113708 sec]
step 600: train loss 1.4282, val loss 1.8047 [112.75946831703186 sec]
step 700: train loss 1.3689, val loss 1.7920 [129.98420405387878 sec]
step 800: train loss 1.3312, val loss 1.8046 [147.22617959976196 sec]
step 900: train loss 1.2847, val loss 1.8068 [164.50592136383057 sec]
step 1000: train loss 1.2335, val loss 1.7848 [181.79425263404846 sec]
step 1100: train loss 1.1974, val loss 1.7925 [199.01947021484375 sec]
step 1200: train loss 1.1532, val loss 1.7974 [216.24441242218018 sec]
step 1300: train loss 1.1248, val loss 1.8145 [233.80089688301086 sec]
step 1400: train loss 1.0806, val loss 1.8170 [252.2407066822052 sec]
1.139607310295105
Total Training Time: 260.1532828807831 seconds

ledgmer scroll me a hindressed and breew siler friended of this learried and
grounderst what may has
BEGINNING (1681983989.663195): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6427, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6538, val loss 4.6457 [13.58164358139038 sec]
step 100: train loss 2.2892, val loss 2.3976 [38.68443512916565 sec]
step 200: train loss 1.9078, val loss 2.1078 [63.801530599594116 sec]
step 300: train loss 1.7027, val loss 1.9676 [88.91091179847717 sec]
step 400: train loss 1.5727, val loss 1.8624 [114.01032495498657 sec]
step 500: train loss 1.4714, val loss 1.8224 [139.05944728851318 sec]
step 600: train loss 1.4063, val loss 1.7977 [164.19263100624084 sec]
step 700: train loss 1.3346, val loss 1.7936 [189.2302610874176 sec]
step 800: train loss 1.2790, val loss 1.8070 [214.38245749473572 sec]
step 900: train loss 1.2351, val loss 1.7952 [239.50972867012024 sec]
step 1000: train loss 1.1839, val loss 1.7955 [264.6061038970947 sec]
step 1100: train loss 1.1397, val loss 1.8239 [289.63554763793945 sec]
step 1200: train loss 1.0936, val loss 1.8171 [314.7072331905365 sec]
step 1300: train loss 1.0565, val loss 1.8105 [339.81150007247925 sec]
step 1400: train loss 1.0227, val loss 1.8384 [364.8771903514862 sec]
1.0690650939941406
Total Training Time: 376.4865882396698 seconds

22
CHAPTER V – A DESPERATE Gratta and conting, Gratta could he reaching of the scent downs. As
ven t
BEGINNING (1681984369.4087405): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5473, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5369, val loss 4.5252 [8.01144528388977 sec]
step 100: train loss 2.3514, val loss 2.4563 [22.324330806732178 sec]
step 200: train loss 1.9424, val loss 2.1283 [36.62596082687378 sec]
step 300: train loss 1.7390, val loss 1.9753 [50.94476342201233 sec]
step 400: train loss 1.6029, val loss 1.8816 [65.2540910243988 sec]
step 500: train loss 1.5008, val loss 1.8237 [79.56630063056946 sec]
step 600: train loss 1.4198, val loss 1.7895 [93.86814498901367 sec]
step 700: train loss 1.3507, val loss 1.7687 [108.19855284690857 sec]
step 800: train loss 1.2954, val loss 1.7518 [122.49184823036194 sec]
step 900: train loss 1.2333, val loss 1.7609 [136.8440225124359 sec]
step 1000: train loss 1.1796, val loss 1.7581 [151.17608785629272 sec]
step 1100: train loss 1.1293, val loss 1.7717 [165.50099349021912 sec]
step 1200: train loss 1.0778, val loss 1.7674 [179.7909369468689 sec]
step 1300: train loss 1.0267, val loss 1.8092 [194.12724542617798 sec]
step 1400: train loss 0.9776, val loss 1.8261 [208.419935464859 sec]
1.0901415348052979
Total Training Time: 214.71838212013245 seconds

to rescrose the order. His was no darts at a spoked. His did not
help. That Chief Gratta, whopeOked.
BEGINNING (1681984585.2440383): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6731, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6670, val loss 4.6763 [14.98523473739624 sec]
step 100: train loss 2.3733, val loss 2.4719 [41.42767262458801 sec]
step 200: train loss 1.9279, val loss 2.1089 [67.85589671134949 sec]
step 300: train loss 1.6763, val loss 1.9336 [94.35303401947021 sec]
step 400: train loss 1.5179, val loss 1.8289 [120.71558785438538 sec]
step 500: train loss 1.4062, val loss 1.7931 [147.13165616989136 sec]
step 600: train loss 1.3071, val loss 1.7605 [173.59071946144104 sec]
step 700: train loss 1.2352, val loss 1.7755 [199.9273087978363 sec]
step 800: train loss 1.1549, val loss 1.7842 [226.50206899642944 sec]
step 900: train loss 1.0799, val loss 1.7801 [253.02016758918762 sec]
step 1000: train loss 1.0066, val loss 1.8305 [279.76589035987854 sec]
step 1100: train loss 0.9346, val loss 1.8306 [306.3087365627289 sec]
step 1200: train loss 0.8715, val loss 1.8892 [332.721843957901 sec]
step 1300: train loss 0.7855, val loss 1.9516 [359.2016530036926 sec]
step 1400: train loss 0.7202, val loss 1.9951 [385.7210314273834 sec]
0.8795342445373535
Total Training Time: 397.31188130378723 seconds

shekels, uf you are it walked suppily, she ed, shall a
lone of of the toors orange bridge. Aidden
to
BEGINNING (1681984984.736898): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.7101, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7155, val loss 4.7069 [21.93403697013855 sec]
step 100: train loss 2.4249, val loss 2.5127 [58.31367588043213 sec]
step 200: train loss 1.9755, val loss 2.1548 [95.06737446784973 sec]
step 300: train loss 1.6855, val loss 1.9294 [131.26602458953857 sec]
step 400: train loss 1.5153, val loss 1.8368 [168.59890174865723 sec]
step 500: train loss 1.3846, val loss 1.7924 [205.01802897453308 sec]
step 600: train loss 1.2716, val loss 1.7575 [241.79859399795532 sec]
step 700: train loss 1.1782, val loss 1.7793 [277.9995107650757 sec]
step 800: train loss 1.0800, val loss 1.7763 [314.08435821533203 sec]
step 900: train loss 0.9879, val loss 1.8260 [350.34332942962646 sec]
step 1000: train loss 0.9131, val loss 1.8268 [386.8946285247803 sec]
step 1100: train loss 0.8146, val loss 1.9179 [423.7659111022949 sec]
step 1200: train loss 0.7340, val loss 1.9934 [459.75852489471436 sec]
step 1300: train loss 0.6542, val loss 2.0438 [496.3189401626587 sec]
step 1400: train loss 0.5775, val loss 2.1283 [532.9471905231476 sec]
0.7003032565116882
Total Training Time: 548.6408638954163 seconds

sAidden. Why This tribe was ruturning a crafty
to him. But do near the others around such a for cubs
BEGINNING (1681985536.7004855): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6102, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6083, val loss 4.6197 [13.24301552772522 sec]
step 100: train loss 2.4306, val loss 2.5284 [35.367340087890625 sec]
step 200: train loss 2.1391, val loss 2.2874 [57.521795988082886 sec]
step 300: train loss 1.8290, val loss 2.0408 [79.61203503608704 sec]
step 400: train loss 1.6267, val loss 1.8973 [101.6852514743805 sec]
step 500: train loss 1.5039, val loss 1.8247 [123.7619891166687 sec]
step 600: train loss 1.3942, val loss 1.7706 [145.87961864471436 sec]
step 700: train loss 1.2999, val loss 1.7368 [167.9769458770752 sec]
step 800: train loss 1.2213, val loss 1.7548 [190.04915308952332 sec]
step 900: train loss 1.1370, val loss 1.7416 [212.11034512519836 sec]
step 1000: train loss 1.0679, val loss 1.7562 [234.23846888542175 sec]
step 1100: train loss 0.9909, val loss 1.7738 [256.3603284358978 sec]
step 1200: train loss 0.9134, val loss 1.8120 [278.48830342292786 sec]
step 1300: train loss 0.8304, val loss 1.8725 [300.5733976364136 sec]
step 1400: train loss 0.7615, val loss 1.9392 [322.6458911895752 sec]
0.9055376052856445
Total Training Time: 332.01679039001465 seconds

the lend Gratta could he himself across and said, "Kir, gran
tatinl to ascore."
"Before Yah! What th
BEGINNING (1681985869.8489816): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5748, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5671, val loss 4.5730 [24.381746530532837 sec]
step 100: train loss 2.4480, val loss 2.5369 [62.13506197929382 sec]
step 200: train loss 2.1546, val loss 2.3018 [100.14747071266174 sec]
step 300: train loss 1.7816, val loss 2.0159 [138.5978708267212 sec]
step 400: train loss 1.5548, val loss 1.8571 [176.328186750412 sec]
step 500: train loss 1.3914, val loss 1.7735 [214.116126537323 sec]
step 600: train loss 1.2725, val loss 1.7610 [251.74059176445007 sec]
step 700: train loss 1.1466, val loss 1.7562 [289.7560751438141 sec]
step 800: train loss 1.0347, val loss 1.7796 [328.54800605773926 sec]
step 900: train loss 0.9200, val loss 1.8236 [366.4332318305969 sec]
step 1000: train loss 0.8131, val loss 1.8731 [404.00005888938904 sec]
step 1100: train loss 0.6926, val loss 1.9635 [441.4751088619232 sec]
step 1200: train loss 0.5841, val loss 2.0811 [479.12870478630066 sec]
step 1300: train loss 0.4816, val loss 2.1440 [521.4964966773987 sec]
step 1400: train loss 0.3858, val loss 2.3091 [559.7147567272186 sec]
0.5747101306915283
Total Training Time: 576.9561700820923 seconds

He disceparated Chief Arphad at had the tribes nor
help use from any traps. Even if he was parlayed.
BEGINNING (1681986449.2227755): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6399, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6363, val loss 4.6522 [2.2034552097320557 sec]
step 100: train loss 2.5240, val loss 2.6009 [5.9710400104522705 sec]
step 200: train loss 2.2705, val loss 2.3754 [9.698553562164307 sec]
step 300: train loss 2.0996, val loss 2.2538 [13.365851163864136 sec]
step 400: train loss 1.9853, val loss 2.1625 [17.02453637123108 sec]
step 500: train loss 1.9084, val loss 2.0916 [20.712858200073242 sec]
step 600: train loss 1.8415, val loss 2.0593 [24.415603399276733 sec]
step 700: train loss 1.7946, val loss 2.0163 [28.018320083618164 sec]
step 800: train loss 1.7380, val loss 1.9580 [31.645094394683838 sec]
step 900: train loss 1.7038, val loss 1.9414 [35.27875828742981 sec]
step 1000: train loss 1.6634, val loss 1.9222 [38.9615638256073 sec]
step 1100: train loss 1.6497, val loss 1.9140 [42.62004542350769 sec]
step 1200: train loss 1.6186, val loss 1.8898 [46.28965163230896 sec]
step 1300: train loss 1.6071, val loss 1.8783 [49.965206146240234 sec]
step 1400: train loss 1.5752, val loss 1.8519 [53.61893391609192 sec]
1.6059993505477905
Total Training Time: 55.117655992507935 seconds

knateed.
"Wo him was of sation the would of have
crowleld a sorting hing ent, he tenturned then at
g
BEGINNING (1681986504.9387996): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6725, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6567, val loss 4.6550 [3.636579990386963 sec]
step 100: train loss 2.4831, val loss 2.5623 [10.084244966506958 sec]
step 200: train loss 2.2317, val loss 2.3410 [16.662266731262207 sec]
step 300: train loss 2.0577, val loss 2.2067 [23.21195340156555 sec]
step 400: train loss 1.9234, val loss 2.1056 [29.697255849838257 sec]
step 500: train loss 1.8281, val loss 2.0367 [36.18267583847046 sec]
step 600: train loss 1.7568, val loss 1.9749 [42.70999240875244 sec]
step 700: train loss 1.6996, val loss 1.9473 [49.173078775405884 sec]
step 800: train loss 1.6569, val loss 1.9137 [55.64933490753174 sec]
step 900: train loss 1.6148, val loss 1.8630 [62.12666583061218 sec]
step 1000: train loss 1.5765, val loss 1.8774 [68.57239103317261 sec]
step 1100: train loss 1.5418, val loss 1.8295 [74.9994649887085 sec]
step 1200: train loss 1.5169, val loss 1.8185 [81.50693488121033 sec]
step 1300: train loss 1.4968, val loss 1.8047 [88.07022213935852 sec]
step 1400: train loss 1.4802, val loss 1.8016 [94.54688930511475 sec]
1.5144473314285278
Total Training Time: 97.37587761878967 seconds

mhad's with a ral
perting. "WHE VE" Gor concerenters,
agon whind looked on said hik?" Perhaps me pos
BEGINNING (1681986603.5007002): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6462, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6334, val loss 4.6293 [5.144932508468628 sec]
step 100: train loss 2.4683, val loss 2.5516 [14.429516792297363 sec]
step 200: train loss 2.2180, val loss 2.3434 [23.701196432113647 sec]
step 300: train loss 2.0270, val loss 2.1941 [32.999083518981934 sec]
step 400: train loss 1.8956, val loss 2.0896 [42.29823112487793 sec]
step 500: train loss 1.8037, val loss 1.9955 [51.77117300033569 sec]
step 600: train loss 1.7241, val loss 1.9810 [61.1459174156189 sec]
step 700: train loss 1.6632, val loss 1.8952 [70.40823435783386 sec]
step 800: train loss 1.6187, val loss 1.8873 [79.71008777618408 sec]
step 900: train loss 1.5654, val loss 1.8440 [88.99024105072021 sec]
step 1000: train loss 1.5388, val loss 1.8378 [98.24656176567078 sec]
step 1100: train loss 1.5082, val loss 1.8164 [107.53871512413025 sec]
step 1200: train loss 1.4621, val loss 1.7808 [117.00396060943604 sec]
step 1300: train loss 1.4530, val loss 1.8189 [126.31861758232117 sec]
step 1400: train loss 1.4259, val loss 1.7789 [135.64992594718933 sec]
1.5608400106430054
Total Training Time: 139.82415556907654 seconds

our coulds facuer of durnemby and
han is;nayah as sationed and yet, as
well nody have. He toidinge. 
BEGINNING (1681986745.0463912): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5923, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6044, val loss 4.6149 [2.4806346893310547 sec]
step 100: train loss 2.5523, val loss 2.6242 [6.675060749053955 sec]
step 200: train loss 2.3835, val loss 2.4790 [10.880539417266846 sec]
step 300: train loss 2.1719, val loss 2.3010 [15.077659130096436 sec]
step 400: train loss 2.0271, val loss 2.1776 [19.266687154769897 sec]
step 500: train loss 1.9407, val loss 2.1324 [23.45751190185547 sec]
step 600: train loss 1.8475, val loss 2.0537 [27.66794490814209 sec]
step 700: train loss 1.7754, val loss 1.9745 [31.852323532104492 sec]
step 800: train loss 1.7182, val loss 1.9570 [36.062665939331055 sec]
step 900: train loss 1.6706, val loss 1.9196 [40.30246138572693 sec]
step 1000: train loss 1.6252, val loss 1.8722 [44.509544134140015 sec]
step 1100: train loss 1.5938, val loss 1.8699 [48.69610285758972 sec]
step 1200: train loss 1.5545, val loss 1.8439 [52.88948631286621 sec]
step 1300: train loss 1.5281, val loss 1.8234 [57.076844930648804 sec]
step 1400: train loss 1.4975, val loss 1.8030 [61.26829242706299 sec]
1.5701196193695068
Total Training Time: 62.974788427352905 seconds

ear they the soon the vell. Gratta wark
no no nodden of they tuonoss hell camp in to but Tak one
sum
BEGINNING (1681986808.6468184): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6364, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6472, val loss 4.6351 [4.350060701370239 sec]
step 100: train loss 2.5072, val loss 2.5821 [11.920109272003174 sec]
step 200: train loss 2.3286, val loss 2.4313 [19.503153800964355 sec]
step 300: train loss 2.1123, val loss 2.2471 [27.077957153320312 sec]
step 400: train loss 1.9396, val loss 2.1137 [34.65589356422424 sec]
step 500: train loss 1.8236, val loss 2.0298 [42.21765375137329 sec]
step 600: train loss 1.7296, val loss 1.9584 [49.795907497406006 sec]
step 700: train loss 1.6684, val loss 1.9299 [57.33794689178467 sec]
step 800: train loss 1.5974, val loss 1.8716 [64.88133978843689 sec]
step 900: train loss 1.5488, val loss 1.8355 [72.42608952522278 sec]
step 1000: train loss 1.5013, val loss 1.8044 [79.98457407951355 sec]
step 1100: train loss 1.4662, val loss 1.7951 [87.55043125152588 sec]
step 1200: train loss 1.4398, val loss 1.7904 [95.11556887626648 sec]
step 1300: train loss 1.3950, val loss 1.7591 [102.6542067527771 sec]
step 1400: train loss 1.3680, val loss 1.7528 [110.23946499824524 sec]
1.412997841835022
Total Training Time: 113.44781470298767 seconds

Gratta smiled. "Sir reach or cances, all over troup as
hereal swomenting their parw rule
somen hough
BEGINNING (1681986923.3019233): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5123, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5202, val loss 4.5211 [6.168169736862183 sec]
step 100: train loss 2.5048, val loss 2.5777 [17.043476819992065 sec]
step 200: train loss 2.3108, val loss 2.4227 [27.9266037940979 sec]
step 300: train loss 2.0753, val loss 2.2269 [38.83677697181702 sec]
step 400: train loss 1.8851, val loss 2.0769 [49.73375582695007 sec]
step 500: train loss 1.7558, val loss 1.9830 [60.670573711395264 sec]
step 600: train loss 1.6763, val loss 1.9175 [71.5748119354248 sec]
step 700: train loss 1.6035, val loss 1.8717 [82.46924328804016 sec]
step 800: train loss 1.5412, val loss 1.8354 [93.39468884468079 sec]
step 900: train loss 1.4898, val loss 1.8120 [104.29672312736511 sec]
step 1000: train loss 1.4316, val loss 1.7854 [115.17286539077759 sec]
step 1100: train loss 1.3834, val loss 1.7779 [126.09123373031616 sec]
step 1200: train loss 1.3644, val loss 1.7565 [136.99618434906006 sec]
step 1300: train loss 1.3233, val loss 1.7382 [147.8827292919159 sec]
step 1400: train loss 1.2908, val loss 1.7375 [158.7868754863739 sec]
1.410097599029541
Total Training Time: 163.51185083389282 seconds

cart in of the ord on. If would clump-joing he
for Ar was warriound no sniffe the firs botwing to ne
BEGINNING (1681987088.606156): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6155, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6061, val loss 4.6142 [3.3793368339538574 sec]
step 100: train loss 2.5518, val loss 2.6246 [9.247981071472168 sec]
step 200: train loss 2.4594, val loss 2.5447 [15.12720537185669 sec]
step 300: train loss 2.3661, val loss 2.4640 [20.99408531188965 sec]
step 400: train loss 2.2345, val loss 2.3485 [26.871605396270752 sec]
step 500: train loss 2.1002, val loss 2.2474 [32.74924445152283 sec]
step 600: train loss 1.9794, val loss 2.1538 [38.617857456207275 sec]
step 700: train loss 1.8841, val loss 2.0831 [44.50621056556702 sec]
step 800: train loss 1.8116, val loss 2.0315 [50.3931348323822 sec]
step 900: train loss 1.7464, val loss 1.9675 [56.28396439552307 sec]
step 1000: train loss 1.6895, val loss 1.9295 [62.15063714981079 sec]
step 1100: train loss 1.6354, val loss 1.8929 [68.01826119422913 sec]
step 1200: train loss 1.5917, val loss 1.8611 [73.89301824569702 sec]
step 1300: train loss 1.5555, val loss 1.8310 [79.77852177619934 sec]
step 1400: train loss 1.5189, val loss 1.8016 [85.66786551475525 sec]
1.650928258895874
Total Training Time: 88.14423155784607 seconds

the Valle beentably, to it the tuonly tuon be knowish
and to flot talrough int. Gratta barriquist.
H
BEGINNING (1681987177.373808): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6267, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6158, val loss 4.6175 [22896.09755039215 sec]
step 100: train loss 2.5240, val loss 2.6049 [22911.360577583313 sec]
step 200: train loss 2.4294, val loss 2.5120 [22924.31755590439 sec]
step 300: train loss 2.3084, val loss 2.4116 [22935.483884334564 sec]
step 400: train loss 2.0997, val loss 2.2474 [22946.45719742775 sec]
step 500: train loss 1.9387, val loss 2.1306 [22957.39885687828 sec]
step 600: train loss 1.8131, val loss 2.0216 [22968.368054151535 sec]
step 700: train loss 1.7167, val loss 1.9524 [22979.28355026245 sec]
step 800: train loss 1.6380, val loss 1.8873 [22990.751483678818 sec]
step 900: train loss 1.5705, val loss 1.8561 [23002.352107286453 sec]
step 1000: train loss 1.5149, val loss 1.8288 [23013.44810271263 sec]
step 1100: train loss 1.4557, val loss 1.7946 [23025.028812885284 sec]
step 1200: train loss 1.4211, val loss 1.7843 [23036.714865922928 sec]
step 1300: train loss 1.3685, val loss 1.7603 [23049.198136806488 sec]
step 1400: train loss 1.3253, val loss 1.7323 [23064.201087236404 sec]
1.369177222251892
Total Training Time: 23069.59202361107 seconds

AcQ oner from this Torial cerch
and, and he oter said to then way have a cer stent,
and his finced t
BEGINNING (1682010248.538624): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6901, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6964, val loss 4.7039 [9.907463312149048 sec]
step 100: train loss 2.5134, val loss 2.5892 [28.394646644592285 sec]
step 200: train loss 2.4214, val loss 2.5179 [47.22422456741333 sec]
step 300: train loss 2.2665, val loss 2.3773 [63.10978627204895 sec]
step 400: train loss 2.0285, val loss 2.1801 [79.75777840614319 sec]
step 500: train loss 1.8688, val loss 2.0589 [96.2983820438385 sec]
step 600: train loss 1.7420, val loss 1.9648 [113.68364906311035 sec]
step 700: train loss 1.6421, val loss 1.9036 [131.05315589904785 sec]
step 800: train loss 1.5675, val loss 1.8546 [148.7582564353943 sec]
step 900: train loss 1.4969, val loss 1.8082 [163.54239416122437 sec]
step 1000: train loss 1.4325, val loss 1.8048 [175.2895863056183 sec]
step 1100: train loss 1.3768, val loss 1.7724 [187.19105577468872 sec]
step 1200: train loss 1.3227, val loss 1.7419 [199.04294896125793 sec]
step 1300: train loss 1.2757, val loss 1.7431 [211.4458155632019 sec]
step 1400: train loss 1.2392, val loss 1.7348 [224.68996787071228 sec]
1.334281325340271
Total Training Time: 230.58062195777893 seconds

trable age." Gratta again as the soon. A turned and ange, and in
the stook of the and scept, but the
BEGINNING (1682010481.2583323): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5894, val loss 4.5946 [3.0106699466705322 sec]
step 100: train loss 2.4875, val loss 2.5666 [7.2954206466674805 sec]
step 200: train loss 2.2229, val loss 2.3336 [11.484179735183716 sec]
step 300: train loss 2.0618, val loss 2.2157 [16.57262945175171 sec]
step 400: train loss 1.9413, val loss 2.1255 [21.5696861743927 sec]
step 500: train loss 1.8507, val loss 2.0395 [26.366456270217896 sec]
step 600: train loss 1.7834, val loss 1.9983 [30.890146255493164 sec]
step 700: train loss 1.7261, val loss 1.9464 [35.41362953186035 sec]
step 800: train loss 1.6894, val loss 1.9478 [40.03815293312073 sec]
step 900: train loss 1.6418, val loss 1.8913 [44.771705627441406 sec]
step 1000: train loss 1.6122, val loss 1.8839 [49.56454348564148 sec]
step 1100: train loss 1.5842, val loss 1.8813 [54.13575100898743 sec]
step 1200: train loss 1.5612, val loss 1.8480 [58.7129864692688 sec]
step 1300: train loss 1.5414, val loss 1.8213 [63.28767275810242 sec]
step 1400: train loss 1.5145, val loss 1.8261 [68.27852725982666 sec]
1.5858043432235718
Total Training Time: 70.22057819366455 seconds

"We musn't de could and giest ally, of man seemed."
"What dep onself the at Ar you
gray spit for lar
BEGINNING (1682010552.1676073): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5894, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5827, val loss 4.5773 [4.353396892547607 sec]
step 100: train loss 2.4644, val loss 2.5345 [12.05586290359497 sec]
step 200: train loss 2.1728, val loss 2.3085 [19.709150552749634 sec]
step 300: train loss 1.9766, val loss 2.1605 [27.41308045387268 sec]
step 400: train loss 1.8378, val loss 2.0356 [35.08003783226013 sec]
step 500: train loss 1.7530, val loss 1.9641 [42.97320747375488 sec]
step 600: train loss 1.6848, val loss 1.9123 [50.72930717468262 sec]
step 700: train loss 1.6272, val loss 1.8712 [58.41645622253418 sec]
step 800: train loss 1.5848, val loss 1.8639 [66.18132328987122 sec]
step 900: train loss 1.5448, val loss 1.8345 [73.90638661384583 sec]
step 1000: train loss 1.5101, val loss 1.8353 [81.61468386650085 sec]
step 1100: train loss 1.4767, val loss 1.8207 [89.27830219268799 sec]
step 1200: train loss 1.4568, val loss 1.8162 [97.04376983642578 sec]
step 1300: train loss 1.4312, val loss 1.7941 [104.8699700832367 sec]
step 1400: train loss 1.3994, val loss 1.7970 [112.65142893791199 sec]
1.4356456995010376
Total Training Time: 116.05802369117737 seconds

grater his for the Pyrrangs, came
of somether graft Gratta said. He
toways guards
down new. Sunies o
BEGINNING (1682010669.61675): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6240, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6238, val loss 4.6205 [6.598679542541504 sec]
step 100: train loss 2.4341, val loss 2.5199 [17.711920022964478 sec]
step 200: train loss 2.1436, val loss 2.2737 [28.644150495529175 sec]
step 300: train loss 1.9395, val loss 2.1178 [39.644896030426025 sec]
step 400: train loss 1.8059, val loss 2.0051 [50.587883710861206 sec]
step 500: train loss 1.7231, val loss 1.9458 [61.714338541030884 sec]
step 600: train loss 1.6399, val loss 1.9053 [72.7176079750061 sec]
step 700: train loss 1.5758, val loss 1.8411 [83.66846013069153 sec]
step 800: train loss 1.5309, val loss 1.8297 [94.69989585876465 sec]
step 900: train loss 1.4885, val loss 1.8124 [105.78044438362122 sec]
step 1000: train loss 1.4512, val loss 1.8070 [116.80534720420837 sec]
step 1100: train loss 1.4159, val loss 1.7620 [127.99591135978699 sec]
step 1200: train loss 1.3859, val loss 1.8006 [139.0147635936737 sec]
step 1300: train loss 1.3581, val loss 1.7752 [149.95206546783447 sec]
step 1400: train loss 1.3290, val loss 1.7621 [160.90232825279236 sec]
1.3920856714248657
Total Training Time: 165.82076025009155 seconds

The simpy. I a shemed back look on at him plead him satbe'd
boursons and by tuons foollowing
about t
BEGINNING (1682010837.443534): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6587, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6694, val loss 4.6684 [3.2192273139953613 sec]
step 100: train loss 2.5248, val loss 2.5964 [8.758033275604248 sec]
step 200: train loss 2.3316, val loss 2.4300 [14.321633577346802 sec]
step 300: train loss 2.1039, val loss 2.2463 [20.019824266433716 sec]
step 400: train loss 1.9507, val loss 2.1365 [25.591662168502808 sec]
step 500: train loss 1.8502, val loss 2.0547 [31.07124376296997 sec]
step 600: train loss 1.7732, val loss 1.9988 [36.58553767204285 sec]
step 700: train loss 1.7077, val loss 1.9439 [42.11922216415405 sec]
step 800: train loss 1.6506, val loss 1.8973 [47.63482069969177 sec]
step 900: train loss 1.6043, val loss 1.8758 [53.13400912284851 sec]
step 1000: train loss 1.5622, val loss 1.8469 [58.6509325504303 sec]
step 1100: train loss 1.5240, val loss 1.8270 [64.14581656455994 sec]
step 1200: train loss 1.4923, val loss 1.8201 [69.7147924900055 sec]
step 1300: train loss 1.4547, val loss 1.7982 [75.25608777999878 sec]
step 1400: train loss 1.4320, val loss 1.7847 [80.77367043495178 sec]
1.56991708278656
Total Training Time: 83.05738663673401 seconds

down ways gattle frembly. I grade pawas half
a wan nearly we meture a before a piese are?".
Tenty a 
BEGINNING (1682010921.2402265): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6470, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6366, val loss 4.6369 [5.680348873138428 sec]
step 100: train loss 2.4916, val loss 2.5701 [15.404892683029175 sec]
step 200: train loss 2.2699, val loss 2.3868 [25.172954320907593 sec]
step 300: train loss 2.0146, val loss 2.1792 [35.4108247756958 sec]
step 400: train loss 1.8554, val loss 2.0525 [45.58856749534607 sec]
step 500: train loss 1.7248, val loss 1.9580 [55.868386030197144 sec]
step 600: train loss 1.6377, val loss 1.8955 [67.50130534172058 sec]
step 700: train loss 1.5692, val loss 1.8376 [78.7931010723114 sec]
step 800: train loss 1.5104, val loss 1.8123 [88.48453545570374 sec]
step 900: train loss 1.4647, val loss 1.7902 [97.53893446922302 sec]
step 1000: train loss 1.4107, val loss 1.7619 [106.58121275901794 sec]
step 1100: train loss 1.3791, val loss 1.7537 [115.66514539718628 sec]
step 1200: train loss 1.3405, val loss 1.7335 [124.64954948425293 sec]
step 1300: train loss 1.3057, val loss 1.7427 [133.68835425376892 sec]
step 1400: train loss 1.2760, val loss 1.7372 [142.72859835624695 sec]
1.3635473251342773
Total Training Time: 146.54544878005981 seconds

an hung death. WalsHered and the more over the
gereather with The brray."
The wany uon chared and An
BEGINNING (1682011068.9703634): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.7043, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7158, val loss 4.7079 [7.375296115875244 sec]
step 100: train loss 2.4764, val loss 2.5559 [20.33487296104431 sec]
step 200: train loss 2.2747, val loss 2.3851 [33.26462769508362 sec]
step 300: train loss 1.9941, val loss 2.1661 [47.24349808692932 sec]
step 400: train loss 1.8230, val loss 2.0282 [60.82006120681763 sec]
step 500: train loss 1.6964, val loss 1.9297 [74.29745173454285 sec]
step 600: train loss 1.5963, val loss 1.8641 [87.75587821006775 sec]
step 700: train loss 1.5238, val loss 1.8134 [101.12277436256409 sec]
step 800: train loss 1.4554, val loss 1.7916 [114.5995409488678 sec]
step 900: train loss 1.4021, val loss 1.7642 [128.21320509910583 sec]
step 1000: train loss 1.3481, val loss 1.7447 [141.67008185386658 sec]
step 1100: train loss 1.3032, val loss 1.7328 [155.17143988609314 sec]
step 1200: train loss 1.2618, val loss 1.7445 [168.3928849697113 sec]
step 1300: train loss 1.2233, val loss 1.7434 [181.34617590904236 sec]
step 1400: train loss 1.1836, val loss 1.7491 [194.41599297523499 sec]
1.229560136795044
Total Training Time: 200.00904989242554 seconds

SEAN McKAY
HAN
CHAPTERATES?" AMBNE
AY
"Nav, theyI have self, quich the
maeuws short powsely paw. We 
BEGINNING (1682011271.0481272): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6004, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6078, val loss 4.6074 [4.620681524276733 sec]
step 100: train loss 2.5468, val loss 2.6236 [12.445835590362549 sec]
step 200: train loss 2.4421, val loss 2.5239 [20.2761492729187 sec]
step 300: train loss 2.3327, val loss 2.4308 [28.110382318496704 sec]
step 400: train loss 2.1571, val loss 2.2872 [35.910131216049194 sec]
step 500: train loss 1.9963, val loss 2.1613 [43.72920203208923 sec]
step 600: train loss 1.8736, val loss 2.0678 [51.59446978569031 sec]
step 700: train loss 1.7856, val loss 1.9977 [59.43395638465881 sec]
step 800: train loss 1.7060, val loss 1.9458 [67.30225729942322 sec]
step 900: train loss 1.6405, val loss 1.8928 [75.11580014228821 sec]
step 1000: train loss 1.5834, val loss 1.8623 [82.9608564376831 sec]
step 1100: train loss 1.5397, val loss 1.8346 [90.80618596076965 sec]
step 1200: train loss 1.4879, val loss 1.7962 [98.63374900817871 sec]
step 1300: train loss 1.4484, val loss 1.7765 [106.4857292175293 sec]
step 1400: train loss 1.4125, val loss 1.7535 [114.35408735275269 sec]
1.4810200929641724
Total Training Time: 117.64230990409851 seconds

fition of thint scavicib, whese worsh pears, be centrels
peopin to right, looke brok as of has hum. 
BEGINNING (1682011389.3204727): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6682, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6685, val loss 4.6691 [8.217626094818115 sec]
step 100: train loss 2.5102, val loss 2.5857 [22.51099443435669 sec]
step 200: train loss 2.4026, val loss 2.4825 [36.833597898483276 sec]
step 300: train loss 2.2188, val loss 2.3321 [51.21468424797058 sec]
step 400: train loss 2.0106, val loss 2.1702 [65.60093116760254 sec]
step 500: train loss 1.8540, val loss 2.0514 [79.94830179214478 sec]
step 600: train loss 1.7436, val loss 1.9539 [94.80353832244873 sec]
step 700: train loss 1.6454, val loss 1.8915 [109.80011010169983 sec]
step 800: train loss 1.5582, val loss 1.8454 [124.59611177444458 sec]
step 900: train loss 1.4910, val loss 1.8126 [139.47128343582153 sec]
step 1000: train loss 1.4348, val loss 1.7724 [154.41493844985962 sec]
step 1100: train loss 1.3863, val loss 1.7596 [173.00123500823975 sec]
step 1200: train loss 1.3332, val loss 1.7374 [189.0083749294281 sec]
step 1300: train loss 1.2899, val loss 1.7347 [209.28533720970154 sec]
step 1400: train loss 1.2420, val loss 1.7308 [227.07808446884155 sec]
1.3094950914382935
Total Training Time: 236.0502371788025 seconds

from Arnd and spoke to back of this he." Arphad
his nodded, but nod dow with a, no boush tenorther
F
BEGINNING (1682011627.0582094): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6466, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6385, val loss 4.6367 [14.63513970375061 sec]
step 100: train loss 2.5101, val loss 2.5787 [38.54236817359924 sec]
step 200: train loss 2.3832, val loss 2.4835 [62.64623546600342 sec]
step 300: train loss 2.1479, val loss 2.2809 [85.64146184921265 sec]
step 400: train loss 1.9124, val loss 2.0936 [112.32058525085449 sec]
step 500: train loss 1.7802, val loss 1.9998 [145.3193769454956 sec]
step 600: train loss 1.6544, val loss 1.9019 [171.02134037017822 sec]
step 700: train loss 1.5508, val loss 1.8375 [196.82088327407837 sec]
step 800: train loss 1.4712, val loss 1.7987 [222.8939390182495 sec]
step 900: train loss 1.4074, val loss 1.7688 [250.02651596069336 sec]
step 1000: train loss 1.3493, val loss 1.7742 [279.48010063171387 sec]
step 1100: train loss 1.2862, val loss 1.7235 [311.6824641227722 sec]
step 1200: train loss 1.2359, val loss 1.7412 [338.71637630462646 sec]
step 1300: train loss 1.1744, val loss 1.7463 [366.9239354133606 sec]
step 1400: train loss 1.1242, val loss 1.7556 [393.3515713214874 sec]
1.2051395177841187
Total Training Time: 403.9966390132904 seconds

"Gor. Ot, if all discuss our seemed for
shel human had im tost?" Gratta was voiged again, but they
m
BEGINNING (1682012032.9826262): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6641, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6623, val loss 4.6673 [3.201216220855713 sec]
step 100: train loss 2.4705, val loss 2.5479 [8.337632656097412 sec]
step 200: train loss 2.1652, val loss 2.2971 [13.399832248687744 sec]
step 300: train loss 1.9910, val loss 2.1599 [18.731844902038574 sec]
step 400: train loss 1.8908, val loss 2.0703 [23.847479104995728 sec]
step 500: train loss 1.8088, val loss 2.0107 [28.9040367603302 sec]
step 600: train loss 1.7389, val loss 1.9630 [34.057575702667236 sec]
step 700: train loss 1.6879, val loss 1.9441 [39.129727840423584 sec]
step 800: train loss 1.6435, val loss 1.8996 [44.15393400192261 sec]
step 900: train loss 1.6075, val loss 1.8785 [49.207786083221436 sec]
step 1000: train loss 1.5766, val loss 1.8553 [54.25030517578125 sec]
step 1100: train loss 1.5429, val loss 1.8317 [59.359251737594604 sec]
step 1200: train loss 1.5198, val loss 1.8397 [64.51715898513794 sec]
step 1300: train loss 1.4929, val loss 1.8268 [69.58305382728577 sec]
step 1400: train loss 1.4714, val loss 1.8057 [74.70919156074524 sec]
1.5701422691345215
Total Training Time: 76.73789954185486 seconds

many tuons where the guard a hard of their the humans. That a did now whered, from the may quet arel
BEGINNING (1682012110.4128146): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6111, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6076, val loss 4.6077 [5.231463670730591 sec]
step 100: train loss 2.4357, val loss 2.5275 [13.905869483947754 sec]
step 200: train loss 2.1108, val loss 2.2488 [22.932928562164307 sec]
step 300: train loss 1.9134, val loss 2.0968 [31.73825693130493 sec]
step 400: train loss 1.7840, val loss 1.9903 [40.45692729949951 sec]
step 500: train loss 1.6938, val loss 1.9286 [49.13820266723633 sec]
step 600: train loss 1.6331, val loss 1.8992 [57.84434509277344 sec]
step 700: train loss 1.5712, val loss 1.8662 [67.02286982536316 sec]
step 800: train loss 1.5324, val loss 1.8356 [75.7648012638092 sec]
step 900: train loss 1.4892, val loss 1.8099 [84.57124829292297 sec]
step 1000: train loss 1.4442, val loss 1.8041 [93.5728850364685 sec]
step 1100: train loss 1.4199, val loss 1.7833 [102.19661974906921 sec]
step 1200: train loss 1.3935, val loss 1.7756 [110.88740706443787 sec]
step 1300: train loss 1.3588, val loss 1.7738 [119.62158179283142 sec]
step 1400: train loss 1.3302, val loss 1.7594 [128.34302067756653 sec]
1.5059479475021362
Total Training Time: 132.01476097106934 seconds

"I am you thave heady a milley."
Chink Taka this live cart anask he his stomal few und Arphad 's rea
BEGINNING (1682012243.728244): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6815, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6765, val loss 4.6772 [6.988974094390869 sec]
step 100: train loss 2.4180, val loss 2.5044 [19.203847646713257 sec]
step 200: train loss 2.0933, val loss 2.2382 [31.396482944488525 sec]
step 300: train loss 1.8791, val loss 2.0721 [43.48595857620239 sec]
step 400: train loss 1.7489, val loss 1.9689 [55.61328482627869 sec]
step 500: train loss 1.6510, val loss 1.9001 [67.89488768577576 sec]
step 600: train loss 1.5830, val loss 1.8564 [80.01158142089844 sec]
step 700: train loss 1.5232, val loss 1.8347 [92.30375838279724 sec]
step 800: train loss 1.4726, val loss 1.8095 [104.3938353061676 sec]
step 900: train loss 1.4295, val loss 1.7837 [116.41024208068848 sec]
step 1000: train loss 1.3887, val loss 1.7750 [128.75311541557312 sec]
step 1100: train loss 1.3556, val loss 1.7651 [140.91755962371826 sec]
step 1200: train loss 1.3236, val loss 1.7536 [157.10138773918152 sec]
step 1300: train loss 1.2955, val loss 1.7607 [168.6271059513092 sec]
step 1400: train loss 1.2636, val loss 1.7477 [180.07169938087463 sec]
1.3001320362091064
Total Training Time: 185.00305271148682 seconds

and to go." The soldiers, whipe's runding
long ago, but soldier the cubits is to int." The kill were
BEGINNING (1682012430.681838): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6827, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6912, val loss 4.6918 [5.805414915084839 sec]
step 100: train loss 2.5221, val loss 2.5965 [15.23732590675354 sec]
step 200: train loss 2.2958, val loss 2.3933 [22.580018520355225 sec]
step 300: train loss 2.0705, val loss 2.2290 [30.108971118927002 sec]
step 400: train loss 1.9156, val loss 2.1019 [37.41064667701721 sec]
step 500: train loss 1.8090, val loss 2.0143 [44.717559814453125 sec]
step 600: train loss 1.7287, val loss 1.9624 [52.04701089859009 sec]
step 700: train loss 1.6638, val loss 1.9191 [59.54984998703003 sec]
step 800: train loss 1.6144, val loss 1.8727 [66.75337672233582 sec]
step 900: train loss 1.5667, val loss 1.8458 [74.21296453475952 sec]
step 1000: train loss 1.5280, val loss 1.8091 [81.53824949264526 sec]
step 1100: train loss 1.4844, val loss 1.7941 [88.86682724952698 sec]
step 1200: train loss 1.4532, val loss 1.7855 [99.705570936203 sec]
step 1300: train loss 1.4166, val loss 1.7729 [113.10979795455933 sec]
step 1400: train loss 1.3947, val loss 1.7638 [120.31012058258057 sec]
1.429570198059082
Total Training Time: 123.23243618011475 seconds

 teter to heath, and so."
Ar!" Gratta come bary a
critherribes they conly cants's gace the had a tyo
BEGINNING (1682012554.723495): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5496, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5414, val loss 4.5381 [7.313221216201782 sec]
step 100: train loss 2.4722, val loss 2.5520 [19.86648416519165 sec]
step 200: train loss 2.2174, val loss 2.3450 [32.216267347335815 sec]
step 300: train loss 1.9545, val loss 2.1295 [44.606398820877075 sec]
step 400: train loss 1.7938, val loss 2.0025 [57.058348655700684 sec]
step 500: train loss 1.6789, val loss 1.9287 [69.44436883926392 sec]
step 600: train loss 1.5864, val loss 1.8686 [81.73136949539185 sec]
step 700: train loss 1.5239, val loss 1.8171 [94.11027240753174 sec]
step 800: train loss 1.4569, val loss 1.7908 [106.56889772415161 sec]
step 900: train loss 1.4075, val loss 1.7565 [119.03882026672363 sec]
step 1000: train loss 1.3542, val loss 1.7461 [131.62710070610046 sec]
step 1100: train loss 1.3214, val loss 1.7530 [144.20407819747925 sec]
step 1200: train loss 1.2781, val loss 1.7356 [156.81138515472412 sec]
step 1300: train loss 1.2437, val loss 1.7208 [169.31803059577942 sec]
step 1400: train loss 1.2022, val loss 1.7269 [181.8902518749237 sec]
1.311023473739624
Total Training Time: 187.05364108085632 seconds

anslead anain though in deceptable, he pathan giver lies to
the ee riveon trickingnges.
"That mees f
BEGINNING (1682012743.2539694): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6379, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6284, val loss 4.6362 [10.234256505966187 sec]
step 100: train loss 2.4613, val loss 2.5430 [28.01168441772461 sec]
step 200: train loss 2.1969, val loss 2.3147 [45.70197510719299 sec]
step 300: train loss 1.9185, val loss 2.0930 [63.47904181480408 sec]
step 400: train loss 1.7541, val loss 1.9713 [81.01815557479858 sec]
step 500: train loss 1.6267, val loss 1.8901 [98.77201986312866 sec]
step 600: train loss 1.5435, val loss 1.8399 [116.36917495727539 sec]
step 700: train loss 1.4564, val loss 1.7935 [134.0190167427063 sec]
step 800: train loss 1.4035, val loss 1.7738 [151.66811776161194 sec]
step 900: train loss 1.3399, val loss 1.7521 [169.37166666984558 sec]
step 1000: train loss 1.2855, val loss 1.7348 [187.14386463165283 sec]
step 1100: train loss 1.2370, val loss 1.7419 [204.78332424163818 sec]
step 1200: train loss 1.1914, val loss 1.7342 [222.692040681839 sec]
step 1300: train loss 1.1531, val loss 1.7425 [240.3816261291504 sec]
step 1400: train loss 1.1107, val loss 1.7517 [258.05584692955017 sec]
1.2247079610824585
Total Training Time: 265.57489109039307 seconds

He joi!"
"Yes, sir!" ofer a quiet
our many way.
I you have will about the brought as ne
gut judge. F
BEGINNING (1682013010.9062006): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6282, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6140, val loss 4.6222 [7.010752201080322 sec]
step 100: train loss 2.5352, val loss 2.6053 [18.85384178161621 sec]
step 200: train loss 2.4301, val loss 2.5184 [30.59479856491089 sec]
step 300: train loss 2.2709, val loss 2.3831 [42.3866605758667 sec]
step 400: train loss 2.0614, val loss 2.2147 [54.159650802612305 sec]
step 500: train loss 1.8993, val loss 2.0775 [65.74828147888184 sec]
step 600: train loss 1.7878, val loss 1.9993 [77.32706308364868 sec]
step 700: train loss 1.7029, val loss 1.9280 [89.12370896339417 sec]
step 800: train loss 1.6311, val loss 1.8900 [100.76046204566956 sec]
step 900: train loss 1.5747, val loss 1.8488 [112.44945240020752 sec]
step 1000: train loss 1.5192, val loss 1.8110 [124.25230360031128 sec]
step 1100: train loss 1.4755, val loss 1.7940 [135.9904568195343 sec]
step 1200: train loss 1.4316, val loss 1.7676 [147.77451848983765 sec]
step 1300: train loss 1.3898, val loss 1.7440 [159.57868719100952 sec]
step 1400: train loss 1.3623, val loss 1.7459 [171.5301866531372 sec]
1.441731333732605
Total Training Time: 176.28782892227173 seconds

could. Onothers overs from to somety of down
the blet: uped his, Leeket a mightionerations. Th eyou.
BEGINNING (1682013187.9769187): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6339, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6403, val loss 4.6320 [12.19393277168274 sec]
step 100: train loss 2.5059, val loss 2.5866 [33.153791427612305 sec]
step 200: train loss 2.3921, val loss 2.4867 [54.16589379310608 sec]
step 300: train loss 2.1867, val loss 2.3099 [74.91029739379883 sec]
step 400: train loss 1.9491, val loss 2.1218 [95.96388745307922 sec]
step 500: train loss 1.7952, val loss 2.0006 [116.88268804550171 sec]
step 600: train loss 1.6792, val loss 1.9115 [137.64965987205505 sec]
step 700: train loss 1.5821, val loss 1.8487 [158.64040637016296 sec]
step 800: train loss 1.4965, val loss 1.7973 [179.8016233444214 sec]
step 900: train loss 1.4305, val loss 1.7828 [200.75847816467285 sec]
step 1000: train loss 1.3657, val loss 1.7420 [221.71239376068115 sec]
step 1100: train loss 1.3112, val loss 1.7266 [242.8024308681488 sec]
step 1200: train loss 1.2607, val loss 1.7222 [263.7256586551666 sec]
step 1300: train loss 1.2139, val loss 1.7186 [284.5076277256012 sec]
step 1400: train loss 1.1672, val loss 1.7159 [305.6823761463165 sec]
1.2641398906707764
Total Training Time: 314.4704020023346 seconds

"I cask to evenamptanure stariyah allowing gave to the
him."
"Hoomey bek him maeuw reprivightful the
BEGINNING (1682013503.9295523): Baseline LR(0.00025) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5368, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5364, val loss 4.5440 [17.021246194839478 sec]
step 100: train loss 2.4949, val loss 2.5777 [46.218682527542114 sec]
step 200: train loss 2.3538, val loss 2.4572 [75.14460945129395 sec]
step 300: train loss 2.1136, val loss 2.2518 [104.2562370300293 sec]
step 400: train loss 1.8771, val loss 2.0663 [133.7430338859558 sec]
step 500: train loss 1.7212, val loss 1.9444 [168.43905997276306 sec]
step 600: train loss 1.6014, val loss 1.8741 [194.93854975700378 sec]
step 700: train loss 1.5010, val loss 1.8005 [222.85601687431335 sec]
step 800: train loss 1.4182, val loss 1.7789 [249.35073971748352 sec]
step 900: train loss 1.3411, val loss 1.7483 [275.13604187965393 sec]
step 1000: train loss 1.2783, val loss 1.7336 [303.7134094238281 sec]
step 1100: train loss 1.2160, val loss 1.7272 [329.27220010757446 sec]
step 1200: train loss 1.1512, val loss 1.7231 [356.98059821128845 sec]
step 1300: train loss 1.0964, val loss 1.7313 [385.42754888534546 sec]
step 1400: train loss 1.0376, val loss 1.7410 [411.5971772670746 sec]
1.1821025609970093
Total Training Time: 422.72253942489624 seconds

anything you." Peoacial
Chans we along that imfe. I learnit will a way lift.
"Surning It returned he
BEGINNING (1682013928.6108813): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6043, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6039, val loss 4.6146 [2.8570668697357178 sec]
step 100: train loss 2.4683, val loss 2.5409 [7.738996744155884 sec]
step 200: train loss 2.1821, val loss 2.3181 [12.606017827987671 sec]
step 300: train loss 2.0219, val loss 2.1769 [17.53684139251709 sec]
step 400: train loss 1.9056, val loss 2.0862 [22.420517206192017 sec]
step 500: train loss 1.8274, val loss 2.0418 [27.443909645080566 sec]
step 600: train loss 1.7693, val loss 1.9965 [32.73035001754761 sec]
step 700: train loss 1.7204, val loss 1.9577 [37.7299439907074 sec]
step 800: train loss 1.6626, val loss 1.9225 [42.45316123962402 sec]
step 900: train loss 1.6335, val loss 1.8942 [47.165268421173096 sec]
step 1000: train loss 1.6101, val loss 1.9074 [51.89494514465332 sec]
step 1100: train loss 1.5808, val loss 1.8704 [56.593512535095215 sec]
step 1200: train loss 1.5492, val loss 1.8498 [61.47011113166809 sec]
step 1300: train loss 1.5171, val loss 1.8471 [67.47466135025024 sec]
step 1400: train loss 1.4977, val loss 1.8483 [72.6444890499115 sec]
1.530448317527771
Total Training Time: 74.86684894561768 seconds

it than on. "May ran segaing Gratta with as hed
our thre our tuon comd. "Ar. Ar he wile as
usuered m
BEGINNING (1682014004.3053787): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5820, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5767, val loss 4.5884 [5.104281425476074 sec]
step 100: train loss 2.4407, val loss 2.5317 [15.197362422943115 sec]
step 200: train loss 2.1456, val loss 2.2922 [24.92660355567932 sec]
step 300: train loss 1.9586, val loss 2.1470 [35.04224467277527 sec]
step 400: train loss 1.8474, val loss 2.0423 [47.527135133743286 sec]
step 500: train loss 1.7466, val loss 1.9851 [57.17377734184265 sec]
step 600: train loss 1.6888, val loss 1.9304 [66.90760278701782 sec]
step 700: train loss 1.6223, val loss 1.9041 [76.16142106056213 sec]
step 800: train loss 1.5795, val loss 1.8533 [85.68474912643433 sec]
step 900: train loss 1.5333, val loss 1.8442 [94.93707752227783 sec]
step 1000: train loss 1.5067, val loss 1.8312 [104.16763591766357 sec]
step 1100: train loss 1.4751, val loss 1.8094 [113.52975821495056 sec]
step 1200: train loss 1.4541, val loss 1.8297 [122.72141361236572 sec]
step 1300: train loss 1.4122, val loss 1.8014 [132.12628531455994 sec]
step 1400: train loss 1.3936, val loss 1.7834 [141.27425336837769 sec]
1.4873688220977783
Total Training Time: 145.52445936203003 seconds

of destiing Beriyah." They ensee.
"Ke!"
Gratta tooked with Gratta. A
change made bodded fent Anayah'
BEGINNING (1682014151.4295604): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6138, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6181, val loss 4.6270 [7.714365243911743 sec]
step 100: train loss 2.4577, val loss 2.5343 [21.061796188354492 sec]
step 200: train loss 2.1571, val loss 2.2830 [34.48759579658508 sec]
step 300: train loss 1.9494, val loss 2.1410 [47.87675929069519 sec]
step 400: train loss 1.8210, val loss 2.0117 [61.183223485946655 sec]
step 500: train loss 1.7134, val loss 1.9635 [74.64547228813171 sec]
step 600: train loss 1.6533, val loss 1.9177 [92.38147497177124 sec]
step 700: train loss 1.5933, val loss 1.8841 [105.64415454864502 sec]
step 800: train loss 1.5391, val loss 1.8476 [119.03670859336853 sec]
step 900: train loss 1.5056, val loss 1.8247 [132.41511917114258 sec]
step 1000: train loss 1.4650, val loss 1.8121 [145.73839163780212 sec]
step 1100: train loss 1.4375, val loss 1.8143 [158.99777913093567 sec]
step 1200: train loss 1.4048, val loss 1.8159 [172.31448364257812 sec]
step 1300: train loss 1.3827, val loss 1.7962 [186.4538118839264 sec]
step 1400: train loss 1.3414, val loss 1.7726 [199.81409883499146 sec]
1.4096256494522095
Total Training Time: 206.1479332447052 seconds

65
CifS AcouTak of Elan. Do themor of pain their lowurection to
for our cartly once more send songed
BEGINNING (1682014359.9827745): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6000, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5894, val loss 4.5906 [3.4383959770202637 sec]
step 100: train loss 2.4949, val loss 2.5732 [9.723960638046265 sec]
step 200: train loss 2.2900, val loss 2.3945 [15.90335202217102 sec]
step 300: train loss 2.0548, val loss 2.2056 [22.203346729278564 sec]
step 400: train loss 1.9054, val loss 2.0918 [28.42437219619751 sec]
step 500: train loss 1.8121, val loss 2.0430 [34.68620991706848 sec]
step 600: train loss 1.7249, val loss 1.9477 [40.90090203285217 sec]
step 700: train loss 1.6617, val loss 1.9039 [47.106468200683594 sec]
step 800: train loss 1.6214, val loss 1.8788 [53.31504988670349 sec]
step 900: train loss 1.5723, val loss 1.8336 [59.56083703041077 sec]
step 1000: train loss 1.5235, val loss 1.8382 [65.764169216156 sec]
step 1100: train loss 1.4986, val loss 1.8020 [71.94185876846313 sec]
step 1200: train loss 1.4586, val loss 1.7862 [78.12940549850464 sec]
step 1300: train loss 1.4318, val loss 1.7853 [84.33430099487305 sec]
step 1400: train loss 1.4033, val loss 1.7567 [90.59839081764221 sec]
1.4600733518600464
Total Training Time: 93.31779527664185 seconds

then his human wonly concil, butthing
a larger. He now, jumpser betherved for could rguzz-growly
and
BEGINNING (1682014454.1535668): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.7096, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6938, val loss 4.6933 [6.322077751159668 sec]
step 100: train loss 2.4769, val loss 2.5620 [17.795902729034424 sec]
step 200: train loss 2.2318, val loss 2.3518 [29.68567657470703 sec]
step 300: train loss 1.9908, val loss 2.1686 [41.238874435424805 sec]
step 400: train loss 1.8372, val loss 2.0441 [52.76361012458801 sec]
step 500: train loss 1.7122, val loss 1.9470 [64.29900002479553 sec]
step 600: train loss 1.6362, val loss 1.8886 [77.2485122680664 sec]
step 700: train loss 1.5670, val loss 1.8468 [88.1519775390625 sec]
step 800: train loss 1.4904, val loss 1.8027 [99.22798156738281 sec]
step 900: train loss 1.4399, val loss 1.7947 [110.48340439796448 sec]
step 1000: train loss 1.4041, val loss 1.7700 [121.47500920295715 sec]
step 1100: train loss 1.3583, val loss 1.7551 [132.38091588020325 sec]
step 1200: train loss 1.3177, val loss 1.7627 [143.41440415382385 sec]
step 1300: train loss 1.2829, val loss 1.7462 [154.35249018669128 sec]
step 1400: train loss 1.2488, val loss 1.7566 [165.35390615463257 sec]
1.3202348947525024
Total Training Time: 170.3126573562622 seconds

the our tenemited unts on the tooss parath retorest offly camp ort.
The many humans tran cell them,

BEGINNING (1682014626.0393734): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5118, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5326, val loss 4.5352 [8.739681243896484 sec]
step 100: train loss 2.4807, val loss 2.5711 [24.588746309280396 sec]
step 200: train loss 2.2476, val loss 2.3858 [40.44931364059448 sec]
step 300: train loss 1.9794, val loss 2.1486 [56.497974157333374 sec]
step 400: train loss 1.7962, val loss 2.0181 [73.433185338974 sec]
step 500: train loss 1.6703, val loss 1.9244 [89.9219765663147 sec]
step 600: train loss 1.5825, val loss 1.8717 [106.40499067306519 sec]
step 700: train loss 1.4997, val loss 1.8286 [123.07787346839905 sec]
step 800: train loss 1.4421, val loss 1.7904 [139.4412522315979 sec]
step 900: train loss 1.3888, val loss 1.7937 [155.53415203094482 sec]
step 1000: train loss 1.3346, val loss 1.7623 [170.90892267227173 sec]
step 1100: train loss 1.2840, val loss 1.7621 [186.26238322257996 sec]
step 1200: train loss 1.2384, val loss 1.7432 [201.6613700389862 sec]
step 1300: train loss 1.2043, val loss 1.7593 [217.03533935546875 sec]
step 1400: train loss 1.1704, val loss 1.7336 [232.60209918022156 sec]
1.293425440788269
Total Training Time: 239.62126898765564 seconds

Sore Arphad and got runiched the goated were will
kinited a conscer's? Was raise dan't for matter re
BEGINNING (1682014868.1812465): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.5266, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5280, val loss 4.5222 [4.947380542755127 sec]
step 100: train loss 2.5168, val loss 2.5976 [13.699761629104614 sec]
step 200: train loss 2.4165, val loss 2.5085 [22.412542581558228 sec]
step 300: train loss 2.2742, val loss 2.3841 [31.346317291259766 sec]
step 400: train loss 2.0875, val loss 2.2446 [40.37966561317444 sec]
step 500: train loss 1.9363, val loss 2.1191 [49.32788681983948 sec]
step 600: train loss 1.8204, val loss 2.0369 [58.31447720527649 sec]
step 700: train loss 1.7370, val loss 1.9745 [67.2775890827179 sec]
step 800: train loss 1.6500, val loss 1.9135 [76.21991038322449 sec]
step 900: train loss 1.5913, val loss 1.8644 [85.2610535621643 sec]
step 1000: train loss 1.5394, val loss 1.8320 [94.21109771728516 sec]
step 1100: train loss 1.4933, val loss 1.8165 [103.18243837356567 sec]
step 1200: train loss 1.4491, val loss 1.7941 [112.17016506195068 sec]
step 1300: train loss 1.4092, val loss 1.7675 [121.11428213119507 sec]
step 1400: train loss 1.3722, val loss 1.7570 [130.11714887619019 sec]
1.4334063529968262
Total Training Time: 133.98187232017517 seconds

turned any had was." Gratta parlace in
cectured attage shown when hild momen stread.
"I sheavel be a
BEGINNING (1682015003.1589022): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6436, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6421, val loss 4.6482 [9.445566415786743 sec]
step 100: train loss 2.4979, val loss 2.5817 [33.71253943443298 sec]
step 200: train loss 2.3815, val loss 2.4870 [50.04411458969116 sec]
step 300: train loss 2.1627, val loss 2.2969 [66.34719324111938 sec]
step 400: train loss 1.9410, val loss 2.1321 [84.02155256271362 sec]
step 500: train loss 1.7885, val loss 2.0168 [101.8901948928833 sec]
step 600: train loss 1.6789, val loss 1.9323 [120.14941334724426 sec]
step 700: train loss 1.5852, val loss 1.8596 [136.38665294647217 sec]
step 800: train loss 1.4956, val loss 1.8189 [152.79223322868347 sec]
step 900: train loss 1.4414, val loss 1.8018 [169.1651132106781 sec]
step 1000: train loss 1.3795, val loss 1.7681 [186.22843050956726 sec]
step 1100: train loss 1.3230, val loss 1.7470 [203.05551433563232 sec]
step 1200: train loss 1.2729, val loss 1.7265 [219.77598118782043 sec]
step 1300: train loss 1.2214, val loss 1.7295 [236.4849967956543 sec]
step 1400: train loss 1.1748, val loss 1.7315 [253.1647093296051 sec]
1.2104601860046387
Total Training Time: 261.47991371154785 seconds

• SEAN McKAY
Namah squal shall of thos the Pyrran turness help dick
fures.
Sunnemy the robes all wou
BEGINNING (1682015266.451954): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6974, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6941, val loss 4.6838 [14.408365726470947 sec]
step 100: train loss 2.4869, val loss 2.5687 [42.74043369293213 sec]
step 200: train loss 2.3583, val loss 2.4557 [67.56471467018127 sec]
step 300: train loss 2.0507, val loss 2.2070 [92.08665490150452 sec]
step 400: train loss 1.8430, val loss 2.0372 [115.97921299934387 sec]
step 500: train loss 1.6870, val loss 1.9324 [139.88454055786133 sec]
step 600: train loss 1.5819, val loss 1.8591 [164.80845427513123 sec]
step 700: train loss 1.4838, val loss 1.8110 [189.4910581111908 sec]
step 800: train loss 1.4063, val loss 1.7801 [213.63555693626404 sec]
step 900: train loss 1.3396, val loss 1.7488 [238.05241680145264 sec]
step 1000: train loss 1.2683, val loss 1.7270 [262.79816794395447 sec]
step 1100: train loss 1.2111, val loss 1.7368 [286.82357120513916 sec]
step 1200: train loss 1.1557, val loss 1.7175 [310.74792671203613 sec]
step 1300: train loss 1.1046, val loss 1.7455 [334.6640968322754 sec]
step 1400: train loss 1.0517, val loss 1.7668 [358.6545088291168 sec]
1.1276371479034424
Total Training Time: 369.3330328464508 seconds

etent is sucand. He sat no so doubt the cubs, so
heard by no ex-tuon raiol from his fathers aways. T
BEGINNING (1682015638.1958869): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5813, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5724, val loss 4.5650 [3.0901525020599365 sec]
step 100: train loss 2.4231, val loss 2.5142 [8.368403911590576 sec]
step 200: train loss 2.1206, val loss 2.2689 [13.668968915939331 sec]
step 300: train loss 1.9452, val loss 2.1275 [18.977193117141724 sec]
step 400: train loss 1.8370, val loss 2.0365 [24.27446413040161 sec]
step 500: train loss 1.7525, val loss 1.9744 [29.54752826690674 sec]
step 600: train loss 1.6993, val loss 1.9339 [34.919294595718384 sec]
step 700: train loss 1.6432, val loss 1.8920 [40.237767696380615 sec]
step 800: train loss 1.6057, val loss 1.8713 [45.52079391479492 sec]
step 900: train loss 1.5741, val loss 1.8508 [50.769535303115845 sec]
step 1000: train loss 1.5390, val loss 1.8510 [56.062180042266846 sec]
step 1100: train loss 1.5075, val loss 1.8121 [61.332194089889526 sec]
step 1200: train loss 1.4863, val loss 1.8109 [66.9857907295227 sec]
step 1300: train loss 1.4555, val loss 1.8104 [72.62858200073242 sec]
step 1400: train loss 1.4267, val loss 1.8012 [78.33160710334778 sec]
1.5418986082077026
Total Training Time: 80.5326840877533 seconds

CHAPTER IV Gratta splecht, onif the sirple?"
"Arna yearge nodded quickled ut with an the
arristurned
BEGINNING (1682015719.497626): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6840, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6833, val loss 4.6775 [5.317931890487671 sec]
step 100: train loss 2.4103, val loss 2.4995 [14.902580976486206 sec]
step 200: train loss 2.0811, val loss 2.2404 [24.609458684921265 sec]
step 300: train loss 1.8936, val loss 2.0855 [34.200499057769775 sec]
step 400: train loss 1.7651, val loss 1.9785 [44.0276083946228 sec]
step 500: train loss 1.6802, val loss 1.9140 [53.49467420578003 sec]
step 600: train loss 1.6243, val loss 1.8742 [63.028205156326294 sec]
step 700: train loss 1.5611, val loss 1.8485 [72.54254484176636 sec]
step 800: train loss 1.5060, val loss 1.8220 [82.23055219650269 sec]
step 900: train loss 1.4641, val loss 1.8026 [92.33396816253662 sec]
step 1000: train loss 1.4182, val loss 1.7893 [102.32810568809509 sec]
step 1100: train loss 1.3852, val loss 1.7937 [111.87650561332703 sec]
step 1200: train loss 1.3723, val loss 1.7855 [121.38511681556702 sec]
step 1300: train loss 1.3243, val loss 1.7740 [130.99529790878296 sec]
step 1400: train loss 1.3113, val loss 1.7772 [140.56834316253662 sec]
1.3893561363220215
Total Training Time: 144.88137817382812 seconds

some wire creaty of to currock
my much, any the jumbe?"
Shill be yellow, teach with Vak my that noSt
BEGINNING (1682015865.9015362): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6608, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6354, val loss 4.6305 [8.180487155914307 sec]
step 100: train loss 2.4047, val loss 2.4945 [21.979553699493408 sec]
step 200: train loss 2.0765, val loss 2.2364 [35.85446095466614 sec]
step 300: train loss 1.8694, val loss 2.0638 [49.77331471443176 sec]
step 400: train loss 1.7410, val loss 1.9717 [63.906137466430664 sec]
step 500: train loss 1.6600, val loss 1.9171 [78.25969743728638 sec]
step 600: train loss 1.5743, val loss 1.8594 [92.37433052062988 sec]
step 700: train loss 1.5112, val loss 1.8365 [106.22793459892273 sec]
step 800: train loss 1.4752, val loss 1.8247 [120.00759363174438 sec]
step 900: train loss 1.4171, val loss 1.7923 [133.7952253818512 sec]
step 1000: train loss 1.3817, val loss 1.8020 [147.7407910823822 sec]
step 1100: train loss 1.3425, val loss 1.7784 [161.48961758613586 sec]
step 1200: train loss 1.3092, val loss 1.7532 [175.88681602478027 sec]
step 1300: train loss 1.2826, val loss 1.7811 [189.9280242919922 sec]
step 1400: train loss 1.2525, val loss 1.7492 [204.11291098594666 sec]
1.395342469215393
Total Training Time: 210.4219868183136 seconds

SEON McKAY
q9ick, shifly foor before he
walkings ht was expened. "He Wiftly for him lead he tacked t
BEGINNING (1682016078.5893826): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6683, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6663, val loss 4.6691 [4.096095085144043 sec]
step 100: train loss 2.4770, val loss 2.5532 [11.220155000686646 sec]
step 200: train loss 2.2166, val loss 2.3348 [18.346893548965454 sec]
step 300: train loss 1.9940, val loss 2.1618 [26.22603964805603 sec]
step 400: train loss 1.8544, val loss 2.0477 [33.292248249053955 sec]
step 500: train loss 1.7419, val loss 1.9688 [41.47589898109436 sec]
step 600: train loss 1.6663, val loss 1.9123 [51.10670018196106 sec]
step 700: train loss 1.5991, val loss 1.8554 [58.41269111633301 sec]
step 800: train loss 1.5502, val loss 1.8619 [65.47015285491943 sec]
step 900: train loss 1.4945, val loss 1.8087 [72.53170704841614 sec]
step 1000: train loss 1.4550, val loss 1.8040 [80.29106545448303 sec]
step 1100: train loss 1.4184, val loss 1.7790 [89.45861506462097 sec]
step 1200: train loss 1.3814, val loss 1.7673 [96.6777970790863 sec]
step 1300: train loss 1.3525, val loss 1.7657 [104.01677870750427 sec]
step 1400: train loss 1.3128, val loss 1.7347 [111.16101026535034 sec]
1.449892520904541
Total Training Time: 114.53440356254578 seconds

smiler. "Command around Ik he will the enembes if sine
at there. "The paths impanint. Gratta ol of t
BEGINNING (1682016194.035061): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5813, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5887, val loss 4.5912 [7.42324161529541 sec]
step 100: train loss 2.4518, val loss 2.5362 [20.526000022888184 sec]
step 200: train loss 2.1551, val loss 2.2904 [34.288408279418945 sec]
step 300: train loss 1.9048, val loss 2.0909 [48.25764298439026 sec]
step 400: train loss 1.7349, val loss 1.9434 [62.47179293632507 sec]
step 500: train loss 1.6334, val loss 1.8956 [75.50041389465332 sec]
step 600: train loss 1.5408, val loss 1.8292 [88.5262086391449 sec]
step 700: train loss 1.4807, val loss 1.8024 [102.59177923202515 sec]
step 800: train loss 1.4175, val loss 1.7748 [115.85686540603638 sec]
step 900: train loss 1.3579, val loss 1.7711 [129.50075769424438 sec]
step 1000: train loss 1.3158, val loss 1.7638 [144.25939321517944 sec]
step 1100: train loss 1.2723, val loss 1.7511 [158.11290287971497 sec]
step 1200: train loss 1.2271, val loss 1.7438 [171.35233855247498 sec]
step 1300: train loss 1.1880, val loss 1.7456 [184.61584329605103 sec]
step 1400: train loss 1.1490, val loss 1.7299 [197.86477422714233 sec]
1.249411940574646
Total Training Time: 203.71243906021118 seconds

Gratta, telled. "Yes, sir this can take in the dustrod's he adjust
over a moore. Soick ent ville sho
BEGINNING (1682016399.281996): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6643, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6596, val loss 4.6575 [12.174509763717651 sec]
step 100: train loss 2.4441, val loss 2.5428 [31.972432613372803 sec]
step 200: train loss 2.1345, val loss 2.2584 [51.03073191642761 sec]
step 300: train loss 1.8560, val loss 2.0554 [70.10572528839111 sec]
step 400: train loss 1.7059, val loss 1.9386 [88.41135048866272 sec]
step 500: train loss 1.5815, val loss 1.8487 [106.5146017074585 sec]
step 600: train loss 1.4902, val loss 1.8022 [126.52808690071106 sec]
step 700: train loss 1.4091, val loss 1.7788 [146.74953699111938 sec]
step 800: train loss 1.3509, val loss 1.7633 [166.66279292106628 sec]
step 900: train loss 1.2799, val loss 1.7552 [186.380797624588 sec]
step 1000: train loss 1.2335, val loss 1.7355 [206.1261932849884 sec]
step 1100: train loss 1.1831, val loss 1.7361 [225.99651861190796 sec]
step 1200: train loss 1.1353, val loss 1.7442 [245.75919842720032 sec]
step 1300: train loss 1.0919, val loss 1.7624 [267.7192041873932 sec]
step 1400: train loss 1.0416, val loss 1.7608 [287.19935870170593 sec]
1.1544549465179443
Total Training Time: 295.851952791214 seconds

me the gan ask you."
"He have the mostes than impensed the give a dayp of
humans, who we looked up.

BEGINNING (1682016697.3833146): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6288, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6223, val loss 4.6209 [6.405013799667358 sec]
step 100: train loss 2.5055, val loss 2.5848 [17.91795516014099 sec]
step 200: train loss 2.3907, val loss 2.4896 [29.42480444908142 sec]
step 300: train loss 2.2064, val loss 2.3337 [40.91486668586731 sec]
step 400: train loss 1.9910, val loss 2.1523 [52.42478561401367 sec]
step 500: train loss 1.8445, val loss 2.0473 [63.92664432525635 sec]
step 600: train loss 1.7324, val loss 1.9666 [76.4348623752594 sec]
step 700: train loss 1.6497, val loss 1.9172 [88.58994126319885 sec]
step 800: train loss 1.5772, val loss 1.8494 [100.07570672035217 sec]
step 900: train loss 1.5164, val loss 1.8185 [111.53470730781555 sec]
step 1000: train loss 1.4605, val loss 1.7836 [120.66834998130798 sec]
step 1100: train loss 1.4143, val loss 1.7724 [131.1467227935791 sec]
step 1200: train loss 1.3682, val loss 1.7523 [142.63096022605896 sec]
step 1300: train loss 1.3364, val loss 1.7496 [154.1092562675476 sec]
step 1400: train loss 1.2911, val loss 1.7359 [165.58743691444397 sec]
1.35930335521698
Total Training Time: 170.57027411460876 seconds

he by I shave this commosing any pats tuons.
"Were – the have oicil as fear." Pelance and got metait
BEGINNING (1682016868.790581): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6930, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6849, val loss 4.6732 [12.012874841690063 sec]
step 100: train loss 2.4830, val loss 2.5670 [33.55547571182251 sec]
step 200: train loss 2.3407, val loss 2.4418 [59.28410863876343 sec]
step 300: train loss 2.0669, val loss 2.2175 [81.27812123298645 sec]
step 400: train loss 1.8518, val loss 2.0629 [103.9072642326355 sec]
step 500: train loss 1.6976, val loss 1.9468 [126.1233081817627 sec]
step 600: train loss 1.5837, val loss 1.8598 [147.70834183692932 sec]
step 700: train loss 1.5011, val loss 1.8117 [169.21316194534302 sec]
step 800: train loss 1.4180, val loss 1.7835 [190.74890303611755 sec]
step 900: train loss 1.3525, val loss 1.7551 [212.26063179969788 sec]
step 1000: train loss 1.2818, val loss 1.7410 [233.71311020851135 sec]
step 1100: train loss 1.2221, val loss 1.7172 [255.1894211769104 sec]
step 1200: train loss 1.1682, val loss 1.7252 [276.71609902381897 sec]
step 1300: train loss 1.1181, val loss 1.7248 [298.2307047843933 sec]
step 1400: train loss 1.0677, val loss 1.7576 [319.85882663726807 sec]
1.14323890209198
Total Training Time: 329.27830362319946 seconds

"I said, a he as had your should not expect two so
treftly. How you."
Gratta smiled take the cubs to
BEGINNING (1682017199.6629207): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.7027, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7085, val loss 4.7050 [17.676876544952393 sec]
step 100: train loss 2.4805, val loss 2.5670 [47.39899730682373 sec]
step 200: train loss 2.3163, val loss 2.4291 [77.10627818107605 sec]
step 300: train loss 1.9986, val loss 2.1693 [106.80576658248901 sec]
step 400: train loss 1.7773, val loss 2.0046 [139.23360419273376 sec]
step 500: train loss 1.6303, val loss 1.9016 [168.52207398414612 sec]
step 600: train loss 1.5067, val loss 1.8242 [194.7291395664215 sec]
step 700: train loss 1.4088, val loss 1.7842 [219.40132355690002 sec]
step 800: train loss 1.3259, val loss 1.7687 [244.59579014778137 sec]
step 900: train loss 1.2478, val loss 1.7410 [269.7697811126709 sec]
step 1000: train loss 1.1793, val loss 1.7527 [294.98418712615967 sec]
step 1100: train loss 1.1032, val loss 1.7415 [323.0527675151825 sec]
step 1200: train loss 1.0370, val loss 1.7559 [352.3697214126587 sec]
step 1300: train loss 0.9672, val loss 1.7777 [381.67460346221924 sec]
step 1400: train loss 0.8904, val loss 1.8148 [410.9224708080292 sec]
0.9833188056945801
Total Training Time: 423.7468264102936 seconds

and nearly waved Gratta open the hundred straight of stry
maeuws.
Gratta moved quickly him he said, 
BEGINNING (1682017625.745451): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6327, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6290, val loss 4.6219 [3.4803340435028076 sec]
step 100: train loss 2.3948, val loss 2.4892 [9.474172353744507 sec]
step 200: train loss 2.0656, val loss 2.2162 [15.462848424911499 sec]
step 300: train loss 1.8891, val loss 2.0800 [21.438273429870605 sec]
step 400: train loss 1.7885, val loss 1.9969 [27.350311517715454 sec]
step 500: train loss 1.7085, val loss 1.9531 [33.22562909126282 sec]
step 600: train loss 1.6442, val loss 1.9189 [39.089648723602295 sec]
step 700: train loss 1.6034, val loss 1.8756 [44.944419860839844 sec]
step 800: train loss 1.5557, val loss 1.8536 [50.8159294128418 sec]
step 900: train loss 1.5220, val loss 1.8475 [56.672240018844604 sec]
step 1000: train loss 1.4910, val loss 1.8326 [62.526854276657104 sec]
step 1100: train loss 1.4597, val loss 1.8038 [68.3489739894867 sec]
step 1200: train loss 1.4270, val loss 1.8064 [74.1923360824585 sec]
step 1300: train loss 1.4000, val loss 1.7961 [80.03804063796997 sec]
step 1400: train loss 1.3856, val loss 1.7889 [85.89694786071777 sec]
1.4642672538757324
Total Training Time: 88.35742568969727 seconds

not deen do
seen all menter. He was
dolded he returned this wall infeares worrea said. "Chief you ha
BEGINNING (1682017714.8714337): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6496, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6561, val loss 4.6542 [5.896818399429321 sec]
step 100: train loss 2.3544, val loss 2.4503 [16.35155463218689 sec]
step 200: train loss 2.0165, val loss 2.1654 [26.815975427627563 sec]
step 300: train loss 1.8178, val loss 2.0178 [37.30519199371338 sec]
step 400: train loss 1.7066, val loss 1.9379 [47.737715005874634 sec]
step 500: train loss 1.6177, val loss 1.8816 [58.186460971832275 sec]
step 600: train loss 1.5500, val loss 1.8471 [68.6431794166565 sec]
step 700: train loss 1.4893, val loss 1.8092 [79.12776041030884 sec]
step 800: train loss 1.4424, val loss 1.8016 [89.58339619636536 sec]
step 900: train loss 1.4057, val loss 1.7847 [100.04300212860107 sec]
step 1000: train loss 1.3670, val loss 1.7860 [110.48422145843506 sec]
step 1100: train loss 1.3356, val loss 1.7704 [120.94458961486816 sec]
step 1200: train loss 1.3012, val loss 1.7693 [131.39299392700195 sec]
step 1300: train loss 1.2764, val loss 1.7911 [143.14725589752197 sec]
step 1400: train loss 1.2475, val loss 1.7693 [154.27029013633728 sec]
1.3737194538116455
Total Training Time: 158.8636019229889 seconds

deepy to reemain. Gratta had had silence hals
be the nail.
Fatta mon the airivalining a happrisonise
BEGINNING (1682017875.2230964): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6515, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6573, val loss 4.6508 [8.388211011886597 sec]
step 100: train loss 2.3593, val loss 2.4535 [23.571353673934937 sec]
step 200: train loss 1.9944, val loss 2.1553 [40.324790716171265 sec]
step 300: train loss 1.8029, val loss 2.0189 [56.98342776298523 sec]
step 400: train loss 1.6581, val loss 1.8994 [72.20669865608215 sec]
step 500: train loss 1.5791, val loss 1.8696 [87.43078446388245 sec]
step 600: train loss 1.5066, val loss 1.8305 [102.60295677185059 sec]
step 700: train loss 1.4479, val loss 1.8149 [117.82200264930725 sec]
step 800: train loss 1.4022, val loss 1.7854 [133.31792449951172 sec]
step 900: train loss 1.3560, val loss 1.7779 [149.07941937446594 sec]
step 1000: train loss 1.3118, val loss 1.7622 [164.84775352478027 sec]
step 1100: train loss 1.2750, val loss 1.7700 [180.55155658721924 sec]
step 1200: train loss 1.2410, val loss 1.7649 [196.3989508152008 sec]
step 1300: train loss 1.2090, val loss 1.7896 [211.80014395713806 sec]
step 1400: train loss 1.1738, val loss 1.7779 [227.11038303375244 sec]
1.2758921384811401
Total Training Time: 233.93649578094482 seconds

between captor cubits from their tribes. Thave lessed
of the prident, and Anayah had threear eyes of
BEGINNING (1682018111.424786): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5968, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5867, val loss 4.5929 [5.053629636764526 sec]
step 100: train loss 2.4581, val loss 2.5452 [13.810192108154297 sec]
step 200: train loss 2.1642, val loss 2.3046 [22.558173418045044 sec]
step 300: train loss 1.9300, val loss 2.1136 [31.251127004623413 sec]
step 400: train loss 1.7915, val loss 2.0101 [40.432138204574585 sec]
step 500: train loss 1.6900, val loss 1.9350 [49.20361828804016 sec]
step 600: train loss 1.6085, val loss 1.8825 [58.23354482650757 sec]
step 700: train loss 1.5474, val loss 1.8437 [66.85357403755188 sec]
step 800: train loss 1.5020, val loss 1.8174 [76.0541775226593 sec]
step 900: train loss 1.4503, val loss 1.7982 [84.77740502357483 sec]
step 1000: train loss 1.4026, val loss 1.7834 [93.35063195228577 sec]
step 1100: train loss 1.3708, val loss 1.7722 [101.91579508781433 sec]
step 1200: train loss 1.3363, val loss 1.7744 [110.52797055244446 sec]
step 1300: train loss 1.2982, val loss 1.7496 [119.15246367454529 sec]
step 1400: train loss 1.2667, val loss 1.7469 [127.90449380874634 sec]
1.3874659538269043
Total Training Time: 131.5940318107605 seconds

yhurght to you decent and mooned, but here, sack wairs
they."
Taeuw of all me think foollight. Nimal
BEGINNING (1682018243.7892683): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6262, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6267, val loss 4.6230 [9.659179449081421 sec]
step 100: train loss 2.4312, val loss 2.5174 [25.8701913356781 sec]
step 200: train loss 2.1109, val loss 2.2447 [43.76917576789856 sec]
step 300: train loss 1.8542, val loss 2.0519 [60.98804831504822 sec]
step 400: train loss 1.6948, val loss 1.9260 [75.07440185546875 sec]
step 500: train loss 1.5783, val loss 1.8636 [87.83476638793945 sec]
step 600: train loss 1.4873, val loss 1.8051 [100.56838059425354 sec]
step 700: train loss 1.4150, val loss 1.7817 [114.41636300086975 sec]
step 800: train loss 1.3611, val loss 1.7496 [131.73766899108887 sec]
step 900: train loss 1.3024, val loss 1.7404 [147.94833636283875 sec]
step 1000: train loss 1.2434, val loss 1.7335 [167.7437264919281 sec]
step 1100: train loss 1.1982, val loss 1.7379 [184.3980565071106 sec]
step 1200: train loss 1.1521, val loss 1.7467 [203.05378413200378 sec]
step 1300: train loss 1.1020, val loss 1.7483 [222.97093963623047 sec]
step 1400: train loss 1.0584, val loss 1.7705 [241.314453125 sec]
1.1135153770446777
Total Training Time: 248.3366174697876 seconds

them. Have whistled will people did what have a company of
this treaty that his eye." Gratta, who pr
BEGINNING (1682018493.6391802): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6160, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6235, val loss 4.6216 [12.856514692306519 sec]
step 100: train loss 2.4382, val loss 2.5256 [37.96699237823486 sec]
step 200: train loss 2.1192, val loss 2.2619 [60.81307029724121 sec]
step 300: train loss 1.8290, val loss 2.0365 [85.70215082168579 sec]
step 400: train loss 1.6577, val loss 1.9186 [111.06282758712769 sec]
step 500: train loss 1.5295, val loss 1.8308 [136.05122995376587 sec]
step 600: train loss 1.4204, val loss 1.7666 [159.07014346122742 sec]
step 700: train loss 1.3538, val loss 1.7748 [181.98031997680664 sec]
step 800: train loss 1.2723, val loss 1.7669 [204.87462830543518 sec]
step 900: train loss 1.2135, val loss 1.7372 [227.7951169013977 sec]
step 1000: train loss 1.1484, val loss 1.7434 [250.71068120002747 sec]
step 1100: train loss 1.0916, val loss 1.7427 [273.65206956863403 sec]
step 1200: train loss 1.0376, val loss 1.7743 [296.53742933273315 sec]
step 1300: train loss 0.9842, val loss 1.7900 [321.4119145870209 sec]
step 1400: train loss 0.9321, val loss 1.8023 [346.5609257221222 sec]
1.0383875370025635
Total Training Time: 356.96392250061035 seconds

small as he blood. "We dis! Chief
Gratta, home slumped different even has bearned on." Gor
wassa and
BEGINNING (1682018852.9918323): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5750, val loss 4.5717 [8.237191438674927 sec]
step 100: train loss 2.4978, val loss 2.5812 [22.987510204315186 sec]
step 200: train loss 2.3497, val loss 2.4519 [37.20028877258301 sec]
step 300: train loss 2.1348, val loss 2.2769 [51.20686745643616 sec]
step 400: train loss 1.9290, val loss 2.1176 [65.51624798774719 sec]
step 500: train loss 1.7879, val loss 1.9986 [80.16347908973694 sec]
step 600: train loss 1.6747, val loss 1.9239 [94.2099256515503 sec]
step 700: train loss 1.5919, val loss 1.8672 [108.48362708091736 sec]
step 800: train loss 1.5202, val loss 1.8215 [122.20494294166565 sec]
step 900: train loss 1.4596, val loss 1.7835 [135.9308226108551 sec]
step 1000: train loss 1.4043, val loss 1.7715 [149.78242921829224 sec]
step 1100: train loss 1.3514, val loss 1.7375 [163.7275788784027 sec]
step 1200: train loss 1.3104, val loss 1.7280 [177.440354347229 sec]
step 1300: train loss 1.2677, val loss 1.7156 [194.10227131843567 sec]
step 1400: train loss 1.2228, val loss 1.7161 [209.7154791355133 sec]
1.2867307662963867
Total Training Time: 217.10568284988403 seconds

them. Arphad
whist ince! Polisies sade fish Clans oncilisies
unbits in the cubs, but wem last. It re
BEGINNING (1682019071.010156): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6212, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6305, val loss 4.6304 [19.715873956680298 sec]
step 100: train loss 2.4766, val loss 2.5568 [43.66400408744812 sec]
step 200: train loss 2.2926, val loss 2.4078 [70.27419638633728 sec]
step 300: train loss 1.9799, val loss 2.1570 [101.01450252532959 sec]
step 400: train loss 1.7840, val loss 2.0125 [127.89582014083862 sec]
step 500: train loss 1.6417, val loss 1.8972 [159.35267400741577 sec]
step 600: train loss 1.5243, val loss 1.8300 [185.90561962127686 sec]
step 700: train loss 1.4340, val loss 1.7802 [212.75829339027405 sec]
step 800: train loss 1.3543, val loss 1.7466 [239.32596492767334 sec]
step 900: train loss 1.2753, val loss 1.7187 [269.2877905368805 sec]
step 1000: train loss 1.2124, val loss 1.7311 [294.45428943634033 sec]
step 1100: train loss 1.1465, val loss 1.7284 [322.27433681488037 sec]
step 1200: train loss 1.0820, val loss 1.7133 [353.38924264907837 sec]
step 1300: train loss 1.0264, val loss 1.7374 [389.9798595905304 sec]
step 1400: train loss 0.9563, val loss 1.7657 [414.9309916496277 sec]
1.0721689462661743
Total Training Time: 427.40914940834045 seconds

the montainly had here boselness, wish
yells the can appearted them. I ent the enty
tranges as over 
BEGINNING (1682019500.0243204): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6251, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6143, val loss 4.6216 [23.106391429901123 sec]
step 100: train loss 2.4714, val loss 2.5484 [69.95388197898865 sec]
step 200: train loss 2.2916, val loss 2.4045 [113.43183135986328 sec]
step 300: train loss 1.9464, val loss 2.1264 [155.39629340171814 sec]
step 400: train loss 1.7144, val loss 1.9493 [197.66601967811584 sec]
step 500: train loss 1.5599, val loss 1.8529 [235.29431772232056 sec]
step 600: train loss 1.4309, val loss 1.7923 [280.45707416534424 sec]
step 700: train loss 1.3310, val loss 1.7542 [325.219464302063 sec]
step 800: train loss 1.2377, val loss 1.7222 [368.9749183654785 sec]
step 900: train loss 1.1652, val loss 1.7303 [416.774516582489 sec]
step 1000: train loss 1.0766, val loss 1.7397 [460.39704418182373 sec]
step 1100: train loss 0.9969, val loss 1.7582 [499.14621925354004 sec]
step 1200: train loss 0.9104, val loss 1.7802 [539.6875467300415 sec]
step 1300: train loss 0.8335, val loss 1.8165 [585.3103423118591 sec]
step 1400: train loss 0.7537, val loss 1.8816 [625.7367444038391 sec]
0.8649119138717651
Total Training Time: 641.8359642028809 seconds

McKAY
ENERKION
lood is the mands, who reaching them back for the celt, and
guard rarised and out the
BEGINNING (1682020144.4080286): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.5943, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5927, val loss 4.5952 [4.231899738311768 sec]
step 100: train loss 2.3861, val loss 2.4871 [12.330634593963623 sec]
step 200: train loss 2.0766, val loss 2.2359 [19.815456867218018 sec]
step 300: train loss 1.9084, val loss 2.1024 [27.197518348693848 sec]
step 400: train loss 1.8022, val loss 2.0226 [34.36380648612976 sec]
step 500: train loss 1.7263, val loss 1.9715 [41.51088213920593 sec]
step 600: train loss 1.6700, val loss 1.9179 [48.703404664993286 sec]
step 700: train loss 1.6126, val loss 1.8900 [55.80567383766174 sec]
step 800: train loss 1.5814, val loss 1.8716 [62.893311738967896 sec]
step 900: train loss 1.5405, val loss 1.8466 [70.06542110443115 sec]
step 1000: train loss 1.5079, val loss 1.8487 [77.2008912563324 sec]
step 1100: train loss 1.4743, val loss 1.8143 [85.63574743270874 sec]
step 1200: train loss 1.4514, val loss 1.8074 [96.45619535446167 sec]
step 1300: train loss 1.4254, val loss 1.7997 [104.17127180099487 sec]
step 1400: train loss 1.3978, val loss 1.7863 [113.15365862846375 sec]
1.502355933189392
Total Training Time: 116.49931859970093 seconds

looked for a guard flal shielY
is inted to were verriwed. This the firestraftes. "Demed drushinfing 
BEGINNING (1682020262.2802422): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.7170, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6961, val loss 4.6878 [8.010778903961182 sec]
step 100: train loss 2.3945, val loss 2.4894 [21.686928510665894 sec]
step 200: train loss 2.0662, val loss 2.2307 [35.119502544403076 sec]
step 300: train loss 1.8903, val loss 2.1132 [48.45784854888916 sec]
step 400: train loss 1.7562, val loss 1.9967 [61.90839409828186 sec]
step 500: train loss 1.6720, val loss 1.9490 [75.28320860862732 sec]
step 600: train loss 1.6070, val loss 1.8745 [89.30478763580322 sec]
step 700: train loss 1.5551, val loss 1.8677 [109.99768376350403 sec]
step 800: train loss 1.5284, val loss 1.8618 [126.16863298416138 sec]
step 900: train loss 1.4780, val loss 1.8325 [140.11909985542297 sec]
step 1000: train loss 1.4296, val loss 1.7960 [153.91069769859314 sec]
step 1100: train loss 1.4016, val loss 1.8094 [167.40749263763428 sec]
step 1200: train loss 1.3710, val loss 1.7959 [181.038010597229 sec]
step 1300: train loss 1.3357, val loss 1.7778 [195.54671955108643 sec]
step 1400: train loss 1.3105, val loss 1.8022 [213.6298930644989 sec]
1.434526801109314
Total Training Time: 221.6160752773285 seconds

 two find the fearfuft they fore.
Your to night all of their the PProintrates."
Pelana's commangotia
BEGINNING (1682020486.5432243): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6378, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6300, val loss 4.6268 [10.733164310455322 sec]
step 100: train loss 2.4297, val loss 2.5185 [30.158469915390015 sec]
step 200: train loss 2.0896, val loss 2.2469 [49.65787744522095 sec]
step 300: train loss 1.8828, val loss 2.0799 [70.72070598602295 sec]
step 400: train loss 1.7457, val loss 1.9727 [91.72793531417847 sec]
step 500: train loss 1.6640, val loss 1.9314 [112.00164604187012 sec]
step 600: train loss 1.5879, val loss 1.8776 [134.42101192474365 sec]
step 700: train loss 1.5190, val loss 1.8514 [157.87790727615356 sec]
step 800: train loss 1.4784, val loss 1.8126 [177.2991533279419 sec]
step 900: train loss 1.4419, val loss 1.8132 [196.72548007965088 sec]
step 1000: train loss 1.3933, val loss 1.8004 [216.2262361049652 sec]
step 1100: train loss 1.3563, val loss 1.8132 [235.66409993171692 sec]
step 1200: train loss 1.3284, val loss 1.7818 [255.14160466194153 sec]
step 1300: train loss 1.2884, val loss 1.7974 [274.6272518634796 sec]
step 1400: train loss 1.2625, val loss 1.7897 [294.08355379104614 sec]
1.2649532556533813
Total Training Time: 303.0833945274353 seconds

Aiddo, bothing, and
quirthers of these arrives. "The smumpost your nusts war brike meant,
as a we Ta
BEGINNING (1682020792.9538987): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.5489, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5470, val loss 4.5488 [4.955132484436035 sec]
step 100: train loss 2.4469, val loss 2.5431 [14.123941898345947 sec]
step 200: train loss 2.1446, val loss 2.2942 [23.270857334136963 sec]
step 300: train loss 1.8993, val loss 2.0930 [32.43607997894287 sec]
step 400: train loss 1.7768, val loss 2.0062 [41.58929514884949 sec]
step 500: train loss 1.6761, val loss 1.9330 [50.743895053863525 sec]
step 600: train loss 1.6033, val loss 1.8938 [59.8927698135376 sec]
step 700: train loss 1.5376, val loss 1.8357 [69.1001079082489 sec]
step 800: train loss 1.4804, val loss 1.8297 [78.29548835754395 sec]
step 900: train loss 1.4282, val loss 1.8003 [87.64683222770691 sec]
step 1000: train loss 1.3883, val loss 1.7955 [97.03345847129822 sec]
step 1100: train loss 1.3507, val loss 1.7888 [106.21692299842834 sec]
step 1200: train loss 1.3095, val loss 1.7784 [115.36694145202637 sec]
step 1300: train loss 1.2755, val loss 1.7737 [124.59505128860474 sec]
step 1400: train loss 1.2426, val loss 1.7758 [133.75821495056152 sec]
1.3072702884674072
Total Training Time: 137.94727063179016 seconds

a truswed
Gratta toward this him, and if was posepon this he kindesso worring to be furse

overiy of
BEGINNING (1682020932.0549889): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6322, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6429, val loss 4.6462 [9.305529117584229 sec]
step 100: train loss 2.4449, val loss 2.5423 [26.76638913154602 sec]
step 200: train loss 2.1284, val loss 2.2694 [44.199849367141724 sec]
step 300: train loss 1.8531, val loss 2.0584 [61.63901877403259 sec]
step 400: train loss 1.7036, val loss 1.9343 [79.57325291633606 sec]
step 500: train loss 1.5991, val loss 1.8883 [98.7175989151001 sec]
step 600: train loss 1.5059, val loss 1.8328 [116.23483037948608 sec]
step 700: train loss 1.4380, val loss 1.8190 [135.53553009033203 sec]
step 800: train loss 1.3793, val loss 1.7918 [154.18102192878723 sec]
step 900: train loss 1.3199, val loss 1.7803 [175.01737761497498 sec]
step 1000: train loss 1.2720, val loss 1.7838 [194.1773543357849 sec]
step 1100: train loss 1.2250, val loss 1.7632 [211.84968328475952 sec]
step 1200: train loss 1.1709, val loss 1.7605 [229.61682653427124 sec]
step 1300: train loss 1.1362, val loss 1.7822 [247.51190209388733 sec]
step 1400: train loss 1.0879, val loss 1.8015 [265.1110134124756 sec]
1.2151954174041748
Total Training Time: 273.3534803390503 seconds

how  a genert humal slaved. They no neaving for tuons
squirre who a carrow, and looked at Arphad qui
BEGINNING (1682021207.7030742): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5781, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5909, val loss 4.5922 [13.880630731582642 sec]
step 100: train loss 2.4705, val loss 2.5575 [39.744186878204346 sec]
step 200: train loss 2.1884, val loss 2.3185 [65.37964534759521 sec]
step 300: train loss 1.8968, val loss 2.1043 [92.99219369888306 sec]
step 400: train loss 1.7042, val loss 1.9379 [121.05761528015137 sec]
step 500: train loss 1.5850, val loss 1.8806 [147.10890197753906 sec]
step 600: train loss 1.4908, val loss 1.8376 [173.4707968235016 sec]
step 700: train loss 1.4110, val loss 1.7963 [199.5206000804901 sec]
step 800: train loss 1.3520, val loss 1.7798 [226.40724873542786 sec]
step 900: train loss 1.2764, val loss 1.7719 [253.5885307788849 sec]
step 1000: train loss 1.2217, val loss 1.7589 [279.65322947502136 sec]
step 1100: train loss 1.1729, val loss 1.7734 [305.4173963069916 sec]
step 1200: train loss 1.1092, val loss 1.7739 [331.2284607887268 sec]
step 1300: train loss 1.0658, val loss 1.7949 [356.97810983657837 sec]
step 1400: train loss 1.0101, val loss 1.8129 [382.8082466125488 sec]
1.0772960186004639
Total Training Time: 394.8888852596283 seconds

cord of the sathem, we Living," Lamek realing. The sat
the Pning sit and consed we scowls, Gratta in
BEGINNING (1682021605.9636781): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6745, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6793, val loss 4.6793 [8.067442655563354 sec]
step 100: train loss 2.4843, val loss 2.5813 [22.659518241882324 sec]
step 200: train loss 2.3220, val loss 2.4345 [37.63813805580139 sec]
step 300: train loss 2.1116, val loss 2.2781 [52.21119570732117 sec]
step 400: train loss 1.8864, val loss 2.0845 [67.40356254577637 sec]
step 500: train loss 1.7366, val loss 1.9534 [82.27679753303528 sec]
step 600: train loss 1.6343, val loss 1.8843 [96.98273229598999 sec]
step 700: train loss 1.5454, val loss 1.8311 [111.54749727249146 sec]
step 800: train loss 1.4809, val loss 1.8260 [126.29047393798828 sec]
step 900: train loss 1.4105, val loss 1.7822 [141.10518956184387 sec]
step 1000: train loss 1.3588, val loss 1.7689 [155.76001572608948 sec]
step 1100: train loss 1.3016, val loss 1.7483 [172.0439076423645 sec]
step 1200: train loss 1.2542, val loss 1.7296 [186.7088623046875 sec]
step 1300: train loss 1.2056, val loss 1.7288 [201.34581112861633 sec]
step 1400: train loss 1.1528, val loss 1.7320 [215.93798065185547 sec]
1.2391341924667358
Total Training Time: 222.45701360702515 seconds

Namal and seemined to again." Anayah looked forward
again and and failish stiled t his are mose aver
BEGINNING (1682021829.60934): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6221, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6205, val loss 4.6191 [15.325650930404663 sec]
step 100: train loss 2.4865, val loss 2.5704 [42.77981162071228 sec]
step 200: train loss 2.3173, val loss 2.4320 [74.00726580619812 sec]
step 300: train loss 1.9928, val loss 2.1752 [101.19906783103943 sec]
step 400: train loss 1.7669, val loss 1.9871 [128.64461159706116 sec]
step 500: train loss 1.6114, val loss 1.8876 [157.81734466552734 sec]
step 600: train loss 1.5002, val loss 1.8237 [185.53740572929382 sec]
step 700: train loss 1.4096, val loss 1.7786 [212.67765712738037 sec]
step 800: train loss 1.3343, val loss 1.7886 [241.99020147323608 sec]
step 900: train loss 1.2539, val loss 1.7499 [269.9112048149109 sec]
step 1000: train loss 1.1900, val loss 1.7368 [297.1804552078247 sec]
step 1100: train loss 1.1235, val loss 1.7552 [323.99432730674744 sec]
step 1200: train loss 1.0602, val loss 1.8012 [351.3373303413391 sec]
step 1300: train loss 0.9916, val loss 1.7935 [378.7756745815277 sec]
step 1400: train loss 0.9250, val loss 1.8120 [406.0441846847534 sec]
1.104871153831482
Total Training Time: 418.15908575057983 seconds

families looked at Anayah. "
bowed scomy
with mes, and Anayah we canot this treaty into the Pyrran C
BEGINNING (1682022250.1040246): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6117, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6126, val loss 4.6176 [22.88257074356079 sec]
step 100: train loss 2.4892, val loss 2.5794 [58.63033437728882 sec]
step 200: train loss 2.3502, val loss 2.4685 [93.27055740356445 sec]
step 300: train loss 2.0542, val loss 2.2251 [127.47387099266052 sec]
step 400: train loss 1.7877, val loss 2.0082 [162.24551224708557 sec]
step 500: train loss 1.6102, val loss 1.9001 [197.60558891296387 sec]
step 600: train loss 1.4751, val loss 1.8227 [232.10723423957825 sec]
step 700: train loss 1.3662, val loss 1.7990 [266.7737195491791 sec]
step 800: train loss 1.2720, val loss 1.7555 [301.40987634658813 sec]
step 900: train loss 1.1958, val loss 1.7604 [335.97182726860046 sec]
step 1000: train loss 1.0960, val loss 1.7693 [373.9007406234741 sec]
step 1100: train loss 1.0149, val loss 1.7859 [409.67094016075134 sec]
step 1200: train loss 0.9335, val loss 1.8243 [445.4552729129791 sec]
step 1300: train loss 0.8529, val loss 1.8640 [480.4689977169037 sec]
step 1400: train loss 0.7616, val loss 1.9026 [517.0442354679108 sec]
0.8693640828132629
Total Training Time: 532.6755895614624 seconds

"Can if Gor ran an of the tuon Tribes an bit
words?" The cillies wital's all and his report wined.
T
BEGINNING (1682022786.3196735): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5105, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5074, val loss 4.4981 [4.432044982910156 sec]
step 100: train loss 2.3309, val loss 2.4307 [12.54423475265503 sec]
step 200: train loss 2.0189, val loss 2.1870 [20.6424617767334 sec]
step 300: train loss 1.8445, val loss 2.0451 [28.818090200424194 sec]
step 400: train loss 1.7470, val loss 1.9843 [37.00025415420532 sec]
step 500: train loss 1.6588, val loss 1.9333 [45.257728815078735 sec]
step 600: train loss 1.5940, val loss 1.9006 [53.37584686279297 sec]
step 700: train loss 1.5357, val loss 1.8558 [61.61943578720093 sec]
step 800: train loss 1.5057, val loss 1.8447 [69.89339637756348 sec]
step 900: train loss 1.4616, val loss 1.8409 [78.28993964195251 sec]
step 1000: train loss 1.4351, val loss 1.8254 [86.58018350601196 sec]
step 1100: train loss 1.4073, val loss 1.8270 [94.8381655216217 sec]
step 1200: train loss 1.3656, val loss 1.7960 [103.03925061225891 sec]
step 1300: train loss 1.3460, val loss 1.8057 [111.26214551925659 sec]
step 1400: train loss 1.3123, val loss 1.8128 [119.54680156707764 sec]
1.384576439857483
Total Training Time: 123.20764183998108 seconds

Priiests monisuach Aidden pould beforel of you mysenter had bors of
32
74
CHAPTHE PERAY
Gratta turne
BEGINNING (1682022910.741441): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.6533, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6573, val loss 4.6644 [8.273559331893921 sec]
step 100: train loss 2.3203, val loss 2.4258 [23.791465044021606 sec]
step 200: train loss 1.9525, val loss 2.1277 [39.166508436203 sec]
step 300: train loss 1.7936, val loss 2.0114 [54.27118754386902 sec]
step 400: train loss 1.6577, val loss 1.9201 [69.88868165016174 sec]
step 500: train loss 1.5743, val loss 1.8672 [85.03964257240295 sec]
step 600: train loss 1.5246, val loss 1.8366 [100.47370052337646 sec]
step 700: train loss 1.4554, val loss 1.8257 [115.89163112640381 sec]
step 800: train loss 1.4071, val loss 1.7955 [131.43999314308167 sec]
step 900: train loss 1.3717, val loss 1.8003 [146.66486287117004 sec]
step 1000: train loss 1.3221, val loss 1.7908 [162.27496814727783 sec]
step 1100: train loss 1.3034, val loss 1.7925 [177.69989323616028 sec]
step 1200: train loss 1.2536, val loss 1.7639 [193.2370753288269 sec]
step 1300: train loss 1.2237, val loss 1.7728 [208.6730933189392 sec]
step 1400: train loss 1.1969, val loss 1.7947 [223.8487720489502 sec]
1.2704590559005737
Total Training Time: 230.91958832740784 seconds

Yes, I will didn't laugh. Gratta monger,
but the enough are to the gold. The said as aerys of the ai
BEGINNING (1682023143.9158645): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.6760, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6856, val loss 4.6808 [11.783510208129883 sec]
step 100: train loss 2.3720, val loss 2.4766 [34.3118155002594 sec]
step 200: train loss 1.9806, val loss 2.1583 [58.701064348220825 sec]
step 300: train loss 1.7901, val loss 2.0041 [80.94106817245483 sec]
step 400: train loss 1.6709, val loss 1.9354 [103.5386893749237 sec]
step 500: train loss 1.5635, val loss 1.8716 [126.19731783866882 sec]
step 600: train loss 1.4982, val loss 1.8367 [149.08640122413635 sec]
step 700: train loss 1.4430, val loss 1.8318 [172.06935691833496 sec]
step 800: train loss 1.3879, val loss 1.8004 [194.2369999885559 sec]
step 900: train loss 1.3381, val loss 1.7953 [216.3718752861023 sec]
step 1000: train loss 1.3025, val loss 1.7839 [238.53265237808228 sec]
step 1100: train loss 1.2595, val loss 1.7992 [260.70561170578003 sec]
step 1200: train loss 1.2191, val loss 1.7834 [282.8539216518402 sec]
step 1300: train loss 1.1791, val loss 1.7794 [305.0440034866333 sec]
step 1400: train loss 1.1546, val loss 1.7976 [327.1868345737457 sec]
1.2436609268188477
Total Training Time: 337.58285546302795 seconds

was standingtly. Do notiant the was still asshoul want wafte the
scorraws were black ting the mient.
BEGINNING (1682023484.8502538): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6244, val loss 4.6251 [8.42689561843872 sec]
step 100: train loss 2.4121, val loss 2.5050 [21.117806673049927 sec]
step 200: train loss 2.0518, val loss 2.2019 [33.09249258041382 sec]
step 300: train loss 1.8488, val loss 2.0542 [44.79722809791565 sec]
step 400: train loss 1.7016, val loss 1.9416 [56.6107120513916 sec]
step 500: train loss 1.6032, val loss 1.8820 [68.27358317375183 sec]
step 600: train loss 1.5169, val loss 1.8231 [80.0387191772461 sec]
step 700: train loss 1.4606, val loss 1.8097 [91.74302077293396 sec]
step 800: train loss 1.4020, val loss 1.7772 [103.70950627326965 sec]
step 900: train loss 1.3528, val loss 1.7749 [115.59625196456909 sec]
step 1000: train loss 1.2992, val loss 1.7494 [127.36795830726624 sec]
step 1100: train loss 1.2613, val loss 1.7538 [139.17410469055176 sec]
step 1200: train loss 1.2217, val loss 1.7568 [150.8614945411682 sec]
step 1300: train loss 1.1840, val loss 1.7617 [162.4839973449707 sec]
step 1400: train loss 1.1456, val loss 1.7753 [174.9727349281311 sec]
1.1932287216186523
Total Training Time: 180.27304697036743 seconds

shoor the Torial."
"Be is Gor thank in that shekels."
yelled with the paralav feen an Gratta saidned
BEGINNING (1682023666.3615487): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5597, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5650, val loss 4.5683 [12.913094282150269 sec]
step 100: train loss 2.4242, val loss 2.5135 [35.510563135147095 sec]
step 200: train loss 2.0303, val loss 2.1840 [58.19911980628967 sec]
step 300: train loss 1.7641, val loss 1.9831 [80.33669805526733 sec]
step 400: train loss 1.6171, val loss 1.8774 [102.3429503440857 sec]
step 500: train loss 1.5028, val loss 1.8207 [124.32086181640625 sec]
step 600: train loss 1.4107, val loss 1.7760 [146.28617453575134 sec]
step 700: train loss 1.3374, val loss 1.7624 [168.2289595603943 sec]
step 800: train loss 1.2768, val loss 1.7700 [190.22650814056396 sec]
step 900: train loss 1.2122, val loss 1.7443 [213.00815868377686 sec]
step 1000: train loss 1.1609, val loss 1.7521 [235.137845993042 sec]
step 1100: train loss 1.1037, val loss 1.7622 [257.190185546875 sec]
step 1200: train loss 1.0486, val loss 1.8030 [279.2335388660431 sec]
step 1300: train loss 0.9944, val loss 1.8137 [301.2413969039917 sec]
step 1400: train loss 0.9392, val loss 1.8373 [323.3217875957489 sec]
1.0270062685012817
Total Training Time: 333.17816710472107 seconds

so whqoilebring thought these rested to face the bridge. He was
so not comeone of tuon first, and ye
BEGINNING (1682024001.9573503): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5349, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5036, val loss 4.5038 [17.835833311080933 sec]
step 100: train loss 2.4407, val loss 2.5367 [49.36216068267822 sec]
step 200: train loss 2.0916, val loss 2.2514 [81.46901679039001 sec]
step 300: train loss 1.8075, val loss 2.0245 [113.0902807712555 sec]
step 400: train loss 1.6146, val loss 1.8978 [145.07527828216553 sec]
step 500: train loss 1.4920, val loss 1.8222 [176.66242909431458 sec]
step 600: train loss 1.3982, val loss 1.8077 [208.84714365005493 sec]
step 700: train loss 1.2967, val loss 1.7704 [245.8748335838318 sec]
step 800: train loss 1.2269, val loss 1.7630 [279.51640915870667 sec]
step 900: train loss 1.1551, val loss 1.7819 [323.85554575920105 sec]
step 1000: train loss 1.0743, val loss 1.7881 [357.2371413707733 sec]
step 1100: train loss 1.0016, val loss 1.8074 [394.1670560836792 sec]
step 1200: train loss 0.9426, val loss 1.8431 [428.676301240921 sec]
step 1300: train loss 0.8670, val loss 1.8661 [461.7651855945587 sec]
step 1400: train loss 0.8003, val loss 1.9171 [496.79622745513916 sec]
0.9504243731498718
Total Training Time: 512.1871311664581 seconds

what was they have not frathers sile that what marched
the bridge, back the Pilisital spit. A jiddim
BEGINNING (1682024518.2830677): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5738, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5751, val loss 4.5764 [16.634182453155518 sec]
step 100: train loss 2.4686, val loss 2.5487 [38.25055193901062 sec]
step 200: train loss 2.2923, val loss 2.4060 [59.96219038963318 sec]
step 300: train loss 2.0035, val loss 2.1770 [83.84184741973877 sec]
step 400: train loss 1.7936, val loss 2.0257 [103.51970481872559 sec]
step 500: train loss 1.6423, val loss 1.9036 [123.54715919494629 sec]
step 600: train loss 1.5293, val loss 1.8327 [144.18744587898254 sec]
step 700: train loss 1.4527, val loss 1.8079 [165.21625542640686 sec]
step 800: train loss 1.3725, val loss 1.7641 [186.78818321228027 sec]
step 900: train loss 1.3167, val loss 1.7477 [210.52620005607605 sec]
step 1000: train loss 1.2525, val loss 1.7617 [235.23975682258606 sec]
step 1100: train loss 1.1919, val loss 1.7628 [257.4078242778778 sec]
step 1200: train loss 1.1352, val loss 1.7545 [280.85263109207153 sec]
step 1300: train loss 1.0786, val loss 1.7712 [304.77375292778015 sec]
step 1400: train loss 1.0265, val loss 1.7903 [328.6288537979126 sec]
1.1227980852127075
Total Training Time: 337.296683549881 seconds

it it Pilisia – Anayah his adge men cauew.
21
Sean laparge rody at the markethed. He opened the horn
BEGINNING (1682024856.9481804): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.7134, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7114, val loss 4.7037 [23.996398210525513 sec]
step 100: train loss 2.4686, val loss 2.5634 [70.81932592391968 sec]
step 200: train loss 2.2539, val loss 2.3780 [135.3895149230957 sec]
step 300: train loss 1.8854, val loss 2.0689 [202.0181601047516 sec]
step 400: train loss 1.6775, val loss 1.9333 [255.7231593132019 sec]
step 500: train loss 1.5250, val loss 1.8455 [308.17315649986267 sec]
step 600: train loss 1.4089, val loss 1.7855 [362.1597673892975 sec]
step 700: train loss 1.3074, val loss 1.7585 [419.9644832611084 sec]
step 800: train loss 1.2146, val loss 1.7419 [540.3348648548126 sec]
step 900: train loss 1.1330, val loss 1.7403 [628.9430248737335 sec]
step 1000: train loss 1.0532, val loss 1.7701 [719.9524788856506 sec]
step 1100: train loss 0.9620, val loss 1.7949 [804.5213310718536 sec]
step 1200: train loss 0.8844, val loss 1.8082 [896.1235685348511 sec]
step 1300: train loss 0.7940, val loss 1.8764 [974.3593738079071 sec]
step 1400: train loss 0.7129, val loss 1.9335 [1073.6529088020325 sec]
0.813141405582428
Total Training Time: 1117.5614528656006 seconds

would sniff. "Come hercouncil
on them, find give again. It offer they ubs because my
hief Ar?" Anaya
BEGINNING (1682025977.1880946): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5723, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5727, val loss 4.5744 [68.03193807601929 sec]
step 100: train loss 2.4756, val loss 2.5577 [202.40879845619202 sec]
step 200: train loss 2.3055, val loss 2.4236 [352.59265398979187 sec]
step 300: train loss 1.9191, val loss 2.1217 [444.21888160705566 sec]
step 400: train loss 1.6646, val loss 1.9450 [577.1640210151672 sec]
step 500: train loss 1.4992, val loss 1.8314 [735.7254958152771 sec]
step 600: train loss 1.3651, val loss 1.8105 [874.061961889267 sec]
step 700: train loss 1.2439, val loss 1.7729 [1012.5513210296631 sec]
step 800: train loss 1.1307, val loss 1.7670 [1156.0562212467194 sec]
step 900: train loss 1.0231, val loss 1.7838 [1282.0110776424408 sec]
step 1000: train loss 0.9221, val loss 1.8377 [1380.1999838352203 sec]
step 1100: train loss 0.8151, val loss 1.8721 [1469.0642066001892 sec]
step 1200: train loss 0.7040, val loss 1.9511 [1556.9309568405151 sec]
step 1300: train loss 0.6047, val loss 2.0873 [1642.756617307663 sec]
step 1400: train loss 0.5045, val loss 2.1309 [1730.907655954361 sec]
0.6741734147071838
Total Training Time: 1775.2003693580627 seconds

– IISIONE OF THE PTEACE
"Yes, we have gan king acting about the liand wall to the Pyrran
77
SEAN McK
BEGINNING (1682027756.5314636): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.5939, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6188, val loss 4.6095 [6.91252064704895 sec]
step 100: train loss 2.2795, val loss 2.3889 [18.30546545982361 sec]
step 200: train loss 1.9498, val loss 2.1291 [30.429895162582397 sec]
step 300: train loss 1.7892, val loss 2.0096 [43.067819118499756 sec]
step 400: train loss 1.6770, val loss 1.9310 [55.80379557609558 sec]
step 500: train loss 1.6003, val loss 1.8824 [67.97735261917114 sec]
step 600: train loss 1.5429, val loss 1.8616 [81.83998417854309 sec]
step 700: train loss 1.4878, val loss 1.8271 [95.13002133369446 sec]
step 800: train loss 1.4525, val loss 1.8220 [107.06563687324524 sec]
step 900: train loss 1.4127, val loss 1.8216 [119.45851278305054 sec]
step 1000: train loss 1.3657, val loss 1.7939 [132.7440710067749 sec]
step 1100: train loss 1.3349, val loss 1.7860 [145.03448104858398 sec]
step 1200: train loss 1.3056, val loss 1.7838 [158.1474962234497 sec]
step 1300: train loss 1.2863, val loss 1.7829 [170.21687841415405 sec]
step 1400: train loss 1.2523, val loss 1.7892 [181.95875334739685 sec]
1.340484380722046
Total Training Time: 187.44670581817627 seconds

nard. Isw Except and stThis all of
the tuons and tace their seen in
halives. He othercles a
direct h
BEGINNING (1682027945.314226): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.5715, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5735, val loss 4.5741 [12.429244756698608 sec]
step 100: train loss 2.2910, val loss 2.4084 [34.10453653335571 sec]
step 200: train loss 1.9228, val loss 2.1173 [59.098934173583984 sec]
step 300: train loss 1.7288, val loss 1.9619 [84.34221863746643 sec]
step 400: train loss 1.6099, val loss 1.8804 [109.53986978530884 sec]
step 500: train loss 1.5256, val loss 1.8505 [135.83268666267395 sec]
step 600: train loss 1.4536, val loss 1.8053 [157.79355883598328 sec]
step 700: train loss 1.3985, val loss 1.8070 [175.4413197040558 sec]
step 800: train loss 1.3455, val loss 1.7831 [193.06228375434875 sec]
step 900: train loss 1.3038, val loss 1.7886 [210.76967668533325 sec]
step 1000: train loss 1.2735, val loss 1.8001 [230.38486886024475 sec]
step 1100: train loss 1.2210, val loss 1.7745 [250.8801248073578 sec]
step 1200: train loss 1.1826, val loss 1.7944 [268.7388651371002 sec]
step 1300: train loss 1.1476, val loss 1.7910 [286.5814163684845 sec]
step 1400: train loss 1.1130, val loss 1.8194 [304.1721725463867 sec]
1.2025172710418701
Total Training Time: 312.1996998786926 seconds

to starl of the maeuws liest
willine. Lameks mach juch did not we
mounce, had said as Cyah and winde
BEGINNING (1682028259.871423): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.5087, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5180, val loss 4.5149 [14.08977484703064 sec]
step 100: train loss 2.3371, val loss 2.4407 [40.330066442489624 sec]
step 200: train loss 1.9453, val loss 2.1332 [66.37113451957703 sec]
step 300: train loss 1.7341, val loss 1.9713 [92.98369717597961 sec]
step 400: train loss 1.6061, val loss 1.8939 [118.76718163490295 sec]
step 500: train loss 1.5065, val loss 1.8474 [144.5333878993988 sec]
step 600: train loss 1.4291, val loss 1.8022 [171.0269911289215 sec]
step 700: train loss 1.3633, val loss 1.7909 [196.8601052761078 sec]
step 800: train loss 1.3149, val loss 1.7869 [223.190660238266 sec]
step 900: train loss 1.2630, val loss 1.7953 [248.90271139144897 sec]
step 1000: train loss 1.2078, val loss 1.7896 [274.5050151348114 sec]
step 1100: train loss 1.1600, val loss 1.7926 [300.30990719795227 sec]
step 1200: train loss 1.1247, val loss 1.8332 [325.8826138973236 sec]
step 1300: train loss 1.0710, val loss 1.8327 [351.579208612442 sec]
step 1400: train loss 1.0393, val loss 1.8509 [377.34145069122314 sec]
1.1394984722137451
Total Training Time: 389.27260398864746 seconds

there. The men and hour, alracked at
Gratta tuning are
other treaty, for you, things fire traps of a
BEGINNING (1682028652.548895): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.5144, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5128, val loss 4.5138 [9.161221265792847 sec]
step 100: train loss 2.3884, val loss 2.4833 [24.001754999160767 sec]
step 200: train loss 2.0067, val loss 2.1827 [40.34439134597778 sec]
step 300: train loss 1.7844, val loss 2.0021 [55.03990459442139 sec]
step 400: train loss 1.6450, val loss 1.9032 [69.77717113494873 sec]
step 500: train loss 1.5615, val loss 1.8449 [84.37416505813599 sec]
step 600: train loss 1.4785, val loss 1.8109 [99.22463369369507 sec]
step 700: train loss 1.4052, val loss 1.7774 [113.87226843833923 sec]
step 800: train loss 1.3494, val loss 1.7579 [128.68624186515808 sec]
step 900: train loss 1.2952, val loss 1.7539 [143.47062921524048 sec]
step 1000: train loss 1.2394, val loss 1.7614 [158.4084415435791 sec]
step 1100: train loss 1.2015, val loss 1.7726 [173.32967042922974 sec]
step 1200: train loss 1.1568, val loss 1.7736 [188.33775639533997 sec]
step 1300: train loss 1.1102, val loss 1.7856 [203.30221343040466 sec]
step 1400: train loss 1.0710, val loss 1.7920 [218.25098609924316 sec]
1.1566786766052246
Total Training Time: 224.82271718978882 seconds

Pelana smiled. "So, sir!"
Gratta proked and at vaging. They walked wite Arphad
chiefs past matneuw o
BEGINNING (1682028878.5506747): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5755, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5816, val loss 4.5851 [15.467182636260986 sec]
step 100: train loss 2.3979, val loss 2.4885 [42.91340184211731 sec]
step 200: train loss 1.9684, val loss 2.1483 [70.22718739509583 sec]
step 300: train loss 1.7263, val loss 1.9505 [97.53218626976013 sec]
step 400: train loss 1.5654, val loss 1.8601 [124.90768718719482 sec]
step 500: train loss 1.4524, val loss 1.8020 [152.27645349502563 sec]
step 600: train loss 1.3466, val loss 1.7672 [179.6440532207489 sec]
step 700: train loss 1.2739, val loss 1.7579 [206.8125298023224 sec]
step 800: train loss 1.1994, val loss 1.7560 [233.94404363632202 sec]
step 900: train loss 1.1297, val loss 1.7596 [261.17030096054077 sec]
step 1000: train loss 1.0663, val loss 1.7798 [288.3136878013611 sec]
step 1100: train loss 0.9975, val loss 1.7823 [315.4394471645355 sec]
step 1200: train loss 0.9259, val loss 1.8199 [342.8052706718445 sec]
step 1300: train loss 0.8694, val loss 1.8663 [370.0073847770691 sec]
step 1400: train loss 0.8023, val loss 1.9040 [397.2826154232025 sec]
0.921923041343689
Total Training Time: 409.1051244735718 seconds

ofs armor, the High Priest ently
downs to greet. There was planners, I rejoice in this responding in
BEGINNING (1682029289.919348): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5973, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5949, val loss 4.5884 [22.8019757270813 sec]
step 100: train loss 2.4379, val loss 2.5236 [60.63710284233093 sec]
step 200: train loss 2.0344, val loss 2.1979 [98.49764752388 sec]
step 300: train loss 1.7415, val loss 1.9665 [136.75141954421997 sec]
step 400: train loss 1.5603, val loss 1.8467 [174.70456385612488 sec]
step 500: train loss 1.4355, val loss 1.8057 [212.49287104606628 sec]
step 600: train loss 1.3185, val loss 1.7705 [251.62866735458374 sec]
step 700: train loss 1.2249, val loss 1.7600 [290.50169610977173 sec]
step 800: train loss 1.1422, val loss 1.7936 [330.0087904930115 sec]
step 900: train loss 1.0517, val loss 1.7766 [368.05902767181396 sec]
step 1000: train loss 0.9795, val loss 1.8051 [406.2880976200104 sec]
step 1100: train loss 0.8991, val loss 1.8525 [444.81620836257935 sec]
step 1200: train loss 0.8151, val loss 1.8982 [482.53480052948 sec]
step 1300: train loss 0.7286, val loss 1.9720 [520.4809815883636 sec]
step 1400: train loss 0.6574, val loss 2.0248 [558.2691237926483 sec]
0.7884666323661804
Total Training Time: 575.330465555191 seconds

him disdarts after embling to the rest of the nation mored
for in their chests. left down to a the t
BEGINNING (1682029869.1053245): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6111, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6174, val loss 4.6135 [13.960939645767212 sec]
step 100: train loss 2.4589, val loss 2.5437 [37.23113203048706 sec]
step 200: train loss 2.2338, val loss 2.3666 [60.55350685119629 sec]
step 300: train loss 1.9241, val loss 2.1171 [84.0049192905426 sec]
step 400: train loss 1.7213, val loss 1.9642 [107.39413666725159 sec]
step 500: train loss 1.5774, val loss 1.8698 [131.06247687339783 sec]
step 600: train loss 1.4758, val loss 1.8051 [154.35915970802307 sec]
step 700: train loss 1.3888, val loss 1.7740 [177.58757066726685 sec]
step 800: train loss 1.3115, val loss 1.7370 [200.83773064613342 sec]
step 900: train loss 1.2416, val loss 1.7426 [224.15451192855835 sec]
step 1000: train loss 1.1701, val loss 1.7433 [247.4608564376831 sec]
step 1100: train loss 1.1159, val loss 1.7557 [270.77205538749695 sec]
step 1200: train loss 1.0440, val loss 1.7674 [294.1152160167694 sec]
step 1300: train loss 0.9783, val loss 1.8013 [317.3827049732208 sec]
step 1400: train loss 0.9125, val loss 1.8074 [340.72311067581177 sec]
1.0051250457763672
Total Training Time: 350.77495884895325 seconds

2014X
All rais 8 – Abight aka~ge to him up. He remain and paras
chain will priect nemy them, they we
BEGINNING (1682030221.1110048): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6437, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6426, val loss 4.6459 [25.339662075042725 sec]
step 100: train loss 2.4575, val loss 2.5451 [65.89697289466858 sec]
step 200: train loss 2.2080, val loss 2.3383 [106.28060698509216 sec]
step 300: train loss 1.8394, val loss 2.0360 [147.03008556365967 sec]
step 400: train loss 1.6168, val loss 1.8982 [188.46344017982483 sec]
step 500: train loss 1.4649, val loss 1.8100 [232.37889122962952 sec]
step 600: train loss 1.3447, val loss 1.7767 [312.69523572921753 sec]
step 700: train loss 1.2338, val loss 1.7431 [363.16515588760376 sec]
step 800: train loss 1.1330, val loss 1.7525 [416.78511214256287 sec]
step 900: train loss 1.0391, val loss 1.7918 [466.85108375549316 sec]
step 1000: train loss 0.9354, val loss 1.7992 [517.477753162384 sec]
step 1100: train loss 0.8297, val loss 1.8506 [583.2647411823273 sec]
step 1200: train loss 0.7322, val loss 1.9166 [636.1883218288422 sec]
step 1300: train loss 0.6282, val loss 1.9975 [684.6009910106659 sec]
step 1400: train loss 0.5278, val loss 2.0934 [734.8976364135742 sec]
0.6966707706451416
Total Training Time: 769.9358439445496 seconds

SEAN McKAYourshing off you, Wy dir not?"
"HEver, the Surely have seen in 1 prisonce.
ir just from an
BEGINNING (1682030993.884682): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6236, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6360, val loss 4.6347 [2.666321277618408 sec]
step 100: train loss 2.5924, val loss 2.6661 [7.330116510391235 sec]
step 200: train loss 2.4154, val loss 2.5003 [11.84374189376831 sec]
step 300: train loss 2.2479, val loss 2.3722 [16.317793130874634 sec]
step 400: train loss 2.1347, val loss 2.2725 [20.77809739112854 sec]
step 500: train loss 2.0528, val loss 2.2087 [26.15836215019226 sec]
step 600: train loss 1.9838, val loss 2.1532 [31.478626251220703 sec]
step 700: train loss 1.9228, val loss 2.0808 [36.3521249294281 sec]
step 800: train loss 1.8743, val loss 2.0592 [40.552542209625244 sec]
step 900: train loss 1.8300, val loss 2.0325 [44.72822022438049 sec]
step 1000: train loss 1.8039, val loss 2.0112 [48.97532868385315 sec]
step 1100: train loss 1.7621, val loss 1.9878 [53.604610443115234 sec]
step 1200: train loss 1.7386, val loss 1.9667 [57.70042943954468 sec]
step 1300: train loss 1.7088, val loss 1.9417 [62.220370292663574 sec]
step 1400: train loss 1.6903, val loss 1.9154 [66.95897054672241 sec]
1.7109637260437012
Total Training Time: 69.08049321174622 seconds

reaked, to whim. A“ by, but mick sport loaght."
sered?" Ar! Aftach abock. Afor twart
he mere so yeh.
BEGINNING (1682031063.695214): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5701, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5970, val loss 4.6004 [4.0807929039001465 sec]
step 100: train loss 2.5564, val loss 2.6320 [11.40362286567688 sec]
step 200: train loss 2.3589, val loss 2.4462 [18.830782651901245 sec]
step 300: train loss 2.1882, val loss 2.3044 [25.9795663356781 sec]
step 400: train loss 2.0494, val loss 2.2075 [33.17057967185974 sec]
step 500: train loss 1.9577, val loss 2.1309 [40.350412130355835 sec]
step 600: train loss 1.8854, val loss 2.0730 [47.52915287017822 sec]
step 700: train loss 1.8138, val loss 2.0288 [54.71528625488281 sec]
step 800: train loss 1.7771, val loss 2.0011 [62.2575740814209 sec]
step 900: train loss 1.7202, val loss 1.9492 [69.55804944038391 sec]
step 1000: train loss 1.6788, val loss 1.9109 [76.92371439933777 sec]
step 1100: train loss 1.6438, val loss 1.8896 [84.42836737632751 sec]
step 1200: train loss 1.6202, val loss 1.8597 [91.55342745780945 sec]
step 1300: train loss 1.5940, val loss 1.8494 [99.22495341300964 sec]
step 1400: train loss 1.5668, val loss 1.8475 [107.22681331634521 sec]
1.6390327215194702
Total Training Time: 110.39354467391968 seconds

smile. Gratta lrike at his freher table,
thuman  of your brike issed any."
Anayah nodded. This you d
BEGINNING (1682031175.5623815): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6234, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6229, val loss 4.6257 [5.885752201080322 sec]
step 100: train loss 2.5501, val loss 2.6228 [16.52818202972412 sec]
step 200: train loss 2.3553, val loss 2.4427 [26.648531913757324 sec]
step 300: train loss 2.1735, val loss 2.2918 [36.790897846221924 sec]
step 400: train loss 2.0394, val loss 2.1983 [47.01467990875244 sec]
step 500: train loss 1.9259, val loss 2.0945 [57.16355872154236 sec]
step 600: train loss 1.8495, val loss 2.0480 [67.35175156593323 sec]
step 700: train loss 1.7747, val loss 1.9989 [77.7468535900116 sec]
step 800: train loss 1.7253, val loss 1.9248 [88.23755741119385 sec]
step 900: train loss 1.6785, val loss 1.9185 [98.37812542915344 sec]
step 1000: train loss 1.6314, val loss 1.8939 [108.438716173172 sec]
step 1100: train loss 1.5943, val loss 1.8519 [118.53679394721985 sec]
step 1200: train loss 1.5736, val loss 1.8437 [128.6241316795349 sec]
step 1300: train loss 1.5362, val loss 1.8246 [138.75395154953003 sec]
step 1400: train loss 1.5033, val loss 1.8058 [149.24158549308777 sec]
1.5154744386672974
Total Training Time: 153.7413821220398 seconds

then 33
5 Mor will Yous are tas of" as a
trang gear rested . And his was down." Away, the smiles wil
BEGINNING (1682031331.219399): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6566, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6356, val loss 4.6319 [2.6911211013793945 sec]
step 100: train loss 2.6055, val loss 2.6694 [7.331779479980469 sec]
step 200: train loss 2.4770, val loss 2.5582 [11.983647346496582 sec]
step 300: train loss 2.3607, val loss 2.4624 [16.653307676315308 sec]
step 400: train loss 2.2255, val loss 2.3353 [21.336204051971436 sec]
step 500: train loss 2.1025, val loss 2.2386 [26.042448043823242 sec]
step 600: train loss 2.0238, val loss 2.1791 [30.72078013420105 sec]
step 700: train loss 1.9516, val loss 2.1258 [35.35244679450989 sec]
step 800: train loss 1.8877, val loss 2.0797 [40.11894178390503 sec]
step 900: train loss 1.8449, val loss 2.0440 [44.767913579940796 sec]
step 1000: train loss 1.7977, val loss 1.9992 [49.42102861404419 sec]
step 1100: train loss 1.7515, val loss 1.9762 [54.14749622344971 sec]
step 1200: train loss 1.7213, val loss 1.9490 [58.88318419456482 sec]
step 1300: train loss 1.6855, val loss 1.9161 [63.51616835594177 sec]
step 1400: train loss 1.6589, val loss 1.9027 [69.25928592681885 sec]
1.7474218606948853
Total Training Time: 71.67067384719849 seconds

afted the his fam to bo comittle. Chies had entreatjoins
WRAC
SEAN McKAY
AVI 5
SYeas cher words from
BEGINNING (1682031403.693149): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6717, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6651, val loss 4.6644 [5.087380409240723 sec]
step 100: train loss 2.5548, val loss 2.6335 [12.931750059127808 sec]
step 200: train loss 2.4271, val loss 2.5114 [20.747085571289062 sec]
step 300: train loss 2.2941, val loss 2.4069 [28.745494604110718 sec]
step 400: train loss 2.1315, val loss 2.2636 [39.31376886367798 sec]
step 500: train loss 2.0040, val loss 2.1612 [47.95594501495361 sec]
step 600: train loss 1.8951, val loss 2.0797 [56.824498891830444 sec]
step 700: train loss 1.8145, val loss 2.0220 [64.90229105949402 sec]
step 800: train loss 1.7477, val loss 1.9668 [72.90317964553833 sec]
step 900: train loss 1.6981, val loss 1.9240 [80.907790184021 sec]
step 1000: train loss 1.6513, val loss 1.9088 [88.8584349155426 sec]
step 1100: train loss 1.6080, val loss 1.8826 [96.83384227752686 sec]
step 1200: train loss 1.5712, val loss 1.8442 [104.79616832733154 sec]
step 1300: train loss 1.5346, val loss 1.8243 [112.89277291297913 sec]
step 1400: train loss 1.5072, val loss 1.8140 [120.96485042572021 sec]
1.5391079187393188
Total Training Time: 124.38819694519043 seconds

creased had tithc py melirese
slaxt.
The irmecian fereen outs. Sneem, for thowle yovery
qure haveriv
BEGINNING (1682031529.389385): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6030, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6216, val loss 4.6227 [6.4260947704315186 sec]
step 100: train loss 2.5396, val loss 2.6141 [17.863376140594482 sec]
step 200: train loss 2.4226, val loss 2.5080 [29.277246713638306 sec]
step 300: train loss 2.2844, val loss 2.3862 [40.8089234828949 sec]
step 400: train loss 2.0999, val loss 2.2463 [52.22855877876282 sec]
step 500: train loss 1.9565, val loss 2.1349 [64.08789706230164 sec]
step 600: train loss 1.8527, val loss 2.0568 [75.58838820457458 sec]
step 700: train loss 1.7644, val loss 1.9734 [87.37352108955383 sec]
step 800: train loss 1.6991, val loss 1.9230 [98.87433218955994 sec]
step 900: train loss 1.6388, val loss 1.8801 [110.30131149291992 sec]
step 1000: train loss 1.5863, val loss 1.8504 [121.95429587364197 sec]
step 1100: train loss 1.5488, val loss 1.8338 [133.6938328742981 sec]
step 1200: train loss 1.5138, val loss 1.8155 [145.27453684806824 sec]
step 1300: train loss 1.4735, val loss 1.8075 [156.99096369743347 sec]
step 1400: train loss 1.4394, val loss 1.7917 [168.52889776229858 sec]
1.4838377237319946
Total Training Time: 173.66572213172913 seconds

beWing to bake, the tast tuon him, en? Yes, shoomare." "Yes, "What
an shook the telp worth comey tre
BEGINNING (1682031705.1730359): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6324, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6248, val loss 4.6308 [3.4530439376831055 sec]
step 100: train loss 2.5960, val loss 2.6792 [9.46904444694519 sec]
step 200: train loss 2.5019, val loss 2.5826 [15.550589799880981 sec]
step 300: train loss 2.4424, val loss 2.5250 [21.53112506866455 sec]
step 400: train loss 2.3755, val loss 2.4612 [27.574874877929688 sec]
step 500: train loss 2.2735, val loss 2.3831 [33.5992169380188 sec]
step 600: train loss 2.1628, val loss 2.2891 [39.60627222061157 sec]
step 700: train loss 2.0601, val loss 2.2087 [45.61680507659912 sec]
step 800: train loss 1.9779, val loss 2.1482 [51.622281551361084 sec]
step 900: train loss 1.9129, val loss 2.0914 [57.61781644821167 sec]
step 1000: train loss 1.8512, val loss 2.0420 [63.627758264541626 sec]
step 1100: train loss 1.7983, val loss 2.0132 [69.61115264892578 sec]
step 1200: train loss 1.7586, val loss 1.9626 [75.72449326515198 sec]
step 1300: train loss 1.7119, val loss 1.9446 [81.77462840080261 sec]
step 1400: train loss 1.6835, val loss 1.9155 [87.80687069892883 sec]
1.784745693206787
Total Training Time: 90.3306212425232 seconds

vustriest. Namal folighties yenty." Gratto eens his a
priestelingsed supeding Chief And brut woulded
BEGINNING (1682031796.2189817): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5941, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5954, val loss 4.5883 [6.178343057632446 sec]
step 100: train loss 2.5655, val loss 2.6343 [17.109534978866577 sec]
step 200: train loss 2.4724, val loss 2.5529 [28.027775526046753 sec]
step 300: train loss 2.4013, val loss 2.4987 [38.920754194259644 sec]
step 400: train loss 2.2997, val loss 2.4049 [49.881638050079346 sec]
step 500: train loss 2.1602, val loss 2.2881 [60.861746072769165 sec]
step 600: train loss 2.0158, val loss 2.1728 [71.79283404350281 sec]
step 700: train loss 1.9156, val loss 2.0932 [82.7507541179657 sec]
step 800: train loss 1.8314, val loss 2.0319 [93.65305256843567 sec]
step 900: train loss 1.7696, val loss 1.9780 [104.60051822662354 sec]
step 1000: train loss 1.7039, val loss 1.9352 [115.57300734519958 sec]
step 1100: train loss 1.6502, val loss 1.9028 [126.61357617378235 sec]
step 1200: train loss 1.6062, val loss 1.8723 [137.54456686973572 sec]
step 1300: train loss 1.5634, val loss 1.8366 [148.46614122390747 sec]
step 1400: train loss 1.5265, val loss 1.8285 [159.39621329307556 sec]
1.621549367904663
Total Training Time: 164.1834008693695 seconds

recouch him raised for like. He wad nothe pina Takain hosh
in spable on our''d standing. Than herpy 
BEGINNING (1682031961.783151): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5824, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5873, val loss 4.5883 [8.872944831848145 sec]
step 100: train loss 2.5555, val loss 2.6346 [24.79178810119629 sec]
step 200: train loss 2.4631, val loss 2.5516 [40.618247509002686 sec]
step 300: train loss 2.3796, val loss 2.4825 [56.42528033256531 sec]
step 400: train loss 2.2398, val loss 2.3669 [72.25376868247986 sec]
step 500: train loss 2.0814, val loss 2.2296 [88.11208391189575 sec]
step 600: train loss 1.9352, val loss 2.1154 [103.91733860969543 sec]
step 700: train loss 1.8276, val loss 2.0178 [119.73433089256287 sec]
step 800: train loss 1.7476, val loss 1.9585 [136.3391363620758 sec]
step 900: train loss 1.6770, val loss 1.9080 [157.41048622131348 sec]
step 1000: train loss 1.6144, val loss 1.8821 [175.79308581352234 sec]
step 1100: train loss 1.5605, val loss 1.8446 [191.56822228431702 sec]
step 1200: train loss 1.5082, val loss 1.8100 [207.31425213813782 sec]
step 1300: train loss 1.4650, val loss 1.7879 [223.11314463615417 sec]
step 1400: train loss 1.4273, val loss 1.7901 [238.86001825332642 sec]
1.495824933052063
Total Training Time: 245.77973699569702 seconds

bild. The he cubrutt itsid of ten the were, as
ange, eyesh he cortared beinge or his deacibs. We ful
BEGINNING (1682032209.3572412): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5879, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5991, val loss 4.6038 [2.444568395614624 sec]
step 100: train loss 2.5815, val loss 2.6407 [6.5179455280303955 sec]
step 200: train loss 2.3459, val loss 2.4408 [10.593589305877686 sec]
step 300: train loss 2.1678, val loss 2.3087 [14.684945821762085 sec]
step 400: train loss 2.0596, val loss 2.2078 [18.7592134475708 sec]
step 500: train loss 1.9733, val loss 2.1389 [22.810558319091797 sec]
step 600: train loss 1.9027, val loss 2.0867 [26.862674951553345 sec]
step 700: train loss 1.8468, val loss 2.0392 [30.951223373413086 sec]
step 800: train loss 1.8002, val loss 2.0044 [35.34900903701782 sec]
step 900: train loss 1.7771, val loss 1.9902 [39.52308750152588 sec]
step 1000: train loss 1.7316, val loss 1.9431 [43.57876420021057 sec]
step 1100: train loss 1.7006, val loss 1.9437 [47.66788697242737 sec]
step 1200: train loss 1.6731, val loss 1.8974 [51.69303631782532 sec]
step 1300: train loss 1.6484, val loss 1.8948 [55.741161584854126 sec]
step 1400: train loss 1.6289, val loss 1.8726 [59.81616687774658 sec]
1.659985899925232
Total Training Time: 61.46416258811951 seconds

land the to seall. Hew werecorly no leared all Maite allies andinted's way to to Aut reple are as be
BEGINNING (1682032271.48151): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5691, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6001, val loss 4.6000 [3.9817519187927246 sec]
step 100: train loss 2.5297, val loss 2.5999 [10.996697187423706 sec]
step 200: train loss 2.3205, val loss 2.4071 [17.972132682800293 sec]
step 300: train loss 2.1123, val loss 2.2625 [24.939022541046143 sec]
step 400: train loss 1.9701, val loss 2.1389 [31.90279245376587 sec]
step 500: train loss 1.8756, val loss 2.0685 [39.192878007888794 sec]
step 600: train loss 1.7963, val loss 2.0190 [46.28282594680786 sec]
step 700: train loss 1.7430, val loss 1.9694 [53.238513708114624 sec]
step 800: train loss 1.6875, val loss 1.9238 [60.20092821121216 sec]
step 900: train loss 1.6466, val loss 1.9012 [67.20814275741577 sec]
step 1000: train loss 1.6081, val loss 1.8845 [74.14511466026306 sec]
step 1100: train loss 1.5745, val loss 1.8580 [81.13477540016174 sec]
step 1200: train loss 1.5480, val loss 1.8397 [88.08499813079834 sec]
step 1300: train loss 1.5141, val loss 1.8219 [95.05829358100891 sec]
step 1400: train loss 1.4880, val loss 1.8151 [102.61937236785889 sec]
1.5407514572143555
Total Training Time: 105.8272168636322 seconds

noClyse was
great have quire, all have to him strelooy walked. "The stend was smiler. Gratta leaked 
BEGINNING (1682032378.5303857): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5381, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5437, val loss 4.5388 [5.549600124359131 sec]
step 100: train loss 2.5044, val loss 2.5801 [15.512672424316406 sec]
step 200: train loss 2.2934, val loss 2.3983 [25.4515061378479 sec]
step 300: train loss 2.0876, val loss 2.2346 [35.454280853271484 sec]
step 400: train loss 1.9371, val loss 2.1029 [45.40333366394043 sec]
step 500: train loss 1.8439, val loss 2.0366 [56.49649667739868 sec]
step 600: train loss 1.7576, val loss 1.9719 [69.64794540405273 sec]
step 700: train loss 1.6870, val loss 1.9420 [80.66343927383423 sec]
step 800: train loss 1.6462, val loss 1.8850 [93.60415863990784 sec]
step 900: train loss 1.5939, val loss 1.8546 [105.16756272315979 sec]
step 1000: train loss 1.5587, val loss 1.8479 [115.0093023777008 sec]
step 1100: train loss 1.5155, val loss 1.8231 [124.83882403373718 sec]
step 1200: train loss 1.4857, val loss 1.8284 [134.7116870880127 sec]
step 1300: train loss 1.4597, val loss 1.7885 [144.53339290618896 sec]
step 1400: train loss 1.4302, val loss 1.7874 [154.43586254119873 sec]
1.5020006895065308
Total Training Time: 158.8099570274353 seconds

ragu to smell." Taka, lower must down the city in clear in the rear too face acan no little.
Gor to 
BEGINNING (1682032539.096897): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6293, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6258, val loss 4.6216 [3.002587080001831 sec]
step 100: train loss 2.5912, val loss 2.6625 [8.02690601348877 sec]
step 200: train loss 2.4434, val loss 2.5264 [13.068541288375854 sec]
step 300: train loss 2.2941, val loss 2.3964 [18.109944105148315 sec]
step 400: train loss 2.1435, val loss 2.2768 [23.17983102798462 sec]
step 500: train loss 2.0284, val loss 2.1915 [28.19202446937561 sec]
step 600: train loss 1.9441, val loss 2.1217 [33.24059247970581 sec]
step 700: train loss 1.8766, val loss 2.0680 [38.30012226104736 sec]
step 800: train loss 1.8247, val loss 2.0191 [43.336655378341675 sec]
step 900: train loss 1.7706, val loss 1.9865 [48.354126930236816 sec]
step 1000: train loss 1.7230, val loss 1.9489 [53.37582492828369 sec]
step 1100: train loss 1.6857, val loss 1.9209 [58.396103620529175 sec]
step 1200: train loss 1.6533, val loss 1.9099 [63.4135217666626 sec]
step 1300: train loss 1.6199, val loss 1.8792 [68.43445181846619 sec]
step 1400: train loss 1.5917, val loss 1.8520 [73.45791816711426 sec]
1.581215739250183
Total Training Time: 75.486008644104 seconds

NENOMEW
SEEKIT ON
EFGE
ACHE to GAN McWIN
coll spooke fil whed mentain dightley. It! He side. Gratta

BEGINNING (1682032615.2094252): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6130, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6017, val loss 4.5866 [5.185563325881958 sec]
step 100: train loss 2.5456, val loss 2.6139 [14.157979011535645 sec]
step 200: train loss 2.3962, val loss 2.4933 [23.14701557159424 sec]
step 300: train loss 2.2376, val loss 2.3440 [32.127363204956055 sec]
step 400: train loss 2.0526, val loss 2.2013 [41.08542203903198 sec]
step 500: train loss 1.9091, val loss 2.0892 [50.04216718673706 sec]
step 600: train loss 1.8179, val loss 2.0220 [59.0166540145874 sec]
step 700: train loss 1.7384, val loss 1.9602 [68.03835821151733 sec]
step 800: train loss 1.6745, val loss 1.9093 [77.01933884620667 sec]
step 900: train loss 1.6285, val loss 1.8782 [85.98532462120056 sec]
step 1000: train loss 1.5747, val loss 1.8529 [94.96976041793823 sec]
step 1100: train loss 1.5283, val loss 1.8149 [104.00551414489746 sec]
step 1200: train loss 1.4961, val loss 1.8097 [112.97867321968079 sec]
step 1300: train loss 1.4588, val loss 1.7958 [121.97053980827332 sec]
step 1400: train loss 1.4352, val loss 1.7757 [130.9413616657257 sec]
1.514421820640564
Total Training Time: 134.73005390167236 seconds

Suon't eyes. "Beriyan lost of wall be him as hem to filly poices
wouldierb find scenting souns.
"The
BEGINNING (1682032751.1440718): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6037, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6093, val loss 4.6166 [7.370988607406616 sec]
step 100: train loss 2.5265, val loss 2.5999 [20.308709621429443 sec]
step 200: train loss 2.3938, val loss 2.4805 [33.24930191040039 sec]
step 300: train loss 2.2052, val loss 2.3276 [46.18266463279724 sec]
step 400: train loss 2.0030, val loss 2.1562 [59.107027530670166 sec]
step 500: train loss 1.8616, val loss 2.0550 [72.0297794342041 sec]
step 600: train loss 1.7552, val loss 1.9741 [84.9567654132843 sec]
step 700: train loss 1.6709, val loss 1.9117 [97.88568997383118 sec]
step 800: train loss 1.6034, val loss 1.8653 [110.91642022132874 sec]
step 900: train loss 1.5518, val loss 1.8419 [123.8400981426239 sec]
step 1000: train loss 1.4960, val loss 1.8176 [136.77721691131592 sec]
step 1100: train loss 1.4545, val loss 1.7818 [149.7597234249115 sec]
step 1200: train loss 1.4141, val loss 1.7696 [162.6898753643036 sec]
step 1300: train loss 1.3718, val loss 1.7599 [175.6335687637329 sec]
step 1400: train loss 1.3444, val loss 1.7468 [188.54441785812378 sec]
1.4264392852783203
Total Training Time: 194.101895570755 seconds

was helping two him a right?"
Gratta is. "And yes."
"He poid up litted out of the great low. If
are 
BEGINNING (1682032947.0158012): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5697, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5656, val loss 4.5655 [4.504614591598511 sec]
step 100: train loss 2.5952, val loss 2.6672 [12.286007165908813 sec]
step 200: train loss 2.4915, val loss 2.5758 [20.085763692855835 sec]
step 300: train loss 2.4247, val loss 2.5141 [27.879273653030396 sec]
step 400: train loss 2.3513, val loss 2.4536 [35.696027517318726 sec]
step 500: train loss 2.2552, val loss 2.3619 [43.49258756637573 sec]
step 600: train loss 2.1290, val loss 2.2652 [51.291067361831665 sec]
step 700: train loss 2.0121, val loss 2.1683 [59.06822109222412 sec]
step 800: train loss 1.9226, val loss 2.1051 [66.85068368911743 sec]
step 900: train loss 1.8568, val loss 2.0531 [74.64611744880676 sec]
step 1000: train loss 1.7938, val loss 1.9898 [82.42840099334717 sec]
step 1100: train loss 1.7355, val loss 1.9606 [90.24217224121094 sec]
step 1200: train loss 1.6868, val loss 1.9068 [98.02028059959412 sec]
step 1300: train loss 1.6408, val loss 1.8792 [105.8110191822052 sec]
step 1400: train loss 1.6046, val loss 1.8657 [113.59766006469727 sec]
1.6721868515014648
Total Training Time: 116.87306094169617 seconds

SEANDmons mattere st. I in they cropping round to
weree pace. One on ither but hisfor creed must
pur
BEGINNING (1682033064.5093613): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5610, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5565, val loss 4.5576 [8.107134103775024 sec]
step 100: train loss 2.5590, val loss 2.6303 [22.36592197418213 sec]
step 200: train loss 2.4537, val loss 2.5393 [36.63403081893921 sec]
step 300: train loss 2.3654, val loss 2.4586 [50.895973682403564 sec]
step 400: train loss 2.2332, val loss 2.3564 [65.14654469490051 sec]
step 500: train loss 2.0670, val loss 2.2168 [85.99641489982605 sec]
step 600: train loss 1.9408, val loss 2.1221 [107.97656154632568 sec]
step 700: train loss 1.8460, val loss 2.0354 [131.31506776809692 sec]
step 800: train loss 1.7572, val loss 1.9814 [152.29491114616394 sec]
step 900: train loss 1.6967, val loss 1.9278 [171.3301272392273 sec]
step 1000: train loss 1.6347, val loss 1.8930 [193.72079849243164 sec]
step 1100: train loss 1.5792, val loss 1.8552 [210.9576518535614 sec]
step 1200: train loss 1.5336, val loss 1.8373 [226.3022768497467 sec]
step 1300: train loss 1.4954, val loss 1.8141 [244.76743125915527 sec]
step 1400: train loss 1.4463, val loss 1.7844 [259.13034200668335 sec]
1.5183299779891968
Total Training Time: 266.1896686553955 seconds

Namil Arphad negotion."
A nour asined look. "The tuon that I like as coviffed.
"Yes, they is tuon yo
BEGINNING (1682033332.0229354): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6267, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6164, val loss 4.6031 [11.784893274307251 sec]
step 100: train loss 2.5395, val loss 2.6084 [33.175596475601196 sec]
step 200: train loss 2.4489, val loss 2.5387 [56.68260335922241 sec]
step 300: train loss 2.3490, val loss 2.4551 [78.15026354789734 sec]
step 400: train loss 2.1576, val loss 2.2886 [99.57547879219055 sec]
step 500: train loss 1.9810, val loss 2.1553 [120.96193528175354 sec]
step 600: train loss 1.8570, val loss 2.0533 [142.39515590667725 sec]
step 700: train loss 1.7516, val loss 1.9702 [163.80513954162598 sec]
step 800: train loss 1.6682, val loss 1.9178 [185.19925475120544 sec]
step 900: train loss 1.5966, val loss 1.8667 [206.62001514434814 sec]
step 1000: train loss 1.5366, val loss 1.8316 [228.00470304489136 sec]
step 1100: train loss 1.4809, val loss 1.7910 [249.46287941932678 sec]
step 1200: train loss 1.4307, val loss 1.7790 [270.87072253227234 sec]
step 1300: train loss 1.3834, val loss 1.7541 [292.3337128162384 sec]
step 1400: train loss 1.3466, val loss 1.7357 [313.7417953014374 sec]
1.417357325553894
Total Training Time: 323.11317324638367 seconds

hereed that They sould out came to couns. Jushe
we not remused in your be welied canted was ix, as
f
BEGINNING (1682033656.9778788): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6230, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6236, val loss 4.6269 [2.820220470428467 sec]
step 100: train loss 2.5713, val loss 2.6395 [7.4628918170928955 sec]
step 200: train loss 2.3207, val loss 2.4202 [12.148271799087524 sec]
step 300: train loss 2.1264, val loss 2.2679 [16.785459995269775 sec]
step 400: train loss 2.0047, val loss 2.1649 [21.48794198036194 sec]
step 500: train loss 1.9223, val loss 2.1054 [26.09246277809143 sec]
step 600: train loss 1.8582, val loss 2.0417 [30.700254201889038 sec]
step 700: train loss 1.7985, val loss 1.9961 [35.3126757144928 sec]
step 800: train loss 1.7597, val loss 1.9646 [40.24894642829895 sec]
step 900: train loss 1.7104, val loss 1.9412 [44.998029947280884 sec]
step 1000: train loss 1.6869, val loss 1.9219 [49.63727140426636 sec]
step 1100: train loss 1.6571, val loss 1.8978 [54.271894454956055 sec]
step 1200: train loss 1.6291, val loss 1.8814 [58.9096999168396 sec]
step 1300: train loss 1.5988, val loss 1.8689 [63.53310418128967 sec]
step 1400: train loss 1.5746, val loss 1.8594 [68.23955726623535 sec]
1.6801388263702393
Total Training Time: 70.06003260612488 seconds

Anayah see! Elying on was must retome was ago, "Oone if this froce
dest bet, Gratta be my motith uct
BEGINNING (1682033727.6877718): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6841, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6950, val loss 4.6848 [4.666304111480713 sec]
step 100: train loss 2.5081, val loss 2.5820 [12.70571231842041 sec]
step 200: train loss 2.2678, val loss 2.3843 [20.637362480163574 sec]
step 300: train loss 2.0715, val loss 2.2097 [28.612820148468018 sec]
step 400: train loss 1.9409, val loss 2.1238 [36.88988780975342 sec]
step 500: train loss 1.8348, val loss 2.0312 [44.81745433807373 sec]
step 600: train loss 1.7513, val loss 1.9605 [52.776362657547 sec]
step 700: train loss 1.6872, val loss 1.9163 [60.79185652732849 sec]
step 800: train loss 1.6443, val loss 1.8939 [68.77672362327576 sec]
step 900: train loss 1.5981, val loss 1.8571 [76.68592834472656 sec]
step 1000: train loss 1.5685, val loss 1.8475 [84.61864352226257 sec]
step 1100: train loss 1.5235, val loss 1.8249 [92.58784890174866 sec]
step 1200: train loss 1.5000, val loss 1.8073 [100.75699877738953 sec]
step 1300: train loss 1.4727, val loss 1.7895 [108.73241972923279 sec]
step 1400: train loss 1.4489, val loss 1.7897 [116.71080541610718 sec]
1.5270320177078247
Total Training Time: 119.9949676990509 seconds

looked and on wether a saws."
Ancennered silend."
49
SEAN McKAMT
McKieding to need to Taking
the tal
BEGINNING (1682033848.8502574): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6278, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6434, val loss 4.6344 [6.479785919189453 sec]
step 100: train loss 2.4819, val loss 2.5570 [17.765159845352173 sec]
step 200: train loss 2.2216, val loss 2.3564 [29.0942120552063 sec]
step 300: train loss 2.0222, val loss 2.1849 [40.40153479576111 sec]
step 400: train loss 1.8748, val loss 2.0673 [52.01396203041077 sec]
step 500: train loss 1.7843, val loss 1.9945 [63.27381658554077 sec]
step 600: train loss 1.7026, val loss 1.9341 [74.56592464447021 sec]
step 700: train loss 1.6494, val loss 1.9046 [85.80059695243835 sec]
step 800: train loss 1.5823, val loss 1.8535 [97.07591557502747 sec]
step 900: train loss 1.5360, val loss 1.8242 [108.37784147262573 sec]
step 1000: train loss 1.4966, val loss 1.8175 [119.96066546440125 sec]
step 1100: train loss 1.4647, val loss 1.7825 [131.20947360992432 sec]
step 1200: train loss 1.4325, val loss 1.7871 [142.6231713294983 sec]
step 1300: train loss 1.4028, val loss 1.7736 [153.89100861549377 sec]
step 1400: train loss 1.3697, val loss 1.7679 [165.23288297653198 sec]
1.4213004112243652
Total Training Time: 170.0378975868225 seconds

McKAY
"WI will not we have sent in not shook
and to warriors of the of can the nut will betight!"
Ta
BEGINNING (1682034020.6192577): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.7053, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7084, val loss 4.7118 [3.656813859939575 sec]
step 100: train loss 2.5773, val loss 2.6444 [9.835319995880127 sec]
step 200: train loss 2.4355, val loss 2.5209 [15.91315221786499 sec]
step 300: train loss 2.2805, val loss 2.3787 [22.003506898880005 sec]
step 400: train loss 2.1104, val loss 2.2486 [28.086714267730713 sec]
step 500: train loss 1.9895, val loss 2.1587 [34.17782640457153 sec]
step 600: train loss 1.8952, val loss 2.0799 [40.269391775131226 sec]
step 700: train loss 1.8179, val loss 2.0222 [46.36142110824585 sec]
step 800: train loss 1.7594, val loss 1.9786 [52.46570348739624 sec]
step 900: train loss 1.7088, val loss 1.9408 [58.56480646133423 sec]
step 1000: train loss 1.6716, val loss 1.9202 [64.67328548431396 sec]
step 1100: train loss 1.6309, val loss 1.8801 [70.82126212120056 sec]
step 1200: train loss 1.5968, val loss 1.8474 [77.02056908607483 sec]
step 1300: train loss 1.5725, val loss 1.8432 [83.12466621398926 sec]
step 1400: train loss 1.5310, val loss 1.8205 [89.19517469406128 sec]
1.575218677520752
Total Training Time: 91.68302750587463 seconds

their wailit mighfed to the horve
Counced would betwore see spaing faing
on bere cappe. The sold Gwa
BEGINNING (1682034112.922328): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5569, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5587, val loss 4.5550 [6.196123361587524 sec]
step 100: train loss 2.5415, val loss 2.6169 [16.94556212425232 sec]
step 200: train loss 2.3722, val loss 2.4630 [27.690531492233276 sec]
step 300: train loss 2.1859, val loss 2.3127 [38.44080400466919 sec]
step 400: train loss 2.0106, val loss 2.1741 [49.2833833694458 sec]
step 500: train loss 1.8731, val loss 2.0554 [60.02330207824707 sec]
step 600: train loss 1.7667, val loss 1.9759 [70.77548885345459 sec]
step 700: train loss 1.6916, val loss 1.9281 [81.51215028762817 sec]
step 800: train loss 1.6219, val loss 1.8698 [92.26004076004028 sec]
step 900: train loss 1.5656, val loss 1.8446 [103.02209258079529 sec]
step 1000: train loss 1.5208, val loss 1.8197 [113.85229754447937 sec]
step 1100: train loss 1.4660, val loss 1.7922 [124.60928511619568 sec]
step 1200: train loss 1.4372, val loss 1.7664 [135.33344435691833 sec]
step 1300: train loss 1.4007, val loss 1.7507 [146.08657670021057 sec]
step 1400: train loss 1.3742, val loss 1.7539 [156.8543517589569 sec]
1.487092137336731
Total Training Time: 161.3915605545044 seconds

Aboutwn his feold where a mornessens the cirty
tuon'sself may huew he hround the airs
comfrom berfus
BEGINNING (1682034275.4954045): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5937, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6007, val loss 4.5993 [8.803478479385376 sec]
step 100: train loss 2.5101, val loss 2.5844 [24.53559112548828 sec]
step 200: train loss 2.3549, val loss 2.4534 [47.07140588760376 sec]
step 300: train loss 2.1366, val loss 2.2721 [67.0031533241272 sec]
step 400: train loss 1.9467, val loss 2.1163 [82.4855432510376 sec]
step 500: train loss 1.8139, val loss 2.0127 [97.9363923072815 sec]
step 600: train loss 1.7093, val loss 1.9344 [113.38880777359009 sec]
step 700: train loss 1.6276, val loss 1.8754 [128.83784532546997 sec]
step 800: train loss 1.5561, val loss 1.8409 [144.3104259967804 sec]
step 900: train loss 1.4929, val loss 1.7960 [159.77216291427612 sec]
step 1000: train loss 1.4478, val loss 1.7832 [175.2239272594452 sec]
step 1100: train loss 1.4020, val loss 1.7478 [190.6655957698822 sec]
step 1200: train loss 1.3583, val loss 1.7495 [206.18683290481567 sec]
step 1300: train loss 1.3240, val loss 1.7316 [221.69754338264465 sec]
step 1400: train loss 1.2871, val loss 1.7344 [237.1481487751007 sec]
1.4023585319519043
Total Training Time: 243.81951308250427 seconds

NG'st would me watcking Thas were nodded. After
the. Eove was axpted the squirrec his bew snay and o
BEGINNING (1682034521.0874245): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6155, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6092, val loss 4.6075 [5.558840274810791 sec]
step 100: train loss 2.5917, val loss 2.6631 [15.289028406143188 sec]
step 200: train loss 2.4868, val loss 2.5637 [25.15405511856079 sec]
step 300: train loss 2.4106, val loss 2.5034 [35.07226777076721 sec]
step 400: train loss 2.3095, val loss 2.4193 [44.92987370491028 sec]
step 500: train loss 2.1837, val loss 2.3042 [54.864988803863525 sec]
step 600: train loss 2.0450, val loss 2.2035 [64.7584912776947 sec]
step 700: train loss 1.9333, val loss 2.1147 [74.62501621246338 sec]
step 800: train loss 1.8485, val loss 2.0425 [84.53874373435974 sec]
step 900: train loss 1.7797, val loss 1.9936 [94.3974666595459 sec]
step 1000: train loss 1.7226, val loss 1.9503 [104.27506446838379 sec]
step 1100: train loss 1.6735, val loss 1.9150 [114.13253664970398 sec]
step 1200: train loss 1.6287, val loss 1.8915 [124.01404309272766 sec]
step 1300: train loss 1.5902, val loss 1.8652 [133.86688876152039 sec]
step 1400: train loss 1.5513, val loss 1.8353 [143.8717441558838 sec]
1.6369327306747437
Total Training Time: 148.03778553009033 seconds

HIND Di:g4
EWE MuzEved to KI390 Mck At Arphad saall a
sprocke of and dabout is, sic. Taka nort at Pe
BEGINNING (1682034669.7839935): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.7079, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7206, val loss 4.7147 [10.189682722091675 sec]
step 100: train loss 2.5541, val loss 2.6282 [28.117096424102783 sec]
step 200: train loss 2.4593, val loss 2.5383 [45.959317684173584 sec]
step 300: train loss 2.3547, val loss 2.4516 [63.84663510322571 sec]
step 400: train loss 2.1878, val loss 2.3086 [81.71170592308044 sec]
step 500: train loss 2.0201, val loss 2.1806 [99.5554358959198 sec]
step 600: train loss 1.8924, val loss 2.0745 [117.55806636810303 sec]
step 700: train loss 1.7927, val loss 1.9972 [135.47552704811096 sec]
step 800: train loss 1.7046, val loss 1.9423 [153.33436346054077 sec]
step 900: train loss 1.6402, val loss 1.8916 [171.29683423042297 sec]
step 1000: train loss 1.5844, val loss 1.8517 [190.8451144695282 sec]
step 1100: train loss 1.5237, val loss 1.8235 [224.0396430492401 sec]
step 1200: train loss 1.4778, val loss 1.8018 [250.37947988510132 sec]
step 1300: train loss 1.4337, val loss 1.7788 [271.5310015678406 sec]
step 1400: train loss 1.3958, val loss 1.7524 [301.59969568252563 sec]
1.4584934711456299
Total Training Time: 310.716881275177 seconds

"Take are wask, tack of yellechep he oped and
up buled, I wishink oth can frelendsed, of my mauw
mai
BEGINNING (1682034981.921146): Baseline LR(0.00016) Heads(6) Embeddings(384) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5510, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5478, val loss 4.5429 [22.467992544174194 sec]
step 100: train loss 2.5245, val loss 2.5951 [52.8553352355957 sec]
step 200: train loss 2.4295, val loss 2.5130 [78.89487290382385 sec]
step 300: train loss 2.3009, val loss 2.4112 [104.7602915763855 sec]
step 400: train loss 2.1084, val loss 2.2554 [130.3504981994629 sec]
step 500: train loss 1.9407, val loss 2.1134 [156.3722677230835 sec]
step 600: train loss 1.8156, val loss 2.0153 [182.22543501853943 sec]
step 700: train loss 1.7147, val loss 1.9480 [207.93783807754517 sec]
step 800: train loss 1.6274, val loss 1.8850 [234.57480120658875 sec]
step 900: train loss 1.5597, val loss 1.8566 [260.4337329864502 sec]
step 1000: train loss 1.4902, val loss 1.8031 [286.17246532440186 sec]
step 1100: train loss 1.4281, val loss 1.7777 [312.35010290145874 sec]
step 1200: train loss 1.3845, val loss 1.7704 [338.168664932251 sec]
step 1300: train loss 1.3407, val loss 1.7537 [363.93641424179077 sec]
step 1400: train loss 1.2879, val loss 1.7393 [389.7981472015381 sec]
1.3717395067214966
Total Training Time: 400.74784231185913 seconds

their opened and the may gotest they may at their was make a
smiled, and to outacch pour at they eye
BEGINNING (1682035384.6999986): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6662, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6557, val loss 4.6542 [2.8980159759521484 sec]
step 100: train loss 2.5431, val loss 2.6242 [7.848630428314209 sec]
step 200: train loss 2.3276, val loss 2.4242 [12.768100500106812 sec]
step 300: train loss 2.1466, val loss 2.2814 [17.571305513381958 sec]
step 400: train loss 2.0318, val loss 2.1951 [22.34117865562439 sec]
step 500: train loss 1.9527, val loss 2.1340 [27.266121864318848 sec]
step 600: train loss 1.8847, val loss 2.0698 [32.46066427230835 sec]
step 700: train loss 1.8165, val loss 2.0274 [37.20170283317566 sec]
step 800: train loss 1.7729, val loss 1.9808 [42.01462149620056 sec]
step 900: train loss 1.7398, val loss 1.9542 [46.79658889770508 sec]
step 1000: train loss 1.7118, val loss 1.9463 [51.6177237033844 sec]
step 1100: train loss 1.6726, val loss 1.9263 [56.41362690925598 sec]
step 1200: train loss 1.6508, val loss 1.8865 [61.12984561920166 sec]
step 1300: train loss 1.6242, val loss 1.9027 [65.88533234596252 sec]
step 1400: train loss 1.5898, val loss 1.8691 [70.63650822639465 sec]
1.7146562337875366
Total Training Time: 72.67900133132935 seconds

20~EAR – A CEKAY
A RIN– FNEW, mil know
area ie too he know is comef?" "I will thalk sight be henroun
BEGINNING (1682035458.1472175): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.6644, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6689, val loss 4.6713 [4.716048240661621 sec]
step 100: train loss 2.5084, val loss 2.5789 [13.362380981445312 sec]
step 200: train loss 2.2915, val loss 2.4015 [22.386466026306152 sec]
step 300: train loss 2.0932, val loss 2.2335 [31.170711755752563 sec]
step 400: train loss 1.9616, val loss 2.1344 [39.818925619125366 sec]
step 500: train loss 1.8462, val loss 2.0518 [48.49388337135315 sec]
step 600: train loss 1.7778, val loss 1.9867 [57.05138826370239 sec]
step 700: train loss 1.7221, val loss 1.9574 [65.62376594543457 sec]
step 800: train loss 1.6810, val loss 1.9122 [74.2316792011261 sec]
step 900: train loss 1.6177, val loss 1.8983 [82.73559236526489 sec]
step 1000: train loss 1.5932, val loss 1.8615 [91.71260738372803 sec]
step 1100: train loss 1.5572, val loss 1.8522 [100.3510947227478 sec]
step 1200: train loss 1.5274, val loss 1.8305 [109.03458452224731 sec]
step 1300: train loss 1.5042, val loss 1.8290 [117.56798553466797 sec]
step 1400: train loss 1.4809, val loss 1.8125 [126.16245579719543 sec]
1.5081021785736084
Total Training Time: 130.03536438941956 seconds

chaid to que cearth his catFing veice a
maeuw offerbution if hind he to the shout in lake as faster.
BEGINNING (1682035589.6451323): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.6174, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5836, val loss 4.5827 [6.729410648345947 sec]
step 100: train loss 2.4942, val loss 2.5702 [19.21969771385193 sec]
step 200: train loss 2.2799, val loss 2.3919 [32.034690856933594 sec]
step 300: train loss 2.0757, val loss 2.2169 [44.38042068481445 sec]
step 400: train loss 1.9426, val loss 2.1077 [56.68893241882324 sec]
step 500: train loss 1.8390, val loss 2.0318 [69.11893463134766 sec]
step 600: train loss 1.7546, val loss 1.9776 [81.37679600715637 sec]
step 700: train loss 1.6947, val loss 1.9505 [94.179758310318 sec]
step 800: train loss 1.6463, val loss 1.8989 [106.59521842002869 sec]
step 900: train loss 1.5986, val loss 1.8737 [118.9653971195221 sec]
step 1000: train loss 1.5638, val loss 1.8577 [131.54090094566345 sec]
step 1100: train loss 1.5247, val loss 1.8405 [143.82021689414978 sec]
step 1200: train loss 1.4899, val loss 1.8210 [156.60897994041443 sec]
step 1300: train loss 1.4570, val loss 1.8099 [168.92503809928894 sec]
step 1400: train loss 1.4391, val loss 1.7877 [181.20449447631836 sec]
1.4957001209259033
Total Training Time: 187.0263512134552 seconds

gain head bring and commanod a looked
at their was usel. This of ofur tent. Aour
dare nored 4cKAY
"D
BEGINNING (1682035778.9244034): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6671, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6507, val loss 4.6527 [3.2391607761383057 sec]
step 100: train loss 2.5573, val loss 2.6349 [8.898326396942139 sec]
step 200: train loss 2.4154, val loss 2.5105 [14.555062532424927 sec]
step 300: train loss 2.2592, val loss 2.3722 [20.234541416168213 sec]
step 400: train loss 2.0967, val loss 2.2374 [25.876677989959717 sec]
step 500: train loss 1.9919, val loss 2.1478 [31.742172241210938 sec]
step 600: train loss 1.9034, val loss 2.0925 [37.63181805610657 sec]
step 700: train loss 1.8277, val loss 2.0180 [43.27537775039673 sec]
step 800: train loss 1.7704, val loss 1.9877 [48.93795371055603 sec]
step 900: train loss 1.7136, val loss 1.9414 [54.643115282058716 sec]
step 1000: train loss 1.6743, val loss 1.9238 [60.3214750289917 sec]
step 1100: train loss 1.6367, val loss 1.9002 [65.98649716377258 sec]
step 1200: train loss 1.5979, val loss 1.8822 [71.64387941360474 sec]
step 1300: train loss 1.5684, val loss 1.8500 [77.35223627090454 sec]
step 1400: train loss 1.5425, val loss 1.8300 [83.04513812065125 sec]
1.6077426671981812
Total Training Time: 85.48547530174255 seconds

with Gratta awas dis neared. In take his sunage
PTER V – V ACDGrata looked up to posy face his myt
g
BEGINNING (1682035865.1930974): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6309, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6334, val loss 4.6298 [5.842419624328613 sec]
step 100: train loss 2.5179, val loss 2.5951 [16.52248215675354 sec]
step 200: train loss 2.3858, val loss 2.4843 [27.006624698638916 sec]
step 300: train loss 2.2062, val loss 2.3350 [37.4494514465332 sec]
step 400: train loss 2.0222, val loss 2.1880 [47.89649152755737 sec]
step 500: train loss 1.8813, val loss 2.0651 [58.348458766937256 sec]
step 600: train loss 1.7877, val loss 1.9977 [68.79516530036926 sec]
step 700: train loss 1.7041, val loss 1.9325 [79.43628478050232 sec]
step 800: train loss 1.6468, val loss 1.9027 [89.92892146110535 sec]
step 900: train loss 1.5863, val loss 1.8684 [100.40289545059204 sec]
step 1000: train loss 1.5423, val loss 1.8451 [110.86981797218323 sec]
step 1100: train loss 1.4914, val loss 1.7979 [121.32626748085022 sec]
step 1200: train loss 1.4663, val loss 1.7939 [131.83909130096436 sec]
step 1300: train loss 1.4270, val loss 1.7669 [142.4277765750885 sec]
step 1400: train loss 1.3939, val loss 1.7592 [152.96241402626038 sec]
1.4346284866333008
Total Training Time: 157.65084171295166 seconds

the learner. "Beft the gathers werk rodic onlighting.
Pelans of Thas's jorse and others, and wore at
BEGINNING (1682036024.3143158): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.5626, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5811, val loss 4.5740 [8.35146951675415 sec]
step 100: train loss 2.5055, val loss 2.5870 [23.626739025115967 sec]
step 200: train loss 2.3678, val loss 2.4656 [38.86898684501648 sec]
step 300: train loss 2.1674, val loss 2.2991 [54.27617955207825 sec]
step 400: train loss 1.9645, val loss 2.1431 [69.51633834838867 sec]
step 500: train loss 1.8451, val loss 2.0347 [84.7936499118805 sec]
step 600: train loss 1.7323, val loss 1.9616 [100.0472559928894 sec]
step 700: train loss 1.6613, val loss 1.9211 [115.45939421653748 sec]
step 800: train loss 1.5889, val loss 1.8648 [131.14572978019714 sec]
step 900: train loss 1.5373, val loss 1.8346 [146.37880635261536 sec]
step 1000: train loss 1.4915, val loss 1.8227 [161.57745885849 sec]
step 1100: train loss 1.4457, val loss 1.7892 [177.1542410850525 sec]
step 1200: train loss 1.3955, val loss 1.7740 [193.02764105796814 sec]
step 1300: train loss 1.3556, val loss 1.7586 [208.60448336601257 sec]
step 1400: train loss 1.3196, val loss 1.7366 [224.02681827545166 sec]
1.3847458362579346
Total Training Time: 231.47882962226868 seconds

worched by relannight they relerve."
"Twank. Namal pust with his paws, sidencens nainable,
67
SEAN p
BEGINNING (1682036258.4121368): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6073, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6059, val loss 4.6001 [4.903959035873413 sec]
step 100: train loss 2.5559, val loss 2.6393 [14.831476926803589 sec]
step 200: train loss 2.4608, val loss 2.5475 [23.636793613433838 sec]
step 300: train loss 2.3796, val loss 2.4756 [32.49790287017822 sec]
step 400: train loss 2.2506, val loss 2.3675 [41.226933002471924 sec]
step 500: train loss 2.1288, val loss 2.2671 [50.026867628097534 sec]
step 600: train loss 2.0164, val loss 2.1815 [58.802276611328125 sec]
step 700: train loss 1.9366, val loss 2.1173 [67.56198453903198 sec]
step 800: train loss 1.8628, val loss 2.0523 [76.2765097618103 sec]
step 900: train loss 1.7985, val loss 2.0037 [84.98573803901672 sec]
step 1000: train loss 1.7443, val loss 1.9650 [93.6438136100769 sec]
step 1100: train loss 1.6917, val loss 1.9146 [102.17806243896484 sec]
step 1200: train loss 1.6415, val loss 1.8858 [110.68220162391663 sec]
step 1300: train loss 1.6096, val loss 1.8677 [119.25057363510132 sec]
step 1400: train loss 1.5738, val loss 1.8395 [127.77229380607605 sec]
1.6926159858703613
Total Training Time: 131.4810791015625 seconds

suien storce strouck, Gratta loRding sientled and as
he lantt toldersimed from tour mil. Gristta not
BEGINNING (1682036390.7413096): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.5643, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5708, val loss 4.5608 [8.960089206695557 sec]
step 100: train loss 2.5275, val loss 2.6095 [25.26822018623352 sec]
step 200: train loss 2.4374, val loss 2.5257 [41.85952401161194 sec]
step 300: train loss 2.3332, val loss 2.4417 [60.3762629032135 sec]
step 400: train loss 2.1579, val loss 2.2906 [77.88121438026428 sec]
step 500: train loss 1.9961, val loss 2.1638 [94.88039922714233 sec]
step 600: train loss 1.8749, val loss 2.0691 [111.74916791915894 sec]
step 700: train loss 1.7762, val loss 2.0005 [129.8134322166443 sec]
step 800: train loss 1.6943, val loss 1.9399 [147.82124400138855 sec]
step 900: train loss 1.6300, val loss 1.8897 [169.1053659915924 sec]
step 1000: train loss 1.5758, val loss 1.8586 [189.42344903945923 sec]
step 1100: train loss 1.5175, val loss 1.8288 [208.68162441253662 sec]
step 1200: train loss 1.4730, val loss 1.8019 [227.4204659461975 sec]
step 1300: train loss 1.4372, val loss 1.8010 [245.9890537261963 sec]
step 1400: train loss 1.3947, val loss 1.7650 [263.2331826686859 sec]
1.4750776290893555
Total Training Time: 271.0594553947449 seconds

tuod to sire. "We he ission bew took most me any Zechariyah
foughial is delowy crossird. Where thive
BEGINNING (1682036663.4804926): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.5554, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5465, val loss 4.5360 [16.622775554656982 sec]
step 100: train loss 2.5134, val loss 2.5890 [48.91660690307617 sec]
step 200: train loss 2.4259, val loss 2.5246 [78.80490040779114 sec]
step 300: train loss 2.2953, val loss 2.4069 [109.48533248901367 sec]
step 400: train loss 2.0842, val loss 2.2384 [138.04928517341614 sec]
step 500: train loss 1.9135, val loss 2.0960 [166.60189414024353 sec]
step 600: train loss 1.7962, val loss 2.0025 [193.1101393699646 sec]
step 700: train loss 1.6917, val loss 1.9295 [217.90937042236328 sec]
step 800: train loss 1.6073, val loss 1.8782 [242.0635154247284 sec]
step 900: train loss 1.5399, val loss 1.8493 [268.41096210479736 sec]
step 1000: train loss 1.4936, val loss 1.8169 [295.1560640335083 sec]
step 1100: train loss 1.4253, val loss 1.7729 [320.4234335422516 sec]
step 1200: train loss 1.3773, val loss 1.7677 [345.3624458312988 sec]
step 1300: train loss 1.3333, val loss 1.7537 [371.0744082927704 sec]
step 1400: train loss 1.2868, val loss 1.7516 [394.96596097946167 sec]
1.38777494430542
Total Training Time: 406.0235900878906 seconds

its discems wortho not we!" Namal
Anayah and rappon with loons burt. As nayah surptand
hum stow
and 
BEGINNING (1682037071.8960614): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.5605, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5762, val loss 4.5757 [3.111341953277588 sec]
step 100: train loss 2.5098, val loss 2.5935 [8.47873306274414 sec]
step 200: train loss 2.2521, val loss 2.3685 [13.754692077636719 sec]
step 300: train loss 2.0957, val loss 2.2285 [19.023507595062256 sec]
step 400: train loss 1.9676, val loss 2.1349 [24.308762550354004 sec]
step 500: train loss 1.8802, val loss 2.0718 [29.626675128936768 sec]
step 600: train loss 1.8124, val loss 2.0390 [34.90521764755249 sec]
step 700: train loss 1.7557, val loss 1.9673 [40.19086480140686 sec]
step 800: train loss 1.7246, val loss 1.9548 [45.56511211395264 sec]
step 900: train loss 1.6626, val loss 1.9223 [50.853076696395874 sec]
step 1000: train loss 1.6325, val loss 1.8860 [56.19200921058655 sec]
step 1100: train loss 1.6018, val loss 1.8650 [62.15631866455078 sec]
step 1200: train loss 1.5802, val loss 1.8639 [67.85651683807373 sec]
step 1300: train loss 1.5587, val loss 1.8424 [74.15270447731018 sec]
step 1400: train loss 1.5332, val loss 1.8377 [79.83387875556946 sec]
1.547789216041565
Total Training Time: 82.12247276306152 seconds

fecushed spen-en othat was
knoth unt.
Gor frof the him. "The them the klowey stoon of maeuws."
Aidde
BEGINNING (1682037154.8400588): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5791, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5790, val loss 4.5809 [5.389098644256592 sec]
step 100: train loss 2.4783, val loss 2.5596 [15.112633228302002 sec]
step 200: train loss 2.2312, val loss 2.3473 [24.72813391685486 sec]
step 300: train loss 2.0150, val loss 2.1908 [34.34717035293579 sec]
step 400: train loss 1.8893, val loss 2.1000 [44.9266471862793 sec]
step 500: train loss 1.7950, val loss 1.9935 [54.73283910751343 sec]
step 600: train loss 1.7359, val loss 1.9615 [64.78585243225098 sec]
step 700: train loss 1.6572, val loss 1.8971 [74.6824197769165 sec]
step 800: train loss 1.6139, val loss 1.8791 [84.61851143836975 sec]
step 900: train loss 1.5618, val loss 1.8590 [95.51767134666443 sec]
step 1000: train loss 1.5228, val loss 1.8263 [106.56996440887451 sec]
step 1100: train loss 1.4883, val loss 1.8032 [116.55714535713196 sec]
step 1200: train loss 1.4677, val loss 1.8050 [126.23332834243774 sec]
step 1300: train loss 1.4342, val loss 1.7712 [135.8973228931427 sec]
step 1400: train loss 1.4077, val loss 1.7810 [145.69225811958313 sec]
1.491679310798645
Total Training Time: 149.9533188343048 seconds

24
CHAPTED – 8% V A DENGra, will the manew
it in is courge for half witenable same
to le! Tbuod have
BEGINNING (1682037306.3592603): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.7670, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7701, val loss 4.7776 [7.779902935028076 sec]
step 100: train loss 2.4591, val loss 2.5454 [22.305020093917847 sec]
step 200: train loss 2.2025, val loss 2.3199 [36.70422887802124 sec]
step 300: train loss 1.9978, val loss 2.1693 [50.88033986091614 sec]
step 400: train loss 1.8437, val loss 2.0433 [65.66240191459656 sec]
step 500: train loss 1.7446, val loss 1.9834 [80.17229270935059 sec]
step 600: train loss 1.6671, val loss 1.9365 [94.92808485031128 sec]
step 700: train loss 1.6025, val loss 1.8838 [109.12641787528992 sec]
step 800: train loss 1.5495, val loss 1.8411 [123.11878275871277 sec]
step 900: train loss 1.5035, val loss 1.8283 [137.00901579856873 sec]
step 1000: train loss 1.4695, val loss 1.8094 [150.96615052223206 sec]
step 1100: train loss 1.4308, val loss 1.7928 [165.09635829925537 sec]
step 1200: train loss 1.4067, val loss 1.8047 [179.0129313468933 sec]
step 1300: train loss 1.3681, val loss 1.7806 [192.94757866859436 sec]
step 1400: train loss 1.3446, val loss 1.7614 [207.01284337043762 sec]
1.4049259424209595
Total Training Time: 213.29505467414856 seconds

His tubs. We will genesed will of the
neerfally. He coldiers for ofser finishta
tuman cubit to them.
BEGINNING (1682037521.9425843): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6593, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6516, val loss 4.6430 [4.185428857803345 sec]
step 100: train loss 2.5353, val loss 2.6224 [11.452720642089844 sec]
step 200: train loss 2.3760, val loss 2.4727 [19.611031770706177 sec]
step 300: train loss 2.1896, val loss 2.3227 [26.93604564666748 sec]
step 400: train loss 2.0228, val loss 2.1872 [34.27673149108887 sec]
step 500: train loss 1.9008, val loss 2.0949 [41.59026741981506 sec]
step 600: train loss 1.8134, val loss 2.0185 [48.897061347961426 sec]
step 700: train loss 1.7440, val loss 1.9781 [56.00633406639099 sec]
step 800: train loss 1.6876, val loss 1.9258 [63.095093965530396 sec]
step 900: train loss 1.6320, val loss 1.8817 [70.22387146949768 sec]
step 1000: train loss 1.5881, val loss 1.8552 [77.45783162117004 sec]
step 1100: train loss 1.5618, val loss 1.8406 [84.55951642990112 sec]
step 1200: train loss 1.5245, val loss 1.8276 [91.64432692527771 sec]
step 1300: train loss 1.4868, val loss 1.8038 [98.76521182060242 sec]
step 1400: train loss 1.4630, val loss 1.8034 [106.28819227218628 sec]
1.536528468132019
Total Training Time: 109.65838027000427 seconds

SEAN McKAY mile! Nard reacited." Gratta stack
Roished ward."
12
SEAN McKAY
Arnayah with that whick t
BEGINNING (1682037632.44437): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6006, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6170, val loss 4.6165 [14.80982780456543 sec]
step 100: train loss 2.5037, val loss 2.5861 [29.764758348464966 sec]
step 200: train loss 2.3322, val loss 2.4341 [45.41983914375305 sec]
step 300: train loss 2.1015, val loss 2.2440 [59.8390793800354 sec]
step 400: train loss 1.9155, val loss 2.0986 [73.6887719631195 sec]
step 500: train loss 1.7995, val loss 1.9993 [88.09691047668457 sec]
step 600: train loss 1.7054, val loss 1.9312 [102.09566879272461 sec]
step 700: train loss 1.6281, val loss 1.8933 [115.21210503578186 sec]
step 800: train loss 1.5636, val loss 1.8349 [129.36634302139282 sec]
step 900: train loss 1.5123, val loss 1.8164 [142.64783239364624 sec]
step 1000: train loss 1.4609, val loss 1.8039 [156.93485569953918 sec]
step 1100: train loss 1.4185, val loss 1.7737 [170.4671769142151 sec]
step 1200: train loss 1.3859, val loss 1.7806 [184.18031644821167 sec]
step 1300: train loss 1.3424, val loss 1.7459 [197.3902142047882 sec]
step 1400: train loss 1.3069, val loss 1.7433 [210.6735987663269 sec]
1.3708043098449707
Total Training Time: 216.36163854599 seconds

the caite ovations and shall shoutrate of mold Trial's for the
honons, give as you motiany."
Name sl
BEGINNING (1682037850.3817956): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6797, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6848, val loss 4.6748 [11.560882091522217 sec]
step 100: train loss 2.4904, val loss 2.5785 [33.22652840614319 sec]
step 200: train loss 2.3086, val loss 2.4174 [52.86544322967529 sec]
step 300: train loss 2.0675, val loss 2.2247 [73.15642809867859 sec]
step 400: train loss 1.8717, val loss 2.0644 [93.53144240379333 sec]
step 500: train loss 1.7416, val loss 1.9713 [113.73130130767822 sec]
step 600: train loss 1.6440, val loss 1.8931 [133.97392082214355 sec]
step 700: train loss 1.5642, val loss 1.8519 [154.36243724822998 sec]
step 800: train loss 1.4931, val loss 1.8195 [174.5875027179718 sec]
step 900: train loss 1.4229, val loss 1.7818 [195.27217769622803 sec]
step 1000: train loss 1.3891, val loss 1.7684 [215.71531772613525 sec]
step 1100: train loss 1.3249, val loss 1.7534 [235.9548089504242 sec]
step 1200: train loss 1.2827, val loss 1.7373 [256.0021343231201 sec]
step 1300: train loss 1.2485, val loss 1.7447 [276.0078537464142 sec]
step 1400: train loss 1.2140, val loss 1.7299 [296.1412601470947 sec]
1.2886229753494263
Total Training Time: 305.1734800338745 seconds

s~opesed.
"My were – may a be attaited them to Yah Emine?"
Anayah vight and looked at to his mes at.
BEGINNING (1682038158.3309386): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.6269, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6305, val loss 4.6365 [6.93105149269104 sec]
step 100: train loss 2.5556, val loss 2.6347 [19.65330719947815 sec]
step 200: train loss 2.4561, val loss 2.5395 [34.018903732299805 sec]
step 300: train loss 2.3467, val loss 2.4582 [46.06222724914551 sec]
step 400: train loss 2.1990, val loss 2.3342 [59.018486738204956 sec]
step 500: train loss 2.0640, val loss 2.2165 [71.99598288536072 sec]
step 600: train loss 1.9673, val loss 2.1500 [84.04769229888916 sec]
step 700: train loss 1.8841, val loss 2.0800 [96.35425853729248 sec]
step 800: train loss 1.8042, val loss 2.0086 [108.48635792732239 sec]
step 900: train loss 1.7399, val loss 1.9599 [120.95630407333374 sec]
step 1000: train loss 1.6790, val loss 1.9290 [133.00028657913208 sec]
step 1100: train loss 1.6278, val loss 1.8853 [145.25835299491882 sec]
step 1200: train loss 1.5801, val loss 1.8581 [157.8639636039734 sec]
step 1300: train loss 1.5402, val loss 1.8322 [170.79258108139038 sec]
step 1400: train loss 1.4990, val loss 1.7963 [183.71930313110352 sec]
1.5592519044876099
Total Training Time: 189.9464294910431 seconds

The gatered and at onew dan, "No hin would et you."
"He to sear?"
"Yon Eryon!" They King Attakly a t
BEGINNING (1682038349.2319143): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5445, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5448, val loss 4.5494 [12.991369009017944 sec]
step 100: train loss 2.5195, val loss 2.6029 [35.18912315368652 sec]
step 200: train loss 2.4171, val loss 2.5087 [59.95415186882019 sec]
step 300: train loss 2.2814, val loss 2.3933 [84.28855586051941 sec]
step 400: train loss 2.0780, val loss 2.2226 [106.32959246635437 sec]
step 500: train loss 1.9091, val loss 2.0878 [133.6083278656006 sec]
step 600: train loss 1.7914, val loss 2.0057 [156.58933091163635 sec]
step 700: train loss 1.6970, val loss 1.9387 [178.43577647209167 sec]
step 800: train loss 1.6206, val loss 1.8807 [200.3087887763977 sec]
step 900: train loss 1.5474, val loss 1.8326 [222.15214943885803 sec]
step 1000: train loss 1.4922, val loss 1.8084 [244.1688175201416 sec]
step 1100: train loss 1.4324, val loss 1.7749 [265.9285521507263 sec]
step 1200: train loss 1.3860, val loss 1.7669 [288.38215041160583 sec]
step 1300: train loss 1.3415, val loss 1.7489 [312.6993782520294 sec]
step 1400: train loss 1.2970, val loss 1.7463 [337.0009114742279 sec]
1.3690770864486694
Total Training Time: 346.52887511253357 seconds

wage to the maeuw lives.". May of Arphad and
your had, "Ach is your comnuese wither growly to two th
BEGINNING (1682038697.6386504): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6305, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6250, val loss 4.6164 [17.92782235145569 sec]
step 100: train loss 2.5084, val loss 2.5908 [51.25384163856506 sec]
step 200: train loss 2.4077, val loss 2.4965 [86.88828778266907 sec]
step 300: train loss 2.2245, val loss 2.3459 [122.93962287902832 sec]
step 400: train loss 1.9924, val loss 2.1647 [154.7809636592865 sec]
step 500: train loss 1.8345, val loss 2.0502 [187.54239177703857 sec]
step 600: train loss 1.7108, val loss 1.9618 [219.25280928611755 sec]
step 700: train loss 1.6101, val loss 1.8782 [252.4244101047516 sec]
step 800: train loss 1.5388, val loss 1.8449 [287.11076974868774 sec]
step 900: train loss 1.4624, val loss 1.7939 [324.8385844230652 sec]
step 1000: train loss 1.3964, val loss 1.7735 [355.7832405567169 sec]
step 1100: train loss 1.3510, val loss 1.7655 [387.72583532333374 sec]
step 1200: train loss 1.2870, val loss 1.7367 [428.1467685699463 sec]
step 1300: train loss 1.2363, val loss 1.7339 [459.3056788444519 sec]
step 1400: train loss 1.1860, val loss 1.7317 [489.9169511795044 sec]
1.274579644203186
Total Training Time: 503.07169699668884 seconds

let be him away trefure that would my chekms ens? Wet chungeed
Bib-We parted to perful this many com
BEGINNING (1682039203.343326): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6374, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6442, val loss 4.6368 [3.711050271987915 sec]
step 100: train loss 2.5109, val loss 2.5929 [10.105238914489746 sec]
step 200: train loss 2.2163, val loss 2.3399 [16.468165397644043 sec]
step 300: train loss 2.0363, val loss 2.1953 [22.831351041793823 sec]
step 400: train loss 1.9118, val loss 2.0910 [29.180798530578613 sec]
step 500: train loss 1.8206, val loss 2.0201 [35.5430269241333 sec]
step 600: train loss 1.7595, val loss 1.9779 [41.95915770530701 sec]
step 700: train loss 1.6945, val loss 1.9401 [48.294201135635376 sec]
step 800: train loss 1.6629, val loss 1.9239 [54.827112674713135 sec]
step 900: train loss 1.6222, val loss 1.8887 [61.48056149482727 sec]
step 1000: train loss 1.5882, val loss 1.8591 [68.16437339782715 sec]
step 1100: train loss 1.5612, val loss 1.8483 [74.83566427230835 sec]
step 1200: train loss 1.5302, val loss 1.8349 [81.50671577453613 sec]
step 1300: train loss 1.5154, val loss 1.8227 [88.06527066230774 sec]
step 1400: train loss 1.4850, val loss 1.8040 [94.64345002174377 sec]
1.5814570188522339
Total Training Time: 97.39708185195923 seconds

Aiddens a launed that toward like and not reacuss free to fitht the had tuons brown his parted. "It 
BEGINNING (1682039301.6628478): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6216, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6332, val loss 4.6419 [6.5494935512542725 sec]
step 100: train loss 2.4522, val loss 2.5388 [18.014060020446777 sec]
step 200: train loss 2.1815, val loss 2.3054 [29.27247142791748 sec]
step 300: train loss 1.9795, val loss 2.1516 [40.476041316986084 sec]
step 400: train loss 1.8398, val loss 2.0380 [52.7749445438385 sec]
step 500: train loss 1.7414, val loss 1.9606 [63.94992661476135 sec]
step 600: train loss 1.6590, val loss 1.9047 [75.12779951095581 sec]
step 700: train loss 1.5982, val loss 1.8550 [86.37922358512878 sec]
step 800: train loss 1.5496, val loss 1.8440 [97.43636465072632 sec]
step 900: train loss 1.5114, val loss 1.8138 [108.62785053253174 sec]
step 1000: train loss 1.4693, val loss 1.7954 [119.69226312637329 sec]
step 1100: train loss 1.4466, val loss 1.7949 [130.77717423439026 sec]
step 1200: train loss 1.4107, val loss 1.7854 [142.1592516899109 sec]
step 1300: train loss 1.3795, val loss 1.7729 [153.17385077476501 sec]
step 1400: train loss 1.3483, val loss 1.7638 [164.24677538871765 sec]
1.4019652605056763
Total Training Time: 169.18061089515686 seconds

WhE Eldong hersuch they enoumang ooc salmed at they caluter and back.
"King bows it you follows?" An
BEGINNING (1682039472.5312638): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6266, val loss 4.6220 [8.836348533630371 sec]
step 100: train loss 2.4394, val loss 2.5220 [24.89293074607849 sec]
step 200: train loss 2.1640, val loss 2.2910 [41.09735631942749 sec]
step 300: train loss 1.9354, val loss 2.1105 [57.5035240650177 sec]
step 400: train loss 1.8034, val loss 1.9954 [73.92703461647034 sec]
step 500: train loss 1.6911, val loss 1.9281 [90.44157934188843 sec]
step 600: train loss 1.6055, val loss 1.8667 [107.29050040245056 sec]
step 700: train loss 1.5498, val loss 1.8342 [123.80437636375427 sec]
step 800: train loss 1.5011, val loss 1.8138 [140.13402438163757 sec]
step 900: train loss 1.4420, val loss 1.7893 [157.31355547904968 sec]
step 1000: train loss 1.4100, val loss 1.7772 [173.52643585205078 sec]
step 1100: train loss 1.3763, val loss 1.7686 [189.47587704658508 sec]
step 1200: train loss 1.3457, val loss 1.7624 [206.56411957740784 sec]
step 1300: train loss 1.3142, val loss 1.7531 [222.61771273612976 sec]
step 1400: train loss 1.2792, val loss 1.7481 [239.02900218963623 sec]
1.3447561264038086
Total Training Time: 246.19868230819702 seconds

General had
evers.
"An!"
Gratta smell with the might be offer
on thespen, but his fand wish spoke yo
BEGINNING (1682039721.26193): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.7205, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7126, val loss 4.7007 [5.102553367614746 sec]
step 100: train loss 2.5350, val loss 2.6133 [14.018773555755615 sec]
step 200: train loss 2.3456, val loss 2.4406 [22.93540596961975 sec]
step 300: train loss 2.1378, val loss 2.2752 [31.917745113372803 sec]
step 400: train loss 1.9592, val loss 2.1398 [40.97372508049011 sec]
step 500: train loss 1.8552, val loss 2.0679 [50.208415508270264 sec]
step 600: train loss 1.7619, val loss 1.9782 [59.20906066894531 sec]
step 700: train loss 1.6950, val loss 1.9257 [68.08964514732361 sec]
step 800: train loss 1.6374, val loss 1.8854 [76.99757790565491 sec]
step 900: train loss 1.5954, val loss 1.8508 [85.89066553115845 sec]
step 1000: train loss 1.5494, val loss 1.8333 [94.74358940124512 sec]
step 1100: train loss 1.5176, val loss 1.8206 [103.58096027374268 sec]
step 1200: train loss 1.4806, val loss 1.7843 [112.51414942741394 sec]
step 1300: train loss 1.4475, val loss 1.7707 [121.64720916748047 sec]
step 1400: train loss 1.4221, val loss 1.7631 [130.63809156417847 sec]
1.5353665351867676
Total Training Time: 134.39278388023376 seconds

lown upons!" Keanned. Or
wa looked. I will at hat want the tisked Pran to came age to
conue. We that
BEGINNING (1682039856.5520205): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5897, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5925, val loss 4.6008 [9.12404465675354 sec]
step 100: train loss 2.4846, val loss 2.5717 [25.224554538726807 sec]
step 200: train loss 2.2864, val loss 2.3996 [41.596357345581055 sec]
step 300: train loss 2.0501, val loss 2.2000 [58.30582904815674 sec]
step 400: train loss 1.8634, val loss 2.0638 [74.95729684829712 sec]
step 500: train loss 1.7366, val loss 1.9589 [91.5962872505188 sec]
step 600: train loss 1.6486, val loss 1.9076 [108.29249358177185 sec]
step 700: train loss 1.5661, val loss 1.8519 [125.10854172706604 sec]
step 800: train loss 1.5050, val loss 1.8048 [141.85047268867493 sec]
step 900: train loss 1.4502, val loss 1.7938 [158.63614344596863 sec]
step 1000: train loss 1.4083, val loss 1.7657 [175.35171103477478 sec]
step 1100: train loss 1.3581, val loss 1.7530 [192.09346556663513 sec]
step 1200: train loss 1.3182, val loss 1.7373 [208.7745397090912 sec]
step 1300: train loss 1.2790, val loss 1.7324 [225.54277682304382 sec]
step 1400: train loss 1.2357, val loss 1.7225 [242.25263381004333 sec]
1.315566897392273
Total Training Time: 249.46276593208313 seconds

do not raise all day a cad.
Anayah the pan tuon we goated us and raised to the Aidden 
it. "I namblo
BEGINNING (1682040107.749707): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6065, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6173, val loss 4.6087 [13.314823865890503 sec]
step 100: train loss 2.4749, val loss 2.5586 [37.36395502090454 sec]
step 200: train loss 2.2843, val loss 2.3872 [61.42023515701294 sec]
step 300: train loss 2.0100, val loss 2.1694 [85.35134530067444 sec]
step 400: train loss 1.8223, val loss 2.0244 [109.30805945396423 sec]
step 500: train loss 1.6917, val loss 1.9302 [133.37506318092346 sec]
step 600: train loss 1.5940, val loss 1.8709 [157.4017493724823 sec]
step 700: train loss 1.5107, val loss 1.8152 [182.05990839004517 sec]
step 800: train loss 1.4429, val loss 1.7764 [206.18940234184265 sec]
step 900: train loss 1.3798, val loss 1.7593 [230.09300875663757 sec]
step 1000: train loss 1.3289, val loss 1.7530 [254.797101020813 sec]
step 1100: train loss 1.2746, val loss 1.7372 [279.65379524230957 sec]
step 1200: train loss 1.2265, val loss 1.7206 [305.16519021987915 sec]
step 1300: train loss 1.1849, val loss 1.7244 [329.3162806034088 sec]
step 1400: train loss 1.1406, val loss 1.7464 [353.4810354709625 sec]
1.1971192359924316
Total Training Time: 364.4661395549774 seconds

the lave if alomact donight royautur lans that soly, anding,
and almong the nexted to ther dozen lir
BEGINNING (1682040475.1106548): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.6474, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6370, val loss 4.6373 [8.949037313461304 sec]
step 100: train loss 2.5438, val loss 2.6234 [25.403371334075928 sec]
step 200: train loss 2.4430, val loss 2.5318 [40.30720925331116 sec]
step 300: train loss 2.3340, val loss 2.4378 [55.16648316383362 sec]
step 400: train loss 2.1802, val loss 2.3047 [69.83093047142029 sec]
step 500: train loss 2.0346, val loss 2.1969 [84.53694009780884 sec]
step 600: train loss 1.9063, val loss 2.0934 [99.19342923164368 sec]
step 700: train loss 1.8095, val loss 2.0126 [113.81725978851318 sec]
step 800: train loss 1.7207, val loss 1.9413 [128.6313054561615 sec]
step 900: train loss 1.6496, val loss 1.8909 [143.3480360507965 sec]
step 1000: train loss 1.5903, val loss 1.8586 [158.11940121650696 sec]
step 1100: train loss 1.5440, val loss 1.8211 [174.31812858581543 sec]
step 1200: train loss 1.4944, val loss 1.7984 [190.24647879600525 sec]
step 1300: train loss 1.4591, val loss 1.7939 [205.0962655544281 sec]
step 1400: train loss 1.4164, val loss 1.7669 [222.03650569915771 sec]
1.507848858833313
Total Training Time: 228.6126527786255 seconds

been the ne."
The waskiled the cord is tractionf the fur of of their lashmsed
him, by seoplial telp.
BEGINNING (1682040704.7137926): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6382, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6295, val loss 4.6286 [16.41672658920288 sec]
step 100: train loss 2.5089, val loss 2.5882 [47.261340379714966 sec]
step 200: train loss 2.4005, val loss 2.4878 [76.47727131843567 sec]
step 300: train loss 2.2216, val loss 2.3431 [103.39576315879822 sec]
step 400: train loss 1.9775, val loss 2.1530 [139.59785509109497 sec]
step 500: train loss 1.8211, val loss 2.0266 [168.57203078269958 sec]
step 600: train loss 1.7036, val loss 1.9355 [195.51971125602722 sec]
step 700: train loss 1.6169, val loss 1.8756 [226.11079668998718 sec]
step 800: train loss 1.5388, val loss 1.8323 [256.07952666282654 sec]
step 900: train loss 1.4787, val loss 1.8070 [289.69070076942444 sec]
step 1000: train loss 1.4126, val loss 1.7675 [320.23331236839294 sec]
step 1100: train loss 1.3646, val loss 1.7468 [351.8256125450134 sec]
step 1200: train loss 1.3143, val loss 1.7453 [381.0566177368164 sec]
step 1300: train loss 1.2649, val loss 1.7277 [410.7257583141327 sec]
step 1400: train loss 1.2239, val loss 1.7287 [437.7615463733673 sec]
1.3044639825820923
Total Training Time: 448.5973711013794 seconds

Vo ore them he seed intering on the enjost."
36
CHAPTER II
CHAPTER V
44
CHAN CES KAY
had 's judgming
BEGINNING (1682041155.173142): Baseline LR(0.00016) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6081, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6022, val loss 4.6127 [23.363695859909058 sec]
step 100: train loss 2.4993, val loss 2.5835 [61.43126654624939 sec]
step 200: train loss 2.3839, val loss 2.4841 [99.59502863883972 sec]
step 300: train loss 2.2005, val loss 2.3227 [142.99445796012878 sec]
step 400: train loss 1.9546, val loss 2.1404 [181.11101055145264 sec]
step 500: train loss 1.7872, val loss 2.0010 [221.39936304092407 sec]
step 600: train loss 1.6594, val loss 1.9189 [262.6547267436981 sec]
step 700: train loss 1.5607, val loss 1.8470 [299.8942983150482 sec]
step 800: train loss 1.4723, val loss 1.8095 [341.17714405059814 sec]
step 900: train loss 1.3914, val loss 1.7719 [379.1650197505951 sec]
step 1000: train loss 1.3321, val loss 1.7476 [418.99899888038635 sec]
step 1100: train loss 1.2694, val loss 1.7345 [455.2896602153778 sec]
step 1200: train loss 1.2111, val loss 1.7220 [491.6670262813568 sec]
step 1300: train loss 1.1572, val loss 1.7159 [532.1950750350952 sec]
step 1400: train loss 1.1056, val loss 1.7427 [574.8490407466888 sec]
1.1985214948654175
Total Training Time: 589.9734816551208 seconds

Nayah. Shad milen, let ouking the Pyrran
Clans! We were peopled up the rimsents have to someonight,

BEGINNING (1682041748.3036573): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(2)
torch.Size([1024, 86])
tensor(4.6151, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6351, val loss 4.6355 [5.579552888870239 sec]
step 100: train loss 2.4842, val loss 2.5533 [14.946922540664673 sec]
step 200: train loss 2.2071, val loss 2.3270 [23.641835689544678 sec]
step 300: train loss 2.0369, val loss 2.2125 [32.268491983413696 sec]
step 400: train loss 1.9187, val loss 2.0966 [41.31828022003174 sec]
step 500: train loss 1.8316, val loss 2.0461 [50.07912611961365 sec]
step 600: train loss 1.7595, val loss 2.0061 [59.049593448638916 sec]
step 700: train loss 1.7139, val loss 1.9462 [68.02264547348022 sec]
step 800: train loss 1.6593, val loss 1.9194 [76.29842495918274 sec]
step 900: train loss 1.6227, val loss 1.8882 [84.49435949325562 sec]
step 1000: train loss 1.5940, val loss 1.8764 [92.57953095436096 sec]
step 1100: train loss 1.5589, val loss 1.8719 [101.40084743499756 sec]
step 1200: train loss 1.5355, val loss 1.8633 [110.31903553009033 sec]
step 1300: train loss 1.5038, val loss 1.8462 [119.30704283714294 sec]
step 1400: train loss 1.4928, val loss 1.8480 [128.4357395172119 sec]
1.583662748336792
Total Training Time: 133.04474401474 seconds

Gratta's held the ouve suuch
fir." Thas censt part anated, "I
am's?"
in was proked appriancould an t
BEGINNING (1682041882.7520213): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(4)
torch.Size([1024, 86])
tensor(4.5582, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5653, val loss 4.5712 [8.603834390640259 sec]
step 100: train loss 2.4558, val loss 2.5441 [24.536842584609985 sec]
step 200: train loss 2.1624, val loss 2.2998 [38.69696259498596 sec]
step 300: train loss 1.9738, val loss 2.1452 [52.77877163887024 sec]
step 400: train loss 1.8429, val loss 2.0418 [67.38407444953918 sec]
step 500: train loss 1.7496, val loss 1.9708 [81.71760439872742 sec]
step 600: train loss 1.6857, val loss 1.9283 [96.29281115531921 sec]
step 700: train loss 1.6186, val loss 1.8892 [111.71117830276489 sec]
step 800: train loss 1.5703, val loss 1.8656 [127.25831365585327 sec]
step 900: train loss 1.5326, val loss 1.8543 [143.78304839134216 sec]
step 1000: train loss 1.5054, val loss 1.8247 [157.7664511203766 sec]
step 1100: train loss 1.4605, val loss 1.8228 [171.78729248046875 sec]
step 1200: train loss 1.4273, val loss 1.7987 [187.00045466423035 sec]
step 1300: train loss 1.4023, val loss 1.7647 [202.49134492874146 sec]
step 1400: train loss 1.3849, val loss 1.7887 [217.8088846206665 sec]
1.5244636535644531
Total Training Time: 225.11475610733032 seconds

urnit frap-the other spear! What over
caul. Prrieling and Gratta had finding ansidering the
ruelt wa
BEGINNING (1682042110.5923126): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(32) Layers(6)
torch.Size([1024, 86])
tensor(4.5832, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5967, val loss 4.5880 [11.214874744415283 sec]
step 100: train loss 2.4698, val loss 2.5593 [32.91820740699768 sec]
step 200: train loss 2.2005, val loss 2.3301 [55.4853937625885 sec]
step 300: train loss 1.9896, val loss 2.1461 [75.64696526527405 sec]
step 400: train loss 1.8533, val loss 2.0483 [97.69316959381104 sec]
step 500: train loss 1.7362, val loss 1.9549 [119.88111925125122 sec]
step 600: train loss 1.6564, val loss 1.9104 [140.29759860038757 sec]
step 700: train loss 1.5976, val loss 1.8953 [162.17860507965088 sec]
step 800: train loss 1.5403, val loss 1.8499 [185.2785758972168 sec]
step 900: train loss 1.4990, val loss 1.8279 [206.71482181549072 sec]
step 1000: train loss 1.4549, val loss 1.8292 [228.63676118850708 sec]
step 1100: train loss 1.4231, val loss 1.7919 [250.17469143867493 sec]
step 1200: train loss 1.3859, val loss 1.7902 [271.82419633865356 sec]
step 1300: train loss 1.3486, val loss 1.8048 [293.92784118652344 sec]
step 1400: train loss 1.3229, val loss 1.7697 [314.57437014579773 sec]
1.3748174905776978
Total Training Time: 323.9550838470459 seconds

racked in the healls. Your conchild his
heare, and he peached hip cebax out at kmusted as yellonger 
BEGINNING (1682042438.1984723): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(2)
torch.Size([2048, 86])
tensor(4.6051, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6061, val loss 4.6126 [5.126148223876953 sec]
step 100: train loss 2.5003, val loss 2.5821 [14.672996282577515 sec]
step 200: train loss 2.3181, val loss 2.4262 [24.145769834518433 sec]
step 300: train loss 2.0894, val loss 2.2401 [33.5665819644928 sec]
step 400: train loss 1.9281, val loss 2.1171 [43.0740852355957 sec]
step 500: train loss 1.8265, val loss 2.0210 [52.72251582145691 sec]
step 600: train loss 1.7385, val loss 1.9648 [62.47117471694946 sec]
step 700: train loss 1.6651, val loss 1.9104 [72.25582075119019 sec]
step 800: train loss 1.6127, val loss 1.8832 [82.07509541511536 sec]
step 900: train loss 1.5636, val loss 1.8435 [91.81868076324463 sec]
step 1000: train loss 1.5345, val loss 1.8262 [101.57411742210388 sec]
step 1100: train loss 1.4907, val loss 1.8162 [111.59112405776978 sec]
step 1200: train loss 1.4470, val loss 1.7775 [121.32672572135925 sec]
step 1300: train loss 1.4190, val loss 1.7737 [131.03930163383484 sec]
step 1400: train loss 1.3886, val loss 1.7934 [140.8723328113556 sec]
1.3968393802642822
Total Training Time: 145.34737086296082 seconds

juch
is ten
to ent and who wold preled about the senmilaps and
hind towing a right throoes aroum.
La
BEGINNING (1682042584.835834): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(4)
torch.Size([2048, 86])
tensor(4.6000, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5982, val loss 4.6014 [9.695980310440063 sec]
step 100: train loss 2.4750, val loss 2.5575 [28.23439908027649 sec]
step 200: train loss 2.2771, val loss 2.3926 [46.67034411430359 sec]
step 300: train loss 2.0015, val loss 2.1816 [65.1604061126709 sec]
step 400: train loss 1.8333, val loss 2.0348 [84.42302513122559 sec]
step 500: train loss 1.7200, val loss 1.9573 [104.27845764160156 sec]
step 600: train loss 1.6342, val loss 1.9149 [122.44684028625488 sec]
step 700: train loss 1.5701, val loss 1.8720 [140.11896920204163 sec]
step 800: train loss 1.5033, val loss 1.8103 [157.75585079193115 sec]
step 900: train loss 1.4400, val loss 1.7916 [175.26063179969788 sec]
step 1000: train loss 1.4010, val loss 1.7982 [192.728262424469 sec]
step 1100: train loss 1.3493, val loss 1.7702 [210.2578296661377 sec]
step 1200: train loss 1.3134, val loss 1.7557 [227.7310562133789 sec]
step 1300: train loss 1.2728, val loss 1.7492 [245.21613788604736 sec]
step 1400: train loss 1.2395, val loss 1.7662 [262.67831587791443 sec]
1.351026177406311
Total Training Time: 270.8186092376709 seconds

had milit rescollong for the city and on tear. Anayah
undershief to last lose him kief him tends aft
BEGINNING (1682042857.956577): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(32) Layers(6)
torch.Size([2048, 86])
tensor(4.6099, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6216, val loss 4.6217 [13.652265787124634 sec]
step 100: train loss 2.4912, val loss 2.5804 [39.37702226638794 sec]
step 200: train loss 2.3074, val loss 2.4325 [65.06035614013672 sec]
step 300: train loss 2.0409, val loss 2.1872 [90.7109055519104 sec]
step 400: train loss 1.8374, val loss 2.0526 [116.40308117866516 sec]
step 500: train loss 1.7046, val loss 1.9403 [142.07070565223694 sec]
step 600: train loss 1.6040, val loss 1.8747 [167.75278759002686 sec]
step 700: train loss 1.5199, val loss 1.8281 [193.37273716926575 sec]
step 800: train loss 1.4541, val loss 1.8104 [219.04795742034912 sec]
step 900: train loss 1.3938, val loss 1.7828 [244.67085099220276 sec]
step 1000: train loss 1.3433, val loss 1.7909 [270.298894405365 sec]
step 1100: train loss 1.2950, val loss 1.7620 [295.9874370098114 sec]
step 1200: train loss 1.2379, val loss 1.7597 [321.6617510318756 sec]
step 1300: train loss 1.2011, val loss 1.7441 [347.23788714408875 sec]
step 1400: train loss 1.1524, val loss 1.7665 [372.85273790359497 sec]
1.2240480184555054
Total Training Time: 384.8315496444702 seconds

landed down. He who longooked for find
the trable the chartes. Think was expeching th was gent for
p
BEGINNING (1682043246.1793468): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(2)
torch.Size([4096, 86])
tensor(4.6677, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6672, val loss 4.6635 [8.030856847763062 sec]
step 100: train loss 2.5132, val loss 2.5941 [22.58785843849182 sec]
step 200: train loss 2.4199, val loss 2.5258 [37.15019631385803 sec]
step 300: train loss 2.2757, val loss 2.3926 [51.655245780944824 sec]
step 400: train loss 2.1026, val loss 2.2553 [66.22409868240356 sec]
step 500: train loss 1.9481, val loss 2.1247 [80.7759964466095 sec]
step 600: train loss 1.8433, val loss 2.0469 [95.35344386100769 sec]
step 700: train loss 1.7538, val loss 1.9943 [109.91356420516968 sec]
step 800: train loss 1.6723, val loss 1.9223 [124.42687702178955 sec]
step 900: train loss 1.6066, val loss 1.8886 [138.99312782287598 sec]
step 1000: train loss 1.5492, val loss 1.8420 [153.52076935768127 sec]
step 1100: train loss 1.5069, val loss 1.8342 [168.10011053085327 sec]
step 1200: train loss 1.4559, val loss 1.7934 [182.68586468696594 sec]
step 1300: train loss 1.4110, val loss 1.7835 [197.2232313156128 sec]
step 1400: train loss 1.3769, val loss 1.7554 [211.76313757896423 sec]
1.4561935663223267
Total Training Time: 218.2847616672516 seconds

hards a flight the
pardeive – as crulds and their of my any glabbte a
laver. Gratta," in wors king a
BEGINNING (1682043465.6427107): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(4)
torch.Size([4096, 86])
tensor(4.6143, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6079, val loss 4.6063 [15.317419290542603 sec]
step 100: train loss 2.4941, val loss 2.5782 [42.48094034194946 sec]
step 200: train loss 2.3822, val loss 2.4751 [69.65616011619568 sec]
step 300: train loss 2.1784, val loss 2.3088 [96.67566108703613 sec]
step 400: train loss 1.9453, val loss 2.1231 [123.84754943847656 sec]
step 500: train loss 1.8003, val loss 2.0073 [151.10840606689453 sec]
step 600: train loss 1.6851, val loss 1.9314 [178.18835067749023 sec]
step 700: train loss 1.6050, val loss 1.8802 [205.84384179115295 sec]
step 800: train loss 1.5179, val loss 1.8248 [233.03649997711182 sec]
step 900: train loss 1.4506, val loss 1.7988 [261.6667456626892 sec]
step 1000: train loss 1.3834, val loss 1.7643 [289.1810646057129 sec]
step 1100: train loss 1.3379, val loss 1.7413 [315.23712372779846 sec]
step 1200: train loss 1.2812, val loss 1.7445 [341.74014353752136 sec]
step 1300: train loss 1.2324, val loss 1.7354 [368.36208271980286 sec]
step 1400: train loss 1.1907, val loss 1.7463 [394.9673750400543 sec]
1.254677414894104
Total Training Time: 406.4580807685852 seconds

howks with out him.
The talked wound have radlized the
ton certant of The yourna's voice stake, and 
BEGINNING (1682043874.3458312): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(32) Layers(6)
torch.Size([4096, 86])
tensor(4.6544, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6600, val loss 4.6660 [21.791576623916626 sec]
step 100: train loss 2.4939, val loss 2.5703 [57.110623836517334 sec]
step 200: train loss 2.3985, val loss 2.4978 [91.50406408309937 sec]
step 300: train loss 2.1947, val loss 2.3249 [125.80997014045715 sec]
step 400: train loss 1.9111, val loss 2.1033 [159.92832112312317 sec]
step 500: train loss 1.7582, val loss 1.9984 [193.57545065879822 sec]
step 600: train loss 1.6137, val loss 1.8942 [227.57578372955322 sec]
step 700: train loss 1.5045, val loss 1.8438 [261.790381193161 sec]
step 800: train loss 1.4185, val loss 1.7880 [295.69007635116577 sec]
step 900: train loss 1.3440, val loss 1.7650 [330.57737827301025 sec]
step 1000: train loss 1.2785, val loss 1.7568 [364.64448642730713 sec]
step 1100: train loss 1.2134, val loss 1.7588 [399.6933617591858 sec]
step 1200: train loss 1.1475, val loss 1.7598 [434.0421853065491 sec]
step 1300: train loss 1.0895, val loss 1.7695 [468.9226977825165 sec]
step 1400: train loss 1.0326, val loss 1.7919 [502.92113614082336 sec]
1.1253012418746948
Total Training Time: 518.5666315555573 seconds

ould the all." Gratta nodded at the air. They no tuon
gan warriors, and swered. The tathe Prince has
BEGINNING (1682044396.434733): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(2)
torch.Size([1536, 86])
tensor(4.6194, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6304, val loss 4.6353 [4.479796886444092 sec]
step 100: train loss 2.4488, val loss 2.5304 [12.649799346923828 sec]
step 200: train loss 2.1393, val loss 2.2826 [20.72811508178711 sec]
step 300: train loss 1.9473, val loss 2.1160 [28.865090131759644 sec]
step 400: train loss 1.8283, val loss 2.0426 [37.026063442230225 sec]
step 500: train loss 1.7482, val loss 1.9756 [45.18061947822571 sec]
step 600: train loss 1.6873, val loss 1.9250 [53.257864236831665 sec]
step 700: train loss 1.6317, val loss 1.9031 [61.64330291748047 sec]
step 800: train loss 1.5908, val loss 1.8921 [69.6986973285675 sec]
step 900: train loss 1.5496, val loss 1.8468 [77.7997944355011 sec]
step 1000: train loss 1.5239, val loss 1.8282 [85.92651581764221 sec]
step 1100: train loss 1.4901, val loss 1.8191 [93.99221730232239 sec]
step 1200: train loss 1.4589, val loss 1.8195 [102.11092925071716 sec]
step 1300: train loss 1.4277, val loss 1.7876 [110.19355893135071 sec]
step 1400: train loss 1.4076, val loss 1.7932 [118.25717544555664 sec]
1.4965977668762207
Total Training Time: 121.87847590446472 seconds

"I human is in the tuons would, but and in trody% maught plans of to ent ing
tuon a guare, down't st
BEGINNING (1682044519.5423496): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(4)
torch.Size([1536, 86])
tensor(4.5644, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5546, val loss 4.5480 [8.304759740829468 sec]
step 100: train loss 2.4118, val loss 2.4964 [23.337281703948975 sec]
step 200: train loss 2.0868, val loss 2.2332 [38.52629780769348 sec]
step 300: train loss 1.8892, val loss 2.0751 [53.62305498123169 sec]
step 400: train loss 1.7656, val loss 1.9791 [68.87912249565125 sec]
step 500: train loss 1.6728, val loss 1.9310 [84.05234980583191 sec]
step 600: train loss 1.5968, val loss 1.8716 [99.15694403648376 sec]
step 700: train loss 1.5548, val loss 1.8530 [114.24272966384888 sec]
step 800: train loss 1.4946, val loss 1.8357 [129.31230878829956 sec]
step 900: train loss 1.4489, val loss 1.8038 [144.5905520915985 sec]
step 1000: train loss 1.4152, val loss 1.8138 [159.66755485534668 sec]
step 1100: train loss 1.3805, val loss 1.7945 [174.85176944732666 sec]
step 1200: train loss 1.3401, val loss 1.7941 [189.93142366409302 sec]
step 1300: train loss 1.3179, val loss 1.7903 [205.27302241325378 sec]
step 1400: train loss 1.2915, val loss 1.8037 [220.37929439544678 sec]
1.3836053609848022
Total Training Time: 227.37035608291626 seconds

He looked back a down. No will negot
28
CHAPTER V – A DESPERAVE
TO OF THE TUON
CHAPTER V
HAV THE – D
BEGINNING (1682044749.1052341): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(48) Layers(6)
torch.Size([1536, 86])
tensor(4.5733, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5793, val loss 4.5874 [11.706263065338135 sec]
step 100: train loss 2.4258, val loss 2.5169 [33.79048538208008 sec]
step 200: train loss 2.1066, val loss 2.2673 [56.045576333999634 sec]
step 300: train loss 1.8771, val loss 2.0699 [78.16196298599243 sec]
step 400: train loss 1.7494, val loss 1.9799 [100.25533509254456 sec]
step 500: train loss 1.6450, val loss 1.8933 [122.60807013511658 sec]
step 600: train loss 1.5606, val loss 1.8594 [144.6864047050476 sec]
step 700: train loss 1.5048, val loss 1.8076 [166.85308957099915 sec]
step 800: train loss 1.4470, val loss 1.8132 [189.0927391052246 sec]
step 900: train loss 1.4100, val loss 1.7944 [211.1586275100708 sec]
step 1000: train loss 1.3525, val loss 1.7782 [233.3539571762085 sec]
step 1100: train loss 1.3196, val loss 1.7713 [255.58085012435913 sec]
step 1200: train loss 1.2826, val loss 1.7697 [277.68504571914673 sec]
step 1300: train loss 1.2447, val loss 1.7986 [299.73669052124023 sec]
step 1400: train loss 1.2145, val loss 1.7854 [321.95446515083313 sec]
1.2824798822402954
Total Training Time: 332.2826852798462 seconds

looked at the heaeuws said, "I undical usuall tuon around himsself. Gratta
was necust to the first o
BEGINNING (1682045084.5926392): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(2)
torch.Size([3072, 86])
tensor(4.6352, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6414, val loss 4.6426 [6.489913702011108 sec]
step 100: train loss 2.4850, val loss 2.5658 [18.056017875671387 sec]
step 200: train loss 2.2498, val loss 2.3713 [29.613897800445557 sec]
step 300: train loss 2.0023, val loss 2.1604 [41.33675932884216 sec]
step 400: train loss 1.8480, val loss 2.0608 [52.925567865371704 sec]
step 500: train loss 1.7351, val loss 1.9514 [64.47600984573364 sec]
step 600: train loss 1.6572, val loss 1.9055 [76.08004379272461 sec]
step 700: train loss 1.5944, val loss 1.8746 [87.6714837551117 sec]
step 800: train loss 1.5419, val loss 1.8461 [99.29003548622131 sec]
step 900: train loss 1.4875, val loss 1.8158 [110.88936066627502 sec]
step 1000: train loss 1.4438, val loss 1.8071 [122.47445106506348 sec]
step 1100: train loss 1.4043, val loss 1.7779 [134.13823413848877 sec]
step 1200: train loss 1.3626, val loss 1.7790 [145.71002578735352 sec]
step 1300: train loss 1.3335, val loss 1.7539 [157.27037644386292 sec]
step 1400: train loss 1.3021, val loss 1.7558 [168.92891192436218 sec]
1.4019627571105957
Total Training Time: 174.04320812225342 seconds

too need be the paw surkels. We for voiced baning to make
surver. "The walls not eards." The was say
BEGINNING (1682045259.78115): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5884, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5730, val loss 4.5718 [12.13553524017334 sec]
step 100: train loss 2.4521, val loss 2.5361 [34.06355810165405 sec]
step 200: train loss 2.2354, val loss 2.3626 [55.95540165901184 sec]
step 300: train loss 1.9505, val loss 2.1351 [77.88275861740112 sec]
step 400: train loss 1.7666, val loss 1.9861 [99.74017429351807 sec]
step 500: train loss 1.6545, val loss 1.9010 [121.62613034248352 sec]
step 600: train loss 1.5581, val loss 1.8476 [143.5791482925415 sec]
step 700: train loss 1.4799, val loss 1.8083 [165.4412019252777 sec]
step 800: train loss 1.4062, val loss 1.7719 [187.32340455055237 sec]
step 900: train loss 1.3555, val loss 1.7799 [209.2715401649475 sec]
step 1000: train loss 1.3072, val loss 1.7651 [231.18932628631592 sec]
step 1100: train loss 1.2543, val loss 1.7443 [253.09848761558533 sec]
step 1200: train loss 1.2070, val loss 1.7451 [275.014680147171 sec]
step 1300: train loss 1.1694, val loss 1.7433 [296.8919415473938 sec]
step 1400: train loss 1.1231, val loss 1.7675 [318.7892806529999 sec]
1.2552658319473267
Total Training Time: 328.609130859375 seconds

sure. Stercheme quiest, he scoplote to moxniff two
recallonsic parts in to tharrop. The fire a maeus
BEGINNING (1682045590.567418): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6279, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6207, val loss 4.6163 [17.7479145526886 sec]
step 100: train loss 2.4638, val loss 2.5462 [48.98761177062988 sec]
step 200: train loss 2.2296, val loss 2.3635 [80.6183385848999 sec]
step 300: train loss 1.9258, val loss 2.1191 [111.91866445541382 sec]
step 400: train loss 1.7383, val loss 1.9652 [143.51282334327698 sec]
step 500: train loss 1.6059, val loss 1.8951 [174.91491532325745 sec]
step 600: train loss 1.5005, val loss 1.8104 [206.2005579471588 sec]
step 700: train loss 1.4188, val loss 1.7919 [237.29784512519836 sec]
step 800: train loss 1.3516, val loss 1.7751 [268.1809022426605 sec]
step 900: train loss 1.2774, val loss 1.7649 [299.1456677913666 sec]
step 1000: train loss 1.2138, val loss 1.7443 [330.065550327301 sec]
step 1100: train loss 1.1589, val loss 1.7635 [361.32024812698364 sec]
step 1200: train loss 1.1096, val loss 1.7777 [391.9166042804718 sec]
step 1300: train loss 1.0521, val loss 1.7863 [424.0343325138092 sec]
step 1400: train loss 0.9968, val loss 1.8113 [455.52264881134033 sec]
1.0582226514816284
Total Training Time: 469.3512008190155 seconds

Thas.?"
"I sent out in sure that that is yes? Whather
truchmelh men bushed his land this
ribe."
The 
BEGINNING (1682046063.2960844): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(2)
torch.Size([6144, 86])
tensor(4.5355, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5389, val loss 4.5272 [10.622799158096313 sec]
step 100: train loss 2.5013, val loss 2.5888 [28.69254732131958 sec]
step 200: train loss 2.3856, val loss 2.4914 [46.82368993759155 sec]
step 300: train loss 2.2151, val loss 2.3437 [64.81720042228699 sec]
step 400: train loss 1.9989, val loss 2.1672 [82.9261326789856 sec]
step 500: train loss 1.8479, val loss 2.0581 [100.97150754928589 sec]
step 600: train loss 1.7360, val loss 1.9603 [119.1305148601532 sec]
step 700: train loss 1.6485, val loss 1.9099 [137.364483833313 sec]
step 800: train loss 1.5656, val loss 1.8542 [155.69541263580322 sec]
step 900: train loss 1.4988, val loss 1.8041 [173.86876130104065 sec]
step 1000: train loss 1.4429, val loss 1.7899 [192.01714205741882 sec]
step 1100: train loss 1.3977, val loss 1.7688 [210.15725469589233 sec]
step 1200: train loss 1.3475, val loss 1.7579 [228.40697288513184 sec]
step 1300: train loss 1.3060, val loss 1.7574 [246.5319356918335 sec]
step 1400: train loss 1.2698, val loss 1.7359 [264.5805184841156 sec]
1.344533085823059
Total Training Time: 272.4384391307831 seconds

Pilesclans you beet that man imple."
His looked, and Graned and smiled told up reace
on if the the i
BEGINNING (1682046336.9291186): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5901, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5900, val loss 4.5993 [19.861937999725342 sec]
step 100: train loss 2.4871, val loss 2.5786 [50.83325004577637 sec]
step 200: train loss 2.3375, val loss 2.4482 [81.26147413253784 sec]
step 300: train loss 2.0569, val loss 2.2151 [112.26447701454163 sec]
step 400: train loss 1.8516, val loss 2.0530 [143.0561764240265 sec]
step 500: train loss 1.6985, val loss 1.9409 [173.33507752418518 sec]
step 600: train loss 1.5901, val loss 1.8831 [208.4220130443573 sec]
step 700: train loss 1.4938, val loss 1.8180 [239.83942937850952 sec]
step 800: train loss 1.4191, val loss 1.7915 [271.33657693862915 sec]
step 900: train loss 1.3537, val loss 1.7704 [302.70329332351685 sec]
step 1000: train loss 1.2863, val loss 1.7330 [334.4049894809723 sec]
step 1100: train loss 1.2262, val loss 1.7484 [365.71910405158997 sec]
step 1200: train loss 1.1638, val loss 1.7423 [396.79900097846985 sec]
step 1300: train loss 1.1094, val loss 1.7438 [428.1824812889099 sec]
step 1400: train loss 1.0516, val loss 1.7515 [459.86077523231506 sec]
1.1581718921661377
Total Training Time: 474.0598635673523 seconds

Anayah the lights of or loges. Fis cubitiat fod exame.
Tha) was were men me tonice swas face-14
mili
BEGINNING (1682046813.3339436): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6985, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6954, val loss 4.6867 [24.724710702896118 sec]
step 100: train loss 2.4797, val loss 2.5727 [68.68906211853027 sec]
step 200: train loss 2.3355, val loss 2.4498 [112.76052141189575 sec]
step 300: train loss 2.0260, val loss 2.1947 [157.29391837120056 sec]
step 400: train loss 1.7882, val loss 2.0145 [201.61363697052002 sec]
step 500: train loss 1.6326, val loss 1.9014 [251.44362425804138 sec]
step 600: train loss 1.5101, val loss 1.8486 [303.9629168510437 sec]
step 700: train loss 1.4042, val loss 1.7819 [348.12465715408325 sec]
step 800: train loss 1.3256, val loss 1.7703 [394.08870577812195 sec]
step 900: train loss 1.2468, val loss 1.7534 [442.21940302848816 sec]
step 1000: train loss 1.1663, val loss 1.7485 [490.7111325263977 sec]
step 1100: train loss 1.0831, val loss 1.7694 [537.8620510101318 sec]
step 1200: train loss 1.0087, val loss 1.7686 [585.4162375926971 sec]
step 1300: train loss 0.9462, val loss 1.8156 [629.6711225509644 sec]
step 1400: train loss 0.8605, val loss 1.8552 [686.3067882061005 sec]
0.9784095883369446
Total Training Time: 714.5874242782593 seconds

Vryth. Prounting coud, without human drunged with
also darts you preves. He had the gathered snarrow
BEGINNING (1682047531.664149): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(2)
torch.Size([2048, 86])
tensor(4.6633, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6475, val loss 4.6428 [5.258007049560547 sec]
step 100: train loss 2.4145, val loss 2.5038 [15.240040302276611 sec]
step 200: train loss 2.0857, val loss 2.2366 [24.86209726333618 sec]
step 300: train loss 1.9094, val loss 2.1028 [34.47938799858093 sec]
step 400: train loss 1.7961, val loss 2.0054 [43.93069934844971 sec]
step 500: train loss 1.7151, val loss 1.9528 [53.6393883228302 sec]
step 600: train loss 1.6535, val loss 1.9171 [64.27753067016602 sec]
step 700: train loss 1.5878, val loss 1.8661 [74.77711510658264 sec]
step 800: train loss 1.5446, val loss 1.8380 [84.51019382476807 sec]
step 900: train loss 1.5010, val loss 1.8269 [94.317715883255 sec]
step 1000: train loss 1.4682, val loss 1.8239 [104.06574869155884 sec]
step 1100: train loss 1.4497, val loss 1.8256 [113.87421011924744 sec]
step 1200: train loss 1.4089, val loss 1.7862 [123.60572147369385 sec]
step 1300: train loss 1.3840, val loss 1.7933 [132.92802023887634 sec]
step 1400: train loss 1.3559, val loss 1.7969 [142.24568486213684 sec]
1.4057854413986206
Total Training Time: 146.44065880775452 seconds

Pelarana's is hir are trickly sfatchied sushed the smile ofter." The would be a simpliese. Some was

BEGINNING (1682047679.2825518): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(4)
torch.Size([2048, 86])
tensor(4.6360, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6014, val loss 4.6044 [9.553926944732666 sec]
step 100: train loss 2.3793, val loss 2.4710 [27.141356468200684 sec]
step 200: train loss 2.0337, val loss 2.1967 [47.687291622161865 sec]
step 300: train loss 1.8327, val loss 2.0286 [65.63476276397705 sec]
step 400: train loss 1.7080, val loss 1.9485 [83.35658001899719 sec]
step 500: train loss 1.6236, val loss 1.8942 [100.9816575050354 sec]
step 600: train loss 1.5367, val loss 1.8472 [118.61787796020508 sec]
step 700: train loss 1.4859, val loss 1.8306 [136.28681588172913 sec]
step 800: train loss 1.4412, val loss 1.8039 [157.2533040046692 sec]
step 900: train loss 1.3902, val loss 1.8093 [174.7646164894104 sec]
step 1000: train loss 1.3527, val loss 1.7963 [195.43140721321106 sec]
step 1100: train loss 1.3081, val loss 1.7792 [215.89732837677002 sec]
step 1200: train loss 1.2788, val loss 1.7848 [237.02703022956848 sec]
step 1300: train loss 1.2486, val loss 1.7865 [254.97388863563538 sec]
step 1400: train loss 1.2120, val loss 1.7834 [272.6735532283783 sec]
1.275955319404602
Total Training Time: 280.8135459423065 seconds

Gratta wol discuerity oach the walls, and will do and rage an humans
rountled onk as had not was a n
BEGINNING (1682047962.7181492): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(32) Batch Size(64) Layers(6)
torch.Size([2048, 86])
tensor(4.6476, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6348, val loss 4.6432 [14.003561019897461 sec]
step 100: train loss 2.3907, val loss 2.4867 [40.10866379737854 sec]
step 200: train loss 2.0384, val loss 2.2032 [65.77946090698242 sec]
step 300: train loss 1.8311, val loss 2.0367 [91.23046636581421 sec]
step 400: train loss 1.6708, val loss 1.9152 [116.81276106834412 sec]
step 500: train loss 1.5861, val loss 1.8783 [142.4304223060608 sec]
step 600: train loss 1.5104, val loss 1.8414 [168.00060629844666 sec]
step 700: train loss 1.4536, val loss 1.7928 [195.10882663726807 sec]
step 800: train loss 1.3951, val loss 1.7866 [223.07774424552917 sec]
step 900: train loss 1.3464, val loss 1.7837 [255.37007999420166 sec]
step 1000: train loss 1.2938, val loss 1.7743 [290.08299946784973 sec]
step 1100: train loss 1.2583, val loss 1.7791 [324.94410467147827 sec]
step 1200: train loss 1.2145, val loss 1.7675 [356.89211678504944 sec]
step 1300: train loss 1.1767, val loss 1.7829 [388.59597730636597 sec]
step 1400: train loss 1.1414, val loss 1.8116 [420.49682211875916 sec]
1.1943501234054565
Total Training Time: 433.9537847042084 seconds

High Priest me gone down almost not sup, and rattion to stump distross
have live at Torial that much
BEGINNING (1682048399.9865835): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(2)
torch.Size([4096, 86])
tensor(4.6728, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6593, val loss 4.6562 [8.073512077331543 sec]
step 100: train loss 2.4663, val loss 2.5549 [22.390129566192627 sec]
step 200: train loss 2.2032, val loss 2.3263 [45.93979859352112 sec]
step 300: train loss 1.9628, val loss 2.1322 [67.91597652435303 sec]
step 400: train loss 1.8030, val loss 2.0029 [83.77973365783691 sec]
step 500: train loss 1.7010, val loss 1.9495 [98.10414934158325 sec]
step 600: train loss 1.6180, val loss 1.8653 [112.43827104568481 sec]
step 700: train loss 1.5519, val loss 1.8367 [127.86679124832153 sec]
step 800: train loss 1.4998, val loss 1.8224 [146.92201566696167 sec]
step 900: train loss 1.4437, val loss 1.7995 [164.19940423965454 sec]
step 1000: train loss 1.3874, val loss 1.7822 [179.78819918632507 sec]
step 1100: train loss 1.3490, val loss 1.7753 [195.0129952430725 sec]
step 1200: train loss 1.3089, val loss 1.7499 [209.33293461799622 sec]
step 1300: train loss 1.2681, val loss 1.7486 [223.62894749641418 sec]
step 1400: train loss 1.2299, val loss 1.7610 [238.70483803749084 sec]
1.298535704612732
Total Training Time: 247.70789051055908 seconds

A NEW Sman way stepped." Sen Gratta looked on all a negotian, mro
his cornizl?"
Gratta
was nodded, s
BEGINNING (1682048648.9220667): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5961, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5963, val loss 4.5987 [17.16122055053711 sec]
step 100: train loss 2.4475, val loss 2.5392 [45.15611696243286 sec]
step 200: train loss 2.1567, val loss 2.2997 [73.35900568962097 sec]
step 300: train loss 1.8723, val loss 2.0683 [102.86873602867126 sec]
step 400: train loss 1.7009, val loss 1.9502 [129.8786656856537 sec]
step 500: train loss 1.5719, val loss 1.8483 [156.52172088623047 sec]
step 600: train loss 1.4805, val loss 1.8098 [183.50583362579346 sec]
step 700: train loss 1.4093, val loss 1.7845 [211.0278079509735 sec]
step 800: train loss 1.3363, val loss 1.7698 [243.23119115829468 sec]
step 900: train loss 1.2810, val loss 1.7629 [272.6938488483429 sec]
step 1000: train loss 1.2251, val loss 1.7528 [300.7200274467468 sec]
step 1100: train loss 1.1722, val loss 1.7662 [327.43296098709106 sec]
step 1200: train loss 1.1189, val loss 1.7610 [354.1016263961792 sec]
step 1300: train loss 1.0676, val loss 1.7764 [380.7311017513275 sec]
step 1400: train loss 1.0215, val loss 1.7830 [407.5023937225342 sec]
1.1612777709960938
Total Training Time: 419.07942748069763 seconds

worth trime,"
GrattaIt– before a reached marchern what Arphad, but
the is a your words of the direvi
BEGINNING (1682049070.2971122): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5453, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5512, val loss 4.5505 [22.130808353424072 sec]
step 100: train loss 2.4517, val loss 2.5579 [58.74901556968689 sec]
step 200: train loss 2.1885, val loss 2.3181 [95.39052939414978 sec]
step 300: train loss 1.8777, val loss 2.0702 [135.56168246269226 sec]
step 400: train loss 1.6888, val loss 1.9338 [174.87488842010498 sec]
step 500: train loss 1.5505, val loss 1.8580 [211.31996273994446 sec]
step 600: train loss 1.4453, val loss 1.8103 [252.9055142402649 sec]
step 700: train loss 1.3577, val loss 1.7559 [293.34941720962524 sec]
step 800: train loss 1.2793, val loss 1.7432 [332.0457465648651 sec]
step 900: train loss 1.2147, val loss 1.7530 [369.6419475078583 sec]
step 1000: train loss 1.1517, val loss 1.7682 [406.16165018081665 sec]
step 1100: train loss 1.0796, val loss 1.7744 [443.19159722328186 sec]
step 1200: train loss 1.0212, val loss 1.7824 [479.6652672290802 sec]
step 1300: train loss 0.9546, val loss 1.8306 [516.7677648067474 sec]
step 1400: train loss 0.8958, val loss 1.8454 [556.3378837108612 sec]
0.9755991101264954
Total Training Time: 572.6672804355621 seconds

"Yes, yir!" all for the purings, but and left we still
as he."
"You will have be two negotiantices, 
BEGINNING (1682049647.0282087): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(2)
torch.Size([8192, 86])
tensor(4.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5865, val loss 4.5793 [15.517563819885254 sec]
step 100: train loss 2.4952, val loss 2.5864 [45.13215231895447 sec]
step 200: train loss 2.3675, val loss 2.4719 [76.13653659820557 sec]
step 300: train loss 2.1622, val loss 2.2994 [102.29088926315308 sec]
step 400: train loss 1.9722, val loss 2.1440 [124.55471611022949 sec]
step 500: train loss 1.8194, val loss 2.0188 [154.27492356300354 sec]
step 600: train loss 1.7086, val loss 1.9425 [176.58132433891296 sec]
step 700: train loss 1.6151, val loss 1.8776 [198.800222158432 sec]
step 800: train loss 1.5408, val loss 1.8453 [225.13542079925537 sec]
step 900: train loss 1.4716, val loss 1.8090 [254.17496538162231 sec]
step 1000: train loss 1.4172, val loss 1.7808 [278.23816418647766 sec]
step 1100: train loss 1.3672, val loss 1.7715 [305.81445574760437 sec]
step 1200: train loss 1.3094, val loss 1.7412 [332.55895471572876 sec]
step 1300: train loss 1.2613, val loss 1.7414 [354.86705923080444 sec]
step 1400: train loss 1.2180, val loss 1.7285 [381.8745005130768 sec]
1.2931668758392334
Total Training Time: 396.5139362812042 seconds

Gratta. "I shall, such of on is the Pyrran Chief Gra
Aidden."
Gratta nodded. "Thank It is fam a
his 
BEGINNING (1682050045.0446734): Baseline LR(0.00016) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6115, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6035, val loss 4.6030 [29.702221632003784 sec]
step 100: train loss 2.4753, val loss 2.5611 [76.07661700248718 sec]
step 200: train loss 2.3211, val loss 2.4311 [122.30623078346252 sec]
step 300: train loss 2.0149, val loss 2.1808 [184.7343144416809 sec]
step 400: train loss 1.7976, val loss 2.0109 [225.87617993354797 sec]
step 500: train loss 1.6502, val loss 1.9136 [264.1705846786499 sec]
step 600: train loss 1.5377, val loss 1.8516 [302.8409254550934 sec]
step 700: train loss 1.4471, val loss 1.8030 [346.3750286102295 sec]
step 800: train loss 1.3644, val loss 1.7763 [390.24788880348206 sec]
step 900: train loss 1.2898, val loss 1.7462 [428.2864055633545 sec]
step 1000: train loss 1.2283, val loss 1.7465 [480.1285800933838 sec]
step 1100: train loss 1.1572, val loss 1.7400 [531.794721364975 sec]
step 1200: train loss 1.0939, val loss 1.7476 [590.1036496162415 sec]
step 1300: train loss 1.0307, val loss 1.7694 [628.9284591674805 sec]
step 1400: train loss 0.9616, val loss 1.7926 [667.89888048172 sec]
1.0667495727539062
Total Training Time: 695.5143551826477 seconds

SEAN McKAY
subefore feliely."
Gratta looked around and alled birdle to fells, cany
guard with that i
