BEGINNING (1682052224.5473914): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6533, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6449, val loss 4.6357 [21.496329307556152 sec]
step 100: train loss 2.3244, val loss 2.4236 [38.95056128501892 sec]
step 200: train loss 1.8846, val loss 2.0887 [68.99483609199524 sec]
step 300: train loss 1.6664, val loss 1.9209 [111.40110111236572 sec]
step 400: train loss 1.5307, val loss 1.8287 [139.47035574913025 sec]
step 500: train loss 1.4287, val loss 1.8084 [163.5792145729065 sec]
step 600: train loss 1.3392, val loss 1.7802 [180.8676881790161 sec]
step 700: train loss 1.2782, val loss 1.7584 [192.82235646247864 sec]
step 800: train loss 1.2081, val loss 1.7633 [204.72425603866577 sec]
step 900: train loss 1.1626, val loss 1.7650 [216.50142073631287 sec]
step 1000: train loss 1.0983, val loss 1.8169 [228.27202582359314 sec]
step 1100: train loss 1.0365, val loss 1.8177 [240.03201055526733 sec]
step 1200: train loss 0.9936, val loss 1.8288 [251.83630061149597 sec]
step 1300: train loss 0.9399, val loss 1.8710 [263.63147807121277 sec]
step 1400: train loss 0.8896, val loss 1.8547 [275.4036955833435 sec]
step 1500: train loss 0.8389, val loss 1.8859 [287.15993785858154 sec]
step 1600: train loss 0.7936, val loss 1.9308 [298.9221315383911 sec]
step 1700: train loss 0.7430, val loss 1.9810 [310.68336272239685 sec]
step 1800: train loss 0.7011, val loss 2.0198 [322.44595885276794 sec]
step 1900: train loss 0.6637, val loss 2.0472 [334.19267106056213 sec]
step 2000: train loss 0.6127, val loss 2.1138 [345.9691755771637 sec]
step 2100: train loss 0.5726, val loss 2.1237 [357.746787071228 sec]
step 2200: train loss 0.5350, val loss 2.1741 [369.4942800998688 sec]
step 2300: train loss 0.4995, val loss 2.2431 [381.2514133453369 sec]
step 2400: train loss 0.4767, val loss 2.3301 [392.9911324977875 sec]
step 2500: train loss 0.4430, val loss 2.3398 [404.7342748641968 sec]
step 2600: train loss 0.4214, val loss 2.3524 [417.03648471832275 sec]
step 2700: train loss 0.3921, val loss 2.4583 [428.8241112232208 sec]
step 2800: train loss 0.3708, val loss 2.4891 [440.59112453460693 sec]
step 2900: train loss 0.3482, val loss 2.5424 [452.3681390285492 sec]
step 3000: train loss 0.3345, val loss 2.6105 [464.8201684951782 sec]
step 3100: train loss 0.3166, val loss 2.6406 [478.6571946144104 sec]
step 3200: train loss 0.3033, val loss 2.6856 [492.40125036239624 sec]
step 3300: train loss 0.2883, val loss 2.7323 [506.3322551250458 sec]
step 3400: train loss 0.2795, val loss 2.7763 [520.3346045017242 sec]
step 3500: train loss 0.2690, val loss 2.8075 [534.0294396877289 sec]
step 3600: train loss 0.2617, val loss 2.8199 [547.769020318985 sec]
step 3700: train loss 0.2520, val loss 2.9370 [561.1967196464539 sec]
step 3800: train loss 0.2441, val loss 2.9350 [574.7399718761444 sec]
step 3900: train loss 0.2427, val loss 2.9447 [586.7355554103851 sec]
step 4000: train loss 0.2344, val loss 2.9914 [598.4647324085236 sec]
step 4100: train loss 0.2310, val loss 3.0696 [610.2102813720703 sec]
step 4200: train loss 0.2303, val loss 3.1080 [621.9518716335297 sec]
step 4300: train loss 0.2248, val loss 3.0998 [633.6956911087036 sec]
step 4400: train loss 0.2182, val loss 3.1734 [645.4183619022369 sec]
step 4500: train loss 0.2166, val loss 3.1600 [657.138751745224 sec]
step 4600: train loss 0.2128, val loss 3.1510 [668.8565728664398 sec]
step 4700: train loss 0.2120, val loss 3.2155 [680.588237285614 sec]
step 4800: train loss 0.2107, val loss 3.2597 [692.3208417892456 sec]
step 4900: train loss 0.2090, val loss 3.2801 [704.1212792396545 sec]
0.3760831356048584
Total Training Time: 709.2264630794525 seconds

SEAN McKAY
space for him to lie down. One of his guards attacked him with the
critwain."
"Yes, sir!"
BEGINNING (1682052943.8461864): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6179, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6294, val loss 4.6239 [9.6113920211792 sec]
step 100: train loss 2.4111, val loss 2.4974 [27.141399383544922 sec]
step 200: train loss 1.9597, val loss 2.1370 [44.1996533870697 sec]
step 300: train loss 1.6968, val loss 1.9504 [61.266966342926025 sec]
step 400: train loss 1.5432, val loss 1.8587 [78.32043504714966 sec]
step 500: train loss 1.4235, val loss 1.7877 [95.54796957969666 sec]
step 600: train loss 1.3326, val loss 1.7621 [113.38879466056824 sec]
step 700: train loss 1.2558, val loss 1.7545 [130.41737604141235 sec]
step 800: train loss 1.1809, val loss 1.7516 [147.52264022827148 sec]
step 900: train loss 1.1191, val loss 1.7828 [164.6222846508026 sec]
step 1000: train loss 1.0481, val loss 1.7866 [181.6939091682434 sec]
step 1100: train loss 0.9852, val loss 1.8296 [198.73488593101501 sec]
step 1200: train loss 0.9151, val loss 1.8446 [215.78804683685303 sec]
step 1300: train loss 0.8683, val loss 1.8773 [232.9174087047577 sec]
step 1400: train loss 0.8041, val loss 1.9166 [249.94465255737305 sec]
step 1500: train loss 0.7503, val loss 1.9473 [266.9867932796478 sec]
step 1600: train loss 0.7002, val loss 2.0261 [284.0457639694214 sec]
step 1700: train loss 0.6468, val loss 2.0568 [301.2149200439453 sec]
step 1800: train loss 0.5916, val loss 2.0884 [318.2997612953186 sec]
step 1900: train loss 0.5493, val loss 2.1671 [335.46618700027466 sec]
step 2000: train loss 0.5032, val loss 2.2302 [352.77869153022766 sec]
step 2100: train loss 0.4609, val loss 2.2655 [369.91106247901917 sec]
step 2200: train loss 0.4236, val loss 2.3450 [386.9896733760834 sec]
step 2300: train loss 0.3989, val loss 2.4099 [404.04180550575256 sec]
step 2400: train loss 0.3696, val loss 2.4880 [421.1992127895355 sec]
step 2500: train loss 0.3443, val loss 2.4745 [438.3069808483124 sec]
step 2600: train loss 0.3260, val loss 2.5348 [455.31591796875 sec]
step 2700: train loss 0.3060, val loss 2.6257 [472.2914819717407 sec]
step 2800: train loss 0.2889, val loss 2.6895 [489.2757318019867 sec]
step 2900: train loss 0.2775, val loss 2.7565 [506.3556561470032 sec]
step 3000: train loss 0.2677, val loss 2.7610 [523.3450081348419 sec]
step 3100: train loss 0.2554, val loss 2.8557 [540.3468546867371 sec]
step 3200: train loss 0.2480, val loss 2.8514 [557.3646268844604 sec]
step 3300: train loss 0.2371, val loss 2.9271 [574.4524562358856 sec]
step 3400: train loss 0.2323, val loss 2.9542 [591.4472246170044 sec]
step 3500: train loss 0.2295, val loss 2.9528 [608.4425592422485 sec]
step 3600: train loss 0.2229, val loss 3.0692 [625.4570803642273 sec]
step 3700: train loss 0.2209, val loss 3.0955 [642.5114481449127 sec]
step 3800: train loss 0.2175, val loss 3.0942 [659.5059883594513 sec]
step 3900: train loss 0.2133, val loss 3.1592 [676.4992792606354 sec]
step 4000: train loss 0.2129, val loss 3.1084 [693.5437784194946 sec]
step 4100: train loss 0.2079, val loss 3.1785 [710.5583648681641 sec]
step 4200: train loss 0.2067, val loss 3.2335 [727.539449930191 sec]
step 4300: train loss 0.2027, val loss 3.2480 [744.5114583969116 sec]
step 4400: train loss 0.2011, val loss 3.3089 [761.5691463947296 sec]
step 4500: train loss 0.2008, val loss 3.3239 [778.5521514415741 sec]
step 4600: train loss 0.2000, val loss 3.3584 [795.5329303741455 sec]
step 4700: train loss 0.1959, val loss 3.3480 [812.5256948471069 sec]
step 4800: train loss 0.1960, val loss 3.3195 [829.5778276920319 sec]
step 4900: train loss 0.1943, val loss 3.3435 [846.5736503601074 sec]
0.2777579128742218
Total Training Time: 854.0670623779297 seconds

The taskmaster different –
but as far as Gratta could discuss what as well
sit down next to him. Gra
BEGINNING (1682053800.080116): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5716, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5675, val loss 4.5740 [10.95106554031372 sec]
step 100: train loss 2.4374, val loss 2.5278 [30.085361003875732 sec]
step 200: train loss 2.0359, val loss 2.2025 [49.48351240158081 sec]
step 300: train loss 1.7105, val loss 1.9607 [68.61298084259033 sec]
step 400: train loss 1.5094, val loss 1.8401 [87.78500604629517 sec]
step 500: train loss 1.3676, val loss 1.7705 [106.83883690834045 sec]
step 600: train loss 1.2500, val loss 1.7266 [126.12865424156189 sec]
step 700: train loss 1.1419, val loss 1.7403 [145.3952670097351 sec]
step 800: train loss 1.0382, val loss 1.7586 [164.35728931427002 sec]
step 900: train loss 0.9510, val loss 1.8097 [183.5907998085022 sec]
step 1000: train loss 0.8543, val loss 1.8450 [202.69645142555237 sec]
step 1100: train loss 0.7534, val loss 1.8764 [221.6403727531433 sec]
step 1200: train loss 0.6539, val loss 1.9775 [240.79609727859497 sec]
step 1300: train loss 0.5680, val loss 2.0764 [260.06737661361694 sec]
step 1400: train loss 0.4961, val loss 2.1446 [279.3167655467987 sec]
step 1500: train loss 0.4202, val loss 2.2500 [298.5614171028137 sec]
step 1600: train loss 0.3553, val loss 2.3620 [317.86348819732666 sec]
step 1700: train loss 0.2961, val loss 2.4290 [336.991464138031 sec]
step 1800: train loss 0.2571, val loss 2.5528 [356.2468378543854 sec]
step 1900: train loss 0.2254, val loss 2.6672 [375.5237579345703 sec]
step 2000: train loss 0.2008, val loss 2.7047 [394.7589294910431 sec]
step 2100: train loss 0.1814, val loss 2.8077 [413.9864249229431 sec]
step 2200: train loss 0.1677, val loss 2.8679 [433.27115511894226 sec]
step 2300: train loss 0.1543, val loss 3.0139 [452.5551519393921 sec]
step 2400: train loss 0.1477, val loss 3.0930 [471.80659651756287 sec]
step 2500: train loss 0.1402, val loss 3.1291 [491.0712797641754 sec]
step 2600: train loss 0.1350, val loss 3.1465 [510.37118101119995 sec]
step 2700: train loss 0.1282, val loss 3.2486 [529.6173570156097 sec]
step 2800: train loss 0.1275, val loss 3.3027 [548.6827273368835 sec]
step 2900: train loss 0.1223, val loss 3.3167 [567.8307411670685 sec]
step 3000: train loss 0.1200, val loss 3.3332 [586.9722054004669 sec]
step 3100: train loss 0.1183, val loss 3.3805 [606.1160669326782 sec]
step 3200: train loss 0.1175, val loss 3.3920 [625.1645889282227 sec]
step 3300: train loss 0.1137, val loss 3.4394 [644.4415059089661 sec]
step 3400: train loss 0.1132, val loss 3.5110 [663.7094919681549 sec]
step 3500: train loss 0.1115, val loss 3.5279 [682.9618213176727 sec]
step 3600: train loss 0.1115, val loss 3.5486 [702.2377610206604 sec]
step 3700: train loss 0.1094, val loss 3.6005 [721.4800615310669 sec]
step 3800: train loss 0.1077, val loss 3.6516 [740.7721834182739 sec]
step 3900: train loss 0.1067, val loss 3.6898 [760.0543127059937 sec]
step 4000: train loss 0.1064, val loss 3.6876 [779.3313875198364 sec]
step 4100: train loss 0.1058, val loss 3.7039 [798.5879235267639 sec]
step 4200: train loss 0.1053, val loss 3.7154 [817.866898059845 sec]
step 4300: train loss 0.1047, val loss 3.7227 [837.1535542011261 sec]
step 4400: train loss 0.1026, val loss 3.7533 [856.3969502449036 sec]
step 4500: train loss 0.1017, val loss 3.7812 [875.6403088569641 sec]
step 4600: train loss 0.1013, val loss 3.8116 [894.8958988189697 sec]
step 4700: train loss 0.1026, val loss 3.8494 [914.1952984333038 sec]
step 4800: train loss 0.1009, val loss 3.8644 [933.4720239639282 sec]
step 4900: train loss 0.1014, val loss 3.9012 [952.6954934597015 sec]
0.17191825807094574
Total Training Time: 961.0095205307007 seconds

amily it be represent and in front front of the words. The fithering the
Tuon Council? Surely the fa
BEGINNING (1682054762.732716): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6580, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6682, val loss 4.6683 [15.977899074554443 sec]
step 100: train loss 2.4641, val loss 2.5583 [43.43290734291077 sec]
step 200: train loss 2.1126, val loss 2.2715 [70.98426508903503 sec]
step 300: train loss 1.7495, val loss 1.9865 [98.39646768569946 sec]
step 400: train loss 1.5293, val loss 1.8600 [125.67900013923645 sec]
step 500: train loss 1.3684, val loss 1.7981 [153.0989727973938 sec]
step 600: train loss 1.2337, val loss 1.7535 [180.41175246238708 sec]
step 700: train loss 1.1251, val loss 1.7596 [207.95310950279236 sec]
step 800: train loss 1.0052, val loss 1.7804 [235.31236386299133 sec]
step 900: train loss 0.8906, val loss 1.8146 [262.7173011302948 sec]
step 1000: train loss 0.7868, val loss 1.8722 [290.16605043411255 sec]
step 1100: train loss 0.6778, val loss 1.9768 [317.580486536026 sec]
step 1200: train loss 0.5756, val loss 2.0181 [345.08929228782654 sec]
step 1300: train loss 0.4760, val loss 2.1628 [372.40768480300903 sec]
step 1400: train loss 0.3943, val loss 2.2477 [399.8041853904724 sec]
step 1500: train loss 0.3244, val loss 2.3758 [427.06209421157837 sec]
step 1600: train loss 0.2660, val loss 2.4878 [454.4022946357727 sec]
step 1700: train loss 0.2258, val loss 2.6595 [481.89855694770813 sec]
step 1800: train loss 0.1966, val loss 2.6973 [509.2464461326599 sec]
step 1900: train loss 0.1755, val loss 2.8488 [536.8294196128845 sec]
step 2000: train loss 0.1602, val loss 2.9252 [564.2855355739594 sec]
step 2100: train loss 0.1479, val loss 3.0575 [591.6683304309845 sec]
step 2200: train loss 0.1387, val loss 3.0961 [619.2613210678101 sec]
step 2300: train loss 0.1326, val loss 3.1620 [646.5348029136658 sec]
step 2400: train loss 0.1287, val loss 3.2179 [673.9358646869659 sec]
step 2500: train loss 0.1242, val loss 3.2967 [701.207001209259 sec]
step 2600: train loss 0.1223, val loss 3.3667 [728.5378317832947 sec]
step 2700: train loss 0.1192, val loss 3.3532 [755.7984457015991 sec]
step 2800: train loss 0.1155, val loss 3.4723 [783.2486898899078 sec]
step 2900: train loss 0.1144, val loss 3.5012 [810.7111401557922 sec]
step 3000: train loss 0.1139, val loss 3.5427 [838.0698072910309 sec]
step 3100: train loss 0.1121, val loss 3.5535 [865.6011166572571 sec]
step 3200: train loss 0.1100, val loss 3.6492 [893.1343748569489 sec]
step 3300: train loss 0.1090, val loss 3.6224 [920.6784727573395 sec]
step 3400: train loss 0.1082, val loss 3.7004 [948.0649061203003 sec]
step 3500: train loss 0.1063, val loss 3.7480 [975.3987774848938 sec]
step 3600: train loss 0.1059, val loss 3.7245 [1003.0156631469727 sec]
step 3700: train loss 0.1055, val loss 3.7539 [1030.4972853660583 sec]
step 3800: train loss 0.1038, val loss 3.7703 [1057.9442737102509 sec]
step 3900: train loss 0.1036, val loss 3.8096 [1085.576967716217 sec]
step 4000: train loss 0.1028, val loss 3.8665 [1112.770545721054 sec]
step 4100: train loss 0.1012, val loss 3.8077 [1140.1740863323212 sec]
step 4200: train loss 0.1015, val loss 3.8499 [1167.5800890922546 sec]
step 4300: train loss 0.1015, val loss 3.8551 [1195.037706375122 sec]
step 4400: train loss 0.1006, val loss 3.8731 [1222.218948841095 sec]
step 4500: train loss 0.0995, val loss 3.9523 [1249.6771125793457 sec]
step 4600: train loss 0.0998, val loss 3.9102 [1276.8475515842438 sec]
step 4700: train loss 0.0990, val loss 3.9247 [1304.0473048686981 sec]
step 4800: train loss 0.0978, val loss 3.9897 [1331.1979248523712 sec]
step 4900: train loss 0.0983, val loss 4.0102 [1358.5840656757355 sec]
0.14685983955860138
Total Training Time: 1370.2299811840057 seconds

to be camp. Slowly Gratta heard nodded and bd smiled. Aidden smell
gaze and out feling the first, wh
BEGINNING (1682056135.1979213): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5592, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5610, val loss 4.5642 [8.126012086868286 sec]
step 100: train loss 2.2917, val loss 2.3986 [22.549309730529785 sec]
step 200: train loss 1.8492, val loss 2.0441 [36.884984254837036 sec]
step 300: train loss 1.6145, val loss 1.9037 [51.23652195930481 sec]
step 400: train loss 1.4795, val loss 1.8117 [65.57954239845276 sec]
step 500: train loss 1.3792, val loss 1.8079 [79.961345911026 sec]
step 600: train loss 1.2811, val loss 1.7461 [94.31641626358032 sec]
step 700: train loss 1.2016, val loss 1.7571 [108.64828062057495 sec]
step 800: train loss 1.1319, val loss 1.7787 [123.00017619132996 sec]
step 900: train loss 1.0674, val loss 1.7885 [137.3514814376831 sec]
step 1000: train loss 0.9911, val loss 1.8192 [151.74730324745178 sec]
step 1100: train loss 0.9360, val loss 1.8312 [166.10566759109497 sec]
step 1200: train loss 0.8693, val loss 1.8671 [180.44415926933289 sec]
step 1300: train loss 0.8173, val loss 1.9126 [194.8014271259308 sec]
step 1400: train loss 0.7588, val loss 1.9697 [209.17633771896362 sec]
step 1500: train loss 0.6956, val loss 2.0088 [223.56425213813782 sec]
step 1600: train loss 0.6381, val loss 2.0363 [237.902193069458 sec]
step 1700: train loss 0.5922, val loss 2.1309 [252.24846935272217 sec]
step 1800: train loss 0.5424, val loss 2.1584 [266.5972213745117 sec]
step 1900: train loss 0.4994, val loss 2.2484 [280.9959809780121 sec]
step 2000: train loss 0.4588, val loss 2.2853 [295.3498718738556 sec]
step 2100: train loss 0.4233, val loss 2.3186 [309.6926097869873 sec]
step 2200: train loss 0.3906, val loss 2.4325 [324.04360580444336 sec]
step 2300: train loss 0.3648, val loss 2.4582 [338.3785605430603 sec]
step 2400: train loss 0.3415, val loss 2.5026 [352.7602379322052 sec]
step 2500: train loss 0.3187, val loss 2.5762 [367.10413432121277 sec]
step 2600: train loss 0.2983, val loss 2.6602 [381.45590710639954 sec]
step 2700: train loss 0.2869, val loss 2.7204 [395.8135733604431 sec]
step 2800: train loss 0.2735, val loss 2.7589 [410.2053084373474 sec]
step 2900: train loss 0.2623, val loss 2.8099 [424.5492355823517 sec]
step 3000: train loss 0.2520, val loss 2.8417 [438.89188528060913 sec]
step 3100: train loss 0.2471, val loss 2.8412 [453.22854495048523 sec]
step 3200: train loss 0.2378, val loss 2.9451 [467.5785758495331 sec]
step 3300: train loss 0.2316, val loss 2.9644 [481.95924830436707 sec]
step 3400: train loss 0.2242, val loss 3.0345 [496.3053069114685 sec]
step 3500: train loss 0.2227, val loss 3.0365 [510.68607449531555 sec]
step 3600: train loss 0.2187, val loss 3.0981 [525.0239777565002 sec]
step 3700: train loss 0.2144, val loss 3.1489 [539.3865511417389 sec]
step 3800: train loss 0.2102, val loss 3.1301 [553.7640743255615 sec]
step 3900: train loss 0.2134, val loss 3.1588 [568.1118495464325 sec]
step 4000: train loss 0.2071, val loss 3.1661 [582.4513339996338 sec]
step 4100: train loss 0.2065, val loss 3.2475 [596.7908861637115 sec]
step 4200: train loss 0.2011, val loss 3.2486 [611.1752388477325 sec]
step 4300: train loss 0.2005, val loss 3.2864 [625.5182275772095 sec]
step 4400: train loss 0.1981, val loss 3.2856 [639.8453159332275 sec]
step 4500: train loss 0.1965, val loss 3.3244 [654.1934473514557 sec]
step 4600: train loss 0.1944, val loss 3.3870 [668.5339112281799 sec]
step 4700: train loss 0.1947, val loss 3.3847 [682.9235601425171 sec]
step 4800: train loss 0.1920, val loss 3.3908 [697.2618253231049 sec]
step 4900: train loss 0.1919, val loss 3.4383 [711.6117861270905 sec]
0.2923130691051483
Total Training Time: 717.8308460712433 seconds

you."
Zechariyah now like for the soon. And darts with the
meant placed a paw around the ensweep you
BEGINNING (1682056854.4904525): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6676, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6745, val loss 4.6687 [11.700081586837769 sec]
step 100: train loss 2.3752, val loss 2.4677 [32.621567487716675 sec]
step 200: train loss 1.8906, val loss 2.0851 [53.49874567985535 sec]
step 300: train loss 1.6238, val loss 1.8905 [74.38290882110596 sec]
step 400: train loss 1.4663, val loss 1.7864 [95.30053234100342 sec]
step 500: train loss 1.3448, val loss 1.7650 [116.18725371360779 sec]
step 600: train loss 1.2475, val loss 1.7460 [137.0785686969757 sec]
step 700: train loss 1.1646, val loss 1.7368 [158.0107090473175 sec]
step 800: train loss 1.0837, val loss 1.7746 [178.90975403785706 sec]
step 900: train loss 1.0017, val loss 1.7941 [199.8041796684265 sec]
step 1000: train loss 0.9218, val loss 1.8425 [220.74295663833618 sec]
step 1100: train loss 0.8540, val loss 1.8715 [241.6240530014038 sec]
step 1200: train loss 0.7825, val loss 1.9066 [262.5225045681 sec]
step 1300: train loss 0.7052, val loss 1.9961 [283.4510850906372 sec]
step 1400: train loss 0.6417, val loss 2.0930 [304.33873105049133 sec]
step 1500: train loss 0.5795, val loss 2.1278 [325.2490584850311 sec]
step 1600: train loss 0.5248, val loss 2.1729 [346.1663181781769 sec]
step 1700: train loss 0.4689, val loss 2.2835 [367.0689251422882 sec]
step 1800: train loss 0.4221, val loss 2.3692 [387.9612686634064 sec]
step 1900: train loss 0.3822, val loss 2.4493 [408.8567123413086 sec]
step 2000: train loss 0.3445, val loss 2.4770 [429.77072620391846 sec]
step 2100: train loss 0.3186, val loss 2.5877 [450.6518642902374 sec]
step 2200: train loss 0.2994, val loss 2.6390 [471.53247690200806 sec]
step 2300: train loss 0.2828, val loss 2.7538 [492.4327368736267 sec]
step 2400: train loss 0.2650, val loss 2.7787 [513.3219723701477 sec]
step 2500: train loss 0.2520, val loss 2.8705 [534.2080335617065 sec]
step 2600: train loss 0.2386, val loss 2.9783 [555.1396040916443 sec]
step 2700: train loss 0.2342, val loss 2.9348 [576.0464792251587 sec]
step 2800: train loss 0.2260, val loss 2.9948 [597.013916015625 sec]
step 2900: train loss 0.2213, val loss 3.1355 [618.0169024467468 sec]
step 3000: train loss 0.2155, val loss 3.0864 [638.9094467163086 sec]
step 3100: train loss 0.2116, val loss 3.2177 [659.80664229393 sec]
step 3200: train loss 0.2108, val loss 3.1864 [680.731338262558 sec]
step 3300: train loss 0.2054, val loss 3.2229 [701.6098937988281 sec]
step 3400: train loss 0.2017, val loss 3.2946 [722.483362197876 sec]
step 3500: train loss 0.2003, val loss 3.2553 [743.4143946170807 sec]
step 3600: train loss 0.1963, val loss 3.3722 [764.2954936027527 sec]
step 3700: train loss 0.1959, val loss 3.3802 [785.1808457374573 sec]
step 3800: train loss 0.1933, val loss 3.3889 [806.086483001709 sec]
step 3900: train loss 0.1930, val loss 3.4568 [826.9993944168091 sec]
step 4000: train loss 0.1917, val loss 3.4260 [847.8824005126953 sec]
step 4100: train loss 0.1900, val loss 3.4771 [868.7707886695862 sec]
step 4200: train loss 0.1882, val loss 3.4767 [889.6880605220795 sec]
step 4300: train loss 0.1881, val loss 3.5095 [910.5821254253387 sec]
step 4400: train loss 0.1866, val loss 3.5421 [931.4902069568634 sec]
step 4500: train loss 0.1868, val loss 3.5584 [952.4108850955963 sec]
step 4600: train loss 0.1841, val loss 3.5866 [973.2943499088287 sec]
step 4700: train loss 0.1851, val loss 3.5320 [994.1719174385071 sec]
step 4800: train loss 0.1823, val loss 3.6045 [1015.0776283740997 sec]
step 4900: train loss 0.1834, val loss 3.6139 [1035.94504570961 sec]
0.26782742142677307
Total Training Time: 1045.136684179306 seconds

neck. Then he unsheathed
a long gold knife from his belt, cut the pigeon
in half, and put the halves
BEGINNING (1682057901.7858193): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5826, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5839, val loss 4.5944 [13.599154472351074 sec]
step 100: train loss 2.4199, val loss 2.5116 [36.328213691711426 sec]
step 200: train loss 1.9699, val loss 2.1397 [59.011698722839355 sec]
step 300: train loss 1.6508, val loss 1.9090 [81.46776509284973 sec]
step 400: train loss 1.4434, val loss 1.7930 [104.11988520622253 sec]
step 500: train loss 1.3055, val loss 1.7516 [127.34942841529846 sec]
step 600: train loss 1.1795, val loss 1.7414 [151.28971099853516 sec]
step 700: train loss 1.0486, val loss 1.7481 [175.7279336452484 sec]
step 800: train loss 0.9393, val loss 1.8016 [199.78583335876465 sec]
step 900: train loss 0.8289, val loss 1.8709 [223.0237057209015 sec]
step 1000: train loss 0.7158, val loss 1.9298 [247.0717649459839 sec]
step 1100: train loss 0.6124, val loss 2.0217 [271.1301622390747 sec]
step 1200: train loss 0.5162, val loss 2.1020 [295.37015175819397 sec]
step 1300: train loss 0.4229, val loss 2.2260 [319.7774884700775 sec]
step 1400: train loss 0.3504, val loss 2.3438 [344.072336435318 sec]
step 1500: train loss 0.2943, val loss 2.4825 [368.5527560710907 sec]
step 1600: train loss 0.2406, val loss 2.6047 [392.44923853874207 sec]
step 1700: train loss 0.2107, val loss 2.7129 [415.0157344341278 sec]
step 1800: train loss 0.1826, val loss 2.8286 [437.1753315925598 sec]
step 1900: train loss 0.1667, val loss 2.8857 [461.22493505477905 sec]
step 2000: train loss 0.1493, val loss 2.9853 [485.53496289253235 sec]
step 2100: train loss 0.1395, val loss 3.0525 [509.2307126522064 sec]
step 2200: train loss 0.1345, val loss 3.1833 [533.5397744178772 sec]
step 2300: train loss 0.1282, val loss 3.2106 [556.8535041809082 sec]
step 2400: train loss 0.1231, val loss 3.2957 [579.9875929355621 sec]
step 2500: train loss 0.1193, val loss 3.3673 [603.2309820652008 sec]
step 2600: train loss 0.1189, val loss 3.4074 [626.3135800361633 sec]
step 2700: train loss 0.1144, val loss 3.4283 [649.7886555194855 sec]
step 2800: train loss 0.1136, val loss 3.4915 [673.1572601795197 sec]
step 2900: train loss 0.1100, val loss 3.5750 [696.4850347042084 sec]
step 3000: train loss 0.1093, val loss 3.5647 [719.6605038642883 sec]
step 3100: train loss 0.1090, val loss 3.5977 [742.7327606678009 sec]
step 3200: train loss 0.1064, val loss 3.5948 [766.3173151016235 sec]
step 3300: train loss 0.1064, val loss 3.6451 [789.5704472064972 sec]
step 3400: train loss 0.1050, val loss 3.7111 [812.5419037342072 sec]
step 3500: train loss 0.1038, val loss 3.7249 [835.9263586997986 sec]
step 3600: train loss 0.1023, val loss 3.7436 [859.483067035675 sec]
step 3700: train loss 0.1020, val loss 3.7617 [882.3842966556549 sec]
step 3800: train loss 0.1016, val loss 3.7857 [905.6504225730896 sec]
step 3900: train loss 0.1004, val loss 3.8758 [928.8691885471344 sec]
step 4000: train loss 0.1003, val loss 3.8787 [952.2856028079987 sec]
step 4100: train loss 0.0994, val loss 3.8907 [975.1926155090332 sec]
step 4200: train loss 0.0991, val loss 3.8928 [998.1416308879852 sec]
step 4300: train loss 0.0975, val loss 3.9583 [1021.1654818058014 sec]
step 4400: train loss 0.0979, val loss 3.9656 [1044.17045712471 sec]
step 4500: train loss 0.0979, val loss 3.9421 [1067.3394629955292 sec]
step 4600: train loss 0.0959, val loss 3.9452 [1090.334363937378 sec]
step 4700: train loss 0.0969, val loss 4.0024 [1113.3525958061218 sec]
step 4800: train loss 0.0959, val loss 3.9870 [1136.4718780517578 sec]
step 4900: train loss 0.0961, val loss 4.0600 [1160.325246334076 sec]
0.15791501104831696
Total Training Time: 1170.4332084655762 seconds

the tuon deovices. The sounds of while cast a splce! Prothere turned to the
Fish Valley."
"For came 
BEGINNING (1682059073.815713): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6558, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6692, val loss 4.6713 [19.94776177406311 sec]
step 100: train loss 2.4607, val loss 2.5469 [52.633103132247925 sec]
step 200: train loss 2.0761, val loss 2.2252 [85.38884997367859 sec]
step 300: train loss 1.6899, val loss 1.9471 [118.10585021972656 sec]
step 400: train loss 1.4601, val loss 1.8077 [150.76292967796326 sec]
step 500: train loss 1.2934, val loss 1.7579 [183.5057611465454 sec]
step 600: train loss 1.1523, val loss 1.7421 [216.33537101745605 sec]
step 700: train loss 1.0187, val loss 1.7532 [249.00985264778137 sec]
step 800: train loss 0.8970, val loss 1.8020 [281.5627386569977 sec]
step 900: train loss 0.7690, val loss 1.9110 [314.3135406970978 sec]
step 1000: train loss 0.6386, val loss 1.9652 [346.99469017982483 sec]
step 1100: train loss 0.5190, val loss 2.0621 [379.57818245887756 sec]
step 1200: train loss 0.4041, val loss 2.2401 [412.28583550453186 sec]
step 1300: train loss 0.3224, val loss 2.3830 [444.9867091178894 sec]
step 1400: train loss 0.2577, val loss 2.5316 [477.74530005455017 sec]
step 1500: train loss 0.2076, val loss 2.6137 [510.42841935157776 sec]
step 1600: train loss 0.1787, val loss 2.7838 [542.9827945232391 sec]
step 1700: train loss 0.1591, val loss 2.8946 [575.5574200153351 sec]
step 1800: train loss 0.1440, val loss 3.0604 [608.1857120990753 sec]
step 1900: train loss 0.1344, val loss 3.0746 [640.7976982593536 sec]
step 2000: train loss 0.1265, val loss 3.1848 [673.4749038219452 sec]
step 2100: train loss 0.1222, val loss 3.2871 [706.1690258979797 sec]
step 2200: train loss 0.1215, val loss 3.3071 [738.774516582489 sec]
step 2300: train loss 0.1156, val loss 3.3683 [771.3610875606537 sec]
step 2400: train loss 0.1141, val loss 3.4672 [804.0272901058197 sec]
step 2500: train loss 0.1122, val loss 3.5380 [836.6028435230255 sec]
step 2600: train loss 0.1092, val loss 3.5262 [869.108503818512 sec]
step 2700: train loss 0.1092, val loss 3.5263 [902.0046832561493 sec]
step 2800: train loss 0.1067, val loss 3.6658 [934.5226402282715 sec]
step 2900: train loss 0.1054, val loss 3.6632 [968.3623452186584 sec]
step 3000: train loss 0.1033, val loss 3.7499 [1000.7018306255341 sec]
step 3100: train loss 0.1032, val loss 3.7442 [1033.1603150367737 sec]
step 3200: train loss 0.1023, val loss 3.7998 [1065.7787733078003 sec]
step 3300: train loss 0.1008, val loss 3.8891 [1098.2366750240326 sec]
step 3400: train loss 0.1003, val loss 3.8772 [1130.573359489441 sec]
step 3500: train loss 0.0986, val loss 3.8882 [1162.9061064720154 sec]
step 3600: train loss 0.0996, val loss 3.9454 [1195.3571226596832 sec]
step 3700: train loss 0.0976, val loss 3.9220 [1227.9229819774628 sec]
step 3800: train loss 0.0973, val loss 3.8950 [1260.2468144893646 sec]
step 3900: train loss 0.0975, val loss 3.9622 [1292.743411540985 sec]
step 4000: train loss 0.0961, val loss 3.9698 [1324.9733283519745 sec]
step 4100: train loss 0.0961, val loss 4.0411 [1357.482854604721 sec]
step 4200: train loss 0.0957, val loss 3.9702 [1390.178395986557 sec]
step 4300: train loss 0.0952, val loss 4.0366 [1422.5640046596527 sec]
step 4400: train loss 0.0944, val loss 4.0714 [1454.933569431305 sec]
step 4500: train loss 0.0943, val loss 4.0563 [1487.6248495578766 sec]
step 4600: train loss 0.0938, val loss 4.1567 [1520.1561887264252 sec]
step 4700: train loss 0.0936, val loss 4.0400 [1552.6766304969788 sec]
step 4800: train loss 0.0931, val loss 4.1258 [1584.8518090248108 sec]
step 4900: train loss 0.0929, val loss 4.1320 [1617.2513599395752 sec]
0.13911037147045135
Total Training Time: 1630.8871057033539 seconds

wearing before the weaker maeuw and forth of the Tuon
Council was meeting. Nearly half of the tuon a
BEGINNING (1682060706.9843378): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.5184, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5191, val loss 4.5352 [11.095351219177246 sec]
step 100: train loss 2.3853, val loss 2.4898 [31.260242700576782 sec]
step 200: train loss 1.9257, val loss 2.1124 [51.44691252708435 sec]
step 300: train loss 1.6796, val loss 1.9319 [71.78977274894714 sec]
step 400: train loss 1.5224, val loss 1.8476 [91.96126317977905 sec]
step 500: train loss 1.4084, val loss 1.7884 [112.1272964477539 sec]
step 600: train loss 1.3183, val loss 1.7779 [132.41189122200012 sec]
step 700: train loss 1.2308, val loss 1.7764 [152.55974960327148 sec]
step 800: train loss 1.1610, val loss 1.7973 [172.6806342601776 sec]
step 900: train loss 1.0896, val loss 1.7940 [192.82614731788635 sec]
step 1000: train loss 1.0192, val loss 1.8391 [212.9032974243164 sec]
step 1100: train loss 0.9580, val loss 1.8321 [233.03662085533142 sec]
step 1200: train loss 0.9040, val loss 1.9125 [253.14063477516174 sec]
step 1300: train loss 0.8258, val loss 1.9380 [273.3976581096649 sec]
step 1400: train loss 0.7630, val loss 1.9683 [293.52445125579834 sec]
step 1500: train loss 0.7099, val loss 2.0324 [313.6744363307953 sec]
step 1600: train loss 0.6508, val loss 2.0993 [333.9667024612427 sec]
step 1700: train loss 0.5941, val loss 2.1604 [354.0912621021271 sec]
step 1800: train loss 0.5350, val loss 2.2816 [374.2143108844757 sec]
step 1900: train loss 0.4911, val loss 2.3048 [394.4030730724335 sec]
step 2000: train loss 0.4533, val loss 2.3869 [414.6060609817505 sec]
step 2100: train loss 0.4122, val loss 2.4145 [434.7072718143463 sec]
step 2200: train loss 0.3805, val loss 2.5259 [454.83945965766907 sec]
step 2300: train loss 0.3523, val loss 2.5552 [474.9723081588745 sec]
step 2400: train loss 0.3296, val loss 2.6755 [495.0651924610138 sec]
step 2500: train loss 0.3083, val loss 2.6894 [515.1946005821228 sec]
step 2600: train loss 0.2871, val loss 2.7616 [535.464029788971 sec]
step 2700: train loss 0.2739, val loss 2.8240 [555.6054062843323 sec]
step 2800: train loss 0.2604, val loss 2.8832 [575.704822063446 sec]
step 2900: train loss 0.2565, val loss 2.9141 [595.8897607326508 sec]
step 3000: train loss 0.2429, val loss 2.9522 [616.2049849033356 sec]
step 3100: train loss 0.2375, val loss 3.0094 [636.3800368309021 sec]
step 3200: train loss 0.2298, val loss 3.0568 [656.5493772029877 sec]
step 3300: train loss 0.2238, val loss 3.1150 [676.6516728401184 sec]
step 3400: train loss 0.2196, val loss 3.1605 [696.7647142410278 sec]
step 3500: train loss 0.2197, val loss 3.1374 [716.918643951416 sec]
step 3600: train loss 0.2126, val loss 3.2548 [737.198988199234 sec]
step 3700: train loss 0.2118, val loss 3.2129 [757.309642791748 sec]
step 3800: train loss 0.2095, val loss 3.2612 [777.3711411952972 sec]
step 3900: train loss 0.2074, val loss 3.3240 [797.5929217338562 sec]
step 4000: train loss 0.2033, val loss 3.3911 [817.681236743927 sec]
step 4100: train loss 0.2029, val loss 3.3720 [837.7720365524292 sec]
step 4200: train loss 0.2000, val loss 3.4840 [857.9344539642334 sec]
step 4300: train loss 0.2003, val loss 3.4670 [878.0567722320557 sec]
step 4400: train loss 0.1977, val loss 3.4709 [898.1393220424652 sec]
step 4500: train loss 0.1947, val loss 3.4833 [918.2848932743073 sec]
step 4600: train loss 0.1944, val loss 3.5283 [938.435408115387 sec]
step 4700: train loss 0.1927, val loss 3.5809 [958.5327892303467 sec]
step 4800: train loss 0.1934, val loss 3.5561 [978.6411442756653 sec]
step 4900: train loss 0.1921, val loss 3.6231 [998.8818483352661 sec]
0.26972007751464844
Total Training Time: 1007.9060397148132 seconds

needs. Were there are they ran guards at the came to him. Ence Gratta
sea and Gor that was followed 
BEGINNING (1682061717.1069825): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.7194, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7261, val loss 4.7249 [16.215112447738647 sec]
step 100: train loss 2.5468, val loss 2.6277 [45.595709562301636 sec]
step 200: train loss 2.1656, val loss 2.3030 [74.54675674438477 sec]
step 300: train loss 1.8395, val loss 2.0645 [103.80700469017029 sec]
step 400: train loss 1.6286, val loss 1.9035 [133.19325017929077 sec]
step 500: train loss 1.4874, val loss 1.8258 [162.24154543876648 sec]
step 600: train loss 1.3760, val loss 1.7881 [191.6714210510254 sec]
step 700: train loss 1.2778, val loss 1.7581 [220.79067158699036 sec]
step 800: train loss 1.2077, val loss 1.7614 [250.21064376831055 sec]
step 900: train loss 1.1224, val loss 1.7786 [279.2166473865509 sec]
step 1000: train loss 1.0596, val loss 1.8221 [308.1471035480499 sec]
step 1100: train loss 0.9881, val loss 1.8441 [337.05673241615295 sec]
step 1200: train loss 0.9097, val loss 1.8521 [365.92249870300293 sec]
step 1300: train loss 0.8419, val loss 1.9077 [395.38195991516113 sec]
step 1400: train loss 0.7651, val loss 1.9383 [424.4030771255493 sec]
step 1500: train loss 0.7108, val loss 1.9878 [453.8510174751282 sec]
step 1600: train loss 0.6379, val loss 2.0656 [483.0801293849945 sec]
step 1700: train loss 0.5830, val loss 2.1367 [513.0932929515839 sec]
step 1800: train loss 0.5195, val loss 2.2162 [542.0013294219971 sec]
step 1900: train loss 0.4692, val loss 2.2853 [571.0907247066498 sec]
step 2000: train loss 0.4299, val loss 2.3417 [600.1444640159607 sec]
step 2100: train loss 0.3847, val loss 2.4587 [629.1096317768097 sec]
step 2200: train loss 0.3472, val loss 2.5564 [658.7781226634979 sec]
step 2300: train loss 0.3277, val loss 2.5787 [687.7165644168854 sec]
step 2400: train loss 0.3014, val loss 2.6533 [716.7769584655762 sec]
step 2500: train loss 0.2870, val loss 2.7858 [745.7199981212616 sec]
step 2600: train loss 0.2704, val loss 2.7922 [774.3390083312988 sec]
step 2700: train loss 0.2532, val loss 2.8406 [803.4255039691925 sec]
step 2800: train loss 0.2444, val loss 2.9261 [833.348554611206 sec]
step 2900: train loss 0.2387, val loss 2.9639 [865.6931233406067 sec]
step 3000: train loss 0.2321, val loss 3.0069 [897.978542804718 sec]
step 3100: train loss 0.2264, val loss 3.0327 [928.566243648529 sec]
step 3200: train loss 0.2185, val loss 3.1434 [959.7235083580017 sec]
step 3300: train loss 0.2154, val loss 3.1369 [990.6222968101501 sec]
step 3400: train loss 0.2106, val loss 3.2185 [1021.445853471756 sec]
step 3500: train loss 0.2114, val loss 3.2217 [1052.2956845760345 sec]
step 3600: train loss 0.2054, val loss 3.2508 [1083.1476335525513 sec]
step 3700: train loss 0.2054, val loss 3.2388 [1114.1667623519897 sec]
step 3800: train loss 0.2023, val loss 3.3041 [1145.2418863773346 sec]
step 3900: train loss 0.2007, val loss 3.3522 [1176.3964145183563 sec]
step 4000: train loss 0.1999, val loss 3.3616 [1207.5159947872162 sec]
step 4100: train loss 0.1982, val loss 3.4068 [1238.5230190753937 sec]
step 4200: train loss 0.1968, val loss 3.4368 [1267.7782611846924 sec]
step 4300: train loss 0.1928, val loss 3.5139 [1296.8578684329987 sec]
step 4400: train loss 0.1950, val loss 3.5060 [1326.353652715683 sec]
step 4500: train loss 0.1931, val loss 3.4982 [1355.4852221012115 sec]
step 4600: train loss 0.1905, val loss 3.5458 [1384.875864982605 sec]
step 4700: train loss 0.1890, val loss 3.4906 [1413.9585235118866 sec]
step 4800: train loss 0.1894, val loss 3.5496 [1443.322458744049 sec]
step 4900: train loss 0.1882, val loss 3.6396 [1472.617509841919 sec]
0.254733145236969
Total Training Time: 1485.8109641075134 seconds

concil just as he had done for Namal.
"To the right is Chief Taka, to the grade as word our
looked l
BEGINNING (1682063206.2222815): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5685, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5704, val loss 4.5669 [19.089595317840576 sec]
step 100: train loss 2.4645, val loss 2.5487 [49.99128532409668 sec]
step 200: train loss 2.1163, val loss 2.2711 [80.80506229400635 sec]
step 300: train loss 1.7157, val loss 1.9627 [111.32907199859619 sec]
step 400: train loss 1.4973, val loss 1.8343 [142.1208028793335 sec]
step 500: train loss 1.3307, val loss 1.7891 [172.93544936180115 sec]
step 600: train loss 1.1863, val loss 1.7665 [203.4311821460724 sec]
step 700: train loss 1.0607, val loss 1.7850 [234.30818152427673 sec]
step 800: train loss 0.9307, val loss 1.8237 [265.13449025154114 sec]
step 900: train loss 0.8165, val loss 1.8859 [296.14265847206116 sec]
step 1000: train loss 0.6879, val loss 2.0120 [327.3808379173279 sec]
step 1100: train loss 0.5715, val loss 2.1102 [358.42062425613403 sec]
step 1200: train loss 0.4589, val loss 2.2615 [389.5255603790283 sec]
step 1300: train loss 0.3705, val loss 2.3572 [420.49204087257385 sec]
step 1400: train loss 0.2954, val loss 2.4645 [451.36422395706177 sec]
step 1500: train loss 0.2357, val loss 2.6140 [482.4183053970337 sec]
step 1600: train loss 0.1996, val loss 2.7626 [513.4014501571655 sec]
step 1700: train loss 0.1774, val loss 2.8773 [544.2951011657715 sec]
step 1800: train loss 0.1611, val loss 3.0025 [575.0901539325714 sec]
step 1900: train loss 0.1458, val loss 3.0562 [605.6637082099915 sec]
step 2000: train loss 0.1364, val loss 3.1707 [636.6039273738861 sec]
step 2100: train loss 0.1297, val loss 3.2866 [667.6231667995453 sec]
step 2200: train loss 0.1240, val loss 3.2960 [698.7985389232635 sec]
step 2300: train loss 0.1206, val loss 3.3893 [729.3203880786896 sec]
step 2400: train loss 0.1173, val loss 3.3986 [760.7183268070221 sec]
step 2500: train loss 0.1153, val loss 3.5171 [791.480325460434 sec]
step 2600: train loss 0.1137, val loss 3.5246 [822.4264497756958 sec]
step 2700: train loss 0.1107, val loss 3.6113 [853.395943403244 sec]
step 2800: train loss 0.1087, val loss 3.5971 [884.3835458755493 sec]
step 2900: train loss 0.1074, val loss 3.7190 [915.3158688545227 sec]
step 3000: train loss 0.1066, val loss 3.6600 [946.3771469593048 sec]
step 3100: train loss 0.1059, val loss 3.7413 [977.1927826404572 sec]
step 3200: train loss 0.1043, val loss 3.7963 [1008.0068349838257 sec]
step 3300: train loss 0.1031, val loss 3.8138 [1038.634271621704 sec]
step 3400: train loss 0.1022, val loss 3.8831 [1069.463860988617 sec]
step 3500: train loss 0.1008, val loss 3.8573 [1100.1092522144318 sec]
step 3600: train loss 0.1010, val loss 3.9414 [1131.384150981903 sec]
step 3700: train loss 0.1006, val loss 3.9088 [1162.3004145622253 sec]
step 3800: train loss 0.0990, val loss 3.9469 [1192.9118871688843 sec]
step 3900: train loss 0.0995, val loss 3.9599 [1223.6217558383942 sec]
step 4000: train loss 0.0983, val loss 3.9126 [1254.5651218891144 sec]
step 4100: train loss 0.0985, val loss 3.9814 [1285.3939492702484 sec]
step 4200: train loss 0.0975, val loss 4.0282 [1316.0512290000916 sec]
step 4300: train loss 0.0965, val loss 4.0767 [1346.911610364914 sec]
step 4400: train loss 0.0964, val loss 4.0630 [1377.5540118217468 sec]
step 4500: train loss 0.0966, val loss 4.0384 [1408.486025094986 sec]
step 4600: train loss 0.0958, val loss 4.1977 [1439.295017004013 sec]
step 4700: train loss 0.0952, val loss 4.0583 [1470.0008780956268 sec]
step 4800: train loss 0.0956, val loss 4.0961 [1500.750067949295 sec]
step 4900: train loss 0.0940, val loss 4.1959 [1532.1600186824799 sec]
0.1412997543811798
Total Training Time: 1545.8367545604706 seconds

Repority is of Torial. – now that was different. They had too seen the
table where two cords away a 
BEGINNING (1682064754.4045594): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6436, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6386, val loss 4.6500 [26.566778421401978 sec]
step 100: train loss 2.5535, val loss 2.6341 [69.10886120796204 sec]
step 200: train loss 2.3912, val loss 2.5007 [110.89646244049072 sec]
step 300: train loss 1.9699, val loss 2.1671 [152.75957369804382 sec]
step 400: train loss 1.6447, val loss 1.9288 [194.64871096611023 sec]
step 500: train loss 1.4187, val loss 1.8180 [236.63662934303284 sec]
step 600: train loss 1.2426, val loss 1.7632 [278.57852506637573 sec]
step 700: train loss 1.0951, val loss 1.7923 [320.4976136684418 sec]
step 800: train loss 0.9455, val loss 1.8222 [362.4539647102356 sec]
step 900: train loss 0.8038, val loss 1.9008 [404.5121428966522 sec]
step 1000: train loss 0.6594, val loss 2.0111 [446.53875613212585 sec]
step 1100: train loss 0.5281, val loss 2.1638 [488.5344843864441 sec]
step 1200: train loss 0.4112, val loss 2.2569 [530.8006913661957 sec]
step 1300: train loss 0.3142, val loss 2.4280 [572.77743268013 sec]
step 1400: train loss 0.2553, val loss 2.5272 [614.7840807437897 sec]
step 1500: train loss 0.2003, val loss 2.7350 [657.4757323265076 sec]
step 1600: train loss 0.1724, val loss 2.8593 [699.4396586418152 sec]
step 1700: train loss 0.1531, val loss 2.9939 [741.4701476097107 sec]
step 1800: train loss 0.1398, val loss 3.0958 [783.4473764896393 sec]
step 1900: train loss 0.1314, val loss 3.1825 [825.4794499874115 sec]
step 2000: train loss 0.1286, val loss 3.2305 [867.5213406085968 sec]
step 2100: train loss 0.1229, val loss 3.3227 [909.6135108470917 sec]
step 2200: train loss 0.1180, val loss 3.3658 [951.6141881942749 sec]
step 2300: train loss 0.1157, val loss 3.4700 [993.6882438659668 sec]
step 2400: train loss 0.1122, val loss 3.5552 [1035.734501361847 sec]
step 2500: train loss 0.1125, val loss 3.5219 [1077.7778389453888 sec]
step 2600: train loss 0.1092, val loss 3.5433 [1119.8642580509186 sec]
step 2700: train loss 0.1079, val loss 3.6247 [1161.885639667511 sec]
step 2800: train loss 0.1061, val loss 3.5755 [1203.9895145893097 sec]
step 2900: train loss 0.1053, val loss 3.6972 [1246.534475326538 sec]
step 3000: train loss 0.1041, val loss 3.6749 [1288.5154542922974 sec]
step 3100: train loss 0.1032, val loss 3.7429 [1330.5747451782227 sec]
step 3200: train loss 0.1038, val loss 3.7799 [1372.7988457679749 sec]
step 3300: train loss 0.1025, val loss 3.7594 [1414.8435318470001 sec]
step 3400: train loss 0.1008, val loss 3.8578 [1456.9022378921509 sec]
step 3500: train loss 0.1000, val loss 3.8072 [1498.9474012851715 sec]
step 3600: train loss 0.0990, val loss 3.8920 [1540.9867310523987 sec]
step 3700: train loss 0.0992, val loss 3.9223 [1583.0353417396545 sec]
step 3800: train loss 0.0983, val loss 3.9139 [1625.0980491638184 sec]
step 3900: train loss 0.0974, val loss 3.9719 [1667.1419146060944 sec]
step 4000: train loss 0.0969, val loss 3.9557 [1709.2377984523773 sec]
step 4100: train loss 0.0968, val loss 3.9592 [1751.2886288166046 sec]
step 4200: train loss 0.0969, val loss 3.9558 [1793.4474394321442 sec]
step 4300: train loss 0.0962, val loss 3.9965 [1835.4784185886383 sec]
step 4400: train loss 0.0956, val loss 4.0118 [1877.5306417942047 sec]
step 4500: train loss 0.0941, val loss 4.0361 [1919.6176373958588 sec]
step 4600: train loss 0.0949, val loss 4.0753 [1961.890529870987 sec]
step 4700: train loss 0.0942, val loss 3.9427 [2004.089548587799 sec]
step 4800: train loss 0.0938, val loss 4.1123 [2046.1793072223663 sec]
step 4900: train loss 0.0940, val loss 4.0936 [2088.296770095825 sec]
0.11566249281167984
Total Training Time: 2107.8502399921417 seconds

worships.
"Yes, the first to roop if their growling. He knew he muster and sat
down again.
Gratta ha
BEGINNING (1682066865.7772794): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5560, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5693, val loss 4.5731 [14.077953100204468 sec]
step 100: train loss 2.3121, val loss 2.4167 [39.25675106048584 sec]
step 200: train loss 1.8482, val loss 2.0595 [64.30780053138733 sec]
step 300: train loss 1.6148, val loss 1.8972 [89.39354586601257 sec]
step 400: train loss 1.4521, val loss 1.8191 [114.57755708694458 sec]
step 500: train loss 1.3205, val loss 1.7795 [139.70149731636047 sec]
step 600: train loss 1.2283, val loss 1.7736 [164.76412272453308 sec]
step 700: train loss 1.1345, val loss 1.7819 [189.79271697998047 sec]
step 800: train loss 1.0471, val loss 1.8097 [214.8985857963562 sec]
step 900: train loss 0.9562, val loss 1.8340 [240.02151250839233 sec]
step 1000: train loss 0.8870, val loss 1.9195 [265.0500695705414 sec]
step 1100: train loss 0.7906, val loss 1.9277 [290.17022919654846 sec]
step 1200: train loss 0.7171, val loss 1.9995 [315.29548478126526 sec]
step 1300: train loss 0.6449, val loss 2.1109 [340.5227847099304 sec]
step 1400: train loss 0.5713, val loss 2.1409 [365.5774657726288 sec]
step 1500: train loss 0.5088, val loss 2.2826 [390.7804651260376 sec]
step 1600: train loss 0.4482, val loss 2.3480 [415.90395402908325 sec]
step 1700: train loss 0.4006, val loss 2.4117 [441.025057554245 sec]
step 1800: train loss 0.3652, val loss 2.5169 [466.12601232528687 sec]
step 1900: train loss 0.3285, val loss 2.5723 [491.21650886535645 sec]
step 2000: train loss 0.3044, val loss 2.6585 [516.3808076381683 sec]
step 2100: train loss 0.2805, val loss 2.7519 [541.5793008804321 sec]
step 2200: train loss 0.2609, val loss 2.7847 [566.647274017334 sec]
step 2300: train loss 0.2491, val loss 2.9390 [591.6693439483643 sec]
step 2400: train loss 0.2400, val loss 2.9816 [616.7991416454315 sec]
step 2500: train loss 0.2317, val loss 3.0496 [641.9671454429626 sec]
step 2600: train loss 0.2269, val loss 3.0691 [667.138573884964 sec]
step 2700: train loss 0.2208, val loss 3.1314 [692.2957570552826 sec]
step 2800: train loss 0.2143, val loss 3.1844 [717.5492441654205 sec]
step 2900: train loss 0.2109, val loss 3.2278 [742.8006563186646 sec]
step 3000: train loss 0.2057, val loss 3.3223 [767.880663394928 sec]
step 3100: train loss 0.2007, val loss 3.3546 [793.1267807483673 sec]
step 3200: train loss 0.1991, val loss 3.3832 [818.2494790554047 sec]
step 3300: train loss 0.1995, val loss 3.3565 [843.4748730659485 sec]
step 3400: train loss 0.1953, val loss 3.4585 [868.6084611415863 sec]
step 3500: train loss 0.1944, val loss 3.4343 [893.7402577400208 sec]
step 3600: train loss 0.1945, val loss 3.4188 [918.8648748397827 sec]
step 3700: train loss 0.1910, val loss 3.5182 [943.8796260356903 sec]
step 3800: train loss 0.1896, val loss 3.5126 [969.0265300273895 sec]
step 3900: train loss 0.1885, val loss 3.5567 [994.1988751888275 sec]
step 4000: train loss 0.1862, val loss 3.6311 [1019.4871115684509 sec]
step 4100: train loss 0.1852, val loss 3.6141 [1044.6146318912506 sec]
step 4200: train loss 0.1839, val loss 3.5749 [1069.7442035675049 sec]
step 4300: train loss 0.1818, val loss 3.6577 [1094.9440758228302 sec]
step 4400: train loss 0.1823, val loss 3.7326 [1120.0648357868195 sec]
step 4500: train loss 0.1823, val loss 3.6815 [1145.290896654129 sec]
step 4600: train loss 0.1813, val loss 3.6942 [1170.402872800827 sec]
step 4700: train loss 0.1815, val loss 3.6947 [1195.5795600414276 sec]
step 4800: train loss 0.1791, val loss 3.7575 [1220.5771703720093 sec]
step 4900: train loss 0.1786, val loss 3.7372 [1245.6957893371582 sec]
0.24650506675243378
Total Training Time: 1256.8803617954254 seconds

we been done, so be
the need for tuon something with humans? Will they be faithful? Yes, they
have g
BEGINNING (1682068124.9078279): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6596, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6573, val loss 4.6669 [20.635851860046387 sec]
step 100: train loss 2.5229, val loss 2.6110 [55.1396906375885 sec]
step 200: train loss 2.0520, val loss 2.2193 [90.1855640411377 sec]
step 300: train loss 1.7470, val loss 1.9862 [124.75826621055603 sec]
step 400: train loss 1.5413, val loss 1.8688 [158.98623704910278 sec]
step 500: train loss 1.3905, val loss 1.7899 [193.56038975715637 sec]
step 600: train loss 1.2733, val loss 1.7871 [227.82496571540833 sec]
step 700: train loss 1.1724, val loss 1.7819 [262.15632581710815 sec]
step 800: train loss 1.0776, val loss 1.7850 [296.3257100582123 sec]
step 900: train loss 0.9706, val loss 1.8025 [330.7876925468445 sec]
step 1000: train loss 0.8752, val loss 1.8748 [365.12322783470154 sec]
step 1100: train loss 0.7981, val loss 1.9445 [399.3587646484375 sec]
step 1200: train loss 0.7030, val loss 2.0045 [433.82855129241943 sec]
step 1300: train loss 0.6238, val loss 2.0665 [468.32483863830566 sec]
step 1400: train loss 0.5451, val loss 2.1380 [502.6167719364166 sec]
step 1500: train loss 0.4772, val loss 2.2665 [537.2981631755829 sec]
step 1600: train loss 0.4172, val loss 2.3352 [571.6992025375366 sec]
step 1700: train loss 0.3694, val loss 2.4213 [606.6302299499512 sec]
step 1800: train loss 0.3258, val loss 2.5580 [641.0765900611877 sec]
step 1900: train loss 0.2972, val loss 2.6367 [675.5990941524506 sec]
step 2000: train loss 0.2744, val loss 2.7503 [710.1345574855804 sec]
step 2100: train loss 0.2577, val loss 2.7921 [744.3575553894043 sec]
step 2200: train loss 0.2453, val loss 2.9157 [778.8499746322632 sec]
step 2300: train loss 0.2341, val loss 2.9523 [813.3108015060425 sec]
step 2400: train loss 0.2230, val loss 3.0087 [848.2177045345306 sec]
step 2500: train loss 0.2181, val loss 3.0939 [882.4770121574402 sec]
step 2600: train loss 0.2140, val loss 3.1457 [916.8467359542847 sec]
step 2700: train loss 0.2091, val loss 3.1590 [951.2718544006348 sec]
step 2800: train loss 0.2043, val loss 3.2415 [985.6341118812561 sec]
step 2900: train loss 0.2007, val loss 3.2532 [1019.8811359405518 sec]
step 3000: train loss 0.1973, val loss 3.3178 [1054.1330225467682 sec]
step 3100: train loss 0.1942, val loss 3.3208 [1088.8376224040985 sec]
step 3200: train loss 0.1954, val loss 3.3974 [1123.1776340007782 sec]
step 3300: train loss 0.1924, val loss 3.4231 [1157.6588582992554 sec]
step 3400: train loss 0.1911, val loss 3.4183 [1191.9714360237122 sec]
step 3500: train loss 0.1887, val loss 3.5036 [1226.1000227928162 sec]
step 3600: train loss 0.1870, val loss 3.5004 [1260.1534912586212 sec]
step 3700: train loss 0.1867, val loss 3.5257 [1294.6468522548676 sec]
step 3800: train loss 0.1849, val loss 3.5613 [1329.3162562847137 sec]
step 3900: train loss 0.1839, val loss 3.5743 [1363.6689517498016 sec]
step 4000: train loss 0.1839, val loss 3.5387 [1397.8818986415863 sec]
step 4100: train loss 0.1830, val loss 3.6007 [1432.4688520431519 sec]
step 4200: train loss 0.1824, val loss 3.6065 [1466.5415697097778 sec]
step 4300: train loss 0.1813, val loss 3.5969 [1500.8652346134186 sec]
step 4400: train loss 0.1777, val loss 3.7250 [1535.4923374652863 sec]
step 4500: train loss 0.1786, val loss 3.6670 [1570.3403720855713 sec]
step 4600: train loss 0.1804, val loss 3.6731 [1604.616541147232 sec]
step 4700: train loss 0.1773, val loss 3.7373 [1638.7825088500977 sec]
step 4800: train loss 0.1767, val loss 3.7144 [1673.1032252311707 sec]
step 4900: train loss 0.1758, val loss 3.7514 [1707.5495357513428 sec]
0.22510837018489838
Total Training Time: 1722.3706197738647 seconds

shown. Still, he was a full reply from Chief Gratta's tent. "Have
Chief Gratta confirmed from you, s
BEGINNING (1682069850.6234436): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6090, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6126, val loss 4.6212 [23.448152542114258 sec]
step 100: train loss 2.4480, val loss 2.5375 [60.785106897354126 sec]
step 200: train loss 2.0120, val loss 2.1828 [98.321608543396 sec]
step 300: train loss 1.6563, val loss 1.9163 [135.34682059288025 sec]
step 400: train loss 1.4144, val loss 1.8029 [172.11867833137512 sec]
step 500: train loss 1.2443, val loss 1.7645 [209.19924139976501 sec]
step 600: train loss 1.0774, val loss 1.7756 [246.19180250167847 sec]
step 700: train loss 0.9282, val loss 1.8323 [283.151673078537 sec]
step 800: train loss 0.7588, val loss 1.9412 [320.1386287212372 sec]
step 900: train loss 0.6078, val loss 2.0413 [357.8899726867676 sec]
step 1000: train loss 0.4701, val loss 2.2012 [394.8478891849518 sec]
step 1100: train loss 0.3541, val loss 2.3808 [431.8056671619415 sec]
step 1200: train loss 0.2623, val loss 2.5180 [468.73919677734375 sec]
step 1300: train loss 0.2122, val loss 2.7273 [505.8617730140686 sec]
step 1400: train loss 0.1760, val loss 2.8856 [542.6781640052795 sec]
step 1500: train loss 0.1531, val loss 2.9741 [579.5436060428619 sec]
step 1600: train loss 0.1393, val loss 3.1165 [616.5185375213623 sec]
step 1700: train loss 0.1283, val loss 3.2030 [653.4233040809631 sec]
step 1800: train loss 0.1238, val loss 3.2975 [690.2833077907562 sec]
step 1900: train loss 0.1193, val loss 3.3630 [727.3034389019012 sec]
step 2000: train loss 0.1154, val loss 3.4179 [764.2152156829834 sec]
step 2100: train loss 0.1114, val loss 3.5010 [801.115730047226 sec]
step 2200: train loss 0.1100, val loss 3.6014 [838.1610262393951 sec]
step 2300: train loss 0.1097, val loss 3.6563 [875.2451505661011 sec]
step 2400: train loss 0.1064, val loss 3.6040 [912.1386513710022 sec]
step 2500: train loss 0.1048, val loss 3.7016 [949.720545053482 sec]
step 2600: train loss 0.1022, val loss 3.7309 [986.7113440036774 sec]
step 2700: train loss 0.1029, val loss 3.7090 [1023.7194931507111 sec]
step 2800: train loss 0.1011, val loss 3.7958 [1060.541366815567 sec]
step 2900: train loss 0.1003, val loss 3.7798 [1098.3032445907593 sec]
step 3000: train loss 0.0988, val loss 3.8473 [1135.2578370571136 sec]
step 3100: train loss 0.0984, val loss 3.9197 [1172.0411338806152 sec]
step 3200: train loss 0.0978, val loss 3.9253 [1208.9278984069824 sec]
step 3300: train loss 0.0969, val loss 3.9613 [1245.987092256546 sec]
step 3400: train loss 0.0964, val loss 3.8928 [1282.8860929012299 sec]
step 3500: train loss 0.0959, val loss 4.0075 [1319.8655569553375 sec]
step 3600: train loss 0.0952, val loss 4.0446 [1356.7793591022491 sec]
step 3700: train loss 0.0943, val loss 3.9730 [1394.2657959461212 sec]
step 3800: train loss 0.0935, val loss 4.0748 [1431.824384689331 sec]
step 3900: train loss 0.0935, val loss 4.0407 [1468.91650724411 sec]
step 4000: train loss 0.0935, val loss 4.1104 [1506.2104759216309 sec]
step 4100: train loss 0.0921, val loss 4.0862 [1543.1783640384674 sec]
step 4200: train loss 0.0922, val loss 4.0514 [1579.9644343852997 sec]
step 4300: train loss 0.0915, val loss 4.1269 [1616.9711430072784 sec]
step 4400: train loss 0.0921, val loss 4.1384 [1653.7815885543823 sec]
step 4500: train loss 0.0915, val loss 4.2148 [1690.8267006874084 sec]
step 4600: train loss 0.0909, val loss 4.2121 [1727.6018421649933 sec]
step 4700: train loss 0.0911, val loss 4.2183 [1764.8647212982178 sec]
step 4800: train loss 0.0905, val loss 4.1863 [1801.8125896453857 sec]
step 4900: train loss 0.0904, val loss 4.3738 [1838.7766778469086 sec]
0.12548236548900604
Total Training Time: 1855.5319647789001 seconds

strong military might will be infiltrated he was
once again sto tring all the fied of
reasing the fa
BEGINNING (1682071708.7061148): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6358, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6315, val loss 4.6349 [6.6267616748809814 sec]
step 100: train loss 2.4414, val loss 2.5208 [18.378287076950073 sec]
step 200: train loss 2.0972, val loss 2.2548 [30.105067491531372 sec]
step 300: train loss 1.8372, val loss 2.0324 [41.84851336479187 sec]
step 400: train loss 1.6881, val loss 1.9358 [53.58603024482727 sec]
step 500: train loss 1.5754, val loss 1.8648 [65.32954692840576 sec]
step 600: train loss 1.4924, val loss 1.8048 [77.0750367641449 sec]
step 700: train loss 1.4200, val loss 1.7843 [88.81764698028564 sec]
step 800: train loss 1.3571, val loss 1.7595 [100.55739116668701 sec]
step 900: train loss 1.3129, val loss 1.7682 [112.30125379562378 sec]
step 1000: train loss 1.2535, val loss 1.7305 [124.04934668540955 sec]
step 1100: train loss 1.2067, val loss 1.7431 [135.81828570365906 sec]
step 1200: train loss 1.1647, val loss 1.7404 [147.59856915473938 sec]
step 1300: train loss 1.1166, val loss 1.7376 [159.37013149261475 sec]
step 1400: train loss 1.0854, val loss 1.7599 [171.16132140159607 sec]
step 1500: train loss 1.0367, val loss 1.7704 [182.92811799049377 sec]
step 1600: train loss 0.9971, val loss 1.7836 [194.70067143440247 sec]
step 1700: train loss 0.9584, val loss 1.8063 [206.47318243980408 sec]
step 1800: train loss 0.9182, val loss 1.8330 [218.25540733337402 sec]
step 1900: train loss 0.8786, val loss 1.8434 [230.0208067893982 sec]
step 2000: train loss 0.8395, val loss 1.8907 [241.82208633422852 sec]
step 2100: train loss 0.8045, val loss 1.8863 [253.58116364479065 sec]
step 2200: train loss 0.7709, val loss 1.9153 [265.3470253944397 sec]
step 2300: train loss 0.7281, val loss 1.9583 [277.1265528202057 sec]
step 2400: train loss 0.6888, val loss 2.0063 [288.8963882923126 sec]
step 2500: train loss 0.6555, val loss 2.0327 [300.66812348365784 sec]
step 2600: train loss 0.6246, val loss 2.0378 [312.4376838207245 sec]
step 2700: train loss 0.5940, val loss 2.0800 [324.20851588249207 sec]
step 2800: train loss 0.5616, val loss 2.1409 [335.98125743865967 sec]
step 2900: train loss 0.5323, val loss 2.1819 [347.76684617996216 sec]
step 3000: train loss 0.5086, val loss 2.1871 [359.5384941101074 sec]
step 3100: train loss 0.4803, val loss 2.2184 [371.3123571872711 sec]
step 3200: train loss 0.4565, val loss 2.2691 [383.0802843570709 sec]
step 3300: train loss 0.4333, val loss 2.2989 [394.85514521598816 sec]
step 3400: train loss 0.4128, val loss 2.3482 [406.64358711242676 sec]
step 3500: train loss 0.3917, val loss 2.4033 [418.4415578842163 sec]
step 3600: train loss 0.3738, val loss 2.4550 [430.20683217048645 sec]
step 3700: train loss 0.3581, val loss 2.4805 [441.97136211395264 sec]
step 3800: train loss 0.3455, val loss 2.4812 [453.7424247264862 sec]
step 3900: train loss 0.3301, val loss 2.5385 [465.5188355445862 sec]
step 4000: train loss 0.3205, val loss 2.5326 [477.302622795105 sec]
step 4100: train loss 0.3075, val loss 2.5988 [489.0679786205292 sec]
step 4200: train loss 0.2953, val loss 2.6564 [500.82866859436035 sec]
step 4300: train loss 0.2829, val loss 2.6867 [512.6007623672485 sec]
step 4400: train loss 0.2757, val loss 2.7466 [524.368613243103 sec]
step 4500: train loss 0.2676, val loss 2.7683 [536.1349971294403 sec]
step 4600: train loss 0.2633, val loss 2.7872 [547.9142661094666 sec]
step 4700: train loss 0.2572, val loss 2.8167 [559.6673095226288 sec]
step 4800: train loss 0.2539, val loss 2.8213 [571.4355053901672 sec]
step 4900: train loss 0.2454, val loss 2.8494 [583.1931383609772 sec]
0.4358851909637451
Total Training Time: 588.3164684772491 seconds

to be certurned and began taking his these
vibrant – had ten guards faint from the body, not even a 
BEGINNING (1682072298.4676974): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6320, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6350, val loss 4.6354 [9.563858270645142 sec]
step 100: train loss 2.4422, val loss 2.5304 [26.722397565841675 sec]
step 200: train loss 2.0903, val loss 2.2396 [43.85373306274414 sec]
step 300: train loss 1.8089, val loss 2.0230 [60.98861122131348 sec]
step 400: train loss 1.6518, val loss 1.9120 [78.12870121002197 sec]
step 500: train loss 1.5267, val loss 1.8379 [95.28138041496277 sec]
step 600: train loss 1.4406, val loss 1.7984 [112.42678475379944 sec]
step 700: train loss 1.3609, val loss 1.7653 [129.56125736236572 sec]
step 800: train loss 1.2921, val loss 1.7497 [146.71000814437866 sec]
step 900: train loss 1.2306, val loss 1.7542 [163.85058093070984 sec]
step 1000: train loss 1.1814, val loss 1.7525 [180.99182796478271 sec]
step 1100: train loss 1.1227, val loss 1.7572 [198.1312379837036 sec]
step 1200: train loss 1.0654, val loss 1.7745 [215.28378176689148 sec]
step 1300: train loss 1.0143, val loss 1.7897 [232.4190444946289 sec]
step 1400: train loss 0.9667, val loss 1.8060 [249.56197786331177 sec]
step 1500: train loss 0.9225, val loss 1.8339 [266.69893169403076 sec]
step 1600: train loss 0.8692, val loss 1.8365 [283.8365385532379 sec]
step 1700: train loss 0.8230, val loss 1.9023 [300.97979712486267 sec]
step 1800: train loss 0.7750, val loss 1.9399 [318.11267852783203 sec]
step 1900: train loss 0.7284, val loss 1.9717 [335.247442483902 sec]
step 2000: train loss 0.6799, val loss 2.0207 [352.3912250995636 sec]
step 2100: train loss 0.6416, val loss 2.0391 [369.51893067359924 sec]
step 2200: train loss 0.5990, val loss 2.0945 [386.65906977653503 sec]
step 2300: train loss 0.5625, val loss 2.1113 [403.80367946624756 sec]
step 2400: train loss 0.5102, val loss 2.1950 [420.9517378807068 sec]
step 2500: train loss 0.4859, val loss 2.1921 [438.0915412902832 sec]
step 2600: train loss 0.4559, val loss 2.2471 [455.2330906391144 sec]
step 2700: train loss 0.4192, val loss 2.3258 [472.3584175109863 sec]
step 2800: train loss 0.3974, val loss 2.3665 [489.5137257575989 sec]
step 2900: train loss 0.3738, val loss 2.4036 [506.66645216941833 sec]
step 3000: train loss 0.3569, val loss 2.5075 [523.8062052726746 sec]
step 3100: train loss 0.3333, val loss 2.5474 [540.9852931499481 sec]
step 3200: train loss 0.3208, val loss 2.5876 [558.1400747299194 sec]
step 3300: train loss 0.3006, val loss 2.6215 [575.2827336788177 sec]
step 3400: train loss 0.2896, val loss 2.6603 [592.4262640476227 sec]
step 3500: train loss 0.2849, val loss 2.6913 [609.5729489326477 sec]
step 3600: train loss 0.2697, val loss 2.7552 [626.6972179412842 sec]
step 3700: train loss 0.2587, val loss 2.7861 [643.853253364563 sec]
step 3800: train loss 0.2497, val loss 2.8141 [660.99272108078 sec]
step 3900: train loss 0.2483, val loss 2.8380 [678.1401958465576 sec]
step 4000: train loss 0.2360, val loss 2.8203 [695.2988312244415 sec]
step 4100: train loss 0.2290, val loss 2.9841 [712.486364364624 sec]
step 4200: train loss 0.2280, val loss 3.0175 [729.6340954303741 sec]
step 4300: train loss 0.2247, val loss 2.9654 [746.7981238365173 sec]
step 4400: train loss 0.2202, val loss 3.0616 [763.9435074329376 sec]
step 4500: train loss 0.2168, val loss 3.0481 [781.1202206611633 sec]
step 4600: train loss 0.2157, val loss 3.0769 [798.2665135860443 sec]
step 4700: train loss 0.2110, val loss 3.1404 [815.4094116687775 sec]
step 4800: train loss 0.2099, val loss 3.1790 [832.5671217441559 sec]
step 4900: train loss 0.2073, val loss 3.1869 [849.7124025821686 sec]
0.34332799911499023
Total Training Time: 857.299473285675 seconds

made a quaster small as high as well. Arphad was
right. These willing to match the valley. Surprise 
BEGINNING (1682073157.9545066): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6814, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6793, val loss 4.6985 [10.985743761062622 sec]
step 100: train loss 2.4702, val loss 2.5596 [30.425562620162964 sec]
step 200: train loss 2.2573, val loss 2.3772 [49.87765574455261 sec]
step 300: train loss 1.9523, val loss 2.1366 [69.25444865226746 sec]
step 400: train loss 1.7557, val loss 1.9790 [88.6515281200409 sec]
step 500: train loss 1.6185, val loss 1.8806 [108.06123614311218 sec]
step 600: train loss 1.5177, val loss 1.8191 [127.47645211219788 sec]
step 700: train loss 1.4276, val loss 1.7804 [146.88242316246033 sec]
step 800: train loss 1.3452, val loss 1.7409 [166.2791519165039 sec]
step 900: train loss 1.2742, val loss 1.7455 [185.67572712898254 sec]
step 1000: train loss 1.2145, val loss 1.7273 [205.088232755661 sec]
step 1100: train loss 1.1496, val loss 1.7268 [224.5021255016327 sec]
step 1200: train loss 1.0903, val loss 1.7419 [243.93546652793884 sec]
step 1300: train loss 1.0252, val loss 1.7578 [263.3316910266876 sec]
step 1400: train loss 0.9675, val loss 1.7676 [282.65743947029114 sec]
step 1500: train loss 0.9170, val loss 1.7924 [302.0465075969696 sec]
step 1600: train loss 0.8553, val loss 1.8444 [25906.41005563736 sec]
step 1700: train loss 0.7892, val loss 1.8815 [25926.143632411957 sec]
step 1800: train loss 0.7267, val loss 1.9237 [25945.471806287766 sec]
step 1900: train loss 0.6695, val loss 1.9576 [25964.80868244171 sec]
step 2000: train loss 0.6147, val loss 2.0263 [25984.322580099106 sec]
step 2100: train loss 0.5619, val loss 2.0571 [26003.652478933334 sec]
step 2200: train loss 0.5070, val loss 2.1303 [26022.618801116943 sec]
step 2300: train loss 0.4591, val loss 2.2091 [26041.734170675278 sec]
step 2400: train loss 0.4158, val loss 2.2537 [26060.634590148926 sec]
step 2500: train loss 0.3705, val loss 2.3434 [26079.70067715645 sec]
step 2600: train loss 0.3338, val loss 2.4254 [26099.52118754387 sec]
step 2700: train loss 0.2980, val loss 2.4690 [26119.444915294647 sec]
step 2800: train loss 0.2732, val loss 2.5504 [26138.434583187103 sec]
step 2900: train loss 0.2451, val loss 2.6227 [26157.39727282524 sec]
step 3000: train loss 0.2242, val loss 2.6454 [26176.365869760513 sec]
step 3100: train loss 0.2080, val loss 2.7458 [26195.20393538475 sec]
step 3200: train loss 0.1936, val loss 2.7948 [26214.68472981453 sec]
step 3300: train loss 0.1800, val loss 2.8322 [26234.568258285522 sec]
step 3400: train loss 0.1688, val loss 2.9023 [26254.153289556503 sec]
step 3500: train loss 0.1615, val loss 2.9585 [26273.750025749207 sec]
step 3600: train loss 0.1527, val loss 3.0232 [26293.215689897537 sec]
step 3700: train loss 0.1452, val loss 3.0662 [26312.763080120087 sec]
step 3800: train loss 0.1417, val loss 3.1473 [26345.079285144806 sec]
step 3900: train loss 0.1365, val loss 3.2064 [26377.490793943405 sec]
step 4000: train loss 0.1359, val loss 3.2389 [26414.776732444763 sec]
step 4100: train loss 0.1309, val loss 3.2575 [26449.676060438156 sec]
step 4200: train loss 0.1268, val loss 3.2856 [26482.839290857315 sec]
step 4300: train loss 0.1248, val loss 3.3697 [26512.388187885284 sec]
step 4400: train loss 0.1226, val loss 3.3856 [26541.147305488586 sec]
step 4500: train loss 0.1205, val loss 3.3926 [26585.086344003677 sec]
step 4600: train loss 0.1187, val loss 3.4756 [26609.872268676758 sec]
step 4700: train loss 0.1183, val loss 3.4079 [26643.205461740494 sec]
step 4800: train loss 0.1168, val loss 3.4950 [26670.891412496567 sec]
step 4900: train loss 0.1151, val loss 3.4789 [26709.896554470062 sec]
0.22151713073253632
Total Training Time: 26724.840651750565 seconds

As stood again.  They were all; Gratta was certain that was
a good as the Tuon Tribes would like to 
BEGINNING (1682099884.77715): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5912, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5927, val loss 4.6015 [21.45888328552246 sec]
step 100: train loss 2.4790, val loss 2.5610 [57.36494207382202 sec]
step 200: train loss 2.3044, val loss 2.4156 [85.42636513710022 sec]
step 300: train loss 1.9503, val loss 2.1239 [113.4540753364563 sec]
step 400: train loss 1.7183, val loss 1.9636 [156.3418629169464 sec]
step 500: train loss 1.5644, val loss 1.8448 [192.08692502975464 sec]
step 600: train loss 1.4466, val loss 1.7965 [245.39879083633423 sec]
step 700: train loss 1.3444, val loss 1.7564 [305.30123710632324 sec]
step 800: train loss 1.2518, val loss 1.7476 [373.84989190101624 sec]
step 900: train loss 1.1682, val loss 1.7394 [435.76810002326965 sec]
step 1000: train loss 1.0933, val loss 1.7718 [646.7258157730103 sec]
step 1100: train loss 1.0139, val loss 1.7994 [847.9373610019684 sec]
step 1200: train loss 0.9256, val loss 1.8066 [1105.1999855041504 sec]
step 1300: train loss 0.8514, val loss 1.8449 [1291.6480469703674 sec]
step 1400: train loss 0.7695, val loss 1.8808 [1337.652099609375 sec]
step 1500: train loss 0.6808, val loss 1.9474 [1369.512843132019 sec]
step 1600: train loss 0.6158, val loss 2.0114 [1549.5473854541779 sec]
step 1700: train loss 0.5363, val loss 2.1066 [1768.6748297214508 sec]
step 1800: train loss 0.4614, val loss 2.1604 [1970.7508239746094 sec]
step 1900: train loss 0.4053, val loss 2.2356 [2258.8906190395355 sec]
step 2000: train loss 0.3451, val loss 2.3540 [2527.4970304965973 sec]
step 2100: train loss 0.2993, val loss 2.4680 [2769.5194611549377 sec]
step 2200: train loss 0.2644, val loss 2.5354 [2903.1466228961945 sec]
step 2300: train loss 0.2299, val loss 2.5962 [3061.909767150879 sec]
step 2400: train loss 0.2051, val loss 2.6433 [3241.9098517894745 sec]
step 2500: train loss 0.1842, val loss 2.7628 [3377.68403673172 sec]
step 2600: train loss 0.1679, val loss 2.8680 [3547.103899002075 sec]
step 2700: train loss 0.1587, val loss 2.9378 [3575.0497896671295 sec]
step 2800: train loss 0.1481, val loss 2.9821 [3604.8383202552795 sec]
step 2900: train loss 0.1419, val loss 3.0510 [3632.9055528640747 sec]
step 3000: train loss 0.1364, val loss 3.1003 [3661.239228248596 sec]
step 3100: train loss 0.1328, val loss 3.1796 [3689.876893758774 sec]
step 3200: train loss 0.1282, val loss 3.2734 [3717.8389649391174 sec]
step 3300: train loss 0.1252, val loss 3.2817 [3748.251651763916 sec]
step 3400: train loss 0.1224, val loss 3.3379 [3777.6494591236115 sec]
step 3500: train loss 0.1210, val loss 3.3408 [3816.5906200408936 sec]
step 3600: train loss 0.1171, val loss 3.4131 [3847.9084327220917 sec]
step 3700: train loss 0.1168, val loss 3.4404 [3880.2091512680054 sec]
step 3800: train loss 0.1157, val loss 3.4567 [3912.780421257019 sec]
step 3900: train loss 0.1138, val loss 3.5467 [4002.90549826622 sec]
step 4000: train loss 0.1123, val loss 3.5366 [4053.7015342712402 sec]
step 4100: train loss 0.1110, val loss 3.5551 [4126.51394867897 sec]
step 4200: train loss 0.1096, val loss 3.5498 [4166.744280099869 sec]
step 4300: train loss 0.1078, val loss 3.5668 [4198.934168815613 sec]
step 4400: train loss 0.1075, val loss 3.6011 [4229.404408216476 sec]
step 4500: train loss 0.1070, val loss 3.6596 [4260.718411207199 sec]
step 4600: train loss 0.1053, val loss 3.7314 [4295.111697673798 sec]
step 4700: train loss 0.1059, val loss 3.6921 [4328.739178657532 sec]
step 4800: train loss 0.1040, val loss 3.7053 [4363.986033201218 sec]
step 4900: train loss 0.1034, val loss 3.7746 [4410.053875923157 sec]
0.16432569921016693
Total Training Time: 4422.732807159424 seconds

Gratta turned to Aidden, "Do you have a hundred tuons in warcrating
for I come to your my princes, b
BEGINNING (1682104309.9757729): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6491, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6518, val loss 4.6465 [8.816129684448242 sec]
step 100: train loss 2.4086, val loss 2.5012 [24.8399076461792 sec]
step 200: train loss 2.0358, val loss 2.1897 [80.78481817245483 sec]
step 300: train loss 1.7887, val loss 2.0019 [106.31129932403564 sec]
step 400: train loss 1.6273, val loss 1.8796 [135.85825181007385 sec]
step 500: train loss 1.5129, val loss 1.8172 [156.53889298439026 sec]
step 600: train loss 1.4349, val loss 1.7773 [175.33077716827393 sec]
step 700: train loss 1.3555, val loss 1.7626 [193.19194412231445 sec]
step 800: train loss 1.3025, val loss 1.7690 [214.04140973091125 sec]
step 900: train loss 1.2356, val loss 1.7549 [230.45011711120605 sec]
step 1000: train loss 1.1732, val loss 1.7270 [246.20814776420593 sec]
step 1100: train loss 1.1253, val loss 1.7488 [261.8057613372803 sec]
step 1200: train loss 1.0745, val loss 1.7768 [281.5226867198944 sec]
step 1300: train loss 1.0294, val loss 1.7542 [303.46042561531067 sec]
step 1400: train loss 0.9816, val loss 1.8063 [323.6617670059204 sec]
step 1500: train loss 0.9329, val loss 1.8439 [343.00852513313293 sec]
step 1600: train loss 0.8794, val loss 1.8571 [367.37371301651 sec]
step 1700: train loss 0.8330, val loss 1.8926 [385.7967414855957 sec]
step 1800: train loss 0.7956, val loss 1.8987 [404.8209443092346 sec]
step 1900: train loss 0.7432, val loss 1.9536 [428.3299415111542 sec]
step 2000: train loss 0.7001, val loss 1.9986 [452.79958415031433 sec]
step 2100: train loss 0.6605, val loss 2.0184 [476.2345290184021 sec]
step 2200: train loss 0.6200, val loss 2.0641 [499.0583608150482 sec]
step 2300: train loss 0.5843, val loss 2.1093 [515.47199177742 sec]
step 2400: train loss 0.5452, val loss 2.1621 [531.5678217411041 sec]
step 2500: train loss 0.5100, val loss 2.2010 [549.8507690429688 sec]
step 2600: train loss 0.4803, val loss 2.2408 [567.5094637870789 sec]
step 2700: train loss 0.4520, val loss 2.2805 [583.219390630722 sec]
step 2800: train loss 0.4257, val loss 2.3211 [598.9217474460602 sec]
step 2900: train loss 0.4021, val loss 2.3791 [614.5968899726868 sec]
step 3000: train loss 0.3743, val loss 2.4224 [630.2128200531006 sec]
step 3100: train loss 0.3599, val loss 2.4295 [645.8311638832092 sec]
step 3200: train loss 0.3403, val loss 2.4965 [661.5566658973694 sec]
step 3300: train loss 0.3259, val loss 2.5444 [681.39439702034 sec]
step 3400: train loss 0.3067, val loss 2.6134 [698.4559783935547 sec]
step 3500: train loss 0.2923, val loss 2.6163 [717.5585458278656 sec]
step 3600: train loss 0.2828, val loss 2.6492 [740.850254535675 sec]
step 3700: train loss 0.2727, val loss 2.7226 [763.911899805069 sec]
step 3800: train loss 0.2657, val loss 2.7649 [785.922709941864 sec]
step 3900: train loss 0.2566, val loss 2.8227 [807.6085951328278 sec]
step 4000: train loss 0.2481, val loss 2.8603 [823.6163833141327 sec]
step 4100: train loss 0.2403, val loss 2.9189 [839.4137046337128 sec]
step 4200: train loss 0.2358, val loss 2.8888 [855.0888221263885 sec]
step 4300: train loss 0.2309, val loss 2.9404 [870.7270655632019 sec]
step 4400: train loss 0.2250, val loss 2.9487 [886.2602829933167 sec]
step 4500: train loss 0.2215, val loss 3.0166 [901.9716441631317 sec]
step 4600: train loss 0.2191, val loss 3.0178 [917.6316859722137 sec]
step 4700: train loss 0.2126, val loss 3.0533 [933.2136232852936 sec]
step 4800: train loss 0.2126, val loss 3.0710 [948.8408057689667 sec]
step 4900: train loss 0.2106, val loss 3.1177 [964.4021141529083 sec]
0.3672126531600952
Total Training Time: 971.1854104995728 seconds

us?"
"Stand."
"Very well. "Then words twenty men if was the nearly
, do the seat here of all that ma
BEGINNING (1682105282.690462): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5308, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5373, val loss 4.5374 [12.911522626876831 sec]
step 100: train loss 2.4228, val loss 2.5144 [35.990835666656494 sec]
step 200: train loss 2.0365, val loss 2.1877 [58.886064767837524 sec]
step 300: train loss 1.7647, val loss 1.9811 [82.02516293525696 sec]
step 400: train loss 1.5852, val loss 1.8629 [104.94859218597412 sec]
step 500: train loss 1.4698, val loss 1.8081 [128.07561945915222 sec]
step 600: train loss 1.3760, val loss 1.7737 [151.03902053833008 sec]
step 700: train loss 1.2912, val loss 1.7372 [174.51335096359253 sec]
step 800: train loss 1.2219, val loss 1.7476 [197.93202209472656 sec]
step 900: train loss 1.1513, val loss 1.7488 [220.76011443138123 sec]
step 1000: train loss 1.0921, val loss 1.7295 [243.82103872299194 sec]
step 1100: train loss 1.0322, val loss 1.7754 [266.95569229125977 sec]
step 1200: train loss 0.9706, val loss 1.7783 [289.9502248764038 sec]
step 1300: train loss 0.9065, val loss 1.8118 [312.9839737415314 sec]
step 1400: train loss 0.8525, val loss 1.8489 [336.0680537223816 sec]
step 1500: train loss 0.7912, val loss 1.8885 [359.1841881275177 sec]
step 1600: train loss 0.7318, val loss 1.9169 [382.14237809181213 sec]
step 1700: train loss 0.6765, val loss 1.9781 [408.5392005443573 sec]
step 1800: train loss 0.6228, val loss 2.0429 [433.0010087490082 sec]
step 1900: train loss 0.5701, val loss 2.1074 [462.47517824172974 sec]
step 2000: train loss 0.5255, val loss 2.1691 [503.6349048614502 sec]
step 2100: train loss 0.4906, val loss 2.2172 [533.0392007827759 sec]
step 2200: train loss 0.4478, val loss 2.2737 [561.1694757938385 sec]
step 2300: train loss 0.4109, val loss 2.3246 [584.4129750728607 sec]
step 2400: train loss 0.3803, val loss 2.3962 [616.5802230834961 sec]
step 2500: train loss 0.3520, val loss 2.4832 [647.1981673240662 sec]
step 2600: train loss 0.3310, val loss 2.4991 [672.4687223434448 sec]
step 2700: train loss 0.3088, val loss 2.5950 [697.0937345027924 sec]
step 2800: train loss 0.2955, val loss 2.6464 [720.1641550064087 sec]
step 2900: train loss 0.2780, val loss 2.6913 [746.6766767501831 sec]
step 3000: train loss 0.2663, val loss 2.7568 [769.6818449497223 sec]
step 3100: train loss 0.2550, val loss 2.7787 [794.9005370140076 sec]
step 3200: train loss 0.2471, val loss 2.8748 [818.0880789756775 sec]
step 3300: train loss 0.2378, val loss 2.8851 [840.8764092922211 sec]
step 3400: train loss 0.2362, val loss 2.9406 [870.3425691127777 sec]
step 3500: train loss 0.2239, val loss 2.9708 [901.9685435295105 sec]
step 3600: train loss 0.2212, val loss 3.0573 [925.4159877300262 sec]
step 3700: train loss 0.2160, val loss 3.0332 [950.1079170703888 sec]
step 3800: train loss 0.2128, val loss 3.1271 [973.5404691696167 sec]
step 3900: train loss 0.2111, val loss 3.0874 [997.0587122440338 sec]
step 4000: train loss 0.2068, val loss 3.1540 [1027.2185652256012 sec]
step 4100: train loss 0.2039, val loss 3.1966 [1053.7301127910614 sec]
step 4200: train loss 0.2002, val loss 3.2331 [1082.5283472537994 sec]
step 4300: train loss 0.2003, val loss 3.2416 [1112.1366019248962 sec]
step 4400: train loss 0.1975, val loss 3.2533 [1141.5858912467957 sec]
step 4500: train loss 0.1973, val loss 3.3095 [1169.2908329963684 sec]
step 4600: train loss 0.1950, val loss 3.3552 [1192.3010153770447 sec]
step 4700: train loss 0.1927, val loss 3.3533 [1227.7059485912323 sec]
step 4800: train loss 0.1913, val loss 3.4107 [1260.2676668167114 sec]
step 4900: train loss 0.1903, val loss 3.4623 [1285.670114517212 sec]
0.28236091136932373
Total Training Time: 1298.4482493400574 seconds

berested something into the city."
Gratta nodded to his assent.
34
CHAPTER IIII
THE PEACE
SEANCERAL 
BEGINNING (1682106583.8909192): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5928, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6115, val loss 4.6105 [19.755433082580566 sec]
step 100: train loss 2.4570, val loss 2.5473 [42.75924777984619 sec]
step 200: train loss 2.2204, val loss 2.3452 [65.62387275695801 sec]
step 300: train loss 1.8997, val loss 2.0829 [100.2535490989685 sec]
step 400: train loss 1.6953, val loss 1.9366 [134.55797863006592 sec]
step 500: train loss 1.5576, val loss 1.8429 [199.82724952697754 sec]
step 600: train loss 1.4384, val loss 1.7854 [247.23067784309387 sec]
step 700: train loss 1.3403, val loss 1.7436 [277.8327045440674 sec]
step 800: train loss 1.2614, val loss 1.7242 [301.8663606643677 sec]
step 900: train loss 1.1863, val loss 1.7290 [325.82972145080566 sec]
step 1000: train loss 1.1106, val loss 1.7139 [354.54343247413635 sec]
step 1100: train loss 1.0437, val loss 1.7317 [383.18889713287354 sec]
step 1200: train loss 0.9655, val loss 1.7564 [422.6735565662384 sec]
step 1300: train loss 0.8989, val loss 1.7893 [447.9712553024292 sec]
step 1400: train loss 0.8237, val loss 1.8366 [478.8231363296509 sec]
step 1500: train loss 0.7571, val loss 1.8621 [552.4334108829498 sec]
step 1600: train loss 0.6888, val loss 1.9454 [583.1269774436951 sec]
step 1700: train loss 0.6202, val loss 2.0063 [602.3336145877838 sec]
step 1800: train loss 0.5485, val loss 2.0661 [621.6391549110413 sec]
step 1900: train loss 0.4892, val loss 2.1251 [640.9409611225128 sec]
step 2000: train loss 0.4328, val loss 2.2318 [660.2565240859985 sec]
step 2100: train loss 0.3801, val loss 2.2724 [679.5798017978668 sec]
step 2200: train loss 0.3325, val loss 2.3355 [698.9215302467346 sec]
step 2300: train loss 0.2934, val loss 2.4287 [718.3155746459961 sec]
step 2400: train loss 0.2603, val loss 2.5331 [737.7358992099762 sec]
step 2500: train loss 0.2331, val loss 2.6045 [757.1688244342804 sec]
step 2600: train loss 0.2097, val loss 2.6902 [776.596657037735 sec]
step 2700: train loss 0.1905, val loss 2.7322 [796.0507922172546 sec]
step 2800: train loss 0.1770, val loss 2.7836 [815.9827110767365 sec]
step 2900: train loss 0.1647, val loss 2.8275 [839.8753488063812 sec]
step 3000: train loss 0.1551, val loss 2.9081 [859.8234069347382 sec]
step 3100: train loss 0.1455, val loss 2.9918 [882.2881510257721 sec]
step 3200: train loss 0.1417, val loss 3.0183 [906.8773334026337 sec]
step 3300: train loss 0.1336, val loss 3.1064 [931.2626671791077 sec]
step 3400: train loss 0.1317, val loss 3.1121 [955.7058503627777 sec]
step 3500: train loss 0.1277, val loss 3.1821 [979.8363184928894 sec]
step 3600: train loss 0.1237, val loss 3.2507 [1003.4838042259216 sec]
step 3700: train loss 0.1209, val loss 3.3121 [1027.6712896823883 sec]
step 3800: train loss 0.1188, val loss 3.3504 [1052.1112089157104 sec]
step 3900: train loss 0.1170, val loss 3.3679 [1076.463219165802 sec]
step 4000: train loss 0.1153, val loss 3.3876 [1100.7616698741913 sec]
step 4100: train loss 0.1135, val loss 3.4400 [1125.2441444396973 sec]
step 4200: train loss 0.1126, val loss 3.4828 [1149.7318415641785 sec]
step 4300: train loss 0.1106, val loss 3.4699 [1174.127613067627 sec]
step 4400: train loss 0.1102, val loss 3.5616 [1198.4933021068573 sec]
step 4500: train loss 0.1087, val loss 3.5602 [1222.936946630478 sec]
step 4600: train loss 0.1084, val loss 3.5758 [1247.101420879364 sec]
step 4700: train loss 0.1070, val loss 3.5805 [1270.537210226059 sec]
step 4800: train loss 0.1059, val loss 3.6622 [1294.1104047298431 sec]
step 4900: train loss 0.1051, val loss 3.6623 [1317.784568309784 sec]
0.18988539278507233
Total Training Time: 1327.8092732429504 seconds

and a small smile and said, "You brought another, we
shall rejoice that we have done. You will dober
BEGINNING (1682107913.3029): Baseline LR(0.0003) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6574, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6641, val loss 4.6723 [19.90008282661438 sec]
step 100: train loss 2.4676, val loss 2.5488 [52.558775901794434 sec]
step 200: train loss 2.2716, val loss 2.3945 [85.25301957130432 sec]
step 300: train loss 1.8913, val loss 2.0856 [117.9530439376831 sec]
step 400: train loss 1.6630, val loss 1.9150 [150.71235060691833 sec]
step 500: train loss 1.5088, val loss 1.8328 [183.4809217453003 sec]
step 600: train loss 1.3742, val loss 1.7641 [216.27637577056885 sec]
step 700: train loss 1.2674, val loss 1.7430 [248.90999364852905 sec]
step 800: train loss 1.1660, val loss 1.7426 [281.5995080471039 sec]
step 900: train loss 1.0762, val loss 1.7512 [314.3957316875458 sec]
step 1000: train loss 0.9798, val loss 1.7732 [347.3531222343445 sec]
step 1100: train loss 0.8847, val loss 1.8089 [383.9427056312561 sec]
step 1200: train loss 0.7936, val loss 1.8516 [421.47432827949524 sec]
step 1300: train loss 0.6974, val loss 1.9237 [472.0876679420471 sec]
step 1400: train loss 0.6101, val loss 1.9968 [517.4890065193176 sec]
step 1500: train loss 0.5221, val loss 2.0941 [551.1746227741241 sec]
step 1600: train loss 0.4415, val loss 2.1561 [593.5929713249207 sec]
step 1700: train loss 0.3736, val loss 2.2855 [627.7178053855896 sec]
step 1800: train loss 0.3102, val loss 2.3766 [667.1732356548309 sec]
step 1900: train loss 0.2672, val loss 2.4523 [707.3460097312927 sec]
step 2000: train loss 0.2255, val loss 2.5930 [736.7394914627075 sec]
step 2100: train loss 0.1964, val loss 2.6999 [765.1013827323914 sec]
step 2200: train loss 0.1795, val loss 2.7876 [793.4851033687592 sec]
step 2300: train loss 0.1604, val loss 2.8741 [827.2485709190369 sec]
step 2400: train loss 0.1497, val loss 2.9678 [866.5531949996948 sec]
step 2500: train loss 0.1414, val loss 3.0122 [899.2176098823547 sec]
step 2600: train loss 0.1324, val loss 3.1168 [935.8261368274689 sec]
step 2700: train loss 0.1287, val loss 3.2098 [976.1272320747375 sec]
step 2800: train loss 0.1248, val loss 3.2659 [1008.1731541156769 sec]
step 2900: train loss 0.1203, val loss 3.3236 [1057.6927571296692 sec]
step 3000: train loss 0.1187, val loss 3.3077 [1128.0486629009247 sec]
step 3100: train loss 0.1165, val loss 3.4681 [1201.4480664730072 sec]
step 3200: train loss 0.1136, val loss 3.4716 [1252.0599510669708 sec]
step 3300: train loss 0.1129, val loss 3.4630 [1289.8048148155212 sec]
step 3400: train loss 0.1109, val loss 3.5220 [1327.9905128479004 sec]
step 3500: train loss 0.1092, val loss 3.5700 [1365.8177275657654 sec]
step 3600: train loss 0.1074, val loss 3.6052 [1406.8771080970764 sec]
step 3700: train loss 0.1065, val loss 3.6646 [1452.1509625911713 sec]
step 3800: train loss 0.1063, val loss 3.6395 [1495.2013745307922 sec]
step 3900: train loss 0.1050, val loss 3.7110 [1534.4780168533325 sec]
step 4000: train loss 0.1039, val loss 3.7355 [1573.0812582969666 sec]
step 4100: train loss 0.1032, val loss 3.7641 [1616.8272528648376 sec]
step 4200: train loss 0.1020, val loss 3.8273 [1655.1247119903564 sec]
step 4300: train loss 0.1012, val loss 3.8581 [1691.1875319480896 sec]
step 4400: train loss 0.1007, val loss 3.8455 [1724.4232957363129 sec]
step 4500: train loss 0.0998, val loss 3.9018 [1756.2336628437042 sec]
step 4600: train loss 0.1002, val loss 3.8901 [1794.6620597839355 sec]
step 4700: train loss 0.0994, val loss 3.8814 [1829.645579814911 sec]
step 4800: train loss 0.0981, val loss 3.9637 [1864.807054758072 sec]
step 4900: train loss 0.0983, val loss 3.9597 [1899.5769095420837 sec]
0.15893268585205078
Total Training Time: 1915.7650866508484 seconds

strategies from his company to look at the scent of
square, O darts!" When turned and we move so qui
BEGINNING (1682109831.5802567): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6628, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6597, val loss 4.6580 [9.832086563110352 sec]
step 100: train loss 2.3980, val loss 2.4947 [28.68412446975708 sec]
step 200: train loss 1.9920, val loss 2.1631 [47.91751050949097 sec]
step 300: train loss 1.7419, val loss 1.9854 [67.13136625289917 sec]
step 400: train loss 1.5757, val loss 1.8686 [85.7677972316742 sec]
step 500: train loss 1.4670, val loss 1.8181 [104.97303795814514 sec]
step 600: train loss 1.3793, val loss 1.7810 [123.9168815612793 sec]
step 700: train loss 1.2956, val loss 1.7765 [143.62145137786865 sec]
step 800: train loss 1.2308, val loss 1.7677 [162.67550563812256 sec]
step 900: train loss 1.1610, val loss 1.7735 [181.77508211135864 sec]
step 1000: train loss 1.0979, val loss 1.7757 [201.1557879447937 sec]
step 1100: train loss 1.0404, val loss 1.8102 [219.56643748283386 sec]
step 1200: train loss 0.9900, val loss 1.8238 [238.31954884529114 sec]
step 1300: train loss 0.9227, val loss 1.8475 [257.2871468067169 sec]
step 1400: train loss 0.8667, val loss 1.9030 [275.8107328414917 sec]
step 1500: train loss 0.8052, val loss 1.9394 [301.2625572681427 sec]
step 1600: train loss 0.7479, val loss 1.9462 [327.9623558521271 sec]
step 1700: train loss 0.6960, val loss 2.0369 [351.4270622730255 sec]
step 1800: train loss 0.6437, val loss 2.0604 [373.5814743041992 sec]
step 1900: train loss 0.5921, val loss 2.1189 [395.7167398929596 sec]
step 2000: train loss 0.5461, val loss 2.1647 [414.968368768692 sec]
step 2100: train loss 0.5019, val loss 2.2401 [432.63491106033325 sec]
step 2200: train loss 0.4589, val loss 2.3156 [455.81841588020325 sec]
step 2300: train loss 0.4261, val loss 2.3536 [479.60478949546814 sec]
step 2400: train loss 0.3924, val loss 2.4089 [508.26567816734314 sec]
step 2500: train loss 0.3648, val loss 2.5239 [535.4536139965057 sec]
step 2600: train loss 0.3328, val loss 2.5345 [554.6335499286652 sec]
step 2700: train loss 0.3178, val loss 2.6101 [573.3165504932404 sec]
step 2800: train loss 0.3023, val loss 2.6544 [591.8474626541138 sec]
step 2900: train loss 0.2823, val loss 2.7147 [609.2503545284271 sec]
step 3000: train loss 0.2666, val loss 2.7497 [626.5960650444031 sec]
step 3100: train loss 0.2617, val loss 2.7999 [652.9073307514191 sec]
step 3200: train loss 0.2523, val loss 2.9042 [672.2506372928619 sec]
step 3300: train loss 0.2402, val loss 2.9370 [695.9622731208801 sec]
step 3400: train loss 0.2386, val loss 2.9774 [713.5150098800659 sec]
step 3500: train loss 0.2290, val loss 3.0214 [731.0566666126251 sec]
step 3600: train loss 0.2240, val loss 3.0551 [748.6096048355103 sec]
step 3700: train loss 0.2209, val loss 3.0520 [768.0611565113068 sec]
step 3800: train loss 0.2136, val loss 3.1152 [789.6892535686493 sec]
step 3900: train loss 0.2121, val loss 3.1562 [815.6180982589722 sec]
step 4000: train loss 0.2111, val loss 3.1894 [837.0986723899841 sec]
step 4100: train loss 0.2051, val loss 3.2471 [854.8722906112671 sec]
step 4200: train loss 0.2028, val loss 3.2804 [872.5258340835571 sec]
step 4300: train loss 0.2039, val loss 3.3180 [890.1232671737671 sec]
step 4400: train loss 0.1988, val loss 3.3505 [907.9421525001526 sec]
step 4500: train loss 0.1974, val loss 3.3673 [926.7988331317902 sec]
step 4600: train loss 0.1948, val loss 3.3729 [944.1233325004578 sec]
step 4700: train loss 0.1940, val loss 3.4368 [961.4224798679352 sec]
step 4800: train loss 0.1929, val loss 3.4977 [978.8524181842804 sec]
step 4900: train loss 0.1921, val loss 3.4432 [996.2145392894745 sec]
0.2903788387775421
Total Training Time: 1004.4348895549774 seconds

"Holding this man?
Water it Anayah asked after the gates. Gratta was
starting to swe had been joined
BEGINNING (1682110838.3702164): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.6136, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6112, val loss 4.6144 [13.277232646942139 sec]
step 100: train loss 2.4409, val loss 2.5307 [38.70740509033203 sec]
step 200: train loss 2.0449, val loss 2.2077 [64.18160080909729 sec]
step 300: train loss 1.7623, val loss 1.9836 [89.59245276451111 sec]
step 400: train loss 1.5809, val loss 1.8779 [115.02045893669128 sec]
step 500: train loss 1.4587, val loss 1.8257 [141.72541403770447 sec]
step 600: train loss 1.3462, val loss 1.7822 [167.69674253463745 sec]
step 700: train loss 1.2590, val loss 1.7634 [193.47822523117065 sec]
step 800: train loss 1.1809, val loss 1.7657 [218.8645796775818 sec]
step 900: train loss 1.1077, val loss 1.7836 [244.30302023887634 sec]
step 1000: train loss 1.0373, val loss 1.7850 [269.76081585884094 sec]
step 1100: train loss 0.9629, val loss 1.8186 [300.51559114456177 sec]
step 1200: train loss 0.8872, val loss 1.8445 [329.9080355167389 sec]
step 1300: train loss 0.8243, val loss 1.8967 [362.62849974632263 sec]
step 1400: train loss 0.7627, val loss 1.9415 [388.8012487888336 sec]
step 1500: train loss 0.6862, val loss 2.0127 [414.44808197021484 sec]
step 1600: train loss 0.6256, val loss 2.0653 [440.3579115867615 sec]
step 1700: train loss 0.5675, val loss 2.1520 [466.04576897621155 sec]
step 1800: train loss 0.5096, val loss 2.2175 [492.747172832489 sec]
step 1900: train loss 0.4579, val loss 2.3009 [521.988085269928 sec]
step 2000: train loss 0.4170, val loss 2.3439 [550.1945595741272 sec]
step 2100: train loss 0.3838, val loss 2.4124 [578.4041147232056 sec]
step 2200: train loss 0.3471, val loss 2.4806 [604.2960336208344 sec]
step 2300: train loss 0.3250, val loss 2.5600 [630.089492559433 sec]
step 2400: train loss 0.3050, val loss 2.6478 [656.0089526176453 sec]
step 2500: train loss 0.2875, val loss 2.6952 [682.0683345794678 sec]
step 2600: train loss 0.2720, val loss 2.7773 [708.1531910896301 sec]
step 2700: train loss 0.2618, val loss 2.8102 [733.4943351745605 sec]
step 2800: train loss 0.2484, val loss 2.8943 [758.8981359004974 sec]
step 2900: train loss 0.2391, val loss 2.9612 [784.3088743686676 sec]
step 3000: train loss 0.2325, val loss 2.9995 [809.7177460193634 sec]
step 3100: train loss 0.2274, val loss 3.0717 [835.1502873897552 sec]
step 3200: train loss 0.2207, val loss 3.0700 [860.6726064682007 sec]
step 3300: train loss 0.2152, val loss 3.2004 [886.0567693710327 sec]
step 3400: train loss 0.2158, val loss 3.1910 [911.489955663681 sec]
step 3500: train loss 0.2117, val loss 3.1636 [936.8707563877106 sec]
step 3600: train loss 0.2078, val loss 3.2201 [962.2301242351532 sec]
step 3700: train loss 0.2056, val loss 3.3030 [987.6667990684509 sec]
step 3800: train loss 0.2027, val loss 3.2972 [1013.0891222953796 sec]
step 3900: train loss 0.2002, val loss 3.2866 [1043.1810247898102 sec]
step 4000: train loss 0.1962, val loss 3.3648 [1074.895967245102 sec]
step 4100: train loss 0.1964, val loss 3.4099 [1101.2836802005768 sec]
step 4200: train loss 0.1948, val loss 3.3809 [1132.0599710941315 sec]
step 4300: train loss 0.1926, val loss 3.4563 [1162.1877467632294 sec]
step 4400: train loss 0.1905, val loss 3.4554 [1188.6083660125732 sec]
step 4500: train loss 0.1919, val loss 3.3888 [1214.373125553131 sec]
step 4600: train loss 0.1894, val loss 3.5101 [1240.2924768924713 sec]
step 4700: train loss 0.1881, val loss 3.5260 [1266.0060954093933 sec]
step 4800: train loss 0.1875, val loss 3.4938 [1297.5799083709717 sec]
step 4900: train loss 0.1864, val loss 3.5433 [1323.1641290187836 sec]
0.24993109703063965
Total Training Time: 1335.300225019455 seconds

Clans and expect them to -Biber ence is and everyone, he surprised
as the remaining warriors said, a
BEGINNING (1682112177.282934): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5004, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.4957, val loss 4.5058 [16.600242376327515 sec]
step 100: train loss 2.4558, val loss 2.5426 [52.6905357837677 sec]
step 200: train loss 2.2158, val loss 2.3536 [106.69056224822998 sec]
step 300: train loss 1.8349, val loss 2.0424 [137.80767107009888 sec]
step 400: train loss 1.6168, val loss 1.8904 [169.149409532547 sec]
step 500: train loss 1.4611, val loss 1.8126 [200.41951394081116 sec]
step 600: train loss 1.3429, val loss 1.7684 [231.88939547538757 sec]
step 700: train loss 1.2456, val loss 1.7720 [263.2803997993469 sec]
step 800: train loss 1.1460, val loss 1.7553 [298.2313449382782 sec]
step 900: train loss 1.0520, val loss 1.7863 [329.4866328239441 sec]
step 1000: train loss 0.9543, val loss 1.8012 [360.83183670043945 sec]
step 1100: train loss 0.8623, val loss 1.8394 [392.2865107059479 sec]
step 1200: train loss 0.7721, val loss 1.9136 [423.6582655906677 sec]
step 1300: train loss 0.6779, val loss 2.0234 [455.06979990005493 sec]
step 1400: train loss 0.5839, val loss 2.0793 [486.5218451023102 sec]
step 1500: train loss 0.4895, val loss 2.1693 [517.8916654586792 sec]
step 1600: train loss 0.4113, val loss 2.2740 [550.064822435379 sec]
step 1700: train loss 0.3491, val loss 2.4115 [581.3398056030273 sec]
step 1800: train loss 0.2883, val loss 2.5141 [612.8350210189819 sec]
step 1900: train loss 0.2468, val loss 2.5759 [644.1596953868866 sec]
step 2000: train loss 0.2109, val loss 2.7386 [675.3854646682739 sec]
step 2100: train loss 0.1854, val loss 2.8054 [706.8677308559418 sec]
step 2200: train loss 0.1668, val loss 2.9265 [738.1679213047028 sec]
step 2300: train loss 0.1528, val loss 2.9952 [769.6109175682068 sec]
step 2400: train loss 0.1458, val loss 3.0446 [800.9243214130402 sec]
step 2500: train loss 0.1374, val loss 3.2048 [832.3617761135101 sec]
step 2600: train loss 0.1298, val loss 3.2330 [864.2644956111908 sec]
step 2700: train loss 0.1267, val loss 3.2497 [895.4567828178406 sec]
step 2800: train loss 0.1218, val loss 3.3527 [926.8321888446808 sec]
step 2900: train loss 0.1206, val loss 3.4168 [958.124552488327 sec]
step 3000: train loss 0.1166, val loss 3.4535 [989.5605199337006 sec]
step 3100: train loss 0.1128, val loss 3.4803 [1020.8884716033936 sec]
step 3200: train loss 0.1113, val loss 3.5917 [1052.1353192329407 sec]
step 3300: train loss 0.1110, val loss 3.5951 [1083.5117213726044 sec]
step 3400: train loss 0.1099, val loss 3.6615 [1116.9357471466064 sec]
step 3500: train loss 0.1080, val loss 3.7134 [1148.2835867404938 sec]
step 3600: train loss 0.1067, val loss 3.7253 [1180.1027364730835 sec]
step 3700: train loss 0.1063, val loss 3.7807 [1211.5765914916992 sec]
step 3800: train loss 0.1052, val loss 3.7666 [1243.498250246048 sec]
step 3900: train loss 0.1033, val loss 3.7995 [1275.7320399284363 sec]
step 4000: train loss 0.1040, val loss 3.8414 [1307.733140707016 sec]
step 4100: train loss 0.1024, val loss 3.8146 [1339.1951229572296 sec]
step 4200: train loss 0.1009, val loss 3.8576 [1370.6388335227966 sec]
step 4300: train loss 0.1002, val loss 3.9469 [1401.8982989788055 sec]
step 4400: train loss 0.1006, val loss 3.9128 [1434.4776706695557 sec]
step 4500: train loss 0.0997, val loss 3.9298 [1465.8093423843384 sec]
step 4600: train loss 0.0996, val loss 3.9806 [1498.4002900123596 sec]
step 4700: train loss 0.0983, val loss 3.9904 [1530.8675785064697 sec]
step 4800: train loss 0.0978, val loss 4.0236 [1568.1061191558838 sec]
step 4900: train loss 0.0985, val loss 3.9803 [1600.2975165843964 sec]
0.1567273586988449
Total Training Time: 1615.442787885666 seconds

cubity in only two hours!" Beriyah
looked at Gratta, held his gaze, and said, "I could name a
double
BEGINNING (1682113795.6125073): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6451, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6433, val loss 4.6441 [27.475972890853882 sec]
step 100: train loss 2.4772, val loss 2.5695 [84.71196341514587 sec]
step 200: train loss 2.2778, val loss 2.3975 [133.20780277252197 sec]
step 300: train loss 1.8760, val loss 2.0805 [179.80998253822327 sec]
step 400: train loss 1.6162, val loss 1.9039 [237.17390608787537 sec]
step 500: train loss 1.4458, val loss 1.8178 [298.7187223434448 sec]
step 600: train loss 1.3071, val loss 1.7781 [345.8937199115753 sec]
step 700: train loss 1.1794, val loss 1.7445 [397.63671946525574 sec]
step 800: train loss 1.0623, val loss 1.7786 [463.04947566986084 sec]
step 900: train loss 0.9492, val loss 1.8235 [520.0802571773529 sec]
step 1000: train loss 0.8255, val loss 1.8571 [577.7176344394684 sec]
step 1100: train loss 0.7092, val loss 1.9517 [639.6943602561951 sec]
step 1200: train loss 0.5939, val loss 2.0391 [697.1489431858063 sec]
step 1300: train loss 0.4851, val loss 2.1508 [746.5907077789307 sec]
step 1400: train loss 0.3912, val loss 2.2794 [797.199967622757 sec]
step 1500: train loss 0.3126, val loss 2.4363 [848.5082807540894 sec]
step 1600: train loss 0.2576, val loss 2.5751 [924.7806332111359 sec]
step 1700: train loss 0.2140, val loss 2.6817 [979.4839496612549 sec]
step 1800: train loss 0.1841, val loss 2.8132 [1025.9157192707062 sec]
step 1900: train loss 0.1620, val loss 2.9305 [1097.4776000976562 sec]
step 2000: train loss 0.1512, val loss 3.0617 [1160.9896035194397 sec]
step 2100: train loss 0.1414, val loss 3.0600 [1208.7197577953339 sec]
step 2200: train loss 0.1345, val loss 3.1581 [1279.4541339874268 sec]
step 2300: train loss 0.1283, val loss 3.2775 [1357.311073064804 sec]
step 2400: train loss 0.1245, val loss 3.2264 [1419.675829410553 sec]
step 2500: train loss 0.1202, val loss 3.3930 [1480.9038376808167 sec]
step 2600: train loss 0.1179, val loss 3.3903 [1547.102638721466 sec]
step 2700: train loss 0.1142, val loss 3.4787 [1611.5118837356567 sec]
step 2800: train loss 0.1126, val loss 3.4808 [1672.8536965847015 sec]
step 2900: train loss 0.1124, val loss 3.5445 [1722.538959980011 sec]
step 3000: train loss 0.1088, val loss 3.5958 [1768.8559238910675 sec]
step 3100: train loss 0.1080, val loss 3.5863 [1821.5203294754028 sec]
step 3200: train loss 0.1073, val loss 3.7202 [1874.72967171669 sec]
step 3300: train loss 0.1065, val loss 3.7181 [1937.5027101039886 sec]
step 3400: train loss 0.1051, val loss 3.7349 [1991.3248488903046 sec]
step 3500: train loss 0.1035, val loss 3.8259 [2044.4671125411987 sec]
step 3600: train loss 0.1032, val loss 3.8162 [2090.710655927658 sec]
step 3700: train loss 0.1028, val loss 3.9079 [2137.70508480072 sec]
step 3800: train loss 0.1016, val loss 3.8760 [2184.1384563446045 sec]
step 3900: train loss 0.1017, val loss 3.9102 [2230.473478078842 sec]
step 4000: train loss 0.1016, val loss 3.8601 [2276.6913933753967 sec]
step 4100: train loss 0.1008, val loss 3.9670 [2322.9461698532104 sec]
step 4200: train loss 0.0998, val loss 3.9270 [2369.2601313591003 sec]
step 4300: train loss 0.0994, val loss 4.0175 [2415.6097660064697 sec]
step 4400: train loss 0.0980, val loss 3.9299 [2462.1763923168182 sec]
step 4500: train loss 0.0983, val loss 3.9838 [2508.7488412857056 sec]
step 4600: train loss 0.0972, val loss 3.9992 [2555.12061214447 sec]
step 4700: train loss 0.0964, val loss 3.9888 [2601.5054049491882 sec]
step 4800: train loss 0.0971, val loss 4.0306 [2647.9166700839996 sec]
step 4900: train loss 0.0965, val loss 4.0675 [2694.422519683838 sec]
0.1333049088716507
Total Training Time: 2716.2841811180115 seconds

75
SEAN McKAY
WO DENEMY
turned to face again emy faced two was silent a
ster there. His father's for
BEGINNING (1682116515.6489198): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.5778, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5948, val loss 4.6027 [11.838143587112427 sec]
step 100: train loss 2.3673, val loss 2.4751 [33.820438861846924 sec]
step 200: train loss 1.9006, val loss 2.0927 [56.074897050857544 sec]
step 300: train loss 1.6618, val loss 1.9186 [78.4245080947876 sec]
step 400: train loss 1.5071, val loss 1.8425 [100.79902195930481 sec]
step 500: train loss 1.3836, val loss 1.7766 [123.23307180404663 sec]
step 600: train loss 1.2950, val loss 1.7758 [145.63744044303894 sec]
step 700: train loss 1.2077, val loss 1.7648 [168.06227660179138 sec]
step 800: train loss 1.1290, val loss 1.7583 [190.5134255886078 sec]
step 900: train loss 1.0555, val loss 1.7923 [212.95445680618286 sec]
step 1000: train loss 0.9877, val loss 1.8100 [235.39874601364136 sec]
step 1100: train loss 0.9116, val loss 1.8627 [257.83416628837585 sec]
step 1200: train loss 0.8466, val loss 1.8919 [280.24247431755066 sec]
step 1300: train loss 0.7759, val loss 1.9294 [302.6987428665161 sec]
step 1400: train loss 0.7080, val loss 1.9900 [325.17495584487915 sec]
step 1500: train loss 0.6392, val loss 2.0960 [347.6587178707123 sec]
step 1600: train loss 0.5706, val loss 2.1389 [370.14390110969543 sec]
step 1700: train loss 0.5191, val loss 2.2190 [392.6446475982666 sec]
step 1800: train loss 0.4703, val loss 2.3290 [415.09139919281006 sec]
step 1900: train loss 0.4209, val loss 2.3482 [437.60479760169983 sec]
step 2000: train loss 0.3818, val loss 2.4323 [460.12334728240967 sec]
step 2100: train loss 0.3481, val loss 2.4753 [482.58801913261414 sec]
step 2200: train loss 0.3250, val loss 2.5693 [505.08203053474426 sec]
step 2300: train loss 0.2996, val loss 2.6288 [527.5592713356018 sec]
step 2400: train loss 0.2793, val loss 2.7198 [550.0653882026672 sec]
step 2500: train loss 0.2644, val loss 2.7838 [572.6085760593414 sec]
step 2600: train loss 0.2508, val loss 2.8853 [595.1308898925781 sec]
step 2700: train loss 0.2417, val loss 2.9134 [617.6058750152588 sec]
step 2800: train loss 0.2327, val loss 2.9517 [640.1918361186981 sec]
step 2900: train loss 0.2263, val loss 3.0090 [662.7068858146667 sec]
step 3000: train loss 0.2214, val loss 3.0900 [685.4723312854767 sec]
step 3100: train loss 0.2159, val loss 3.0817 [708.0087890625 sec]
step 3200: train loss 0.2131, val loss 3.1809 [730.4936857223511 sec]
step 3300: train loss 0.2065, val loss 3.2529 [753.0546045303345 sec]
step 3400: train loss 0.2053, val loss 3.2146 [775.4992754459381 sec]
step 3500: train loss 0.2010, val loss 3.3151 [797.9692757129669 sec]
step 3600: train loss 0.1985, val loss 3.3485 [820.3866696357727 sec]
step 3700: train loss 0.1957, val loss 3.3680 [842.8704950809479 sec]
step 3800: train loss 0.1950, val loss 3.3685 [865.3072581291199 sec]
step 3900: train loss 0.1938, val loss 3.3615 [887.6898241043091 sec]
step 4000: train loss 0.1905, val loss 3.4545 [910.087494134903 sec]
step 4100: train loss 0.1890, val loss 3.4792 [932.4744918346405 sec]
step 4200: train loss 0.1880, val loss 3.5709 [953.9185132980347 sec]
step 4300: train loss 0.1865, val loss 3.5258 [973.8512432575226 sec]
step 4400: train loss 0.1859, val loss 3.5587 [993.8154048919678 sec]
step 4500: train loss 0.1857, val loss 3.5551 [1013.8295819759369 sec]
step 4600: train loss 0.1833, val loss 3.5811 [1033.9139642715454 sec]
step 4700: train loss 0.1833, val loss 3.5755 [1053.9951272010803 sec]
step 4800: train loss 0.1812, val loss 3.6206 [1074.10001039505 sec]
step 4900: train loss 0.1812, val loss 3.6519 [1094.2149903774261 sec]
0.24413804709911346
Total Training Time: 1103.4313702583313 seconds

would not have capture his bed within. "I will take
these within." Taka dropped to alerfude that gav
BEGINNING (1682117621.369307): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5479, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5617, val loss 4.5689 [15.981700897216797 sec]
step 100: train loss 2.4118, val loss 2.5030 [45.665114641189575 sec]
step 200: train loss 1.9775, val loss 2.1552 [75.41838574409485 sec]
step 300: train loss 1.6879, val loss 1.9305 [105.18712496757507 sec]
step 400: train loss 1.5148, val loss 1.8459 [134.96327757835388 sec]
step 500: train loss 1.3781, val loss 1.7813 [164.77481293678284 sec]
step 600: train loss 1.2694, val loss 1.7648 [194.56530475616455 sec]
step 700: train loss 1.1743, val loss 1.7696 [224.33994030952454 sec]
step 800: train loss 1.0829, val loss 1.7903 [254.1407492160797 sec]
step 900: train loss 0.9936, val loss 1.8240 [283.9515812397003 sec]
step 1000: train loss 0.9095, val loss 1.8787 [313.79132771492004 sec]
step 1100: train loss 0.8248, val loss 1.9195 [343.65344882011414 sec]
step 1200: train loss 0.7393, val loss 1.9733 [373.49404668807983 sec]
step 1300: train loss 0.6538, val loss 2.0386 [403.3809669017792 sec]
step 1400: train loss 0.5756, val loss 2.1225 [433.26377153396606 sec]
step 1500: train loss 0.5143, val loss 2.1814 [463.15126061439514 sec]
step 1600: train loss 0.4555, val loss 2.2991 [493.0476050376892 sec]
step 1700: train loss 0.3979, val loss 2.4072 [523.0411968231201 sec]
step 1800: train loss 0.3609, val loss 2.4968 [553.0895924568176 sec]
step 1900: train loss 0.3315, val loss 2.5364 [583.3641538619995 sec]
step 2000: train loss 0.3003, val loss 2.6507 [613.3944675922394 sec]
step 2100: train loss 0.2772, val loss 2.7424 [643.6804087162018 sec]
step 2200: train loss 0.2595, val loss 2.7907 [673.973334312439 sec]
step 2300: train loss 0.2448, val loss 2.8403 [704.2220985889435 sec]
step 2400: train loss 0.2378, val loss 2.9567 [734.5019609928131 sec]
step 2500: train loss 0.2280, val loss 2.9834 [764.7984299659729 sec]
step 2600: train loss 0.2221, val loss 3.0561 [795.0654799938202 sec]
step 2700: train loss 0.2161, val loss 3.1434 [825.318852186203 sec]
step 2800: train loss 0.2100, val loss 3.1796 [855.5928461551666 sec]
step 2900: train loss 0.2062, val loss 3.2429 [885.8325479030609 sec]
step 3000: train loss 0.2013, val loss 3.2300 [916.1090388298035 sec]
step 3100: train loss 0.1999, val loss 3.3206 [946.3581447601318 sec]
step 3200: train loss 0.1982, val loss 3.3543 [976.6338555812836 sec]
step 3300: train loss 0.1958, val loss 3.3389 [1006.921023607254 sec]
step 3400: train loss 0.1920, val loss 3.3910 [1037.2050158977509 sec]
step 3500: train loss 0.1930, val loss 3.4088 [1067.4902780056 sec]
step 3600: train loss 0.1896, val loss 3.5067 [1097.7299666404724 sec]
step 3700: train loss 0.1892, val loss 3.4900 [1128.0188767910004 sec]
step 3800: train loss 0.1871, val loss 3.5193 [1158.3869380950928 sec]
step 3900: train loss 0.1867, val loss 3.5062 [1188.6787981987 sec]
step 4000: train loss 0.1848, val loss 3.6318 [1218.9327783584595 sec]
step 4100: train loss 0.1846, val loss 3.5693 [1249.230787038803 sec]
step 4200: train loss 0.1817, val loss 3.6967 [1279.51007604599 sec]
step 4300: train loss 0.1814, val loss 3.6996 [1309.782541513443 sec]
step 4400: train loss 0.1809, val loss 3.6351 [1340.0644118785858 sec]
step 4500: train loss 0.1811, val loss 3.6570 [1370.355108499527 sec]
step 4600: train loss 0.1813, val loss 3.6700 [1400.6597464084625 sec]
step 4700: train loss 0.1785, val loss 3.7179 [1430.9289617538452 sec]
step 4800: train loss 0.1779, val loss 3.7286 [1461.2596435546875 sec]
step 4900: train loss 0.1795, val loss 3.6953 [1491.7173917293549 sec]
0.2307611107826233
Total Training Time: 1505.6585018634796 seconds

Gratta turned to controe of
the centered and seemed the city infection their hearts.
"Do the Pyrran 
BEGINNING (1682119130.7599127): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6145, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6158, val loss 4.6263 [20.08478808403015 sec]
step 100: train loss 2.4422, val loss 2.5387 [57.361337184906006 sec]
step 200: train loss 2.1467, val loss 2.2880 [94.70530009269714 sec]
step 300: train loss 1.7711, val loss 2.0002 [131.88921332359314 sec]
step 400: train loss 1.5524, val loss 1.8538 [168.9493646621704 sec]
step 500: train loss 1.3955, val loss 1.7824 [206.04799032211304 sec]
step 600: train loss 1.2740, val loss 1.7587 [243.21966361999512 sec]
step 700: train loss 1.1528, val loss 1.7513 [280.36469054222107 sec]
step 800: train loss 1.0496, val loss 1.7864 [319.6740880012512 sec]
step 900: train loss 0.9359, val loss 1.7890 [357.0102264881134 sec]
step 1000: train loss 0.8219, val loss 1.8898 [395.0539548397064 sec]
step 1100: train loss 0.7121, val loss 1.9623 [440.0339195728302 sec]
step 1200: train loss 0.5981, val loss 2.0901 [484.7415130138397 sec]
step 1300: train loss 0.4944, val loss 2.1776 [543.4231612682343 sec]
step 1400: train loss 0.3975, val loss 2.3341 [596.6677405834198 sec]
step 1500: train loss 0.3176, val loss 2.4452 [644.1394724845886 sec]
step 1600: train loss 0.2574, val loss 2.5384 [687.4004499912262 sec]
step 1700: train loss 0.2179, val loss 2.6630 [751.7232649326324 sec]
step 1800: train loss 0.1839, val loss 2.8326 [803.7523293495178 sec]
step 1900: train loss 0.1629, val loss 2.8983 [847.1369478702545 sec]
step 2000: train loss 0.1465, val loss 3.0783 [891.7182266712189 sec]
step 2100: train loss 0.1391, val loss 3.0917 [939.8504414558411 sec]
step 2200: train loss 0.1311, val loss 3.2514 [981.5417730808258 sec]
step 2300: train loss 0.1264, val loss 3.3273 [1027.5363793373108 sec]
step 2400: train loss 0.1208, val loss 3.3343 [1071.0355994701385 sec]
step 2500: train loss 0.1177, val loss 3.4566 [1121.5272099971771 sec]
step 2600: train loss 0.1147, val loss 3.5083 [1172.508954524994 sec]
step 2700: train loss 0.1144, val loss 3.5406 [1213.8293471336365 sec]
step 2800: train loss 0.1120, val loss 3.5795 [1255.1364843845367 sec]
step 2900: train loss 0.1083, val loss 3.6117 [1299.4586114883423 sec]
step 3000: train loss 0.1066, val loss 3.7077 [1356.010508775711 sec]
step 3100: train loss 0.1070, val loss 3.7581 [1411.2081279754639 sec]
step 3200: train loss 0.1045, val loss 3.7556 [1452.311255455017 sec]
step 3300: train loss 0.1027, val loss 3.8023 [1499.9694533348083 sec]
step 3400: train loss 0.1021, val loss 3.8877 [1556.9442222118378 sec]
step 3500: train loss 0.1011, val loss 3.8727 [1606.295562505722 sec]
step 3600: train loss 0.1009, val loss 3.9246 [1649.5891995429993 sec]
step 3700: train loss 0.0991, val loss 3.9601 [1693.3503148555756 sec]
step 3800: train loss 0.0990, val loss 3.9411 [1741.742896080017 sec]
step 3900: train loss 0.0990, val loss 3.9713 [1790.199904680252 sec]
step 4000: train loss 0.0978, val loss 3.9992 [1831.344563961029 sec]
step 4100: train loss 0.0969, val loss 4.0593 [1872.4827659130096 sec]
step 4200: train loss 0.0968, val loss 4.0710 [1913.5677609443665 sec]
step 4300: train loss 0.0963, val loss 4.0800 [1954.749314069748 sec]
step 4400: train loss 0.0948, val loss 4.0707 [1995.7955107688904 sec]
step 4500: train loss 0.0952, val loss 4.0927 [2037.5256254673004 sec]
step 4600: train loss 0.0944, val loss 4.1258 [2081.5397551059723 sec]
step 4700: train loss 0.0944, val loss 4.1923 [2128.6640326976776 sec]
step 4800: train loss 0.0941, val loss 4.1713 [2178.4545855522156 sec]
step 4900: train loss 0.0937, val loss 4.2442 [2219.4080679416656 sec]
0.13747638463974
Total Training Time: 2241.2659220695496 seconds

Arphad.
"Ten not worry, unless I had a faster secution, but I so what consideringly into the hills
o
BEGINNING (1682121375.5409486): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6768, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6709, val loss 4.6665 [6.3973166942596436 sec]
step 100: train loss 2.4498, val loss 2.5457 [17.385525703430176 sec]
step 200: train loss 2.1656, val loss 2.3112 [28.22327733039856 sec]
step 300: train loss 1.9022, val loss 2.1003 [42.30562782287598 sec]
step 400: train loss 1.7408, val loss 1.9601 [53.921536445617676 sec]
step 500: train loss 1.6233, val loss 1.8787 [65.2264506816864 sec]
step 600: train loss 1.5347, val loss 1.8399 [75.7379240989685 sec]
step 700: train loss 1.4747, val loss 1.7971 [85.92263531684875 sec]
step 800: train loss 1.4140, val loss 1.7647 [98.49711084365845 sec]
step 900: train loss 1.3603, val loss 1.7671 [113.6192376613617 sec]
step 1000: train loss 1.3121, val loss 1.7426 [125.38751435279846 sec]
step 1100: train loss 1.2678, val loss 1.7450 [137.67791366577148 sec]
step 1200: train loss 1.2274, val loss 1.7250 [148.7203652858734 sec]
step 1300: train loss 1.1896, val loss 1.7460 [159.20903420448303 sec]
step 1400: train loss 1.1455, val loss 1.7537 [171.98926258087158 sec]
step 1500: train loss 1.1146, val loss 1.7529 [184.59375286102295 sec]
step 1600: train loss 1.0815, val loss 1.7662 [197.28210353851318 sec]
step 1700: train loss 1.0411, val loss 1.7671 [212.56547284126282 sec]
step 1800: train loss 1.0064, val loss 1.7816 [225.11897325515747 sec]
step 1900: train loss 0.9668, val loss 1.8128 [243.78982043266296 sec]
step 2000: train loss 0.9307, val loss 1.8169 [258.94778203964233 sec]
step 2100: train loss 0.8988, val loss 1.8118 [272.313937664032 sec]
step 2200: train loss 0.8625, val loss 1.8606 [283.3915240764618 sec]
step 2300: train loss 0.8322, val loss 1.8561 [294.0312485694885 sec]
step 2400: train loss 0.7962, val loss 1.9018 [304.5981955528259 sec]
step 2500: train loss 0.7596, val loss 1.9426 [318.93695521354675 sec]
step 2600: train loss 0.7262, val loss 1.9862 [330.5709240436554 sec]
step 2700: train loss 0.6974, val loss 1.9790 [341.33423233032227 sec]
step 2800: train loss 0.6715, val loss 2.0037 [351.8440878391266 sec]
step 2900: train loss 0.6434, val loss 2.0403 [362.26419019699097 sec]
step 3000: train loss 0.6083, val loss 2.0899 [372.56150817871094 sec]
step 3100: train loss 0.5818, val loss 2.0989 [387.53210401535034 sec]
step 3200: train loss 0.5525, val loss 2.1575 [405.51345348358154 sec]
step 3300: train loss 0.5274, val loss 2.1907 [418.93198466300964 sec]
step 3400: train loss 0.5075, val loss 2.2121 [432.1782238483429 sec]
step 3500: train loss 0.4852, val loss 2.2606 [442.85219144821167 sec]
step 3600: train loss 0.4626, val loss 2.3088 [453.7686598300934 sec]
step 3700: train loss 0.4406, val loss 2.2955 [468.2624456882477 sec]
step 3800: train loss 0.4269, val loss 2.3198 [480.25626039505005 sec]
step 3900: train loss 0.4001, val loss 2.3693 [493.0219769477844 sec]
step 4000: train loss 0.3833, val loss 2.4569 [506.1915171146393 sec]
step 4100: train loss 0.3761, val loss 2.4551 [517.9118852615356 sec]
step 4200: train loss 0.3541, val loss 2.4767 [529.6982507705688 sec]
step 4300: train loss 0.3423, val loss 2.5216 [540.1784183979034 sec]
step 4400: train loss 0.3318, val loss 2.5452 [550.4633841514587 sec]
step 4500: train loss 0.3204, val loss 2.5817 [560.7896721363068 sec]
step 4600: train loss 0.3056, val loss 2.6296 [571.0510833263397 sec]
step 4700: train loss 0.3022, val loss 2.6487 [581.2251298427582 sec]
step 4800: train loss 0.2905, val loss 2.6424 [591.7808458805084 sec]
step 4900: train loss 0.2860, val loss 2.6863 [602.1368134021759 sec]
0.47859540581703186
Total Training Time: 606.932816028595 seconds

moen closely and so the encounter. "There. We should
rouse quickly through.." Anodded and looked at 
BEGINNING (1682121984.2634947): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5985, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5922, val loss 4.5962 [7.852602481842041 sec]
step 100: train loss 2.4454, val loss 2.5424 [25.511702060699463 sec]
step 200: train loss 2.1511, val loss 2.2902 [45.374056816101074 sec]
step 300: train loss 1.8840, val loss 2.0815 [60.38227987289429 sec]
step 400: train loss 1.7074, val loss 1.9375 [77.9271194934845 sec]
step 500: train loss 1.5786, val loss 1.8445 [95.14263105392456 sec]
step 600: train loss 1.4880, val loss 1.8123 [110.29236030578613 sec]
step 700: train loss 1.4034, val loss 1.7647 [125.2530198097229 sec]
step 800: train loss 1.3463, val loss 1.7684 [140.48587846755981 sec]
step 900: train loss 1.2838, val loss 1.7531 [155.59051775932312 sec]
step 1000: train loss 1.2287, val loss 1.7399 [173.86067295074463 sec]
step 1100: train loss 1.1816, val loss 1.7520 [188.91030263900757 sec]
step 1200: train loss 1.1290, val loss 1.7412 [203.6937506198883 sec]
step 1300: train loss 1.0890, val loss 1.7683 [218.35040473937988 sec]
step 1400: train loss 1.0438, val loss 1.7674 [233.4232587814331 sec]
step 1500: train loss 1.0018, val loss 1.7719 [248.65677738189697 sec]
step 1600: train loss 0.9485, val loss 1.8033 [263.61946511268616 sec]
step 1700: train loss 0.9071, val loss 1.8322 [282.7492501735687 sec]
step 1800: train loss 0.8658, val loss 1.8492 [301.0614342689514 sec]
step 1900: train loss 0.8187, val loss 1.8651 [316.3807098865509 sec]
step 2000: train loss 0.7759, val loss 1.9253 [331.68834257125854 sec]
step 2100: train loss 0.7409, val loss 1.9272 [346.9300363063812 sec]
step 2200: train loss 0.6929, val loss 1.9721 [362.03370785713196 sec]
step 2300: train loss 0.6527, val loss 2.0114 [377.4182753562927 sec]
step 2400: train loss 0.6154, val loss 2.0533 [392.28785705566406 sec]
step 2500: train loss 0.5720, val loss 2.0914 [407.24864196777344 sec]
step 2600: train loss 0.5431, val loss 2.1632 [422.2985806465149 sec]
step 2700: train loss 0.5026, val loss 2.1652 [437.44648790359497 sec]
step 2800: train loss 0.4742, val loss 2.2050 [459.34294390678406 sec]
step 2900: train loss 0.4445, val loss 2.2880 [476.81995034217834 sec]
step 3000: train loss 0.4170, val loss 2.3118 [494.3536047935486 sec]
step 3100: train loss 0.3921, val loss 2.3926 [510.2595248222351 sec]
step 3200: train loss 0.3709, val loss 2.4304 [527.6823632717133 sec]
step 3300: train loss 0.3503, val loss 2.4853 [543.1544556617737 sec]
step 3400: train loss 0.3330, val loss 2.5322 [558.8023455142975 sec]
step 3500: train loss 0.3179, val loss 2.5444 [574.6377940177917 sec]
step 3600: train loss 0.3078, val loss 2.5496 [592.5480065345764 sec]
step 3700: train loss 0.2931, val loss 2.6194 [608.1152954101562 sec]
step 3800: train loss 0.2782, val loss 2.6696 [623.944970369339 sec]
step 3900: train loss 0.2736, val loss 2.7045 [639.9726414680481 sec]
step 4000: train loss 0.2618, val loss 2.7584 [654.8909614086151 sec]
step 4100: train loss 0.2542, val loss 2.7818 [669.7743582725525 sec]
step 4200: train loss 0.2488, val loss 2.8177 [684.6794333457947 sec]
step 4300: train loss 0.2413, val loss 2.8642 [699.5713999271393 sec]
step 4400: train loss 0.2379, val loss 2.8620 [714.7751240730286 sec]
step 4500: train loss 0.2291, val loss 2.9368 [729.6704640388489 sec]
step 4600: train loss 0.2247, val loss 3.0304 [744.5512480735779 sec]
step 4700: train loss 0.2218, val loss 3.0265 [759.4139883518219 sec]
step 4800: train loss 0.2201, val loss 3.0708 [774.5899991989136 sec]
step 4900: train loss 0.2155, val loss 3.0690 [789.3741629123688 sec]
0.35232213139533997
Total Training Time: 796.2952983379364 seconds

his day Arphad smiled and said to the walls, the
Tuon Council was about to me tweakes next to him. G
BEGINNING (1682122783.0772507): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5772, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5747, val loss 4.5806 [9.316607236862183 sec]
step 100: train loss 2.4822, val loss 2.5696 [26.42132067680359 sec]
step 200: train loss 2.3273, val loss 2.4358 [43.60176610946655 sec]
step 300: train loss 2.0309, val loss 2.1946 [62.83855581283569 sec]
step 400: train loss 1.8190, val loss 2.0347 [81.83298993110657 sec]
step 500: train loss 1.6801, val loss 1.9279 [98.80117201805115 sec]
step 600: train loss 1.5739, val loss 1.8557 [116.91042351722717 sec]
step 700: train loss 1.4897, val loss 1.8009 [134.3303153514862 sec]
step 800: train loss 1.4027, val loss 1.7710 [153.26129341125488 sec]
step 900: train loss 1.3324, val loss 1.7465 [171.0896544456482 sec]
step 1000: train loss 1.2701, val loss 1.7238 [190.25107669830322 sec]
step 1100: train loss 1.2093, val loss 1.7165 [208.2207682132721 sec]
step 1200: train loss 1.1582, val loss 1.7117 [225.47108244895935 sec]
step 1300: train loss 1.1038, val loss 1.7373 [245.4186623096466 sec]
step 1400: train loss 1.0511, val loss 1.7338 [262.7512879371643 sec]
step 1500: train loss 1.0059, val loss 1.7677 [280.19037222862244 sec]
step 1600: train loss 0.9497, val loss 1.7890 [297.611221075058 sec]
step 1700: train loss 0.8933, val loss 1.7958 [315.16150403022766 sec]
step 1800: train loss 0.8415, val loss 1.8388 [333.6229684352875 sec]
step 1900: train loss 0.7852, val loss 1.8399 [351.31826996803284 sec]
step 2000: train loss 0.7315, val loss 1.9104 [368.8873007297516 sec]
step 2100: train loss 0.6835, val loss 1.9299 [386.71600580215454 sec]
step 2200: train loss 0.6273, val loss 1.9710 [407.89928555488586 sec]
step 2300: train loss 0.5791, val loss 2.0251 [430.1691942214966 sec]
step 2400: train loss 0.5290, val loss 2.0594 [450.9933784008026 sec]
step 2500: train loss 0.4918, val loss 2.1144 [470.9952895641327 sec]
step 2600: train loss 0.4478, val loss 2.1571 [491.3111469745636 sec]
step 2700: train loss 0.4023, val loss 2.2291 [511.4932429790497 sec]
step 2800: train loss 0.3664, val loss 2.2901 [532.7267518043518 sec]
step 2900: train loss 0.3323, val loss 2.3385 [551.1829242706299 sec]
step 3000: train loss 0.3057, val loss 2.3788 [572.0281801223755 sec]
step 3100: train loss 0.2784, val loss 2.4580 [590.1726486682892 sec]
step 3200: train loss 0.2532, val loss 2.5317 [611.8601505756378 sec]
step 3300: train loss 0.2350, val loss 2.6110 [631.5135447978973 sec]
step 3400: train loss 0.2158, val loss 2.6397 [649.9622316360474 sec]
step 3500: train loss 0.2002, val loss 2.7513 [676.1817574501038 sec]
step 3600: train loss 0.1869, val loss 2.7437 [698.1476457118988 sec]
step 3700: train loss 0.1755, val loss 2.8207 [718.3823730945587 sec]
step 3800: train loss 0.1640, val loss 2.8655 [735.6019034385681 sec]
step 3900: train loss 0.1604, val loss 2.9031 [753.291825056076 sec]
step 4000: train loss 0.1523, val loss 2.9580 [778.2571880817413 sec]
step 4100: train loss 0.1469, val loss 2.9827 [805.5948305130005 sec]
step 4200: train loss 0.1394, val loss 3.0386 [825.1051495075226 sec]
step 4300: train loss 0.1378, val loss 3.0315 [844.9560883045197 sec]
step 4400: train loss 0.1344, val loss 3.1276 [866.5816943645477 sec]
step 4500: train loss 0.1311, val loss 3.1419 [884.3931677341461 sec]
step 4600: train loss 0.1281, val loss 3.1894 [904.5685954093933 sec]
step 4700: train loss 0.1259, val loss 3.2339 [923.201557636261 sec]
step 4800: train loss 0.1232, val loss 3.2386 [943.7643311023712 sec]
step 4900: train loss 0.1216, val loss 3.2663 [962.6499350070953 sec]
0.26230499148368835
Total Training Time: 975.3630149364471 seconds

Gratta had gone a convenaptions ensued."
"Startered, so that the darkness in the purvats when the cu
BEGINNING (1682123760.205926): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.7085, val loss 4.7054 [17.954345226287842 sec]
step 100: train loss 2.4803, val loss 2.5599 [47.40018606185913 sec]
step 200: train loss 2.3285, val loss 2.4406 [87.32270002365112 sec]
step 300: train loss 2.0040, val loss 2.1697 [124.92228126525879 sec]
step 400: train loss 1.7896, val loss 1.9996 [150.29171347618103 sec]
step 500: train loss 1.6217, val loss 1.8906 [175.47183561325073 sec]
step 600: train loss 1.5052, val loss 1.8311 [200.61034870147705 sec]
step 700: train loss 1.4005, val loss 1.7754 [225.73818707466125 sec]
step 800: train loss 1.3172, val loss 1.7587 [250.88348650932312 sec]
step 900: train loss 1.2387, val loss 1.7448 [277.06313848495483 sec]
step 1000: train loss 1.1640, val loss 1.7302 [306.3846802711487 sec]
step 1100: train loss 1.0989, val loss 1.7319 [346.58501863479614 sec]
step 1200: train loss 1.0332, val loss 1.7633 [384.30487847328186 sec]
step 1300: train loss 0.9610, val loss 1.7896 [416.60576367378235 sec]
step 1400: train loss 0.8821, val loss 1.8135 [450.2377407550812 sec]
step 1500: train loss 0.8070, val loss 1.8643 [488.9242205619812 sec]
step 1600: train loss 0.7354, val loss 1.8991 [518.8731255531311 sec]
step 1700: train loss 0.6605, val loss 1.9658 [544.2014482021332 sec]
step 1800: train loss 0.5988, val loss 2.0618 [569.5037729740143 sec]
step 1900: train loss 0.5318, val loss 2.0834 [594.826869726181 sec]
step 2000: train loss 0.4673, val loss 2.1640 [620.0940322875977 sec]
step 2100: train loss 0.4083, val loss 2.2477 [645.3668246269226 sec]
step 2200: train loss 0.3588, val loss 2.2995 [670.615903377533 sec]
step 2300: train loss 0.3074, val loss 2.4388 [695.8198335170746 sec]
step 2400: train loss 0.2733, val loss 2.4663 [721.0215709209442 sec]
step 2500: train loss 0.2417, val loss 2.5362 [746.213063955307 sec]
step 2600: train loss 0.2157, val loss 2.6477 [771.3812310695648 sec]
step 2700: train loss 0.1946, val loss 2.7295 [796.5565655231476 sec]
step 2800: train loss 0.1809, val loss 2.8095 [821.663382768631 sec]
step 2900: train loss 0.1644, val loss 2.8924 [846.7413203716278 sec]
step 3000: train loss 0.1542, val loss 2.9416 [871.8284201622009 sec]
step 3100: train loss 0.1491, val loss 2.9859 [896.8996601104736 sec]
step 3200: train loss 0.1416, val loss 3.0552 [921.9665155410767 sec]
step 3300: train loss 0.1371, val loss 3.0735 [947.0210697650909 sec]
step 3400: train loss 0.1309, val loss 3.1700 [972.0285830497742 sec]
step 3500: train loss 0.1289, val loss 3.1649 [997.0844759941101 sec]
step 3600: train loss 0.1260, val loss 3.2512 [1022.0911116600037 sec]
step 3700: train loss 0.1236, val loss 3.2746 [1047.0936524868011 sec]
step 3800: train loss 0.1209, val loss 3.3587 [1072.0873529911041 sec]
step 3900: train loss 0.1185, val loss 3.3722 [1097.089524269104 sec]
step 4000: train loss 0.1173, val loss 3.4007 [1122.0817608833313 sec]
step 4100: train loss 0.1145, val loss 3.4578 [1147.083755493164 sec]
step 4200: train loss 0.1139, val loss 3.4759 [1172.0652523040771 sec]
step 4300: train loss 0.1126, val loss 3.4617 [1197.051434993744 sec]
step 4400: train loss 0.1118, val loss 3.5596 [1221.9976091384888 sec]
step 4500: train loss 0.1106, val loss 3.5449 [1246.9328033924103 sec]
step 4600: train loss 0.1095, val loss 3.5991 [1271.8827738761902 sec]
step 4700: train loss 0.1101, val loss 3.6158 [1296.8300721645355 sec]
step 4800: train loss 0.1070, val loss 3.6444 [1321.795667886734 sec]
step 4900: train loss 0.1073, val loss 3.6875 [1346.7416245937347 sec]
0.1841936856508255
Total Training Time: 1358.1053285598755 seconds

nearly have them honor. GrWhen they already the tuons, gray
diead the gathered to Yehu, sir!"
Gratta
BEGINNING (1682125120.7984118): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6467, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6322, val loss 4.6316 [6.747140645980835 sec]
step 100: train loss 2.4455, val loss 2.5273 [18.795352935791016 sec]
step 200: train loss 2.1084, val loss 2.2529 [30.840713262557983 sec]
step 300: train loss 1.8469, val loss 2.0427 [42.98827123641968 sec]
step 400: train loss 1.6955, val loss 1.9411 [55.29071283340454 sec]
step 500: train loss 1.5850, val loss 1.8670 [67.72517156600952 sec]
step 600: train loss 1.4982, val loss 1.8191 [80.1113953590393 sec]
step 700: train loss 1.4212, val loss 1.7712 [92.30297327041626 sec]
step 800: train loss 1.3600, val loss 1.7620 [104.50745296478271 sec]
step 900: train loss 1.2979, val loss 1.7517 [116.86995315551758 sec]
step 1000: train loss 1.2542, val loss 1.7391 [129.02436470985413 sec]
step 1100: train loss 1.2054, val loss 1.7386 [141.23156070709229 sec]
step 1200: train loss 1.1622, val loss 1.7327 [153.4465730190277 sec]
step 1300: train loss 1.1137, val loss 1.7603 [165.56781673431396 sec]
step 1400: train loss 1.0708, val loss 1.7459 [177.71509504318237 sec]
step 1500: train loss 1.0295, val loss 1.7665 [189.87275671958923 sec]
step 1600: train loss 0.9892, val loss 1.7918 [202.05913043022156 sec]
step 1700: train loss 0.9439, val loss 1.7996 [214.36397075653076 sec]
step 1800: train loss 0.9043, val loss 1.8163 [227.80884408950806 sec]
step 1900: train loss 0.8673, val loss 1.8450 [239.90643739700317 sec]
step 2000: train loss 0.8249, val loss 1.8728 [258.5776000022888 sec]
step 2100: train loss 0.7931, val loss 1.9060 [272.60540080070496 sec]
step 2200: train loss 0.7557, val loss 1.9305 [289.2300901412964 sec]
step 2300: train loss 0.7191, val loss 1.9642 [305.448935508728 sec]
step 2400: train loss 0.6788, val loss 1.9951 [323.499632358551 sec]
step 2500: train loss 0.6421, val loss 2.0456 [337.6132142543793 sec]
step 2600: train loss 0.6116, val loss 2.0497 [351.58465576171875 sec]
step 2700: train loss 0.5737, val loss 2.1074 [365.3791925907135 sec]
step 2800: train loss 0.5460, val loss 2.1412 [382.2381172180176 sec]
step 2900: train loss 0.5201, val loss 2.1901 [399.37607741355896 sec]
step 3000: train loss 0.4910, val loss 2.2291 [417.89181756973267 sec]
step 3100: train loss 0.4625, val loss 2.2751 [431.7701919078827 sec]
step 3200: train loss 0.4426, val loss 2.3073 [446.10442566871643 sec]
step 3300: train loss 0.4151, val loss 2.3561 [462.153192281723 sec]
step 3400: train loss 0.3958, val loss 2.3853 [476.45987606048584 sec]
step 3500: train loss 0.3776, val loss 2.3987 [493.78390669822693 sec]
step 3600: train loss 0.3619, val loss 2.4613 [507.6478548049927 sec]
step 3700: train loss 0.3435, val loss 2.5058 [520.7920823097229 sec]
step 3800: train loss 0.3285, val loss 2.5465 [533.9865427017212 sec]
step 3900: train loss 0.3141, val loss 2.5770 [546.5786118507385 sec]
step 4000: train loss 0.3051, val loss 2.5886 [559.0498464107513 sec]
step 4100: train loss 0.2920, val loss 2.6458 [571.401074886322 sec]
step 4200: train loss 0.2806, val loss 2.6879 [584.1418879032135 sec]
step 4300: train loss 0.2736, val loss 2.7247 [596.4619364738464 sec]
step 4400: train loss 0.2632, val loss 2.7695 [608.9591062068939 sec]
step 4500: train loss 0.2562, val loss 2.7843 [625.0267100334167 sec]
step 4600: train loss 0.2487, val loss 2.8014 [638.0434420108795 sec]
step 4700: train loss 0.2444, val loss 2.8603 [651.611624956131 sec]
step 4800: train loss 0.2400, val loss 2.8865 [665.6339254379272 sec]
step 4900: train loss 0.2359, val loss 2.8450 [679.3246402740479 sec]
0.4205390214920044
Total Training Time: 684.8831789493561 seconds

swarm that must." The between Taka was guarded
by to them, we will assue my general? We did not have
BEGINNING (1682125807.426485): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.6353, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6351, val loss 4.6427 [9.6753830909729 sec]
step 100: train loss 2.4290, val loss 2.5262 [28.811861276626587 sec]
step 200: train loss 2.0822, val loss 2.2388 [47.11530923843384 sec]
step 300: train loss 1.8107, val loss 2.0241 [65.59570145606995 sec]
step 400: train loss 1.6283, val loss 1.8873 [85.14294362068176 sec]
step 500: train loss 1.5181, val loss 1.8393 [103.54956412315369 sec]
step 600: train loss 1.4153, val loss 1.7696 [122.8613703250885 sec]
step 700: train loss 1.3438, val loss 1.7587 [143.66569805145264 sec]
step 800: train loss 1.2682, val loss 1.7274 [164.06164169311523 sec]
step 900: train loss 1.2116, val loss 1.7487 [184.45714044570923 sec]
step 1000: train loss 1.1471, val loss 1.7573 [203.35976362228394 sec]
step 1100: train loss 1.0977, val loss 1.7433 [221.9569742679596 sec]
step 1200: train loss 1.0359, val loss 1.7577 [240.3503496646881 sec]
step 1300: train loss 0.9847, val loss 1.7848 [260.4636845588684 sec]
step 1400: train loss 0.9263, val loss 1.8188 [283.27798533439636 sec]
step 1500: train loss 0.8833, val loss 1.8549 [303.1485650539398 sec]
step 1600: train loss 0.8296, val loss 1.8661 [322.8280086517334 sec]
step 1700: train loss 0.7701, val loss 1.8954 [342.2218334674835 sec]
step 1800: train loss 0.7239, val loss 1.9480 [361.75371265411377 sec]
step 1900: train loss 0.6721, val loss 1.9760 [381.4763557910919 sec]
step 2000: train loss 0.6258, val loss 2.0433 [404.85306882858276 sec]
step 2100: train loss 0.5750, val loss 2.1031 [428.5242509841919 sec]
step 2200: train loss 0.5349, val loss 2.1414 [448.07661986351013 sec]
step 2300: train loss 0.5000, val loss 2.1485 [467.3913505077362 sec]
step 2400: train loss 0.4600, val loss 2.2109 [485.751558303833 sec]
step 2500: train loss 0.4291, val loss 2.3152 [504.30398321151733 sec]
step 2600: train loss 0.3989, val loss 2.3402 [523.3665804862976 sec]
step 2700: train loss 0.3673, val loss 2.4048 [544.1460342407227 sec]
step 2800: train loss 0.3448, val loss 2.4551 [563.1625237464905 sec]
step 2900: train loss 0.3204, val loss 2.5036 [582.2519752979279 sec]
step 3000: train loss 0.3066, val loss 2.5816 [603.106550693512 sec]
step 3100: train loss 0.2902, val loss 2.6117 [625.9102084636688 sec]
step 3200: train loss 0.2725, val loss 2.6854 [648.4508049488068 sec]
step 3300: train loss 0.2635, val loss 2.7373 [666.6913411617279 sec]
step 3400: train loss 0.2551, val loss 2.7547 [684.6914293766022 sec]
step 3500: train loss 0.2453, val loss 2.7969 [703.4387176036835 sec]
step 3600: train loss 0.2398, val loss 2.8414 [722.5402715206146 sec]
step 3700: train loss 0.2320, val loss 2.9083 [747.9808914661407 sec]
step 3800: train loss 0.2275, val loss 2.9227 [766.6326467990875 sec]
step 3900: train loss 0.2233, val loss 2.9474 [785.272453546524 sec]
step 4000: train loss 0.2182, val loss 2.9908 [804.5373549461365 sec]
step 4100: train loss 0.2147, val loss 3.0456 [824.2951099872589 sec]
step 4200: train loss 0.2100, val loss 3.1066 [853.7975435256958 sec]
step 4300: train loss 0.2091, val loss 3.1174 [878.4548456668854 sec]
step 4400: train loss 0.2053, val loss 3.1235 [902.3110499382019 sec]
step 4500: train loss 0.2016, val loss 3.1564 [931.1351163387299 sec]
step 4600: train loss 0.2001, val loss 3.2155 [950.1480360031128 sec]
step 4700: train loss 0.1984, val loss 3.2323 [977.5878665447235 sec]
step 4800: train loss 0.1971, val loss 3.2396 [1011.6435286998749 sec]
step 4900: train loss 0.1951, val loss 3.2745 [1033.2707512378693 sec]
0.3052957355976105
Total Training Time: 1045.4170651435852 seconds

far Yah Elyon will deces, myself as you do taken to their
placks. General Beriyah as he had man sold
BEGINNING (1682126855.3131204): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5491, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5566, val loss 4.5569 [16.15114140510559 sec]
step 100: train loss 2.4744, val loss 2.5531 [47.52038836479187 sec]
step 200: train loss 2.2910, val loss 2.4036 [70.28912472724915 sec]
step 300: train loss 1.9843, val loss 2.1542 [93.63813209533691 sec]
step 400: train loss 1.7744, val loss 1.9988 [120.49508428573608 sec]
step 500: train loss 1.6354, val loss 1.8916 [155.74909687042236 sec]
step 600: train loss 1.5278, val loss 1.8240 [185.4147081375122 sec]
step 700: train loss 1.4337, val loss 1.7826 [208.871253490448 sec]
step 800: train loss 1.3675, val loss 1.7608 [232.01529836654663 sec]
step 900: train loss 1.2922, val loss 1.7434 [259.5985050201416 sec]
step 1000: train loss 1.2223, val loss 1.7288 [281.7929992675781 sec]
step 1100: train loss 1.1579, val loss 1.7192 [304.4898998737335 sec]
step 1200: train loss 1.0975, val loss 1.7251 [327.18324875831604 sec]
step 1300: train loss 1.0362, val loss 1.7265 [349.8391902446747 sec]
step 1400: train loss 0.9767, val loss 1.7646 [372.54480814933777 sec]
step 1500: train loss 0.9147, val loss 1.7928 [395.552880525589 sec]
step 1600: train loss 0.8584, val loss 1.8386 [418.13187980651855 sec]
step 1700: train loss 0.7867, val loss 1.8482 [441.0356168746948 sec]
step 1800: train loss 0.7288, val loss 1.8813 [463.4536921977997 sec]
step 1900: train loss 0.6680, val loss 1.9450 [486.06093168258667 sec]
step 2000: train loss 0.6122, val loss 1.9939 [509.0622065067291 sec]
step 2100: train loss 0.5527, val loss 2.0449 [532.7998905181885 sec]
step 2200: train loss 0.4944, val loss 2.1042 [559.5842831134796 sec]
step 2300: train loss 0.4475, val loss 2.1774 [584.5852913856506 sec]
step 2400: train loss 0.3972, val loss 2.2609 [611.9988393783569 sec]
step 2500: train loss 0.3562, val loss 2.3087 [634.8725221157074 sec]
step 2600: train loss 0.3201, val loss 2.3971 [656.8829090595245 sec]
step 2700: train loss 0.2842, val loss 2.4652 [679.0681731700897 sec]
step 2800: train loss 0.2556, val loss 2.5093 [703.2449049949646 sec]
step 2900: train loss 0.2338, val loss 2.5954 [724.8670010566711 sec]
step 3000: train loss 0.2142, val loss 2.6410 [747.4074635505676 sec]
step 3100: train loss 0.1946, val loss 2.6988 [773.3082828521729 sec]
step 3200: train loss 0.1823, val loss 2.7854 [796.1736023426056 sec]
step 3300: train loss 0.1685, val loss 2.8455 [819.56658411026 sec]
step 3400: train loss 0.1623, val loss 2.8963 [843.7414228916168 sec]
step 3500: train loss 0.1526, val loss 2.9346 [868.0947012901306 sec]
step 3600: train loss 0.1464, val loss 2.9905 [892.1454317569733 sec]
step 3700: train loss 0.1400, val loss 3.0425 [916.4955852031708 sec]
step 3800: train loss 0.1362, val loss 3.0876 [940.6071178913116 sec]
step 3900: train loss 0.1323, val loss 3.1306 [964.5718350410461 sec]
step 4000: train loss 0.1296, val loss 3.2203 [988.8771877288818 sec]
step 4100: train loss 0.1237, val loss 3.2632 [1013.0354096889496 sec]
step 4200: train loss 0.1240, val loss 3.2435 [1037.5718839168549 sec]
step 4300: train loss 0.1209, val loss 3.2937 [1061.7566950321198 sec]
step 4400: train loss 0.1187, val loss 3.3304 [1086.4367089271545 sec]
step 4500: train loss 0.1172, val loss 3.3920 [1110.5608468055725 sec]
step 4600: train loss 0.1155, val loss 3.4376 [1134.5612511634827 sec]
step 4700: train loss 0.1150, val loss 3.4210 [1160.055969953537 sec]
step 4800: train loss 0.1123, val loss 3.4491 [1188.289870262146 sec]
step 4900: train loss 0.1127, val loss 3.5195 [1214.7208931446075 sec]
0.22688081860542297
Total Training Time: 1228.3273537158966 seconds

Gratta's stomach growled into the river, his
front perhaps with the doors and trees and detail said,
BEGINNING (1682128085.6088762): Baseline LR(0.00025) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.6121, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5996, val loss 4.6065 [17.421022176742554 sec]
step 100: train loss 2.4695, val loss 2.5533 [58.19882798194885 sec]
step 200: train loss 2.2931, val loss 2.4060 [102.15230536460876 sec]
step 300: train loss 1.9514, val loss 2.1402 [137.13496708869934 sec]
step 400: train loss 1.7181, val loss 1.9515 [184.231431722641 sec]
step 500: train loss 1.5653, val loss 1.8593 [222.02397513389587 sec]
step 600: train loss 1.4447, val loss 1.8034 [260.493310213089 sec]
step 700: train loss 1.3348, val loss 1.7496 [300.27184319496155 sec]
step 800: train loss 1.2483, val loss 1.7545 [343.86364698410034 sec]
step 900: train loss 1.1656, val loss 1.7317 [377.68749237060547 sec]
step 1000: train loss 1.0827, val loss 1.7300 [412.0290548801422 sec]
step 1100: train loss 1.0069, val loss 1.7589 [448.2909219264984 sec]
step 1200: train loss 0.9155, val loss 1.7776 [482.9979326725006 sec]
step 1300: train loss 0.8289, val loss 1.8586 [518.2371482849121 sec]
step 1400: train loss 0.7520, val loss 1.8876 [550.8833329677582 sec]
step 1500: train loss 0.6622, val loss 1.9624 [583.6475114822388 sec]
step 1600: train loss 0.5862, val loss 2.0328 [616.6109447479248 sec]
step 1700: train loss 0.5128, val loss 2.1138 [701.4003221988678 sec]
step 1800: train loss 0.4348, val loss 2.1889 [753.100359916687 sec]
step 1900: train loss 0.3748, val loss 2.2692 [813.7066285610199 sec]
step 2000: train loss 0.3164, val loss 2.3879 [852.4728844165802 sec]
step 2100: train loss 0.2669, val loss 2.5358 [950.2542145252228 sec]
step 2200: train loss 0.2332, val loss 2.5556 [987.7790138721466 sec]
step 2300: train loss 0.2048, val loss 2.6626 [1029.9679162502289 sec]
step 2400: train loss 0.1823, val loss 2.7262 [1065.7178819179535 sec]
step 2500: train loss 0.1684, val loss 2.8273 [1105.256554365158 sec]
step 2600: train loss 0.1588, val loss 2.8809 [1141.0767979621887 sec]
step 2700: train loss 0.1471, val loss 2.9842 [1179.7885291576385 sec]
step 2800: train loss 0.1373, val loss 3.0660 [1211.7949981689453 sec]
step 2900: train loss 0.1348, val loss 3.1351 [1244.0977363586426 sec]
step 3000: train loss 0.1287, val loss 3.1949 [1276.35649228096 sec]
step 3100: train loss 0.1250, val loss 3.2595 [1308.6287531852722 sec]
step 3200: train loss 0.1213, val loss 3.2915 [1340.8749816417694 sec]
step 3300: train loss 0.1186, val loss 3.3498 [1373.104653120041 sec]
step 3400: train loss 0.1170, val loss 3.3800 [1405.3032455444336 sec]
step 3500: train loss 0.1145, val loss 3.4487 [1437.5003504753113 sec]
step 3600: train loss 0.1140, val loss 3.4490 [1469.6765899658203 sec]
step 3700: train loss 0.1119, val loss 3.4655 [1501.8431825637817 sec]
step 3800: train loss 0.1099, val loss 3.5567 [1533.951485157013 sec]
step 3900: train loss 0.1086, val loss 3.6217 [1566.0470774173737 sec]
step 4000: train loss 0.1085, val loss 3.6113 [1598.1457159519196 sec]
step 4100: train loss 0.1069, val loss 3.6526 [1630.2425439357758 sec]
step 4200: train loss 0.1066, val loss 3.6737 [1662.2858538627625 sec]
step 4300: train loss 0.1060, val loss 3.6654 [1694.6021738052368 sec]
step 4400: train loss 0.1039, val loss 3.7313 [1726.6241991519928 sec]
step 4500: train loss 0.1033, val loss 3.7596 [1758.723197221756 sec]
step 4600: train loss 0.1020, val loss 3.7666 [1790.7300658226013 sec]
step 4700: train loss 0.1016, val loss 3.7571 [1822.7965321540833 sec]
step 4800: train loss 0.1017, val loss 3.8118 [1854.8554637432098 sec]
step 4900: train loss 0.1006, val loss 3.8424 [1886.8895289897919 sec]
0.1643008291721344
Total Training Time: 1901.5075662136078 seconds

as well."
"Yes, sir!" one of the waater crowd as the forest.
Some were starting to scorch. He kicked
BEGINNING (1682129989.6307404): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(4)
torch.Size([3072, 86])
tensor(4.6550, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6537, val loss 4.6477 [9.260252952575684 sec]
step 100: train loss 2.4067, val loss 2.5102 [26.71975541114807 sec]
step 200: train loss 2.0373, val loss 2.2087 [44.24775743484497 sec]
step 300: train loss 1.7884, val loss 2.0092 [61.790724754333496 sec]
step 400: train loss 1.6167, val loss 1.8856 [79.35546445846558 sec]
step 500: train loss 1.5104, val loss 1.8320 [96.95087504386902 sec]
step 600: train loss 1.4244, val loss 1.8008 [114.4976077079773 sec]
step 700: train loss 1.3493, val loss 1.7744 [132.04541516304016 sec]
step 800: train loss 1.2844, val loss 1.7775 [149.5724732875824 sec]
step 900: train loss 1.2122, val loss 1.7583 [167.20640444755554 sec]
step 1000: train loss 1.1586, val loss 1.7637 [185.01948952674866 sec]
step 1100: train loss 1.1028, val loss 1.7783 [202.487450838089 sec]
step 1200: train loss 1.0499, val loss 1.7829 [220.00464534759521 sec]
step 1300: train loss 0.9819, val loss 1.8292 [237.54614925384521 sec]
step 1400: train loss 0.9361, val loss 1.8174 [254.87636280059814 sec]
step 1500: train loss 0.8804, val loss 1.8611 [272.2144055366516 sec]
step 1600: train loss 0.8311, val loss 1.9057 [289.61020016670227 sec]
step 1700: train loss 0.7831, val loss 1.9479 [306.9338164329529 sec]
step 1800: train loss 0.7255, val loss 2.0099 [324.2273180484772 sec]
step 1900: train loss 0.6741, val loss 2.0406 [341.5182030200958 sec]
step 2000: train loss 0.6226, val loss 2.0658 [358.82277488708496 sec]
step 2100: train loss 0.5754, val loss 2.1344 [376.0779962539673 sec]
step 2200: train loss 0.5307, val loss 2.1767 [393.3457570075989 sec]
step 2300: train loss 0.4917, val loss 2.2447 [410.68928360939026 sec]
step 2400: train loss 0.4524, val loss 2.3158 [427.5249693393707 sec]
step 2500: train loss 0.4223, val loss 2.3563 [443.1075828075409 sec]
step 2600: train loss 0.3916, val loss 2.4238 [458.7178428173065 sec]
step 2700: train loss 0.3659, val loss 2.4692 [474.3811004161835 sec]
step 2800: train loss 0.3410, val loss 2.5922 [490.0556092262268 sec]
step 2900: train loss 0.3238, val loss 2.6140 [505.7478561401367 sec]
step 3000: train loss 0.3002, val loss 2.6216 [521.4610486030579 sec]
step 3100: train loss 0.2852, val loss 2.7153 [537.2102570533752 sec]
step 3200: train loss 0.2758, val loss 2.7601 [553.0712425708771 sec]
step 3300: train loss 0.2617, val loss 2.8427 [568.8335089683533 sec]
step 3400: train loss 0.2520, val loss 2.8515 [584.613055229187 sec]
step 3500: train loss 0.2473, val loss 2.8673 [600.4055161476135 sec]
step 3600: train loss 0.2404, val loss 2.9579 [616.2323250770569 sec]
step 3700: train loss 0.2288, val loss 2.9921 [632.0337951183319 sec]
step 3800: train loss 0.2279, val loss 3.0499 [647.8466985225677 sec]
step 3900: train loss 0.2244, val loss 3.1090 [663.6451969146729 sec]
step 4000: train loss 0.2183, val loss 3.1439 [679.4971566200256 sec]
step 4100: train loss 0.2152, val loss 3.1986 [695.3354761600494 sec]
step 4200: train loss 0.2098, val loss 3.1378 [711.1728522777557 sec]
step 4300: train loss 0.2078, val loss 3.2426 [727.0004580020905 sec]
step 4400: train loss 0.2062, val loss 3.2376 [742.8613967895508 sec]
step 4500: train loss 0.2036, val loss 3.3219 [758.7675840854645 sec]
step 4600: train loss 0.2005, val loss 3.3214 [774.6266710758209 sec]
step 4700: train loss 0.1981, val loss 3.3363 [790.4876503944397 sec]
step 4800: train loss 0.1970, val loss 3.3513 [806.345742225647 sec]
step 4900: train loss 0.1956, val loss 3.3575 [822.2662363052368 sec]
0.2992570102214813
Total Training Time: 829.7488360404968 seconds

Half any men any elso smell. "We will
witness my oath pard ao."
Gratta nodded, unsured his warriors 
BEGINNING (1682130821.7567718): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(48) Layers(6)
torch.Size([3072, 86])
tensor(4.5946, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6281, val loss 4.6203 [12.262662887573242 sec]
step 100: train loss 2.4409, val loss 2.5307 [35.76141595840454 sec]
step 200: train loss 2.0824, val loss 2.2382 [59.23753023147583 sec]
step 300: train loss 1.7989, val loss 2.0042 [82.6413893699646 sec]
step 400: train loss 1.6095, val loss 1.8948 [106.04072666168213 sec]
step 500: train loss 1.4892, val loss 1.8346 [129.54137992858887 sec]
step 600: train loss 1.3846, val loss 1.7916 [152.98767018318176 sec]
step 700: train loss 1.2979, val loss 1.7668 [176.45669722557068 sec]
step 800: train loss 1.2199, val loss 1.7606 [199.90307784080505 sec]
step 900: train loss 1.1514, val loss 1.7869 [223.36375427246094 sec]
step 1000: train loss 1.0784, val loss 1.7845 [246.83197855949402 sec]
step 1100: train loss 1.0034, val loss 1.8290 [270.2413582801819 sec]
step 1200: train loss 0.9429, val loss 1.8182 [293.69870805740356 sec]
step 1300: train loss 0.8753, val loss 1.8705 [317.16687750816345 sec]
step 1400: train loss 0.8091, val loss 1.8929 [340.61954164505005 sec]
step 1500: train loss 0.7508, val loss 1.9778 [364.09775161743164 sec]
step 1600: train loss 0.6768, val loss 2.0070 [387.6230592727661 sec]
step 1700: train loss 0.6281, val loss 2.0871 [411.10539197921753 sec]
step 1800: train loss 0.5703, val loss 2.1403 [434.57261061668396 sec]
step 1900: train loss 0.5126, val loss 2.2237 [458.1380970478058 sec]
step 2000: train loss 0.4675, val loss 2.2952 [481.61487793922424 sec]
step 2100: train loss 0.4243, val loss 2.3688 [505.18981409072876 sec]
step 2200: train loss 0.3946, val loss 2.4476 [528.734415769577 sec]
step 2300: train loss 0.3578, val loss 2.4811 [552.2524704933167 sec]
step 2400: train loss 0.3249, val loss 2.5852 [575.8843746185303 sec]
step 2500: train loss 0.3077, val loss 2.6241 [599.3499119281769 sec]
step 2600: train loss 0.2911, val loss 2.7202 [622.8464024066925 sec]
step 2700: train loss 0.2737, val loss 2.7473 [646.4182920455933 sec]
step 2800: train loss 0.2592, val loss 2.8758 [669.9102153778076 sec]
step 2900: train loss 0.2490, val loss 2.8234 [693.379664182663 sec]
step 3000: train loss 0.2429, val loss 2.9124 [717.0036718845367 sec]
step 3100: train loss 0.2343, val loss 2.9935 [740.456912279129 sec]
step 3200: train loss 0.2277, val loss 3.0674 [763.9477772712708 sec]
step 3300: train loss 0.2214, val loss 3.1073 [787.541880607605 sec]
step 3400: train loss 0.2172, val loss 3.1591 [811.0915179252625 sec]
step 3500: train loss 0.2147, val loss 3.1332 [835.5072903633118 sec]
step 3600: train loss 0.2102, val loss 3.1863 [861.5565690994263 sec]
step 3700: train loss 0.2097, val loss 3.2854 [886.095541715622 sec]
step 3800: train loss 0.2034, val loss 3.2637 [910.8286397457123 sec]
step 3900: train loss 0.2040, val loss 3.2898 [935.1397960186005 sec]
step 4000: train loss 0.1995, val loss 3.3490 [959.4413585662842 sec]
step 4100: train loss 0.1979, val loss 3.4157 [983.9390444755554 sec]
step 4200: train loss 0.1992, val loss 3.4174 [1008.2923891544342 sec]
step 4300: train loss 0.1964, val loss 3.3755 [1033.0434122085571 sec]
step 4400: train loss 0.1941, val loss 3.4297 [1057.5296776294708 sec]
step 4500: train loss 0.1913, val loss 3.4983 [1082.0723786354065 sec]
step 4600: train loss 0.1921, val loss 3.3951 [1106.8684360980988 sec]
step 4700: train loss 0.1900, val loss 3.5139 [1131.3517363071442 sec]
step 4800: train loss 0.1877, val loss 3.5723 [1155.914938211441 sec]
step 4900: train loss 0.1883, val loss 3.5247 [1180.7108480930328 sec]
0.274723082780838
Total Training Time: 1192.3262147903442 seconds

Biblicans me to their cells, even if we captured by
humans, Chief Gratta? You are far too powerful f
BEGINNING (1682132018.5013576): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.5625, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5684, val loss 4.5681 [18.891401290893555 sec]
step 100: train loss 2.4658, val loss 2.5557 [54.37921905517578 sec]
step 200: train loss 2.2605, val loss 2.3904 [89.79118275642395 sec]
step 300: train loss 1.9064, val loss 2.0916 [125.17505359649658 sec]
step 400: train loss 1.6868, val loss 1.9293 [160.32041430473328 sec]
step 500: train loss 1.5241, val loss 1.8352 [192.9462833404541 sec]
step 600: train loss 1.4104, val loss 1.7781 [225.94066071510315 sec]
step 700: train loss 1.3108, val loss 1.7524 [258.9499566555023 sec]
step 800: train loss 1.2266, val loss 1.7702 [291.61593079566956 sec]
step 900: train loss 1.1362, val loss 1.7421 [324.27308464050293 sec]
step 1000: train loss 1.0524, val loss 1.7627 [356.7937650680542 sec]
step 1100: train loss 0.9685, val loss 1.7866 [389.5528280735016 sec]
step 1200: train loss 0.8832, val loss 1.8411 [421.8696162700653 sec]
step 1300: train loss 0.7988, val loss 1.8613 [454.2398171424866 sec]
step 1400: train loss 0.7126, val loss 1.9430 [486.34635734558105 sec]
step 1500: train loss 0.6274, val loss 2.0233 [518.483460187912 sec]
step 1600: train loss 0.5483, val loss 2.0912 [550.6953191757202 sec]
step 1700: train loss 0.4664, val loss 2.2060 [582.7683825492859 sec]
step 1800: train loss 0.4057, val loss 2.2967 [614.761638879776 sec]
step 1900: train loss 0.3395, val loss 2.3474 [646.7054646015167 sec]
step 2000: train loss 0.2938, val loss 2.5181 [678.5481667518616 sec]
step 2100: train loss 0.2508, val loss 2.5716 [710.4356105327606 sec]
step 2200: train loss 0.2153, val loss 2.6619 [742.1794037818909 sec]
step 2300: train loss 0.1907, val loss 2.8131 [774.0632286071777 sec]
step 2400: train loss 0.1752, val loss 2.8490 [805.987832069397 sec]
step 2500: train loss 0.1599, val loss 2.9384 [837.7964582443237 sec]
step 2600: train loss 0.1496, val loss 3.0426 [869.6791062355042 sec]
step 2700: train loss 0.1435, val loss 3.0791 [901.5661854743958 sec]
step 2800: train loss 0.1366, val loss 3.1998 [933.4471707344055 sec]
step 2900: train loss 0.1296, val loss 3.2712 [965.2559201717377 sec]
step 3000: train loss 0.1282, val loss 3.2561 [997.1234831809998 sec]
step 3100: train loss 0.1236, val loss 3.3481 [1028.9552557468414 sec]
step 3200: train loss 0.1198, val loss 3.4105 [1060.8693804740906 sec]
step 3300: train loss 0.1179, val loss 3.4418 [1092.7375962734222 sec]
step 3400: train loss 0.1161, val loss 3.4941 [1124.4397838115692 sec]
step 3500: train loss 0.1137, val loss 3.5418 [1156.1163537502289 sec]
step 3600: train loss 0.1128, val loss 3.5732 [1187.9555566310883 sec]
step 3700: train loss 0.1103, val loss 3.6216 [1219.632580280304 sec]
step 3800: train loss 0.1098, val loss 3.6298 [1251.4623954296112 sec]
step 3900: train loss 0.1074, val loss 3.6506 [1283.126719713211 sec]
step 4000: train loss 0.1063, val loss 3.7264 [1314.7186167240143 sec]
step 4100: train loss 0.1067, val loss 3.7934 [1346.2394008636475 sec]
step 4200: train loss 0.1052, val loss 3.7836 [1378.0349538326263 sec]
step 4300: train loss 0.1050, val loss 3.8013 [1409.7395408153534 sec]
step 4400: train loss 0.1040, val loss 3.8576 [1441.2030215263367 sec]
step 4500: train loss 0.1035, val loss 3.8603 [1472.4737751483917 sec]
step 4600: train loss 0.1032, val loss 3.8617 [1503.6715931892395 sec]
step 4700: train loss 0.1018, val loss 3.9007 [1534.8613469600677 sec]
step 4800: train loss 0.1009, val loss 3.8942 [1565.1889457702637 sec]
step 4900: train loss 0.1008, val loss 3.8989 [1593.3613607883453 sec]
0.1568261682987213
Total Training Time: 1606.4097905158997 seconds

was came, should different –
we take the troop to the smaeuws. "Do not worrin3
SEAN McKAY
Elyon as t
BEGINNING (1682133627.2775493): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5898, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6006, val loss 4.5984 [22.44763422012329 sec]
step 100: train loss 2.4885, val loss 2.5703 [64.36887764930725 sec]
step 200: train loss 2.2988, val loss 2.4158 [106.38216829299927 sec]
step 300: train loss 1.9234, val loss 2.1051 [148.39987635612488 sec]
step 400: train loss 1.6726, val loss 1.9369 [190.4645607471466 sec]
step 500: train loss 1.4804, val loss 1.8226 [232.5585708618164 sec]
step 600: train loss 1.3574, val loss 1.7698 [274.71861577033997 sec]
step 700: train loss 1.2362, val loss 1.7637 [316.87713837623596 sec]
step 800: train loss 1.1209, val loss 1.7537 [359.11708402633667 sec]
step 900: train loss 1.0224, val loss 1.7914 [401.25771403312683 sec]
step 1000: train loss 0.9262, val loss 1.8267 [443.39885354042053 sec]
step 1100: train loss 0.8080, val loss 1.8666 [485.5695550441742 sec]
step 1200: train loss 0.6959, val loss 1.9567 [527.7473139762878 sec]
step 1300: train loss 0.5941, val loss 2.0346 [570.013674736023 sec]
step 1400: train loss 0.4954, val loss 2.1440 [612.2921769618988 sec]
step 1500: train loss 0.4119, val loss 2.2472 [654.5882422924042 sec]
step 1600: train loss 0.3382, val loss 2.3799 [696.8605713844299 sec]
step 1700: train loss 0.2728, val loss 2.5319 [739.1197738647461 sec]
step 1800: train loss 0.2258, val loss 2.6456 [781.3975675106049 sec]
step 1900: train loss 0.1955, val loss 2.7995 [823.6702148914337 sec]
step 2000: train loss 0.1747, val loss 2.8772 [865.969033241272 sec]
step 2100: train loss 0.1586, val loss 2.9630 [908.2820360660553 sec]
step 2200: train loss 0.1454, val loss 3.0742 [950.6088478565216 sec]
step 2300: train loss 0.1382, val loss 3.0957 [992.84157538414 sec]
step 2400: train loss 0.1387, val loss 3.2040 [1035.1936917304993 sec]
step 2500: train loss 0.1276, val loss 3.2951 [1077.4649364948273 sec]
step 2600: train loss 0.1239, val loss 3.3372 [1119.7348761558533 sec]
step 2700: train loss 0.1207, val loss 3.3599 [1162.0847578048706 sec]
step 2800: train loss 0.1183, val loss 3.4900 [1204.4412972927094 sec]
step 2900: train loss 0.1155, val loss 3.5101 [1246.710155248642 sec]
step 3000: train loss 0.1150, val loss 3.5780 [1289.0358562469482 sec]
step 3100: train loss 0.1126, val loss 3.6266 [1331.394352197647 sec]
step 3200: train loss 0.1102, val loss 3.5881 [1373.7131929397583 sec]
step 3300: train loss 0.1093, val loss 3.7009 [1415.989060163498 sec]
step 3400: train loss 0.1079, val loss 3.6709 [1458.4450194835663 sec]
step 3500: train loss 0.1072, val loss 3.7951 [1500.8080670833588 sec]
step 3600: train loss 0.1059, val loss 3.7375 [1543.154512643814 sec]
step 3700: train loss 0.1050, val loss 3.8055 [1585.5110993385315 sec]
step 3800: train loss 0.1041, val loss 3.8698 [1627.8941719532013 sec]
step 3900: train loss 0.1046, val loss 3.8776 [1670.2146670818329 sec]
step 4000: train loss 0.1024, val loss 3.9546 [1712.4326584339142 sec]
step 4100: train loss 0.1023, val loss 3.9106 [1754.7298638820648 sec]
step 4200: train loss 0.1006, val loss 3.9735 [1797.1064519882202 sec]
step 4300: train loss 0.1008, val loss 3.9476 [1839.4031820297241 sec]
step 4400: train loss 0.1006, val loss 3.9517 [1881.7511093616486 sec]
step 4500: train loss 0.0998, val loss 4.0080 [1924.0747375488281 sec]
step 4600: train loss 0.1002, val loss 3.9656 [1966.3829622268677 sec]
step 4700: train loss 0.0992, val loss 4.0573 [2008.692049741745 sec]
step 4800: train loss 0.0984, val loss 4.0257 [2051.024358034134 sec]
step 4900: train loss 0.0976, val loss 4.0642 [2093.3726592063904 sec]
0.1399877369403839
Total Training Time: 2113.007032394409 seconds

consider that
enough. For Yah Elyon would certainly avenge us, if you
are not there." Namal turned a
BEGINNING (1682135744.3927): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(4)
torch.Size([4096, 86])
tensor(4.6399, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6468, val loss 4.6458 [11.208938837051392 sec]
step 100: train loss 2.3955, val loss 2.4848 [31.644532680511475 sec]
step 200: train loss 1.9829, val loss 2.1515 [52.07305431365967 sec]
step 300: train loss 1.7143, val loss 1.9562 [72.66145586967468 sec]
step 400: train loss 1.5598, val loss 1.8552 [93.1022744178772 sec]
step 500: train loss 1.4470, val loss 1.8023 [113.51037502288818 sec]
step 600: train loss 1.3522, val loss 1.7786 [134.02527284622192 sec]
step 700: train loss 1.2818, val loss 1.7789 [154.49550008773804 sec]
step 800: train loss 1.2000, val loss 1.7513 [174.91970586776733 sec]
step 900: train loss 1.1377, val loss 1.7714 [195.32587838172913 sec]
step 1000: train loss 1.0643, val loss 1.7722 [215.9532287120819 sec]
step 1100: train loss 0.9898, val loss 1.8081 [236.52543950080872 sec]
step 1200: train loss 0.9377, val loss 1.8488 [256.99159836769104 sec]
step 1300: train loss 0.8644, val loss 1.8791 [277.5963773727417 sec]
step 1400: train loss 0.8037, val loss 1.9250 [298.0463442802429 sec]
step 1500: train loss 0.7358, val loss 2.0032 [318.45538306236267 sec]
step 1600: train loss 0.6677, val loss 2.0440 [339.031863451004 sec]
step 1700: train loss 0.6160, val loss 2.0859 [359.45758986473083 sec]
step 1800: train loss 0.5607, val loss 2.1690 [379.8935475349426 sec]
step 1900: train loss 0.5112, val loss 2.2200 [400.46857261657715 sec]
step 2000: train loss 0.4665, val loss 2.2823 [420.88363814353943 sec]
step 2100: train loss 0.4200, val loss 2.3640 [441.3025641441345 sec]
step 2200: train loss 0.3852, val loss 2.4224 [461.7641532421112 sec]
step 2300: train loss 0.3543, val loss 2.4650 [482.25490736961365 sec]
step 2400: train loss 0.3254, val loss 2.5599 [502.6686329841614 sec]
step 2500: train loss 0.3086, val loss 2.5933 [523.1473259925842 sec]
step 2600: train loss 0.2884, val loss 2.6664 [543.6993379592896 sec]
step 2700: train loss 0.2694, val loss 2.7693 [564.1525256633759 sec]
step 2800: train loss 0.2608, val loss 2.8565 [584.5444798469543 sec]
step 2900: train loss 0.2456, val loss 2.8711 [605.0577261447906 sec]
step 3000: train loss 0.2377, val loss 2.9125 [625.437840461731 sec]
step 3100: train loss 0.2298, val loss 2.9933 [645.8656198978424 sec]
step 3200: train loss 0.2240, val loss 3.0394 [666.3882064819336 sec]
step 3300: train loss 0.2217, val loss 3.0884 [686.759742975235 sec]
step 3400: train loss 0.2149, val loss 3.1443 [707.1549165248871 sec]
step 3500: train loss 0.2088, val loss 3.1535 [727.6606531143188 sec]
step 3600: train loss 0.2084, val loss 3.1602 [748.0595002174377 sec]
step 3700: train loss 0.2056, val loss 3.2417 [768.5149643421173 sec]
step 3800: train loss 0.2000, val loss 3.2870 [788.9011950492859 sec]
step 3900: train loss 0.2008, val loss 3.2852 [809.406919002533 sec]
step 4000: train loss 0.1986, val loss 3.3041 [829.8313024044037 sec]
step 4100: train loss 0.1960, val loss 3.3422 [850.256507396698 sec]
step 4200: train loss 0.1924, val loss 3.4158 [870.809091091156 sec]
step 4300: train loss 0.1910, val loss 3.4464 [891.256395816803 sec]
step 4400: train loss 0.1901, val loss 3.4283 [911.6363081932068 sec]
step 4500: train loss 0.1888, val loss 3.4845 [932.1691398620605 sec]
step 4600: train loss 0.1880, val loss 3.5327 [952.6648056507111 sec]
step 4700: train loss 0.1865, val loss 3.5804 [973.2333781719208 sec]
step 4800: train loss 0.1871, val loss 3.4917 [993.7433469295502 sec]
step 4900: train loss 0.1840, val loss 3.5730 [1014.1467254161835 sec]
0.2708388864994049
Total Training Time: 1023.4511625766754 seconds

"Don't follow m the maeuws! Tatter was
looking at him, a nowing his to he captort and wait if four t
BEGINNING (1682136770.3451223): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(64) Batch Size(64) Layers(6)
torch.Size([4096, 86])
tensor(4.5393, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5253, val loss 4.5262 [16.309741497039795 sec]
step 100: train loss 2.4230, val loss 2.5226 [46.434617042541504 sec]
step 200: train loss 2.0098, val loss 2.1733 [76.42594838142395 sec]
step 300: train loss 1.7279, val loss 1.9672 [106.61240363121033 sec]
step 400: train loss 1.5460, val loss 1.8596 [136.69316124916077 sec]
step 500: train loss 1.4176, val loss 1.7991 [166.7901475429535 sec]
step 600: train loss 1.3190, val loss 1.7660 [196.8379786014557 sec]
step 700: train loss 1.2285, val loss 1.7628 [226.8878209590912 sec]
step 800: train loss 1.1376, val loss 1.7682 [256.93941283226013 sec]
step 900: train loss 1.0535, val loss 1.7734 [286.96445059776306 sec]
step 1000: train loss 0.9778, val loss 1.8344 [317.0990915298462 sec]
step 1100: train loss 0.8970, val loss 1.8854 [347.1322979927063 sec]
step 1200: train loss 0.8177, val loss 1.9041 [377.2852032184601 sec]
step 1300: train loss 0.7319, val loss 1.9468 [407.29336190223694 sec]
step 1400: train loss 0.6621, val loss 2.0401 [437.4243104457855 sec]
step 1500: train loss 0.5921, val loss 2.1491 [467.43627524375916 sec]
step 1600: train loss 0.5272, val loss 2.1818 [497.54804587364197 sec]
step 1700: train loss 0.4643, val loss 2.2612 [527.5477664470673 sec]
step 1800: train loss 0.4172, val loss 2.3702 [557.6404068470001 sec]
step 1900: train loss 0.3790, val loss 2.4368 [587.683696269989 sec]
step 2000: train loss 0.3469, val loss 2.5041 [617.6758072376251 sec]
step 2100: train loss 0.3113, val loss 2.6411 [647.7414073944092 sec]
step 2200: train loss 0.2875, val loss 2.6993 [677.7604794502258 sec]
step 2300: train loss 0.2729, val loss 2.7171 [707.8963069915771 sec]
step 2400: train loss 0.2576, val loss 2.8201 [737.9041512012482 sec]
step 2500: train loss 0.2420, val loss 2.8855 [768.0482227802277 sec]
step 2600: train loss 0.2343, val loss 2.8888 [798.0438516139984 sec]
step 2700: train loss 0.2254, val loss 2.9648 [828.1630976200104 sec]
step 2800: train loss 0.2195, val loss 3.1062 [858.1370933055878 sec]
step 2900: train loss 0.2133, val loss 3.0795 [888.3129861354828 sec]
step 3000: train loss 0.2088, val loss 3.1206 [918.2843511104584 sec]
step 3100: train loss 0.2054, val loss 3.2460 [948.2805211544037 sec]
step 3200: train loss 0.2042, val loss 3.2109 [978.4948728084564 sec]
step 3300: train loss 0.2002, val loss 3.3236 [1008.4954268932343 sec]
step 3400: train loss 0.1994, val loss 3.2730 [1039.7740287780762 sec]
step 3500: train loss 0.1961, val loss 3.3129 [1069.8610577583313 sec]
step 3600: train loss 0.1948, val loss 3.3806 [1100.017323732376 sec]
step 3700: train loss 0.1913, val loss 3.3862 [1130.0109477043152 sec]
step 3800: train loss 0.1904, val loss 3.4556 [1160.125628232956 sec]
step 3900: train loss 0.1894, val loss 3.4720 [1190.1401991844177 sec]
step 4000: train loss 0.1872, val loss 3.5313 [1220.2305262088776 sec]
step 4100: train loss 0.1861, val loss 3.5480 [1250.1921384334564 sec]
step 4200: train loss 0.1853, val loss 3.5282 [1280.2025017738342 sec]
step 4300: train loss 0.1832, val loss 3.6040 [1310.225711107254 sec]
step 4400: train loss 0.1842, val loss 3.6166 [1340.2037060260773 sec]
step 4500: train loss 0.1817, val loss 3.6078 [1370.3037576675415 sec]
step 4600: train loss 0.1834, val loss 3.6257 [1400.382637500763 sec]
step 4700: train loss 0.1822, val loss 3.6127 [1430.4926812648773 sec]
step 4800: train loss 0.1805, val loss 3.6880 [1460.4880051612854 sec]
step 4900: train loss 0.1794, val loss 3.7464 [1490.582756280899 sec]
0.24262864887714386
Total Training Time: 1504.43457365036 seconds

words slung to a eat, if he command the started was
only two cords after the bridge toward him. Anay
BEGINNING (1682138278.6125267): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5757, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5789, val loss 4.5768 [19.888311624526978 sec]
step 100: train loss 2.4495, val loss 2.5424 [56.5261344909668 sec]
step 200: train loss 2.1898, val loss 2.3272 [100.27551889419556 sec]
step 300: train loss 1.8265, val loss 2.0315 [140.2682921886444 sec]
step 400: train loss 1.6187, val loss 1.8953 [177.44568705558777 sec]
step 500: train loss 1.4607, val loss 1.8050 [214.67369651794434 sec]
step 600: train loss 1.3386, val loss 1.7655 [251.96715426445007 sec]
step 700: train loss 1.2405, val loss 1.7527 [289.2150580883026 sec]
step 800: train loss 1.1393, val loss 1.7518 [326.4592037200928 sec]
step 900: train loss 1.0487, val loss 1.7864 [363.7349350452423 sec]
step 1000: train loss 0.9523, val loss 1.8071 [400.9454050064087 sec]
step 1100: train loss 0.8612, val loss 1.8543 [438.24066710472107 sec]
step 1200: train loss 0.7620, val loss 1.9035 [475.4661343097687 sec]
step 1300: train loss 0.6558, val loss 2.0182 [512.8220436573029 sec]
step 1400: train loss 0.5633, val loss 2.0709 [550.1537051200867 sec]
step 1500: train loss 0.4758, val loss 2.1614 [587.4200739860535 sec]
step 1600: train loss 0.3926, val loss 2.2933 [624.695413351059 sec]
step 1700: train loss 0.3266, val loss 2.3922 [662.0323867797852 sec]
step 1800: train loss 0.2708, val loss 2.4825 [699.3540434837341 sec]
step 1900: train loss 0.2295, val loss 2.6011 [736.5853431224823 sec]
step 2000: train loss 0.1985, val loss 2.7390 [773.8460545539856 sec]
step 2100: train loss 0.1764, val loss 2.8556 [811.2360169887543 sec]
step 2200: train loss 0.1566, val loss 2.9565 [848.5618705749512 sec]
step 2300: train loss 0.1441, val loss 3.0516 [885.9014761447906 sec]
step 2400: train loss 0.1375, val loss 3.1446 [923.1657412052155 sec]
step 2500: train loss 0.1290, val loss 3.2066 [962.5730834007263 sec]
step 2600: train loss 0.1249, val loss 3.3062 [1007.2213413715363 sec]
step 2700: train loss 0.1213, val loss 3.3700 [1053.32320022583 sec]
step 2800: train loss 0.1173, val loss 3.4246 [1107.6064212322235 sec]
step 2900: train loss 0.1166, val loss 3.4481 [1149.9929008483887 sec]
step 3000: train loss 0.1134, val loss 3.5022 [1191.8455023765564 sec]
step 3100: train loss 0.1116, val loss 3.5174 [1233.290432691574 sec]
step 3200: train loss 0.1094, val loss 3.6423 [1274.5619373321533 sec]
step 3300: train loss 0.1084, val loss 3.6474 [1315.4249534606934 sec]
step 3400: train loss 0.1064, val loss 3.7393 [1356.2477130889893 sec]
step 3500: train loss 0.1057, val loss 3.7408 [1397.033795118332 sec]
step 3600: train loss 0.1043, val loss 3.7860 [1437.7155611515045 sec]
step 3700: train loss 0.1025, val loss 3.8091 [1478.2872428894043 sec]
step 3800: train loss 0.1018, val loss 3.8911 [1518.8597617149353 sec]
step 3900: train loss 0.1012, val loss 3.8751 [1559.1785809993744 sec]
step 4000: train loss 0.1005, val loss 3.8637 [1599.492659330368 sec]
step 4100: train loss 0.0997, val loss 3.8981 [1639.81192111969 sec]
step 4200: train loss 0.0995, val loss 3.9718 [1680.2379775047302 sec]
step 4300: train loss 0.0976, val loss 4.0118 [1720.4833781719208 sec]
step 4400: train loss 0.0992, val loss 4.0622 [1760.6790771484375 sec]
step 4500: train loss 0.0976, val loss 4.0145 [1800.9091057777405 sec]
step 4600: train loss 0.0970, val loss 4.0607 [1841.0244784355164 sec]
step 4700: train loss 0.0974, val loss 4.0699 [1881.135085105896 sec]
step 4800: train loss 0.0964, val loss 4.1152 [1921.2737321853638 sec]
step 4900: train loss 0.0951, val loss 4.0780 [1961.485030889511 sec]
0.1502833515405655
Total Training Time: 1980.2301633358002 seconds

a smell of thonishmen. After 62
CHAPTER V – A DESPERATE GAMBIT
cellent chief, who is great among the
