BEGINNING (1682143035.3953848): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6802, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6779, val loss 4.6736 [22.100358486175537 sec]
step 100: train loss 2.5540, val loss 2.6344 [64.0577929019928 sec]
step 200: train loss 2.3989, val loss 2.5130 [106.15827131271362 sec]
step 300: train loss 1.9687, val loss 2.1571 [148.32127380371094 sec]
step 400: train loss 1.6221, val loss 1.8990 [190.61550951004028 sec]
step 500: train loss 1.4166, val loss 1.8133 [232.9606556892395 sec]
step 600: train loss 1.2538, val loss 1.7742 [275.06566405296326 sec]
step 700: train loss 1.0948, val loss 1.7672 [317.36501145362854 sec]
step 800: train loss 0.9534, val loss 1.8229 [359.62829208374023 sec]
step 900: train loss 0.8145, val loss 1.8951 [401.8573508262634 sec]
step 1000: train loss 0.6677, val loss 1.9854 [444.24913787841797 sec]
step 1100: train loss 0.5369, val loss 2.1182 [486.6293947696686 sec]
step 1200: train loss 0.4181, val loss 2.2461 [529.0383195877075 sec]
step 1300: train loss 0.3210, val loss 2.4317 [571.5144510269165 sec]
step 1400: train loss 0.2468, val loss 2.5727 [613.8963651657104 sec]
step 1500: train loss 0.2014, val loss 2.7371 [656.3544437885284 sec]
step 1600: train loss 0.1770, val loss 2.8212 [698.824734210968 sec]
step 1700: train loss 0.1539, val loss 2.9603 [741.2298002243042 sec]
step 1800: train loss 0.1390, val loss 3.0932 [783.7649688720703 sec]
step 1900: train loss 0.1317, val loss 3.1500 [859.71049451828 sec]
step 2000: train loss 0.1247, val loss 3.2552 [919.4278972148895 sec]
step 2100: train loss 0.1207, val loss 3.3529 [961.9444417953491 sec]
step 2200: train loss 0.1171, val loss 3.3738 [1004.4745609760284 sec]
step 2300: train loss 0.1146, val loss 3.4592 [1047.0864765644073 sec]
step 2400: train loss 0.1128, val loss 3.5254 [1089.7386357784271 sec]
step 2500: train loss 0.1119, val loss 3.4975 [1132.3450245857239 sec]
step 2600: train loss 0.1077, val loss 3.5889 [1175.1327884197235 sec]
step 2700: train loss 0.1073, val loss 3.6437 [1217.7918021678925 sec]
step 2800: train loss 0.1055, val loss 3.6468 [1260.5122916698456 sec]
step 2900: train loss 0.1047, val loss 3.6949 [1303.1321997642517 sec]
step 3000: train loss 0.1028, val loss 3.7623 [1345.7650692462921 sec]
step 3100: train loss 0.1018, val loss 3.7773 [1388.461933374405 sec]
step 3200: train loss 0.1030, val loss 3.7961 [1431.0641160011292 sec]
step 3300: train loss 0.1014, val loss 3.8042 [1473.6959853172302 sec]
step 3400: train loss 0.1007, val loss 3.8028 [1516.4105656147003 sec]
step 3500: train loss 0.0998, val loss 3.9078 [1559.141979932785 sec]
step 3600: train loss 0.0985, val loss 3.9659 [1601.830528497696 sec]
step 3700: train loss 0.0984, val loss 3.8804 [1644.5530619621277 sec]
step 3800: train loss 0.0977, val loss 3.9986 [1687.4268870353699 sec]
step 3900: train loss 0.0971, val loss 4.0371 [1730.1741547584534 sec]
step 4000: train loss 0.0970, val loss 3.9218 [1772.8623926639557 sec]
step 4100: train loss 0.0962, val loss 3.9292 [1815.9289951324463 sec]
step 4200: train loss 0.0957, val loss 4.0394 [1858.5950891971588 sec]
step 4300: train loss 0.0960, val loss 4.0454 [1901.02983212471 sec]
step 4400: train loss 0.0960, val loss 3.9980 [1943.4875657558441 sec]
step 4500: train loss 0.0948, val loss 4.0463 [1986.0031390190125 sec]
step 4600: train loss 0.0941, val loss 4.0414 [2028.522928237915 sec]
step 4700: train loss 0.0943, val loss 4.0655 [2070.9977197647095 sec]
step 4800: train loss 0.0936, val loss 4.1184 [2113.375056743622 sec]
step 4900: train loss 0.0931, val loss 4.2056 [2155.733840227127 sec]
step 5000: train loss 0.0925, val loss 4.2560 [2198.0774619579315 sec]
step 5100: train loss 0.0931, val loss 4.2451 [2240.424332380295 sec]
step 5200: train loss 0.0932, val loss 4.1911 [2282.802829504013 sec]
step 5300: train loss 0.0921, val loss 4.2410 [2325.1252720355988 sec]
step 5400: train loss 0.0914, val loss 4.1966 [2367.480630874634 sec]
step 5500: train loss 0.0922, val loss 4.2616 [2409.8278794288635 sec]
step 5600: train loss 0.0913, val loss 4.2236 [2452.1290938854218 sec]
step 5700: train loss 0.0917, val loss 4.2248 [2494.4758660793304 sec]
step 5800: train loss 0.0909, val loss 4.3009 [2536.806153535843 sec]
step 5900: train loss 0.0907, val loss 4.2535 [2579.181131839752 sec]
step 6000: train loss 0.0903, val loss 4.2599 [2621.5384323596954 sec]
step 6100: train loss 0.0897, val loss 4.3464 [2663.9005064964294 sec]
step 6200: train loss 0.0899, val loss 4.2995 [2706.204041957855 sec]
step 6300: train loss 0.0892, val loss 4.3537 [2748.5475311279297 sec]
step 6400: train loss 0.0896, val loss 4.3833 [2790.889876127243 sec]
step 6500: train loss 0.0894, val loss 4.4160 [2833.2192001342773 sec]
step 6600: train loss 0.0892, val loss 4.3573 [2875.56533908844 sec]
step 6700: train loss 0.0890, val loss 4.4665 [2917.867767572403 sec]
step 6800: train loss 0.0898, val loss 4.4123 [2960.191708087921 sec]
step 6900: train loss 0.0888, val loss 4.5099 [3002.518931388855 sec]
step 7000: train loss 0.0884, val loss 4.4742 [3044.8467268943787 sec]
step 7100: train loss 0.0889, val loss 4.5465 [3087.1671710014343 sec]
step 7200: train loss 0.0882, val loss 4.5474 [3129.473000764847 sec]
step 7300: train loss 0.0877, val loss 4.5008 [3171.7729222774506 sec]
step 7400: train loss 0.0887, val loss 4.5690 [3214.1191070079803 sec]
step 7500: train loss 0.0879, val loss 4.4603 [3256.4269330501556 sec]
step 7600: train loss 0.0882, val loss 4.5852 [3298.707566022873 sec]
step 7700: train loss 0.0872, val loss 4.5090 [3341.015959262848 sec]
step 7800: train loss 0.0870, val loss 4.5129 [3383.2806148529053 sec]
step 7900: train loss 0.0870, val loss 4.6001 [3425.5710077285767 sec]
step 8000: train loss 0.0872, val loss 4.5270 [3467.844629764557 sec]
step 8100: train loss 0.0867, val loss 4.6414 [3510.1707239151 sec]
step 8200: train loss 0.0871, val loss 4.5491 [3552.5008800029755 sec]
step 8300: train loss 0.0861, val loss 4.5278 [3594.831426382065 sec]
step 8400: train loss 0.0863, val loss 4.6164 [3637.097254514694 sec]
step 8500: train loss 0.0861, val loss 4.5129 [3679.4131031036377 sec]
step 8600: train loss 0.0858, val loss 4.6181 [3721.6753866672516 sec]
step 8700: train loss 0.0861, val loss 4.6551 [3763.989064693451 sec]
step 8800: train loss 0.0855, val loss 4.6569 [3806.340360403061 sec]
step 8900: train loss 0.0852, val loss 4.7135 [3848.6222105026245 sec]
step 9000: train loss 0.0854, val loss 4.6215 [3890.928029537201 sec]
step 9100: train loss 0.0855, val loss 4.7074 [3933.2282910346985 sec]
step 9200: train loss 0.0854, val loss 4.7303 [3975.500363588333 sec]
step 9300: train loss 0.0852, val loss 4.7255 [4017.7582383155823 sec]
step 9400: train loss 0.0845, val loss 4.6863 [4060.037406206131 sec]
step 9500: train loss 0.0847, val loss 4.7232 [4102.34582400322 sec]
step 9600: train loss 0.0847, val loss 4.6966 [4144.612881422043 sec]
step 9700: train loss 0.0850, val loss 4.7279 [4186.8964149951935 sec]
step 9800: train loss 0.0847, val loss 4.7328 [4229.1936647892 sec]
step 9900: train loss 0.0847, val loss 4.7664 [4271.483476161957 sec]
0.10206529498100281
Total Training Time: 4291.077628612518 seconds

taskmaster winced. Whether it was from the implied
yoursuing through the camp toward him. He stood
a
BEGINNING (1682147332.198735): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.5977, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5933, val loss 4.5880 [19.916279077529907 sec]
step 100: train loss 2.4490, val loss 2.5431 [56.646281480789185 sec]
step 200: train loss 2.0115, val loss 2.1911 [93.38059949874878 sec]
step 300: train loss 1.6352, val loss 1.9087 [130.06328201293945 sec]
step 400: train loss 1.4047, val loss 1.7990 [166.80892610549927 sec]
step 500: train loss 1.2234, val loss 1.7621 [203.62223482131958 sec]
step 600: train loss 1.0729, val loss 1.7782 [240.68841576576233 sec]
step 700: train loss 0.9071, val loss 1.8152 [277.75960397720337 sec]
step 800: train loss 0.7602, val loss 1.9552 [314.9259991645813 sec]
step 900: train loss 0.6117, val loss 2.0271 [352.17439699172974 sec]
step 1000: train loss 0.4687, val loss 2.1728 [389.37845969200134 sec]
step 1100: train loss 0.3529, val loss 2.3960 [426.6133646965027 sec]
step 1200: train loss 0.2704, val loss 2.5462 [463.84221625328064 sec]
step 1300: train loss 0.2113, val loss 2.7037 [501.0596477985382 sec]
step 1400: train loss 0.1765, val loss 2.8803 [538.3001911640167 sec]
step 1500: train loss 0.1554, val loss 2.9857 [575.5426619052887 sec]
step 1600: train loss 0.1421, val loss 3.0608 [612.4850099086761 sec]
step 1700: train loss 0.1321, val loss 3.2424 [649.3367865085602 sec]
step 1800: train loss 0.1229, val loss 3.3471 [686.1126852035522 sec]
step 1900: train loss 0.1205, val loss 3.3620 [722.8785648345947 sec]
step 2000: train loss 0.1154, val loss 3.4096 [759.6756544113159 sec]
step 2100: train loss 0.1137, val loss 3.5026 [796.3931927680969 sec]
step 2200: train loss 0.1108, val loss 3.5787 [833.1253311634064 sec]
step 2300: train loss 0.1080, val loss 3.5964 [869.8788685798645 sec]
step 2400: train loss 0.1087, val loss 3.6188 [906.6171967983246 sec]
step 2500: train loss 0.1051, val loss 3.6401 [943.3372232913971 sec]
step 2600: train loss 0.1036, val loss 3.7453 [980.0369758605957 sec]
step 2700: train loss 0.1030, val loss 3.7696 [1016.7405083179474 sec]
step 2800: train loss 0.1022, val loss 3.7862 [1053.4553587436676 sec]
step 2900: train loss 0.1001, val loss 3.8743 [1090.1996440887451 sec]
step 3000: train loss 0.0994, val loss 3.8462 [1126.9444978237152 sec]
step 3100: train loss 0.0992, val loss 3.8823 [1163.639259338379 sec]
step 3200: train loss 0.0982, val loss 3.9485 [1200.2976841926575 sec]
step 3300: train loss 0.0967, val loss 3.9835 [1236.9718225002289 sec]
step 3400: train loss 0.0974, val loss 3.9789 [1273.728667974472 sec]
step 3500: train loss 0.0960, val loss 4.0500 [1310.434487581253 sec]
step 3600: train loss 0.0963, val loss 4.0158 [1347.1304006576538 sec]
step 3700: train loss 0.0950, val loss 4.0759 [1383.829217672348 sec]
step 3800: train loss 0.0943, val loss 4.0513 [1420.5173542499542 sec]
step 3900: train loss 0.0943, val loss 4.1286 [1457.194768190384 sec]
step 4000: train loss 0.0936, val loss 4.1410 [1493.9196462631226 sec]
step 4100: train loss 0.0942, val loss 4.1679 [1530.6041493415833 sec]
step 4200: train loss 0.0929, val loss 4.1001 [1567.2704243659973 sec]
step 4300: train loss 0.0927, val loss 4.1887 [1603.94770860672 sec]
step 4400: train loss 0.0928, val loss 4.2033 [1640.623479604721 sec]
step 4500: train loss 0.0918, val loss 4.1666 [1677.322252035141 sec]
step 4600: train loss 0.0916, val loss 4.2302 [1713.9950222969055 sec]
step 4700: train loss 0.0916, val loss 4.2296 [1750.659681558609 sec]
step 4800: train loss 0.0908, val loss 4.3184 [1787.3413531780243 sec]
step 4900: train loss 0.0907, val loss 4.2956 [1824.0263228416443 sec]
step 5000: train loss 0.0910, val loss 4.3278 [1860.686366558075 sec]
step 5100: train loss 0.0906, val loss 4.3180 [1897.5057196617126 sec]
step 5200: train loss 0.0901, val loss 4.2455 [1934.6601178646088 sec]
step 5300: train loss 0.0900, val loss 4.3410 [1971.7425956726074 sec]
step 5400: train loss 0.0901, val loss 4.3815 [2008.4678423404694 sec]
step 5500: train loss 0.0897, val loss 4.2991 [2045.1424136161804 sec]
step 5600: train loss 0.0890, val loss 4.3625 [2081.8355844020844 sec]
step 5700: train loss 0.0882, val loss 4.3779 [2118.4721982479095 sec]
step 5800: train loss 0.0886, val loss 4.4140 [2155.1684584617615 sec]
step 5900: train loss 0.0886, val loss 4.4669 [2191.844715118408 sec]
step 6000: train loss 0.0879, val loss 4.4240 [2228.503908634186 sec]
step 6100: train loss 0.0875, val loss 4.4847 [2265.1577565670013 sec]
step 6200: train loss 0.0878, val loss 4.3882 [2301.808914899826 sec]
step 6300: train loss 0.0874, val loss 4.5660 [2338.4934129714966 sec]
step 6400: train loss 0.0871, val loss 4.4899 [2375.173668384552 sec]
step 6500: train loss 0.0868, val loss 4.5635 [2411.869542360306 sec]
step 6600: train loss 0.0868, val loss 4.5048 [2448.5481371879578 sec]
step 6700: train loss 0.0865, val loss 4.5249 [2485.2133548259735 sec]
step 6800: train loss 0.0863, val loss 4.5685 [2521.8850071430206 sec]
step 6900: train loss 0.0864, val loss 4.6011 [2558.5162122249603 sec]
step 7000: train loss 0.0862, val loss 4.5624 [2595.171063899994 sec]
step 7100: train loss 0.0860, val loss 4.6216 [2631.83776307106 sec]
step 7200: train loss 0.0860, val loss 4.6329 [2668.486783027649 sec]
step 7300: train loss 0.0858, val loss 4.5620 [2705.1082484722137 sec]
step 7400: train loss 0.0864, val loss 4.6010 [2741.7718210220337 sec]
step 7500: train loss 0.0852, val loss 4.6658 [2778.4395430088043 sec]
step 7600: train loss 0.0858, val loss 4.5844 [2815.1240627765656 sec]
step 7700: train loss 0.0847, val loss 4.7346 [2851.738142490387 sec]
step 7800: train loss 0.0852, val loss 4.6771 [2888.4045197963715 sec]
step 7900: train loss 0.0852, val loss 4.6631 [2925.05331492424 sec]
step 8000: train loss 0.0848, val loss 4.6873 [2961.6767342090607 sec]
step 8100: train loss 0.0846, val loss 4.7311 [2998.330506324768 sec]
step 8200: train loss 0.0839, val loss 4.7338 [3034.9445128440857 sec]
step 8300: train loss 0.0846, val loss 4.7422 [3071.62895655632 sec]
step 8400: train loss 0.0845, val loss 4.7728 [3108.2571058273315 sec]
step 8500: train loss 0.0845, val loss 4.7564 [3144.9395015239716 sec]
step 8600: train loss 0.0836, val loss 4.7770 [3181.5751388072968 sec]
step 8700: train loss 0.0838, val loss 4.7595 [3218.2048971652985 sec]
step 8800: train loss 0.0840, val loss 4.7716 [3254.860560655594 sec]
step 8900: train loss 0.0839, val loss 4.7908 [3291.4743795394897 sec]
step 9000: train loss 0.0837, val loss 4.7693 [3328.1304104328156 sec]
step 9100: train loss 0.0831, val loss 4.8459 [3364.76474404335 sec]
step 9200: train loss 0.0828, val loss 4.7949 [3401.4311220645905 sec]
step 9300: train loss 0.0836, val loss 4.8467 [3438.0537371635437 sec]
step 9400: train loss 0.0824, val loss 4.8813 [3474.6738243103027 sec]
step 9500: train loss 0.0833, val loss 4.8066 [3511.327959060669 sec]
step 9600: train loss 0.0832, val loss 4.9265 [3547.9568371772766 sec]
step 9700: train loss 0.0827, val loss 4.9207 [3584.6138033866882 sec]
step 9800: train loss 0.0831, val loss 4.8894 [3621.218910217285 sec]
step 9900: train loss 0.0826, val loss 4.8811 [3657.888897418976 sec]
0.10216736793518066
Total Training Time: 3674.6733345985413 seconds

"Perhaps my twenty men altogether could free the
cubs. The odds would be tight, and we would want to
BEGINNING (1682151009.4726508): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6309, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6310, val loss 4.6257 [22.730262517929077 sec]
step 100: train loss 2.4703, val loss 2.5568 [65.13190841674805 sec]
step 200: train loss 2.2646, val loss 2.3876 [107.46780681610107 sec]
step 300: train loss 1.8672, val loss 2.0623 [149.8352870941162 sec]
step 400: train loss 1.6077, val loss 1.8827 [192.2313027381897 sec]
step 500: train loss 1.4358, val loss 1.7997 [234.58519625663757 sec]
step 600: train loss 1.2961, val loss 1.7757 [276.9237425327301 sec]
step 700: train loss 1.1766, val loss 1.7674 [319.27127981185913 sec]
step 800: train loss 1.0580, val loss 1.7772 [361.6605966091156 sec]
step 900: train loss 0.9429, val loss 1.8280 [404.02473306655884 sec]
step 1000: train loss 0.8228, val loss 1.8907 [446.3977062702179 sec]
step 1100: train loss 0.7071, val loss 1.9565 [488.7056694030762 sec]
step 1200: train loss 0.5895, val loss 2.0439 [531.074355840683 sec]
step 1300: train loss 0.4808, val loss 2.1720 [573.4207935333252 sec]
step 1400: train loss 0.3911, val loss 2.3064 [615.7481806278229 sec]
step 1500: train loss 0.3163, val loss 2.4249 [658.1589541435242 sec]
step 1600: train loss 0.2574, val loss 2.5417 [700.51340508461 sec]
step 1700: train loss 0.2102, val loss 2.6629 [742.8881590366364 sec]
step 1800: train loss 0.1843, val loss 2.8076 [785.2765035629272 sec]
step 1900: train loss 0.1648, val loss 2.9141 [827.6326637268066 sec]
step 2000: train loss 0.1510, val loss 2.9741 [870.0202531814575 sec]
step 2100: train loss 0.1417, val loss 3.0898 [912.4068698883057 sec]
step 2200: train loss 0.1326, val loss 3.1800 [954.7833733558655 sec]
step 2300: train loss 0.1286, val loss 3.2198 [997.1242978572845 sec]
step 2400: train loss 0.1251, val loss 3.3131 [1039.449584722519 sec]
step 2500: train loss 0.1229, val loss 3.3408 [1081.7707076072693 sec]
step 2600: train loss 0.1179, val loss 3.4231 [1124.13436627388 sec]
step 2700: train loss 0.1163, val loss 3.5424 [1166.4779546260834 sec]
step 2800: train loss 0.1137, val loss 3.5803 [1208.8055939674377 sec]
step 2900: train loss 0.1125, val loss 3.5756 [1251.1699628829956 sec]
step 3000: train loss 0.1111, val loss 3.6041 [1293.472116947174 sec]
step 3100: train loss 0.1094, val loss 3.6850 [1335.7717740535736 sec]
step 3200: train loss 0.1083, val loss 3.7215 [1378.1185111999512 sec]
step 3300: train loss 0.1068, val loss 3.7450 [1420.4533836841583 sec]
step 3400: train loss 0.1062, val loss 3.7338 [1462.8041195869446 sec]
step 3500: train loss 0.1037, val loss 3.8185 [1505.1246304512024 sec]
step 3600: train loss 0.1039, val loss 3.7885 [1547.5116262435913 sec]
step 3700: train loss 0.1036, val loss 3.8587 [1589.871559381485 sec]
step 3800: train loss 0.1030, val loss 3.9179 [1632.1983089447021 sec]
step 3900: train loss 0.1030, val loss 3.9127 [1674.549348115921 sec]
step 4000: train loss 0.1017, val loss 3.9051 [1716.9061379432678 sec]
step 4100: train loss 0.1001, val loss 3.9796 [1759.2279188632965 sec]
step 4200: train loss 0.1014, val loss 3.9079 [1801.5361778736115 sec]
step 4300: train loss 0.0996, val loss 3.9218 [1843.9233767986298 sec]
step 4400: train loss 0.0987, val loss 4.0448 [1886.2527058124542 sec]
step 4500: train loss 0.0977, val loss 4.0648 [1928.5740005970001 sec]
step 4600: train loss 0.0974, val loss 4.0404 [1970.8970093727112 sec]
step 4700: train loss 0.0974, val loss 3.9839 [2013.2158463001251 sec]
step 4800: train loss 0.0969, val loss 4.0567 [2055.5744965076447 sec]
step 4900: train loss 0.0970, val loss 4.1202 [2097.90137219429 sec]
step 5000: train loss 0.0965, val loss 4.0233 [2140.253701686859 sec]
step 5100: train loss 0.0952, val loss 4.1023 [2182.543709039688 sec]
step 5200: train loss 0.0951, val loss 4.1487 [2224.848879098892 sec]
step 5300: train loss 0.0946, val loss 4.1178 [2267.1534633636475 sec]
step 5400: train loss 0.0949, val loss 4.1385 [2309.4715015888214 sec]
step 5500: train loss 0.0955, val loss 4.1506 [2351.760068178177 sec]
step 5600: train loss 0.0943, val loss 4.1754 [2394.0557186603546 sec]
step 5700: train loss 0.0947, val loss 4.1979 [2436.4133384227753 sec]
step 5800: train loss 0.0929, val loss 4.2375 [2478.6866462230682 sec]
step 5900: train loss 0.0932, val loss 4.2431 [2520.9972932338715 sec]
step 6000: train loss 0.0935, val loss 4.1785 [2563.2711000442505 sec]
step 6100: train loss 0.0925, val loss 4.2291 [2605.5606396198273 sec]
step 6200: train loss 0.0921, val loss 4.3061 [2647.8722012043 sec]
step 6300: train loss 0.0918, val loss 4.2332 [2690.1639206409454 sec]
step 6400: train loss 0.0910, val loss 4.2885 [2732.487860918045 sec]
step 6500: train loss 0.0915, val loss 4.3092 [2774.816819667816 sec]
step 6600: train loss 0.0907, val loss 4.3393 [2817.1058304309845 sec]
step 6700: train loss 0.0909, val loss 4.3373 [2859.362824201584 sec]
step 6800: train loss 0.0906, val loss 4.2699 [2901.646812438965 sec]
step 6900: train loss 0.0905, val loss 4.3044 [2943.930934906006 sec]
step 7000: train loss 0.0904, val loss 4.3487 [2986.2051906585693 sec]
step 7100: train loss 0.0900, val loss 4.3558 [3028.6034169197083 sec]
step 7200: train loss 0.0899, val loss 4.3853 [3070.8884534835815 sec]
step 7300: train loss 0.0894, val loss 4.3886 [3113.179340362549 sec]
step 7400: train loss 0.0891, val loss 4.3556 [3155.459354877472 sec]
step 7500: train loss 0.0894, val loss 4.4705 [3197.711575984955 sec]
step 7600: train loss 0.0892, val loss 4.3572 [3240.010542869568 sec]
step 7700: train loss 0.0891, val loss 4.3874 [3282.2762773036957 sec]
step 7800: train loss 0.0885, val loss 4.4619 [3324.5159080028534 sec]
step 7900: train loss 0.0887, val loss 4.4476 [3366.818583726883 sec]
step 8000: train loss 0.0885, val loss 4.4880 [3409.092418909073 sec]
step 8100: train loss 0.0878, val loss 4.4347 [3451.3555340766907 sec]
step 8200: train loss 0.0872, val loss 4.5018 [3493.650363922119 sec]
step 8300: train loss 0.0883, val loss 4.4631 [3535.919932126999 sec]
step 8400: train loss 0.0876, val loss 4.5165 [3578.173327445984 sec]
step 8500: train loss 0.0872, val loss 4.5514 [3620.4306650161743 sec]
step 8600: train loss 0.0871, val loss 4.4284 [3662.7190754413605 sec]
step 8700: train loss 0.0869, val loss 4.5016 [3704.995833158493 sec]
step 8800: train loss 0.0864, val loss 4.5242 [3747.2614731788635 sec]
step 8900: train loss 0.0867, val loss 4.5512 [3789.5185058116913 sec]
step 9000: train loss 0.0867, val loss 4.5729 [3831.7690484523773 sec]
step 9100: train loss 0.0866, val loss 4.5217 [3874.0370643138885 sec]
step 9200: train loss 0.0868, val loss 4.5178 [3916.274721622467 sec]
step 9300: train loss 0.0862, val loss 4.5667 [3958.5894372463226 sec]
step 9400: train loss 0.0858, val loss 4.6029 [4000.8346803188324 sec]
step 9500: train loss 0.0858, val loss 4.5970 [4043.0424592494965 sec]
step 9600: train loss 0.0860, val loss 4.5641 [4085.329038143158 sec]
step 9700: train loss 0.0854, val loss 4.6442 [4127.573220252991 sec]
step 9800: train loss 0.0859, val loss 4.6068 [4169.824619054794 sec]
step 9900: train loss 0.0851, val loss 4.5999 [4212.061567783356 sec]
0.108600914478302
Total Training Time: 4231.715777397156 seconds

ssir. It was hemmed with a blue trim. Namal's voice
"These mauew who his people will and stronger. G
BEGINNING (1682155244.8404584): Baseline LR(0.0003) Heads(12) Embeddings(768) Block Size(128) Batch Size(64) Layers(4)
torch.Size([8192, 86])
tensor(4.6689, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6724, val loss 4.6601 [19.845069408416748 sec]
step 100: train loss 2.4470, val loss 2.5340 [56.75481104850769 sec]
step 200: train loss 2.1469, val loss 2.2911 [93.38192749023438 sec]
step 300: train loss 1.7524, val loss 1.9766 [130.03418493270874 sec]
step 400: train loss 1.5357, val loss 1.8452 [166.69917035102844 sec]
step 500: train loss 1.3855, val loss 1.7803 [203.32956528663635 sec]
step 600: train loss 1.2502, val loss 1.7475 [239.99735283851624 sec]
step 700: train loss 1.1424, val loss 1.7475 [276.6016116142273 sec]
step 800: train loss 1.0266, val loss 1.7746 [313.2651197910309 sec]
step 900: train loss 0.9196, val loss 1.8029 [349.8987600803375 sec]
step 1000: train loss 0.8003, val loss 1.8948 [386.49731945991516 sec]
step 1100: train loss 0.6854, val loss 1.9554 [423.17414808273315 sec]
step 1200: train loss 0.5754, val loss 2.0772 [459.78200602531433 sec]
step 1300: train loss 0.4748, val loss 2.1766 [496.43025851249695 sec]
step 1400: train loss 0.3797, val loss 2.2693 [533.0071754455566 sec]
step 1500: train loss 0.3091, val loss 2.4196 [569.6218068599701 sec]
step 1600: train loss 0.2530, val loss 2.5614 [606.2602860927582 sec]
step 1700: train loss 0.2142, val loss 2.6375 [642.8849937915802 sec]
step 1800: train loss 0.1778, val loss 2.7715 [679.4953966140747 sec]
step 1900: train loss 0.1595, val loss 2.9200 [716.11314868927 sec]
step 2000: train loss 0.1452, val loss 3.0656 [752.7088077068329 sec]
step 2100: train loss 0.1363, val loss 3.0573 [789.3088190555573 sec]
step 2200: train loss 0.1298, val loss 3.1968 [825.9114844799042 sec]
step 2300: train loss 0.1245, val loss 3.2293 [862.4818954467773 sec]
step 2400: train loss 0.1200, val loss 3.2738 [899.0728158950806 sec]
step 2500: train loss 0.1170, val loss 3.3510 [935.6935646533966 sec]
step 2600: train loss 0.1144, val loss 3.4413 [972.3152184486389 sec]
step 2700: train loss 0.1114, val loss 3.5250 [1008.9005689620972 sec]
step 2800: train loss 0.1111, val loss 3.5915 [1045.4867897033691 sec]
step 2900: train loss 0.1085, val loss 3.5808 [1082.0577733516693 sec]
step 3000: train loss 0.1066, val loss 3.7158 [1118.6310369968414 sec]
step 3100: train loss 0.1060, val loss 3.6507 [1155.2290654182434 sec]
step 3200: train loss 0.1040, val loss 3.7468 [1191.8111600875854 sec]
step 3300: train loss 0.1035, val loss 3.6722 [1228.4369838237762 sec]
step 3400: train loss 0.1023, val loss 3.7405 [1265.044559955597 sec]
step 3500: train loss 0.1010, val loss 3.8351 [1301.6752104759216 sec]
step 3600: train loss 0.1010, val loss 3.9107 [1338.272188425064 sec]
step 3700: train loss 0.0992, val loss 3.8648 [1374.8987283706665 sec]
step 3800: train loss 0.0988, val loss 3.8667 [1411.4370758533478 sec]
step 3900: train loss 0.1001, val loss 3.8767 [1448.081970691681 sec]
step 4000: train loss 0.0982, val loss 3.9306 [1484.6873831748962 sec]
step 4100: train loss 0.0971, val loss 4.0131 [1521.3073313236237 sec]
step 4200: train loss 0.0963, val loss 3.9932 [1557.9476935863495 sec]
step 4300: train loss 0.0962, val loss 3.9918 [1594.5795540809631 sec]
step 4400: train loss 0.0950, val loss 4.0533 [1631.2114946842194 sec]
step 4500: train loss 0.0965, val loss 4.0682 [1667.793393611908 sec]
step 4600: train loss 0.0953, val loss 4.0816 [1704.3952822685242 sec]
step 4700: train loss 0.0940, val loss 4.1142 [1740.9944899082184 sec]
step 4800: train loss 0.0935, val loss 4.1267 [1777.5882558822632 sec]
step 4900: train loss 0.0933, val loss 4.1796 [1814.1859390735626 sec]
step 5000: train loss 0.0931, val loss 4.1732 [1850.8033957481384 sec]
step 5100: train loss 0.0924, val loss 4.1733 [1887.3950474262238 sec]
step 5200: train loss 0.0917, val loss 4.1358 [1923.9701282978058 sec]
step 5300: train loss 0.0929, val loss 4.2125 [1960.6151823997498 sec]
step 5400: train loss 0.0913, val loss 4.1715 [1997.1665523052216 sec]
step 5500: train loss 0.0909, val loss 4.2610 [2033.786545753479 sec]
step 5600: train loss 0.0912, val loss 4.3020 [2070.3565723896027 sec]
step 5700: train loss 0.0921, val loss 4.2582 [2106.936356782913 sec]
step 5800: train loss 0.0902, val loss 4.2777 [2143.5225780010223 sec]
step 5900: train loss 0.0902, val loss 4.3466 [2180.0925183296204 sec]
step 6000: train loss 0.0897, val loss 4.3119 [2216.6704947948456 sec]
step 6100: train loss 0.0900, val loss 4.3430 [2253.2392358779907 sec]
step 6200: train loss 0.0894, val loss 4.4171 [2289.855325460434 sec]
step 6300: train loss 0.0894, val loss 4.3695 [2326.4627137184143 sec]
step 6400: train loss 0.0886, val loss 4.3652 [2363.0339517593384 sec]
step 6500: train loss 0.0879, val loss 4.4702 [2399.630592107773 sec]
step 6600: train loss 0.0888, val loss 4.3937 [2436.1926577091217 sec]
step 6700: train loss 0.0882, val loss 4.3587 [2472.7653427124023 sec]
step 6800: train loss 0.0880, val loss 4.4882 [2509.314473390579 sec]
step 6900: train loss 0.0880, val loss 4.4252 [2545.874551296234 sec]
step 7000: train loss 0.0880, val loss 4.4542 [2582.4513959884644 sec]
step 7100: train loss 0.0882, val loss 4.4491 [2619.0370030403137 sec]
step 7200: train loss 0.0874, val loss 4.5138 [2655.604214668274 sec]
step 7300: train loss 0.0873, val loss 4.4263 [2692.216953754425 sec]
step 7400: train loss 0.0866, val loss 4.6020 [2728.7888593673706 sec]
step 7500: train loss 0.0868, val loss 4.5777 [2765.3785135746 sec]
step 7600: train loss 0.0860, val loss 4.6078 [2801.9260568618774 sec]
step 7700: train loss 0.0866, val loss 4.5465 [2838.467204093933 sec]
step 7800: train loss 0.0861, val loss 4.5937 [2875.0255596637726 sec]
step 7900: train loss 0.0863, val loss 4.5967 [2911.604328393936 sec]
step 8000: train loss 0.0858, val loss 4.6591 [2948.163561820984 sec]
step 8100: train loss 0.0858, val loss 4.6146 [2984.702215194702 sec]
step 8200: train loss 0.0852, val loss 4.5740 [3021.264045715332 sec]
step 8300: train loss 0.0846, val loss 4.6916 [3057.8295018672943 sec]
step 8400: train loss 0.0853, val loss 4.6893 [3094.390864133835 sec]
step 8500: train loss 0.0849, val loss 4.7209 [25568.2415869236 sec]
step 8600: train loss 0.0848, val loss 4.6801 [25604.485159397125 sec]
step 8700: train loss 0.0851, val loss 4.6895 [25640.55192065239 sec]
step 8800: train loss 0.0846, val loss 4.6771 [25676.74967265129 sec]
step 8900: train loss 0.0847, val loss 4.6719 [25713.29211783409 sec]
step 9000: train loss 0.0842, val loss 4.7114 [25749.932691812515 sec]
step 9100: train loss 0.0842, val loss 4.7856 [25786.754257678986 sec]
step 9200: train loss 0.0843, val loss 4.8092 [25823.46851181984 sec]
step 9300: train loss 0.0839, val loss 4.7849 [25860.428162813187 sec]
step 9400: train loss 0.0837, val loss 4.7919 [25897.536907672882 sec]
step 9500: train loss 0.0842, val loss 4.7983 [25933.979569911957 sec]
step 9600: train loss 0.0845, val loss 4.7498 [25970.496787309647 sec]
step 9700: train loss 0.0829, val loss 4.7885 [26006.898946762085 sec]
step 9800: train loss 0.0833, val loss 4.8398 [26043.36244916916 sec]
step 9900: train loss 0.0832, val loss 4.7406 [26079.883533477783 sec]
0.1105182021856308
Total Training Time: 26096.65041089058 seconds

carted his head and saluted with his left
hand overs in the pastingle a reached the familiar smell o
BEGINNING (1682181343.9154832): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(64) Layers(6)
torch.Size([8192, 86])
tensor(4.5896, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5906, val loss 4.5921 [15.47118091583252 sec]
step 100: train loss 2.4736, val loss 2.5620 [43.83664512634277 sec]
step 200: train loss 2.1248, val loss 2.2656 [72.35126280784607 sec]
step 300: train loss 1.7269, val loss 1.9750 [100.80600333213806 sec]
step 400: train loss 1.4784, val loss 1.8049 [129.7368357181549 sec]
step 500: train loss 1.3031, val loss 1.7513 [159.07547044754028 sec]
step 600: train loss 1.1586, val loss 1.7545 [187.8532247543335 sec]
step 700: train loss 1.0224, val loss 1.7911 [216.34675550460815 sec]
step 800: train loss 0.8771, val loss 1.8211 [244.86916637420654 sec]
step 900: train loss 0.7490, val loss 1.8816 [273.34140276908875 sec]
step 1000: train loss 0.6216, val loss 1.9807 [301.791298866272 sec]
step 1100: train loss 0.4945, val loss 2.1121 [330.2924473285675 sec]
step 1200: train loss 0.3930, val loss 2.2407 [358.77930974960327 sec]
step 1300: train loss 0.3061, val loss 2.4248 [387.27969241142273 sec]
step 1400: train loss 0.2484, val loss 2.4989 [416.2905340194702 sec]
step 1500: train loss 0.2043, val loss 2.6852 [444.6484079360962 sec]
step 1600: train loss 0.1723, val loss 2.8066 [473.4412212371826 sec]
step 1700: train loss 0.1543, val loss 2.9349 [501.9061207771301 sec]
step 1800: train loss 0.1440, val loss 2.9671 [530.3797497749329 sec]
step 1900: train loss 0.1318, val loss 3.0908 [558.9140121936798 sec]
step 2000: train loss 0.1278, val loss 3.1724 [587.4105088710785 sec]
step 2100: train loss 0.1241, val loss 3.2420 [615.8452596664429 sec]
step 2200: train loss 0.1194, val loss 3.3390 [644.3388130664825 sec]
step 2300: train loss 0.1155, val loss 3.3162 [672.7851433753967 sec]
step 2400: train loss 0.1136, val loss 3.4325 [701.3054485321045 sec]
step 2500: train loss 0.1110, val loss 3.4997 [729.7846648693085 sec]
step 2600: train loss 0.1097, val loss 3.5187 [758.2944047451019 sec]
step 2700: train loss 0.1080, val loss 3.5654 [786.794337272644 sec]
step 2800: train loss 0.1051, val loss 3.6209 [815.318953037262 sec]
step 2900: train loss 0.1045, val loss 3.6596 [843.8809721469879 sec]
step 3000: train loss 0.1048, val loss 3.6616 [872.3750913143158 sec]
step 3100: train loss 0.1043, val loss 3.6984 [900.8207597732544 sec]
step 3200: train loss 0.1020, val loss 3.7686 [929.3465619087219 sec]
step 3300: train loss 0.1010, val loss 3.7595 [957.7491612434387 sec]
step 3400: train loss 0.1007, val loss 3.8024 [986.2931859493256 sec]
step 3500: train loss 0.0997, val loss 3.8161 [1014.7535393238068 sec]
step 3600: train loss 0.0988, val loss 3.7681 [1043.2200422286987 sec]
step 3700: train loss 0.0974, val loss 3.8937 [1071.724256515503 sec]
step 3800: train loss 0.0975, val loss 3.9303 [1100.1937000751495 sec]
step 3900: train loss 0.0975, val loss 3.8832 [1128.687527179718 sec]
step 4000: train loss 0.0961, val loss 3.9386 [1157.1921677589417 sec]
step 4100: train loss 0.0959, val loss 3.9494 [1185.68404006958 sec]
step 4200: train loss 0.0963, val loss 3.9659 [1214.2049045562744 sec]
step 4300: train loss 0.0945, val loss 3.9572 [1242.7178432941437 sec]
step 4400: train loss 0.0940, val loss 3.9648 [1271.2156467437744 sec]
step 4500: train loss 0.0940, val loss 4.0411 [1299.6806108951569 sec]
step 4600: train loss 0.0941, val loss 4.0175 [1328.1685795783997 sec]
step 4700: train loss 0.0932, val loss 4.0234 [1356.612058877945 sec]
step 4800: train loss 0.0937, val loss 4.0485 [1385.2080812454224 sec]
step 4900: train loss 0.0927, val loss 4.0762 [1413.692762851715 sec]
step 5000: train loss 0.0925, val loss 4.1191 [1442.1915390491486 sec]
step 5100: train loss 0.0921, val loss 4.0841 [1470.7220187187195 sec]
step 5200: train loss 0.0917, val loss 4.1371 [1499.2765057086945 sec]
step 5300: train loss 0.0911, val loss 4.1551 [1527.8003842830658 sec]
step 5400: train loss 0.0909, val loss 4.1788 [1556.311334848404 sec]
step 5500: train loss 0.0913, val loss 4.1490 [1584.8232414722443 sec]
step 5600: train loss 0.0908, val loss 4.1293 [1613.3116145133972 sec]
step 5700: train loss 0.0909, val loss 4.1486 [1641.8083174228668 sec]
step 5800: train loss 0.0902, val loss 4.2504 [1670.2880816459656 sec]
step 5900: train loss 0.0892, val loss 4.2563 [1698.7710638046265 sec]
step 6000: train loss 0.0901, val loss 4.2909 [1727.2831497192383 sec]
step 6100: train loss 0.0890, val loss 4.3077 [1755.8822355270386 sec]
step 6200: train loss 0.0890, val loss 4.2452 [1784.429228067398 sec]
step 6300: train loss 0.0889, val loss 4.2658 [1812.9629211425781 sec]
step 6400: train loss 0.0889, val loss 4.3046 [1841.4828202724457 sec]
step 6500: train loss 0.0880, val loss 4.3814 [1869.9629051685333 sec]
step 6600: train loss 0.0884, val loss 4.3461 [1898.5058739185333 sec]
step 6700: train loss 0.0884, val loss 4.3179 [1927.0422673225403 sec]
step 6800: train loss 0.0881, val loss 4.3026 [1955.5603902339935 sec]
step 6900: train loss 0.0883, val loss 4.3016 [1984.0542783737183 sec]
step 7000: train loss 0.0874, val loss 4.3706 [2012.5522422790527 sec]
step 7100: train loss 0.0875, val loss 4.3466 [2041.063437461853 sec]
step 7200: train loss 0.0870, val loss 4.4279 [2069.6069412231445 sec]
step 7300: train loss 0.0870, val loss 4.4011 [2098.159756422043 sec]
step 7400: train loss 0.0865, val loss 4.4137 [2126.678679704666 sec]
step 7500: train loss 0.0867, val loss 4.4101 [2155.2236082553864 sec]
step 7600: train loss 0.0874, val loss 4.4818 [2183.7445056438446 sec]
step 7700: train loss 0.0859, val loss 4.4321 [2212.243595600128 sec]
step 7800: train loss 0.0863, val loss 4.4314 [2240.7476201057434 sec]
step 7900: train loss 0.0863, val loss 4.4694 [2269.268719434738 sec]
step 8000: train loss 0.0864, val loss 4.4974 [2297.7548367977142 sec]
step 8100: train loss 0.0851, val loss 4.4880 [2326.2551493644714 sec]
step 8200: train loss 0.0857, val loss 4.5015 [2354.760461807251 sec]
step 8300: train loss 0.0858, val loss 4.4850 [2383.2742116451263 sec]
step 8400: train loss 0.0855, val loss 4.5138 [2411.7615978717804 sec]
step 8500: train loss 0.0857, val loss 4.5023 [2440.3564903736115 sec]
step 8600: train loss 0.0856, val loss 4.5618 [2468.8568708896637 sec]
step 8700: train loss 0.0853, val loss 4.5473 [2497.351047515869 sec]
step 8800: train loss 0.0846, val loss 4.4863 [2525.8477959632874 sec]
step 8900: train loss 0.0848, val loss 4.5544 [2554.3721256256104 sec]
step 9000: train loss 0.0850, val loss 4.5554 [2582.8880064487457 sec]
step 9100: train loss 0.0843, val loss 4.5777 [2611.4906799793243 sec]
step 9200: train loss 0.0849, val loss 4.6530 [2640.059067249298 sec]
step 9300: train loss 0.0846, val loss 4.5598 [2668.60494017601 sec]
step 9400: train loss 0.0841, val loss 4.5926 [2697.181411743164 sec]
step 9500: train loss 0.0838, val loss 4.6445 [2725.6731083393097 sec]
step 9600: train loss 0.0842, val loss 4.6579 [2754.170238018036 sec]
step 9700: train loss 0.0838, val loss 4.6618 [2782.6775851249695 sec]
step 9800: train loss 0.0838, val loss 4.5747 [2811.195172071457 sec]
step 9900: train loss 0.0837, val loss 4.7643 [2839.707619667053 sec]
0.1101197600364685
Total Training Time: 2852.691764354706 seconds

to gat. He the scowl on his face would have caused Gratta to be
wary in any other meeting, but to fa
BEGINNING (1682184199.0580835): Baseline LR(0.00025) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.5824, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.5780, val loss 4.5841 [22.74733066558838 sec]
step 100: train loss 2.4768, val loss 2.5582 [65.17643976211548 sec]
step 200: train loss 2.2832, val loss 2.4162 [107.61521100997925 sec]
step 300: train loss 1.9039, val loss 2.0991 [149.99185466766357 sec]
step 400: train loss 1.6560, val loss 1.9257 [192.49355483055115 sec]
step 500: train loss 1.4802, val loss 1.8355 [234.95435070991516 sec]
step 600: train loss 1.3507, val loss 1.7795 [277.39625000953674 sec]
step 700: train loss 1.2405, val loss 1.7510 [319.85794162750244 sec]
step 800: train loss 1.1352, val loss 1.7398 [362.3062183856964 sec]
step 900: train loss 1.0271, val loss 1.7604 [404.6779866218567 sec]
step 1000: train loss 0.9285, val loss 1.8045 [447.0466516017914 sec]
step 1100: train loss 0.8284, val loss 1.8859 [489.42540860176086 sec]
step 1200: train loss 0.7136, val loss 1.9505 [531.7389242649078 sec]
step 1300: train loss 0.6111, val loss 2.0370 [574.1327600479126 sec]
step 1400: train loss 0.5188, val loss 2.1163 [616.5880935192108 sec]
step 1500: train loss 0.4226, val loss 2.2648 [658.9554884433746 sec]
step 1600: train loss 0.3426, val loss 2.3371 [701.3650224208832 sec]
step 1700: train loss 0.2870, val loss 2.4593 [743.7257971763611 sec]
step 1800: train loss 0.2357, val loss 2.6093 [786.1229159832001 sec]
step 1900: train loss 0.2029, val loss 2.6839 [828.5390048027039 sec]
step 2000: train loss 0.1780, val loss 2.8352 [870.8905794620514 sec]
step 2100: train loss 0.1599, val loss 2.9262 [913.3069767951965 sec]
step 2200: train loss 0.1494, val loss 3.0035 [955.6739659309387 sec]
step 2300: train loss 0.1413, val loss 3.1073 [998.0075330734253 sec]
step 2400: train loss 0.1317, val loss 3.1728 [1040.4407923221588 sec]
step 2500: train loss 0.1291, val loss 3.2381 [1082.8860704898834 sec]
step 2600: train loss 0.1241, val loss 3.3623 [1125.2810661792755 sec]
step 2700: train loss 0.1217, val loss 3.4008 [1167.654854297638 sec]
step 2800: train loss 0.1190, val loss 3.4353 [1210.0231807231903 sec]
step 2900: train loss 0.1164, val loss 3.4385 [1252.4849932193756 sec]
step 3000: train loss 0.1150, val loss 3.5029 [1294.8487639427185 sec]
step 3100: train loss 0.1110, val loss 3.5753 [1337.0079312324524 sec]
step 3200: train loss 0.1112, val loss 3.6027 [1379.243134021759 sec]
step 3300: train loss 0.1109, val loss 3.6255 [1421.4271943569183 sec]
step 3400: train loss 0.1079, val loss 3.6894 [1463.6143016815186 sec]
step 3500: train loss 0.1066, val loss 3.7466 [1505.807939529419 sec]
step 3600: train loss 0.1061, val loss 3.7102 [1547.9936609268188 sec]
step 3700: train loss 0.1056, val loss 3.7622 [1590.2293334007263 sec]
step 3800: train loss 0.1049, val loss 3.8169 [1632.4815521240234 sec]
step 3900: train loss 0.1030, val loss 3.9149 [1674.7553541660309 sec]
step 4000: train loss 0.1045, val loss 3.9033 [1717.049292087555 sec]
step 4100: train loss 0.1025, val loss 3.8689 [1759.2641685009003 sec]
step 4200: train loss 0.1016, val loss 3.9215 [1801.491253376007 sec]
step 4300: train loss 0.1002, val loss 3.9598 [1843.7689576148987 sec]
step 4400: train loss 0.1003, val loss 4.0267 [1886.0192794799805 sec]
step 4500: train loss 0.0993, val loss 3.9828 [1928.2889001369476 sec]
step 4600: train loss 0.1002, val loss 3.8980 [1970.6032800674438 sec]
step 4700: train loss 0.0988, val loss 4.0375 [2012.8905725479126 sec]
step 4800: train loss 0.0979, val loss 4.0550 [2055.1716091632843 sec]
step 4900: train loss 0.0975, val loss 4.0856 [2097.4354939460754 sec]
step 5000: train loss 0.0978, val loss 4.0733 [2139.647752761841 sec]
step 5100: train loss 0.0969, val loss 4.0358 [2181.9189517498016 sec]
step 5200: train loss 0.0966, val loss 4.0557 [2224.31046295166 sec]
step 5300: train loss 0.0961, val loss 4.1215 [2266.6497831344604 sec]
step 5400: train loss 0.0953, val loss 4.1053 [2308.945453643799 sec]
step 5500: train loss 0.0952, val loss 4.1369 [2351.2224287986755 sec]
step 5600: train loss 0.0950, val loss 4.1998 [2393.495628118515 sec]
step 5700: train loss 0.0950, val loss 4.0940 [2435.806116104126 sec]
step 5800: train loss 0.0946, val loss 4.1770 [2478.1062870025635 sec]
step 5900: train loss 0.0933, val loss 4.2262 [2520.3743999004364 sec]
step 6000: train loss 0.0936, val loss 4.2277 [2562.6983802318573 sec]
step 6100: train loss 0.0931, val loss 4.1876 [2604.979820728302 sec]
step 6200: train loss 0.0933, val loss 4.1815 [2647.2959322929382 sec]
step 6300: train loss 0.0927, val loss 4.1847 [2689.595974445343 sec]
step 6400: train loss 0.0931, val loss 4.2042 [2731.8704211711884 sec]
step 6500: train loss 0.0915, val loss 4.3355 [2774.1833968162537 sec]
step 6600: train loss 0.0917, val loss 4.3379 [2816.457696914673 sec]
step 6700: train loss 0.0913, val loss 4.3665 [2858.7929952144623 sec]
step 6800: train loss 0.0913, val loss 4.3886 [2901.1444997787476 sec]
step 6900: train loss 0.0918, val loss 4.2518 [2943.4420697689056 sec]
step 7000: train loss 0.0908, val loss 4.3426 [2985.744107246399 sec]
step 7100: train loss 0.0909, val loss 4.3946 [3028.077761411667 sec]
step 7200: train loss 0.0900, val loss 4.3914 [3070.3943033218384 sec]
step 7300: train loss 0.0904, val loss 4.3020 [3112.688019514084 sec]
step 7400: train loss 0.0892, val loss 4.3911 [3155.0248308181763 sec]
step 7500: train loss 0.0899, val loss 4.4263 [3197.3489139080048 sec]
step 7600: train loss 0.0893, val loss 4.3027 [3239.689977169037 sec]
step 7700: train loss 0.0892, val loss 4.3494 [3282.0027985572815 sec]
step 7800: train loss 0.0892, val loss 4.3794 [3324.2991540431976 sec]
step 7900: train loss 0.0887, val loss 4.4218 [3366.6114406585693 sec]
step 8000: train loss 0.0885, val loss 4.4128 [3408.8883945941925 sec]
step 8100: train loss 0.0883, val loss 4.4602 [3451.25888299942 sec]
step 8200: train loss 0.0880, val loss 4.4930 [3493.69797205925 sec]
step 8300: train loss 0.0884, val loss 4.4481 [3536.040685415268 sec]
step 8400: train loss 0.0885, val loss 4.4386 [3578.3577263355255 sec]
step 8500: train loss 0.0880, val loss 4.5468 [3620.6823382377625 sec]
step 8600: train loss 0.0877, val loss 4.4896 [3662.985424518585 sec]
step 8700: train loss 0.0873, val loss 4.5078 [3705.3053851127625 sec]
step 8800: train loss 0.0875, val loss 4.5000 [3747.636829137802 sec]
step 8900: train loss 0.0876, val loss 4.4605 [3790.008828639984 sec]
step 9000: train loss 0.0870, val loss 4.4783 [3832.4154188632965 sec]
step 9100: train loss 0.0866, val loss 4.5525 [3874.7400732040405 sec]
step 9200: train loss 0.0867, val loss 4.5269 [3917.0800433158875 sec]
step 9300: train loss 0.0867, val loss 4.5733 [3959.41410112381 sec]
step 9400: train loss 0.0866, val loss 4.5815 [4001.7379117012024 sec]
step 9500: train loss 0.0859, val loss 4.5778 [4044.0508558750153 sec]
step 9600: train loss 0.0860, val loss 4.5408 [4086.4358546733856 sec]
step 9700: train loss 0.0868, val loss 4.5503 [4128.729149580002 sec]
step 9800: train loss 0.0856, val loss 4.5154 [4171.055058956146 sec]
step 9900: train loss 0.0859, val loss 4.5799 [4213.370509386063 sec]
0.10622028261423111
Total Training Time: 4232.986152410507 seconds

over a millennium. Arphad spoke, "General Beriyah, any
human or tuon brave enough to fight certainly
BEGINNING (1682188435.7195284): Baseline LR(0.0006) Heads(12) Embeddings(768) Block Size(128) Batch Size(48) Layers(4)
torch.Size([6144, 86])
tensor(4.6424, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6387, val loss 4.6353 [15.424402475357056 sec]
step 100: train loss 2.4715, val loss 2.5681 [44.03419470787048 sec]
step 200: train loss 2.0953, val loss 2.2518 [72.65714573860168 sec]
step 300: train loss 1.7475, val loss 1.9909 [101.2827980518341 sec]
step 400: train loss 1.5155, val loss 1.8545 [129.90147161483765 sec]
step 500: train loss 1.3374, val loss 1.7830 [158.61283326148987 sec]
step 600: train loss 1.1996, val loss 1.7710 [187.2457239627838 sec]
step 700: train loss 1.0599, val loss 1.8056 [215.86369371414185 sec]
step 800: train loss 0.9273, val loss 1.8417 [244.50072741508484 sec]
step 900: train loss 0.7957, val loss 1.8815 [273.13217759132385 sec]
step 1000: train loss 0.6723, val loss 2.0111 [301.76475286483765 sec]
step 1100: train loss 0.5439, val loss 2.1109 [330.3970603942871 sec]
step 1200: train loss 0.4395, val loss 2.2511 [359.04092597961426 sec]
step 1300: train loss 0.3499, val loss 2.3936 [387.67090940475464 sec]
step 1400: train loss 0.2784, val loss 2.5408 [416.30808997154236 sec]
step 1500: train loss 0.2257, val loss 2.6563 [450.9973089694977 sec]
step 1600: train loss 0.1903, val loss 2.7972 [479.6667582988739 sec]
step 1700: train loss 0.1688, val loss 2.9548 [508.3441548347473 sec]
step 1800: train loss 0.1542, val loss 3.0386 [537.0018975734711 sec]
step 1900: train loss 0.1434, val loss 3.0890 [565.6458342075348 sec]
step 2000: train loss 0.1345, val loss 3.1859 [594.2831280231476 sec]
step 2100: train loss 0.1266, val loss 3.3028 [622.9321846961975 sec]
step 2200: train loss 0.1232, val loss 3.3538 [651.5737476348877 sec]
step 2300: train loss 0.1199, val loss 3.4425 [680.2217004299164 sec]
step 2400: train loss 0.1174, val loss 3.4344 [708.8814480304718 sec]
step 2500: train loss 0.1137, val loss 3.4920 [737.5257656574249 sec]
step 2600: train loss 0.1113, val loss 3.5884 [766.1758439540863 sec]
step 2700: train loss 0.1115, val loss 3.5864 [794.8256056308746 sec]
step 2800: train loss 0.1087, val loss 3.6533 [823.4853649139404 sec]
step 2900: train loss 0.1069, val loss 3.6619 [852.134937286377 sec]
step 3000: train loss 0.1059, val loss 3.6623 [880.7824246883392 sec]
step 3100: train loss 0.1046, val loss 3.7018 [909.4229412078857 sec]
step 3200: train loss 0.1049, val loss 3.7653 [938.0802805423737 sec]
step 3300: train loss 0.1030, val loss 3.8250 [966.7362442016602 sec]
step 3400: train loss 0.1021, val loss 3.7967 [995.3739709854126 sec]
step 3500: train loss 0.1014, val loss 3.9054 [1024.0247716903687 sec]
step 3600: train loss 0.1013, val loss 3.8830 [1052.6605303287506 sec]
step 3700: train loss 0.0998, val loss 3.9369 [1081.3137459754944 sec]
step 3800: train loss 0.0991, val loss 3.8824 [1109.9613873958588 sec]
step 3900: train loss 0.0995, val loss 3.9207 [1138.5966005325317 sec]
step 4000: train loss 0.0979, val loss 3.9666 [1167.238327741623 sec]
step 4100: train loss 0.0978, val loss 3.9525 [1195.8663516044617 sec]
step 4200: train loss 0.0971, val loss 4.0597 [1224.4881417751312 sec]
step 4300: train loss 0.0965, val loss 4.0695 [1253.1156585216522 sec]
step 4400: train loss 0.0964, val loss 4.1115 [1281.751737356186 sec]
step 4500: train loss 0.0970, val loss 4.0611 [1310.3917605876923 sec]
step 4600: train loss 0.0953, val loss 4.0872 [1339.0203123092651 sec]
step 4700: train loss 0.0952, val loss 4.0850 [1367.6768794059753 sec]
step 4800: train loss 0.0944, val loss 4.1370 [1396.3042418956757 sec]
step 4900: train loss 0.0955, val loss 4.0493 [1424.944085597992 sec]
step 5000: train loss 0.0939, val loss 4.1263 [1453.567843914032 sec]
step 5100: train loss 0.0942, val loss 4.1971 [1482.199913740158 sec]
step 5200: train loss 0.0958, val loss 4.2017 [1510.8318617343903 sec]
step 5300: train loss 0.0927, val loss 4.1995 [1539.475459098816 sec]
step 5400: train loss 0.0929, val loss 4.1648 [1568.111707687378 sec]
step 5500: train loss 0.0927, val loss 4.2382 [1596.768893957138 sec]
step 5600: train loss 0.0912, val loss 4.2170 [1625.416787147522 sec]
step 5700: train loss 0.0917, val loss 4.2256 [1654.0544114112854 sec]
step 5800: train loss 0.0914, val loss 4.3127 [1682.6927716732025 sec]
step 5900: train loss 0.0918, val loss 4.3335 [1711.3212263584137 sec]
step 6000: train loss 0.0905, val loss 4.3045 [1739.950826883316 sec]
step 6100: train loss 0.0912, val loss 4.3228 [1768.5898444652557 sec]
step 6200: train loss 0.0908, val loss 4.3424 [1797.215712070465 sec]
step 6300: train loss 0.0915, val loss 4.3784 [1825.8737807273865 sec]
step 6400: train loss 0.0907, val loss 4.4132 [1854.507094860077 sec]
step 6500: train loss 0.0911, val loss 4.4314 [1883.147619009018 sec]
step 6600: train loss 0.0907, val loss 4.3831 [1911.7713305950165 sec]
step 6700: train loss 0.0904, val loss 4.4201 [1940.4093639850616 sec]
step 6800: train loss 0.0896, val loss 4.4023 [1969.0434312820435 sec]
step 6900: train loss 0.0896, val loss 4.4193 [1997.6610715389252 sec]
step 7000: train loss 0.0903, val loss 4.4693 [2026.2954487800598 sec]
step 7100: train loss 0.0897, val loss 4.4672 [2054.925901889801 sec]
step 7200: train loss 0.0891, val loss 4.5088 [2083.5762255191803 sec]
step 7300: train loss 0.0881, val loss 4.4539 [2112.2070841789246 sec]
step 7400: train loss 0.0885, val loss 4.4765 [2140.8209438323975 sec]
step 7500: train loss 0.0882, val loss 4.4861 [2169.450432777405 sec]
step 7600: train loss 0.0886, val loss 4.5306 [2198.0888686180115 sec]
step 7700: train loss 0.0884, val loss 4.5563 [2226.7202155590057 sec]
step 7800: train loss 0.0882, val loss 4.5928 [2255.3518805503845 sec]
step 7900: train loss 0.0884, val loss 4.5697 [2283.990043401718 sec]
step 8000: train loss 0.0878, val loss 4.6035 [2312.6245744228363 sec]
step 8100: train loss 0.0876, val loss 4.5803 [2341.2664823532104 sec]
step 8200: train loss 0.0874, val loss 4.5173 [2369.920573949814 sec]
step 8300: train loss 0.0871, val loss 4.5936 [2398.569475889206 sec]
step 8400: train loss 0.0875, val loss 4.5568 [2427.2261044979095 sec]
step 8500: train loss 0.0863, val loss 4.5918 [2455.8717839717865 sec]
step 8600: train loss 0.0866, val loss 4.5853 [2484.5305535793304 sec]
step 8700: train loss 0.0861, val loss 4.6915 [2513.166151046753 sec]
step 8800: train loss 0.0865, val loss 4.6955 [2541.8219192028046 sec]
step 8900: train loss 0.0861, val loss 4.5714 [2570.4465539455414 sec]
step 9000: train loss 0.0866, val loss 4.5709 [2599.0806968212128 sec]
step 9100: train loss 0.0862, val loss 4.6820 [2627.7281572818756 sec]
step 9200: train loss 0.0862, val loss 4.5906 [2656.3631620407104 sec]
step 9300: train loss 0.0861, val loss 4.7325 [2685.011831998825 sec]
step 9400: train loss 0.0860, val loss 4.7107 [2713.658144235611 sec]
step 9500: train loss 0.0858, val loss 4.7332 [2742.3079409599304 sec]
step 9600: train loss 0.0856, val loss 4.6962 [2770.9424908161163 sec]
step 9700: train loss 0.0853, val loss 4.7286 [2799.586187362671 sec]
step 9800: train loss 0.0852, val loss 4.7897 [2828.207053422928 sec]
step 9900: train loss 0.0849, val loss 4.7006 [2856.835433959961 sec]
0.11132630705833435
Total Training Time: 2870.0378243923187 seconds

me t me I wish to exchange. I may offer one of the
children of my princes, but come  the femies of 
BEGINNING (1682191308.2263405): Baseline LR(0.0006) Heads(8) Embeddings(512) Block Size(128) Batch Size(48) Layers(6)
torch.Size([6144, 86])
tensor(4.6801, device='cuda:0', grad_fn=<NllLossBackward0>)
step 0: train loss 4.6952, val loss 4.6859 [12.384953260421753 sec]
step 100: train loss 2.4792, val loss 2.5621 [34.96409225463867 sec]
step 200: train loss 2.1568, val loss 2.2935 [57.55464839935303 sec]
step 300: train loss 1.7727, val loss 2.0063 [80.14157509803772 sec]
step 400: train loss 1.5515, val loss 1.8554 [102.7350754737854 sec]
step 500: train loss 1.3812, val loss 1.7914 [125.31274962425232 sec]
step 600: train loss 1.2432, val loss 1.7682 [147.8852686882019 sec]
step 700: train loss 1.1219, val loss 1.7603 [170.46281743049622 sec]
step 800: train loss 1.0036, val loss 1.7786 [193.04966735839844 sec]
step 900: train loss 0.8950, val loss 1.8091 [215.62706565856934 sec]
step 1000: train loss 0.7929, val loss 1.8977 [238.21086955070496 sec]
step 1100: train loss 0.6679, val loss 1.9315 [260.8160035610199 sec]
step 1200: train loss 0.5630, val loss 2.0684 [283.409921169281 sec]
step 1300: train loss 0.4716, val loss 2.1856 [306.02354860305786 sec]
step 1400: train loss 0.3918, val loss 2.2776 [328.6425971984863 sec]
step 1500: train loss 0.3123, val loss 2.3814 [351.2262110710144 sec]
step 1600: train loss 0.2696, val loss 2.5409 [373.8115875720978 sec]
step 1700: train loss 0.2209, val loss 2.6538 [396.4074115753174 sec]
step 1800: train loss 0.1913, val loss 2.7702 [418.98613595962524 sec]
step 1900: train loss 0.1722, val loss 2.8717 [441.58046555519104 sec]
step 2000: train loss 0.1573, val loss 2.9352 [464.1898338794708 sec]
step 2100: train loss 0.1473, val loss 3.0063 [486.7895448207855 sec]
step 2200: train loss 0.1375, val loss 3.0907 [509.4503540992737 sec]
step 2300: train loss 0.1331, val loss 3.1560 [532.0688462257385 sec]
step 2400: train loss 0.1276, val loss 3.2795 [554.6700985431671 sec]
step 2500: train loss 0.1247, val loss 3.3034 [577.2735378742218 sec]
step 2600: train loss 0.1201, val loss 3.3990 [599.8892223834991 sec]
step 2700: train loss 0.1188, val loss 3.3687 [622.4843609333038 sec]
step 2800: train loss 0.1174, val loss 3.3950 [645.0966238975525 sec]
step 2900: train loss 0.1143, val loss 3.4385 [667.6831245422363 sec]
step 3000: train loss 0.1128, val loss 3.5036 [690.2534341812134 sec]
step 3100: train loss 0.1113, val loss 3.5800 [712.8351891040802 sec]
step 3200: train loss 0.1093, val loss 3.6369 [735.4335889816284 sec]
step 3300: train loss 0.1086, val loss 3.6230 [758.0285983085632 sec]
step 3400: train loss 0.1074, val loss 3.6968 [780.6046679019928 sec]
step 3500: train loss 0.1061, val loss 3.7331 [803.1925382614136 sec]
step 3600: train loss 0.1058, val loss 3.6753 [825.7805361747742 sec]
step 3700: train loss 0.1051, val loss 3.7385 [848.3549435138702 sec]
step 3800: train loss 0.1047, val loss 3.7621 [870.9485609531403 sec]
step 3900: train loss 0.1032, val loss 3.8079 [893.5371141433716 sec]
step 4000: train loss 0.1024, val loss 3.8402 [916.1364047527313 sec]
step 4100: train loss 0.1011, val loss 3.8407 [938.7321348190308 sec]
step 4200: train loss 0.1011, val loss 3.8088 [961.3120107650757 sec]
step 4300: train loss 0.1003, val loss 3.8125 [983.8945116996765 sec]
step 4400: train loss 0.0995, val loss 3.9260 [1006.5006875991821 sec]
step 4500: train loss 0.0999, val loss 3.8809 [1029.079178094864 sec]
step 4600: train loss 0.0998, val loss 3.8903 [1051.666971206665 sec]
step 4700: train loss 0.0984, val loss 3.8495 [1074.2414011955261 sec]
step 4800: train loss 0.0982, val loss 3.9816 [1096.814352273941 sec]
step 4900: train loss 0.0980, val loss 3.9437 [1119.3902208805084 sec]
step 5000: train loss 0.0967, val loss 4.0078 [1141.9599034786224 sec]
step 5100: train loss 0.0964, val loss 4.0612 [1164.5403454303741 sec]
step 5200: train loss 0.0972, val loss 3.9833 [1187.1486566066742 sec]
step 5300: train loss 0.0973, val loss 4.0471 [1209.729520559311 sec]
step 5400: train loss 0.0963, val loss 3.9996 [1232.313398361206 sec]
step 5500: train loss 0.0958, val loss 4.0101 [1254.917338848114 sec]
step 5600: train loss 0.0946, val loss 4.0527 [1277.5040628910065 sec]
step 5700: train loss 0.0949, val loss 4.0250 [1300.0966835021973 sec]
step 5800: train loss 0.0959, val loss 4.0551 [1322.6909124851227 sec]
step 5900: train loss 0.0942, val loss 4.1298 [1345.2763330936432 sec]
step 6000: train loss 0.0939, val loss 4.0731 [1367.855045080185 sec]
step 6100: train loss 0.0936, val loss 4.1704 [1390.4522495269775 sec]
step 6200: train loss 0.0929, val loss 4.1971 [1413.0453968048096 sec]
step 6300: train loss 0.0933, val loss 4.0160 [1435.6463866233826 sec]
step 6400: train loss 0.0930, val loss 4.1762 [1458.2435286045074 sec]
step 6500: train loss 0.0934, val loss 4.1619 [1480.839838027954 sec]
step 6600: train loss 0.0924, val loss 4.1690 [1503.442745923996 sec]
step 6700: train loss 0.0926, val loss 4.1917 [1526.0323247909546 sec]
step 6800: train loss 0.0920, val loss 4.2046 [1548.6181259155273 sec]
step 6900: train loss 0.0921, val loss 4.1821 [1571.2074284553528 sec]
step 7000: train loss 0.0920, val loss 4.2201 [1593.8060147762299 sec]
step 7100: train loss 0.0917, val loss 4.1533 [1616.40323138237 sec]
step 7200: train loss 0.0916, val loss 4.2992 [1638.9853539466858 sec]
step 7300: train loss 0.0909, val loss 4.2631 [1661.5795533657074 sec]
step 7400: train loss 0.0907, val loss 4.3414 [1684.2050569057465 sec]
step 7500: train loss 0.0903, val loss 4.3484 [1706.8305101394653 sec]
step 7600: train loss 0.0899, val loss 4.3265 [1729.4401438236237 sec]
step 7700: train loss 0.0904, val loss 4.3516 [1752.0580155849457 sec]
step 7800: train loss 0.0897, val loss 4.3403 [1774.689810037613 sec]
step 7900: train loss 0.0899, val loss 4.2941 [1797.2983303070068 sec]
step 8000: train loss 0.0894, val loss 4.3536 [1819.91268324852 sec]
step 8100: train loss 0.0891, val loss 4.3670 [1842.5291328430176 sec]
step 8200: train loss 0.0898, val loss 4.2992 [1865.1167373657227 sec]
step 8300: train loss 0.0898, val loss 4.3890 [1887.7197258472443 sec]
step 8400: train loss 0.0890, val loss 4.4091 [1910.3441648483276 sec]
step 8500: train loss 0.0892, val loss 4.3340 [1932.9396142959595 sec]
step 8600: train loss 0.0885, val loss 4.4896 [1955.5412414073944 sec]
step 8700: train loss 0.0884, val loss 4.4285 [1978.1603076457977 sec]
step 8800: train loss 0.0889, val loss 4.4952 [2000.7606344223022 sec]
step 8900: train loss 0.0889, val loss 4.4361 [2023.382875919342 sec]
step 9000: train loss 0.0876, val loss 4.4434 [2046.0052635669708 sec]
step 9100: train loss 0.0873, val loss 4.5039 [2068.5923902988434 sec]
step 9200: train loss 0.0881, val loss 4.4269 [2091.2165203094482 sec]
step 9300: train loss 0.0874, val loss 4.4593 [2113.833253622055 sec]
step 9400: train loss 0.0875, val loss 4.4780 [2136.429430246353 sec]
step 9500: train loss 0.0872, val loss 4.4627 [2159.0117795467377 sec]
step 9600: train loss 0.0868, val loss 4.5424 [2181.6210644245148 sec]
step 9700: train loss 0.0871, val loss 4.5198 [2204.255986213684 sec]
step 9800: train loss 0.0868, val loss 4.4524 [2226.8594999313354 sec]
step 9900: train loss 0.0868, val loss 4.4448 [2249.4556064605713 sec]
0.11124012619256973
Total Training Time: 2259.6847853660583 seconds

Pyrran tord him, but his wound was to be
trusted. This human's slow reactions proved the treaty?" A 
